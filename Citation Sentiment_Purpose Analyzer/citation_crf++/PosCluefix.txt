given that each token has on the average more than 2 possible tags, the procedural description above is very inefficient for all but very short sentences.
however, the observation that our constraints are localized to a window of a small number of tokens say at most 5 tokens in a sequence, suggests a more efficient scheme originally used by <tref>church 1988</tref>.
the last few years have seen the great success of stochastic part-of-speech pos taggers <tref>church, 1988</tref>: <ref>kupiec, 1992</ref>; charniak et m , 1993; <ref>brill, 1992</ref>; <ref>nagata, 1994</ref>.
the stochastic approach generally attains 94 to 96 accuracy and replaces the labor-intensive compilation of linguistics rules by using an automated learning algorithm.
a recent trend in natural language processing has been toward a greater emphasis on statistical approaches, beginning with the success of statistical part-of-speech tagging programs <tref>church 1988</tref>, and continuing with other work using statistical part-of-speech tagging programs, such as bbn plum <ref>weischedel et al 1993</ref> and nyu proteus <ref>grishman and sterling 1993</ref>.
nevertheless, most natural language systems remain primarily rule based, and even systems that do use statistical techniques, such as att chronus <ref>levin and pieraccini 1995</ref>, continue to require a significant rule based component.
disambiguation of capitalized words is usually handled by pos taggers, which treat capitalized words in the same way as other categories, that is, by accounting for the immediate syntactic context and using estimates collected from a training corpus.
<ref>as church 1988</ref> rightly pointed out, however, proper nouns and capitalized words are particularly problematic: some capitalized words are proper nouns and some are not.
for example, the capitalized word acts is found twice in the brown corpus, both times as a proper noun in a title.
the morphological ambiguity will differ depending on the level of tagging used in each case, as shown in table 2.
there are two kinds of methods for morphological disambiguation: on one hand, statistical methods need little effort and obtain very good results <tref>church, 1988</tref>; cutting el al, 1992, at least when applied to english, but when we try to apply them to basque we encounter additional problems; on the other hand, some rule-based systems <ref>brill, 1992</ref>; <ref>voutilainen et al, 1992</ref> are at least as good as statistical systems and are better adapted to free-order languages and agglutinative languages.
so, we 381 have selected one of each group: constraint grammar formalism <ref>karlsson et al, 1995</ref> and the hmm based tatoo tagger <ref>armstrong et al, 1995</ref>, which has been designed to be applied it to the output of a morphological analyser and the tagset can be switched easily without changing the input text.
because of fundamental differences in tagging strategy between the penn treebank project and the brown project, the resulting mapping is about 9 inaccurate, given the tagging guidelines of the penn treebank project as given in 40 pages of explicit tagging guidelines.
this material is then hand-corrected by our annotators; the result is consistent within annotators to about 3 cf.
deducing linguistic structure from the statistics of large corpora eric brill david magerman mitchell marcus beatrice santorini department of computer and information science university of pennsylvania philadelphia, pa 19104 1 introduction within the last two years, approaches using both stochastic and symbolic techniques have proved adequate to deduce lexical ambiguity resolution rules with less than 3-4 error rate, when trained on moderate sized 500k word corpora of english text eg <tref>church, 1988</tref>; <ref>hindle, 1989</ref>.
the success of these techniques suggests that much of the grammatical structure of language may be derived automatically through distributional analysis, an approach attempted and abandoned in the 1950s.
we describe here two experiments to see how far purely distributional techniques can be pushed to automatically provide both a set of part of speech tags for english, and a grammatical analysis of free english text.
this line of research was motivated by a series of successful applications of mutual information statistics to other problems in natural language processing.
in the last decade, research in speech recognition <ref>jelinek 1985</ref>, noun classification <ref>hindle 1988</ref>, predicate argument relations <ref>church  hanks 1989</ref>, and other areas have shown that mutual information statistics provide a wealth of information for solving these problems.
it is a function of the probabilities of the two events: mz, u  log u xzpvy in this paper, the events x and y will be part-of-speech n-grams instead of single parts-of-speech, as in some earlier work.
4 concluding remarks though there can be little doubt that the ruling system of bakeoffs actively encourages a degree of oneupmanship, our paper and our software are not offered in a competitive spirit.
as we said at the out211 set, we dont necessarily believe hunpos to be in any way better than tnt, and certainly the main ideas have been pioneered by <ref>derose 1988</ref>, <tref>church 1988</tref>, and others long before this generation of hmm work.
but to improve the results beyond what a basic hmm can achieve one needs to tune the system, and progress can only be made if the experiments are end to end replicable.
there is no doubt many other systems could be tweaked further and improve on our results what matters is that anybody could now also tweak hunpos without any restriction to improve the state of the art.
thus, examples 3-5 illustrate how the syntactic context of a word can help determine its meaning.
22 motivation from previous work 221 parsing in recent years, the success of statistical parsing techniques can be attributed to several factors, such as the increasing size of computing machinery to accommodate larger models, the availability of resources such as the penn treebank <ref>marcus et al , 1993</ref> and the success of machine learning techniques for lowerlevel nlp problems, such as part-of-speech tagging <tref>church, 1988</tref>; <ref>brill, 1995</ref>, and ppattachment <ref>brill and resnik, 1994</ref>; <ref>collins and brooks, 1995</ref>.
however, perhaps even more significant has been the lexicalization of the grammar formalisms being probabilistically modeled: crucially, all the recent, successful statistical parsers have in some way made use of bilexical dependencies.
recent research advances may lead to the development of viable book indexing methods for chinese books.
these include the availability of efficient and high precision word segmentation methods for chinese text <ref>chang et al , 1991</ref>; <ref>sproat and shih, 1990</ref>; <ref>wang et al , 1990</ref>, the availability of statistical analysis of a chinese corpus <ref>liu et al , 1975</ref> and large-scale electronic chinese dictionaries with partof-speech information <ref>chang et al , 1988</ref>; bdc, 1992, the corpus-based statistical part-of-speech tagger <tref>church, 1988</tref>; <ref>derose, 1988</ref>; <ref>beale, 1988</ref>, as well as phrasal and clausal analyzers <tref>church 1988</tref>; <ref>ejerhed 1990</ref> 2.
given the text of a book, an indexing system, must perform some kind of phrasal and statistical analysis in order to produce a list of candidate indexes and their occurrence statistics in order to generate indexes as shown in figure 1 which is an excerpt from the reconstruction of indexes of a book on transformational grammar for mandarin chinese <ref>tang, 1977</ref>.
much recent research in the field of natural language processing nlp has focused on an empirical, corpus-based approach <ref>church and mercer, 1993</ref>.
the high accuracy achieved by a corpus-based approach to part-of-speech tagging and noun phrase parsing, as demonstrated by <tref>church, 1988</tref>, has inspired similar approaches to other problems in natural language processing, including syntactic parsing and word sense disambiguation wsd.
the availability of large quantities of part-ofspeech tagged and syntactically parsed sentences like the penn treebank corpus <ref>marcus, santorini, and marcinkiewicz, 1993</ref> has contributed greatly to the development of robust, broad coverage partof-speech taggers and syntactic parsers.
the penn treebank corpus contains a sufficient number of partof-speech tagged and syntactically parsed sentences to serve as adequate training material for building broad coverage part-of-speech taggers and parsers.
this is quite feasible using statistical taggers like those of <ref>garside 1987</ref>, <tref>church 1988</tref> or <ref>foster 1991</ref> which achieve performance upwards of 97 on unrestricted text.
they are well enough concemcd to allude at times even the human reader and no automatic term-recognition system has attempted to distinguish such terms, despite the prevalence ofpolysemy in such fields as the social sciences riggs, 1993 and the importance for purposes of terminological standardization that deviant usage be tracked.
on sentences with <40 words, the former model performs at 69 precision, 75 recall, and the latter at 77 precision and 78 recall.
ever since the success of hmms application to part-of-speech tagging in <tref>church, 1988</tref>, machine learning approaches to natural language processing have steadily become more widespread.
many machine learning approaches let the data speak for itself data ipsa loquuntur, as it were, allowing the modeler to focus on what features of the data are important, rather than on the complicated interaction of such features, as had often been the case with hand-crafted nlp systems.
to see whether our four hypotheses in italics above effectively addressed the four concerns above, we chose to test the hypotheses on two well-known problems: ambiguity both at the structural level and at the part-of-speech level and inferring syntactic and semantic information about unknown words.
guided by the past success of probabilistic models in speech processing, we have integrated probabilistic models into our language processing systems.
the effectiveness of such models is well known <ref>derose 1988</ref>; <tref>church 1988</tref>; <ref>kupiec 1989</ref>; <ref>jelinek 1985</ref>, and they are currently in use in parsers eg de <ref>marcken 1990</ref>.
our work is an incremental improvement on these models in three ways: 1 much less training data than theoretically required proved adequate; 2 we integrated a probabilistic model of word features to handle unknown words uniformly within the probabilistic model and measured its contribution; and 3 we have applied the forward-backward algorithm to accurately compute the most likely tag set.
in section 3, we demonstrate that probability models can improve the performance of knowledge-based syntactic and semantic processing in dealing with structural ambiguity and with unknown words.
<ref>hearst 1991</ref> presented an effective approach to modeling local contextual evidence, while <ref>resnik 1993</ref> gave a classic treatment of the use of word classes in selectional constraints.
preprocessing the corpus with a part of speech tagger phrasal verbs involving the preposition to raise an interesting problem because of the possible confusion with the infinitive marker to.
we have found that if we first tag every word in the corpus with a part of speech using a method such as <tref>church 1988</tref> or <ref>derose 1988</ref>, and then measure associations between tagged words, we can identify interesting contrasts between verbs associated with a following preposition toin and verbs associated with a following infinitive marker toto.
the score identifies quite a number of verbs associated in an interesting way with to; restricting our attention to pairs with a score of 30 or more, there are 768 verbs associated with the preposition toin and 551 verbs with the infinitive marker toto.
however, as <tref>church 1988</tref> rightly pointed out proper nouns and capitalized words are particularly problematic: some capitalized words are proper nouns and some are not.
for example, the capitalized word acts is found twice in brown corpus, both times as a proper noun in a title.
given that each token has on the average more than 2 possible tags, the procedural description above is very inefficient for m1 but very short sentences.
however, the observation that our constraints are localized to a window of a small number of tokens say at most 5 tokens in a sequence, suggests a more efficient scheme originally used by <tref>church 1988</tref>.
they report rates of correctly tagged words which are comparable to that presented by <tref>church 1988</tref> and <ref>kempe 1993</ref>.
in the area of speech recognition neural networks have been used for a decade r, ow.
indeed, recent increased interest in the problem of disambiguating lexical category in english has led to significant progress in developing effective programs for assigning lexical category in unrestricted text.
the most successful and comprehensive of these are based on probabilistic modeling of category sequence and word category <ref>church 1987</ref>; <ref>garside, leech and sampson 1987</ref>; <ref>derose 1988</ref>.
these stochastic methods show impressive performance: church reports a success rate of 95 to 99, and shows a sample text with an error rate of less than one percent.
what may seem particularly surprising is that these methods succeed essentially without reference to syntactic structure; purely surface lexical patterns are involved.
excellent methods have been developed for part-of-speech pos tagging using stochastic models trained on partially tagged corpora <tref>church, 1988</tref>; cutting, <ref>kupiec, pedersen  sibun, 1992</ref>.
semantic issues have been addressed, particularly for sense disambiguation, by using large contexts, eg, 50 nearby words <ref>gale, church  yarowsky, 1992</ref> or by reference to on-line dictionaries <ref>krovetz, 1991</ref>; <ref>lesk, 1986</ref>; <ref>liddy  paik, 1992</ref>; <ref>zernik, 1991</ref>.
more recently, methods to work with entirely untagged corpora have been developed which show great promise <ref>brill  marcus, 1992</ref>; <ref>finch  chater, 1992</ref>; <ref>myaeng  li, 1992</ref>; <ref>schutze, 1992</ref>.
much of this work offers the prospect that a disambiguation system might be able to input unrestricted text and tag each word with the most likely sense with fairly reasonable accuracy and efficiency, just as part of speech taggers eg , <tref>church 1988</tref> can now input unrestricted text and assign each word with the most likely part of speech with fairly reasonable accuracy and efficiency.
the availability of massive lexicographic databases offers a promising route to overcoming the knowledge acquisition bottleneck.
more than thirty years ago, bari-<ref>iillel 1960</ref> predicted that it would be futile to write expert-system-like rules by-hand as they had been doing at georgetown at the time because there would be no way to scale up such rules to cope with unrestricted input.
the most successful achievements so far in the domain of large-scale morphological disambiguation of running text have been those for english reported by <ref>garside, leech, and sampson 1987</ref>, on tagging the lob corpus, and <tref>church 1988</tref>, on assigning part-of-speech labels and parsing noun phrases.
success rates ranging between 95-99 are reported, depending on how success is defined.
additionally, there is a slight but not significant improvement of tagging accuracy.
in the past decade, the speech recognition community has had huge successes in applying hidden markov models, or hmms to their problems.
more recently, the natural language processing community has effectively employed these models for part-ofspeech tagging, as in the seminal <tref>church, 1988</tref> and other, more recent efforts <ref>weischedel et al , 1993</ref>.
we would now propose that hmms have successfully been applied to the problem of name-finding.
we have built a named-entity ne recognition system using a slightly-modified version of an hmm; we call our system nymble.
once the base formalism has been decided upon we currently are using lexicalized multi-component tags with substitution and adjunction, a simple translation strategy from a source string to a target is to parse the string using an appropriate tag parser for the base formalism.
pd productdisplay i p i   j,r i  note that each probability on the right represents the syntactic/semantic preference of a dependency of two lexical items.
we can readily see that the model is very similar to lpcfg models.
a more linguistically motivated approach is to expand the domain of productions downward to incorporate more tree structures.
in particular, we show how a fixed constituency can be maintained at the level of the elementary trees of lexicalized tags and yet be able to achieve the kind of flexibility needed for dealing with the so-called non-constituents.
recently there has been a gain in interest in the so-called mildly context-sensitive formalisms vijay-<ref>shanker, 1987</ref>; <ref>weir, 1988</ref>; <ref>joshi, vijayshanker, and weir, 1991</ref>; vijay-<ref>shanker and weir, 1993a</ref> that generate only a small superset of context-free languages.
one such formalism is lexicalized tree-adjoining grammar ltag schabes, abeill, and <ref>joshi, 1988</ref>; <ref>abeillfi et al , 1990</ref>; <ref>joshi and schabes, 1992</ref>, which provides a number of attractive properties at the cost of decreased efficiency, on6-time in the worst case <ref>vijayshanker, 1987</ref>; <ref>schabes, 1991</ref>; <ref>lang, 1990</ref>; <ref>vijayshanker and weir, 1993b</ref>.
much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation.
at the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as lexical functional grammar lfg <ref>bresnan, 1982</ref>, lexicalized tree adjoining grammar ltag <tref>schabes et al , 1988</tref>, headdriven phrase structure grammar hpsg <ref>pollard and sag, 1994</ref> and combinatory categorial grammar ccg <ref>steedman, 2000</ref>, which represent deep syntactic structures that cannot be expressed in a shallower formalism designed to represent only aspects of surface syntax, such as the dependency formalism used in current mainstream dependency parsing.
we present a novel framework that combines strengths from surface syntactic parsing and deep syntactic parsing, specifically by combining dependency and hpsg parsing.
we show that, by using surface dependencies to constrain the application of wide-coverage hpsg rules, we can benefit from a number of parsing techniques designed for high-accuracy dependency parsing, while actually performing deep syntactic analysis.
this paper will concentrate on context-free grammars cfg and their associated parsers.
hence, the parsing techniques and tools described here can be applied to most tags used for nlp, with, in the worst case, a light over-generation which can be easily and efficiently eliminated in a complementary pass.
this is indeed what we have achieved with a tag automatically extracted from villemonte de <ref>la clergerie, 2005</ref>s large-coverage factorized french tag, as we will see in section 4.
we can thus define lexical transfer rules that avoid the defects of a mere word-to-word approach but still benefit from the simplicity and elegance of a lexical approach.
we are indebted to stuart shieber for his valuable comments.
3 a tag analysis the tag formalism for a recent introduction, see <ref>joshi 1987a</ref> is well suited for linguistic description because 1 it provides a larger domain of locality than a cfg or other augmented cfg-based formalisms such as tlpsg or lfg, and 2 it allows factoring of recursion from the domain of dependencies.
the tree will contain the lexical item, and all of its syntac3some verbs allow scrambling out of their complements more freely than others.
it appears that all subject-control verbs and most object-control verbs governing the dative allow scrambling fairly fely, while scrambling with objectcontrol verbs governing the accusative is more restricted cir.
along with the independent development of parsing techniques for individual grammar formalisms, some of them have been adapted to other formalisms <tref>schabes et al , 1988</tref>; van <ref>noord, 1994</ref>; <ref>yoshida et al , 1999</ref>; <ref>torisawa et al , 2000</ref>.
if we could identify an algorithmic difference that causes performance difference, it would reveal advantages and disadvantages of the different realizations.
the parser achieves an ogn6-time worst case behavior, og2n4-time for unambiguous grammars and linear time for a large class of grammars.
the parser uses the following two-pass parsing strategy originally defined for lexicalized grammars <tref>schabes et al , 1988</tref> which improves its performance in practice <ref>schabes and joshi, 1990</ref>:  in the first step the parser will select, the set of structures corresponding to each word in the sentence.
this information is particularly useful for a top-down component of the parser <ref>schabes and joshi, 1990</ref>.
all the syntactic concepts of lexicalized tag such as the grouping of the trees in tree families which represents the possible variants on a basic subcategorization frame are accessible through mouse-sensitive items.
that is, they used deep analysis as a preprocessor to obtain useful features for training a probabilistic model or statistical classifier of a semantic argument identifier.
as for lexicalized tags, in <tref>schabes et al , 1988</tref> a two step algorithm has been presented: during the first step the trees corresponding to the input string are selected and in the second step the input string is parsed with respect to this set of trees.
another paper by <ref>schabes and joshi 1989</ref> shows how parsing strategies can take advantage of lexicalization in order to improve parsers performance.
two major advantages have been discussed in the cited work: grammar filtering the parser can use only a subset of the entire grammar and bottom-up information further constraints are imposed on the way trees can be combined.
in <ref>kroch and joshi, 1985</ref> a detailed discussion of the linguistic relevance of tags can be found.
lexicalized tree adjoining grammars <tref>schabes et al , 1988</tref> are a refinement of tags such that each elementary tree is associated with a lexieal item, called the anchor of the tree.
notably, the association between elementary trees and anchors improves also parsing performance, as will be discussed below.
a more linguistically motivated approach is to expand the domain of productions downward to incorporate more tree structures.
indeed, recent increased interest in the problem of disambiguating lexical category in english has led to significant progress in developing effective programs for assigning lexical category in unrestricted text.
the most successful and comprehensive of these are based on probabilistic modeling of category sequence and word category <ref>church 1987</ref>; <ref>garside, leech and sampson 1987</ref>; <tref>derose 1988</tref>.
these stochastic methods show impressive performance: church reports a success rate of 95 to 99, and shows a sample text with an error rate of less than one percent.
what may seem particularly surprising is that these methods succeed essentially without reference to syntactic structure; purely surface lexical patterns are involved.
4 concluding remarks though there can be little doubt that the ruling system of bakeoffs actively encourages a degree of oneupmanship, our paper and our software are not offered in a competitive spirit.
as we said at the out211 set, we dont necessarily believe hunpos to be in any way better than tnt, and certainly the main ideas have been pioneered by <tref>derose 1988</tref>, <ref>church 1988</ref>, and others long before this generation of hmm work.
but to improve the results beyond what a basic hmm can achieve one needs to tune the system, and progress can only be made if the experiments are end to end replicable.
there is no doubt many other systems could be tweaked further and improve on our results what matters is that anybody could now also tweak hunpos without any restriction to improve the state of the art.
<ref>recently, brill 1992</ref> described a rule-based tagger that performs as well as taggers based upon probabilistic models and overcomes the limitations common in rule-based approaches to language processing: it is robust and the rules are automatically ac mitsubishi electric research laboratories, 201 broadway, cambridge, ma 02139.
unknown words unknown common words unknown proper nouns tagger guesser metrics error error coverage error coverage hmm xerox mean 17851643 30022169 37567270 10785563 63797113 s-error 0484710 0469922 1687396 0613745 1714969 hmm cascade mean 12378716 21266264 36507909 7776456 64795969 s-error 0917656 0403957 2336381 0853958 2206457 brill brill mean 14688501 27411736 38998687 6439525 62160917 s-error 0908172 0539634 2627234 0501082 4010992 brill cascade mean 11327863 20986240 37933048 5548990 63816586 s-error 0761576 0480798 2353510 0561009 3775991 the brown corpus, we obtained the error rate mean 0 4003093 with the standard error deb0155599.
the brill tagger showed some better results: error rate mean 0 3327366 with the standard error debo 123903.
it is interesting to note that the most commonly used method is viterbi tagging see <tref>derose 1988</tref>; <ref>church 1989</ref> although it is not the optimal method for evaluation at word level.
the reasons for this preference are presumably that:  viterbi tagging is simpler to implement than ml tagging and requires less computation although they both have the same asymptotic complexity  viterbi tagging provides the best interpretation for the sentence, which is linguistically appealing  ml tagging may produce sequences of tags that are linguistically impossible because the choice of a tag depends on all contexts taken together.
however, in our experiments, we will show that viterbi and ml tagging result in very similar performance.
although methods for unsupervised training of hmms do exist, training is usually done in a supervised way by estimation of the above probabilities from relative frequencies in the training data.
in van <ref>halteren, zavrel, and daelemans 1998</ref> we used a straightforward implementation of hmms, which turned out to have the worst accuracy of the four competing methods.
in the present work, we have replaced this by the tnt system we will refer to this tagger as hmm below.
a special lexicon had to be developed to map spoken langnage variants onto their canonical written language forms.
to see whether our four hypotheses in italics above effectively addressed the four concerns above, we chose to test the hypotheses on two well-known problems: ambiguity both at the structural level and at the part-of-speech level and inferring syntactic and semantic information about unknown words.
guided by the past success of probabilistic models in speech processing, we have integrated probabilistic models into our language processing systems.
the effectiveness of such models is well known <tref>derose 1988</tref>; <ref>church 1988</ref>; <ref>kupiec 1989</ref>; <ref>jelinek 1985</ref>, and they are currently in use in parsers eg de <ref>marcken 1990</ref>.
our work is an incremental improvement on these models in three ways: 1 much less training data than theoretically required proved adequate; 2 we integrated a probabilistic model of word features to handle unknown words uniformly within the probabilistic model and measured its contribution; and 3 we have applied the forward-backward algorithm to accurately compute the most likely tag set.
in section 3, we demonstrate that probability models can improve the performance of knowledge-based syntactic and semantic processing in dealing with structural ambiguity and with unknown words.
 finally, there is a large body of work, eg, <tref>moens and steedman 1988</tref>, <ref>passoneau 1988</ref>, <ref>webber 1988</ref>, <ref>hwang 1992</ref>, <ref>song and cohen 1991</ref>, that has focused on a computational analysis of tense and aspect.
while the work on event chronologies is based on some of the notions developed in that body of work, we hope to further exploit insights from previous work.
in the work described here, we investigate how far we can get by focusing our attention only on discourse markers and lexicogrammatical constructs that can be detected by a shallow analysis of natural language texts.
the intuition behind our choice relies on the following facts:  psycholinguistic and other empirical research <ref>kintsch, 1977</ref>; <ref>schiffrin, 1987</ref>; <ref>segal, duchan, and scott, 1991</ref>; <ref>cahn, 1992</ref>; <ref>sanders, spooren, and noordman, 1992</ref>; <ref>hirschberg and litman, 1993</ref>; <ref>knott, 1995</ref>; <ref>costermans and fayol, 1997</ref> has shown that discourse markers are consistently used by human subjects both as cohesive ties between adjacent clauses and as macroconnectors between larger textual units.
the feature specification of this ompositionally derived accomplishment is therefore identical to that of a sentence containing a telic accomplishment verb, such as destroy.
so our semantics of the perfect is like that in <tref>moens and steedman 1988</tref>: a perfect transforms an event into a consequent state, and asserts that the consequent state holds.
as for accomplishments, we can assume that they can be decomposed into several stages, according to <tref>moens and steedman, 1988</tref>: first a preparatory phase, second a culmination or achievement we are not concerned here with the result state.
we can then say that imp refers only to the preparatory phase, so that the term of the eventuality loses all relevance.
this explains the so-called imperfective paradox: it is possible to use imp even though the eventuality never reaches its term: 6 a i1 traversait la rue quand la voiture la 6cras6 he was crossing the street when the car hit him b  i1 traversa la rue quand la voiture la 6cras6 he crossed the street when the car hit him as for achievements, we can assume that they are reduced to a culmination.
an obvious first step, which we are currently working on, is to include a linguisticallymotivatedtemporalontology <ref>moensandsteedman, 1988</ref>, which will be separate from the existing domain ontology.
we also need better techniques for communicating the temporal relationships between events in cases where they are not listed in chronological order <ref>oberlander and lascarides, 1992</ref>.
6 discussion two discourse analysts from edinburgh university, dr andy mckinlay and dr chris mcvittie, kindly examined and compared some of the human and bt45 texts.
the domain is limited to trajectoryof-motion events specified by the verbs run, jog, sit is worth noting that as an alternative to positing a lexical ambiguity, one could just as easily invoke a coercion operator on an event predicate pz mapping it to the process predicate he.
plod, and walk; the locative prepositions to, towards, from, away from, along, eastwards, westwards, and to and back; various landmarks; the distance adverbials n miles; the frequency adverbials twice and n times; and finally the temporal adverbials for and in.
2 3 theory 31 ontology various authors including <ref>link, 1983</ref>, <ref>bach, 1986</ref>, <ref>krifka, 1989</ref>, <ref>eberle, 1990</ref> have proposed modeltheoretic treatments in which a parallel ontological distinction is made between substances and things, processes and events, etc a similarly parallel distinction is employed here, but in a rather different way: unlike the above treatments, the present account models substances, processes, and other such entities as abstract kinds whose realizations vary in amount.
extending their ontology, the same distinction is assumed to hold not only in the domain of materials but also in the domain of eventualities, and derivatively in the domains of space and time as well.
note that in a terminal drs ready for an embedding test, all the auxiliary rpts disappear do not participate in the embedding.
the perfect is analyzed by using the notion of a nucleus <tref>moens and steedman, 1988</tref> to account for the inner structure of an eventuality.
aspectual classification is necessary for interpreting temporal modifiers and assessing temporal entailments <tref>moens and steedman, 1988</tref>; <ref>dorr, 1992</ref>; <ref>klavans, 1994</ref>, and is therefore a necessary component for applications that perform certain language interpretation, summarization, information retrieval, and machine translation tasks.
aspectual classification is a diflqcult problem because many verbs, like have, are aspectually ambiguous.
according to <ref>bennett et al , 1990</ref> in the spirit of <tref>moens and steedman, 1988</tref>, predicates are allowed to undergo an atomicity coercion in which an inherently non-atomic predicate such as dio may become atomic under certain conditions.
given the featural scheme that is imposed on top of the lexical-semantic framework, it is easy to specify coercion functions for each language.
in light of these observations, the lexicm-semantic structure adopted for unitran is an augmented form of jackendoffs representation in which events are distinguished from states as before, but they are further subdivided into activities, achievements, and accomplishments.
the subdivision is achieved by means of three features proposed by <ref>bennett et al , 1990</ref> following the framework of <tref>moens and steedman, 1988</tref> in the spirit of <ref>dowty, 1979</ref> and <ref>vendler, 1967</ref>: dynamic ie , events vs states, as in the jackendoff framework, :t:telic i e, culminative events transitions vs nonculminative events activities, and atomic ie , point events vs extended events.
this featural system is imposed on top of the lexical-semantic framework proposed by jackendoff.
for example, the primitive go would be annotated with the features d,t,-a for the verb destroy, but d,t,a for the verb obliterate, thus providing the appropriate distinction for cases such as 12.
figure 2 relates the four types of lexical-semantic frameworks outlined above.
note that the system of features proposed by <ref>bennett et al , 1990</ref> and <tref>moens and steedman, 1988</tref> provide the finest tuning given that five distinct categories of predicates are identified by the feature settings.
this system is essentially equivalent to the dowty/vendler proposal, but features are used to distinguish the categories more precisely.
in the next section, we will see how the tense and aspect structure described in section 21 and the lexicm-semantic representation described in this section are combined to provide the framework for generating a target-language surface form.
<ref>dowty 1986</ref> and <tref>moens and steedman 1988</tref> decisively questioned the coherence of the class of achievement verbs, arguing that not all of them are non-durative.
as noted above, vendler identifies punctual events through the conjunction of the positive at and negative finish tests.
we impose this system of features on top of the current lexical-semantic framework.
for example, the lexical entry for all three verbs, ransack, obliterate, and destroy, would contain the following lexical-semantic representation: 6 event cause thing x, event goloc thing x, position toloc x john, property destroyed the three verbs would then be distinguished by annotating this representation with the aspectual features d,t,-a for the verb ransack, d,t,-a for the verb destroy, and d,t,a for the verb obliterate, thus providing the appropriate distinction for cases such as 4.
s: juan le dio una puflajada a marls john gave a knife-wound to mary s: juan le dio pufialadas a marls john gave knife-wounds to mary b duratlve divergence, e: john met/knew mary 4 s: juan coaoci6 a marls john met mary s: juan conoci a mrfa john knew merit figure 1: three levels of mt divergences et el.
1990 have examined aspect and verb semantics within the context of machine translation in the spirit of <tref>moens and steedman 1988</tref>.
knowledge of lexical aspect, eg, atelicity, is therefore required for interpreting event sequences in discourse <ref>dowty, 1986</ref>; <tref>moens and steedman, 1988</tref>; <ref>passoneau, 1988</ref>, interfacing to temporal databases <ref>androutsopoulos, 1996</ref>, processing temporal modifiers <ref>antonisse, 1994</ref>, describing allowable alternations and their semantic effects <ref>resnik, 1996</ref>; <ref>tenny, 1994</ref>, and selecting tense and lexical items for natural language generation <ref>dorr and olsen, 1996</ref>; <ref>klavans and chodorow, 1992</ref>, cf.
we show that it is possible to represent lexical aspect--both verbal and compositional--on a large scale, using lexical conceptual structure lcs representations of verbs in the classes cataloged by <ref>levin 1993</ref>.
knowledge of lexical aspect--how verbs denote situations as developing or holding in time--is required for interpreting event sequences in discourse <ref>dowty, 1986</ref>; <tref>moens and steedman, 1988</tref>; <ref>passoneau, 1988</ref>, interfacing to temporal databases <ref>androutsopoulos, 1996</ref>, processing temporal modifiers <ref>antonisse, 1994</ref>, describing allowable alternations and their semantic effects <ref>resnik, 1996</ref>; <ref>tenny, 1994</ref>, and for selecting tense and lexical items for natural language generation dorr and olsen.
as with 36d, the relevant elements of 37b can be represented as   then r   after s  turn right on county line   e 3 :turn-rightyou, county line and the unresolved interpretation of 37b is thus  xafterx, eve 3  aftere 3, ev 560 computational linguistics volume 29, number 4 as for resolving ev, in a well-known article, <tref>moens and steedman 1988</tref> discuss several ways in which an eventuality of one type eg , a process can be coerced into an eventuality of another type eg , an accomplishment, which moens and steedman call a culminated process.
in this case, the matrix argument of then the eventuality of turning right on county line can be used to coerce the process eventuality in 37b into a culminated process of going west on lancaster avenue until county line.
we treat this coercion as a type of associative or bridging inference, as in the examples discussed in section 31.
this allows a modular representation of the semantics of temporal adverbials like until and by, and also aids in the generation of tense and aspect.
this system looks at the mechanics of how the alternatives can be generated from the initial data, but we will have less to say about choosing between them.
ve adopt and extend the feature-based framework proposed by <ref>bennett et al , 1990</ref> in the spirit of <tref>moens and steedman, 1988</tref>.
they uses three features: dynamic, telic, and atomic.
in principle, it is possible for this second module to detect aspectual transformations that apply to any input clause, independent of the manner in which the core constituents interact to produce its fundamental aspectual class.
600 siegel and mckeown improving aspectual classification 26 applications of aspectual classification aspectual classification is a required component of applications that perform natural language interpretation, natural language generation, summarization, information retrieval, and machine translation tasks <tref>moens and steedman 1988</tref>; <ref>klavans and chodorow 1992</ref>; <ref>klavans 1994</ref>; <ref>dorr 1992</ref>; <ref>wiebe et al 1997</ref>.
these applications require the ability to reason about time, ie, temporal reasoning.
598 siegel and mckeown improving aspectual classification table 3 several aspectual entailments.
if a clause occurring: necessarily entails: then it must be: in past progressive tense as argument of stopped in simple present tense past tense reading past tense reading the habitual reading nonculminated event nonculminated event or state event 23 interpreting temporal connectives and modifiers several researchers have developed models that incorporate aspectual class to assess temporal constraints between connected clauses <ref>hwang and schubert 1991</ref>; <ref>schubert and hwang 1990</ref>; <ref>dorr 1992</ref>; <ref>passonneau 1988</ref>; <tref>moens and steedman 1988</tref>; <ref>hitzeman, moens, and grover 1994</ref>.
for example, in interpreting, 7 she had good strength when objectively tested.
an understanding system can recognize the aspectual transformations that have affected a clause only after establishing the clauses fundamental aspectual category.
linguistic models motivate the division between a module that first detects fundamental aspect and a second that detects aspectual transformations <ref>hwang and schubert 1991</ref>; <ref>schubert and hwang 1990</ref>; <ref>dorr 1992</ref>; <ref>passonneau 1988</ref>; <tref>moens and steedman 1988</tref>; <ref>hitzeman, moens, and grover 1994</ref>.
in principle, it is possible for this second module to detect aspectual transformations that apply to any input clause, independent of the manner in which the core constituents interact to produce its fundamental aspectual class.
600 siegel and mckeown improving aspectual classification 26 applications of aspectual classification aspectual classification is a required component of applications that perform natural language interpretation, natural language generation, summarization, information retrieval, and machine translation tasks <tref>moens and steedman 1988</tref>; <ref>klavans and chodorow 1992</ref>; <ref>klavans 1994</ref>; <ref>dorr 1992</ref>; <ref>wiebe et al 1997</ref>.
for example, the progressive marker is constrained to appear with an extended event.
e-mail: kathycscolumbiaedu  2001 association for computational linguistics computational linguistics volume 26, number 4 aspectual classification is necessary for interpreting temporal modifiers and assessing temporal entailments <tref>moens and steedman 1988</tref>; <ref>dorr 1992</ref>; <ref>klavans 1994</ref> and is therefore a required component for applications that perform certain natural language interpretation, generation, summarization, information retrieval, and machine translation tasks.
each of these applications requires the ability to reason about time.
a verbs aspectual category can be predicted by co-occurrence frequencies between the verb and linguistic phenomena such as the progressive tense and certain temporal modifiers <ref>klavans and chodorow 1992</ref>.
aspectual classification is also a necessary prerequisite for interpreting certain adverbial adjuncts, as well as identifying temporal constraints between sentences in a discourse <tref>moens and steedman 1988</tref>; <ref>dorr 1992</ref>; <ref>klavans 1994</ref>.
in addition to the two distinctions described in the previous section, atomicity distinguishes punctual events eg , she noticed the picture on the wall from extended events, which have a time duration eg , she ran to the store.
that is, the fundamental aspectual category is the category the clause would have if it were stripped of any and all aspectual markers that induce an aspectual transformation, as well as all components of the clauses pragmatic context that induce a transformation.
aspect in natural language because, in general, the sequential order of clauses is not enough to determine the underlying chronological order, aspectual classification is required for interpreting 596 siegel and mckeown improving aspectual classification table 1 aspectual classes.
culminated nonculminated events punctual extended culmination culminated process recognize build a house point process hiccup run, swim, walk states understand even the simplest narratives in natural language.
the imperfective paradox and trajectory-of-motion events  michael white department of computer and information science university of pennsylvania philadelphia, pa, usa mwhit el inc c is upenn, edu abstract in the first part of the paper, i present a new treatment of the imperfictive paradox <ref>dowty 1979</ref> for the restricted case of trajectoryof-motion events.
bach 1986:12 summarizes the imperfective paradox <ref>dowty 1979</ref> as follows: how can we characterize the meaning of a progressive sentence like la 17 on the basis of the meaning of a simple sentence like lb 18 when la can be true of a history without lb ever being true.
<ref>white 1993</ref>.
5much as in <tref>moens and steedman 1988</tref> and <ref>jackendoff 1991</ref>, the introduction of gr is necessary to avoid having an ill-sorted formula.
284 the manner of motion supplied by a verb, it does nevertheless permit one to consider factors such as the normal speed as well as the meanings of the prepositions 10, lowards, etc by making two additional restrictive assumptions, namely that these events be of constant velocity and in one dimension, i have been able to construct and implement an algorithm which determines whether a specified sequence of such events is or is not possible under certain situationally supplied constraints.
these constraints include the locations of various landmarks assumed to remain stationary and the minimum, maximum, and normal rates associated with various manners of motion eg running, jogging for a given individual.
capitalizing on bachs insight, i present in the first part of the paper a new treatment of the imperfective paradox which relies on the possibility of having actual events standing in the part-of relation to hypothetical super-events.
1 in particular, the present treatment correctly accounts not only for what 2a fails to entail -namely, that john eventually reaches the museum -but also for what 2a does in fact entail -namely, that john follows by jogging at least an initial part of a path that leads to the museum.
as for accomplishments, we can assume that they can be decomposed into several stages, according to <tref>moens and steedman, 1988</tref>: first  preparatory phase, second a cuhnination or achievement we are not concerned here with the result state.
we can then say that imp refers only to the preparatory phase, so that the term of the eventuality loses all relevance.
this explains the so-called imperfective paradox: it is possible to use imp even though the eventnality never reaches its term: 6 a i1 traversait la rue quand la voiture la ras6 he was crossing the street when the car hit him b  i1 traversa la rue quand la voiture la 6cras6 ile crossed the street when the car hit him as for achievements, we can assume that they are reduced to a culmination.
aspectual classification is a necessary component for a system that analyzes temporal constraints, or performs lexical choice and tense selection in machine translation <tref>moens and steedman, 1988</tref>; <ref>passonneau, 1988</ref>; <ref>doff, 1992</ref>; <ref>klavans, 1994</ref>.
specifically, this technique takes advantage of linguistic constraints that pertain to aspect, eg, only clauses that describe an event can appear in the progressive.
by using tags we get the additional benefit of an existing parser that yields derivations and derived trees fiom which we can construct the compositional semantics of a given sentence.
we decompose each event e into a tripartite structure in a manner similar to <tref>moens and steedman 1988</tref>, introducing a time function for each predicate to specify whether the predicate is true in the preparatory dringe, cuhnination erde, or consequent resll:e stage of an event.
hfitial trees capture tile semantics of the basic senses of verbs in each class.
for example, many ithese restrictions are more like preferences that generate a preferred reading of a sentence.
aspectual classification is necessary for interpreting temporal modifiers and assessing temporal entailments <ref>vendler, 1967</ref>; <ref>dowty, 1979</ref>; <tref>moens and steedman, 1988</tref>; <ref>dorr, 1992</ref>, and is therefore a necessary component for applications that perform certain natural language interpretation, natural language generation, summarization, information retrieval, and machine translation tasks.
although an aspectual lexicon of verbs would suffice to classify many clauses by their main verb only, a verbs primary class is often domaindependent <ref>siegel, 1998b</ref>.
culm events states punctual extended culm culm process recognize build a house nonpoint process culm hiccup run, swim understand 2 aspect in natural language table 1 summarizes the three aspectual distinctions, which compose five aspectual categories.
in addition to the two distinctions described in the previous section, atomicity distinguishes events according to whether they have a time duration punctual versus extended.
3 3many of these issues are discussed in the cl special issue on tense and aspect <ref>june, 1988</ref> in articles by hinniche, moens and steedman, nakhimovsky, passoneau, and webber.
for example, passoneau demonstrates how, without an ccurate specification of the pectual tendencies of the verb coupled with the effect of temporal and aspectual adjuncts, messages, which tend to be in the present tense, ttre not correctly understood nor generated in the pundit system.
it is also clear that events are not undifferentiated masses, but rather have subparts that can be picked out by the choice of phrase type or the addition of adverbial phrases.
when the children crossed the road, a they waited for the teacher to give a signal b they stepped onto its concrete surface as if it were about to swallow them up.
stative belongs to the aktionsart categorisation of verbs distinguishing it from verbs of activity, achievement and accomplishment, which is orthogonal to the categorisation of verbs into semantic fields <ref>vendler, 1967</ref>, <tref>moens  steedman 1988</tref>, <ref>amaro, 2006</ref>.
differences in annotation could be due to the differences in interpretations of the event; however, we found that the vast majority of radically different judgments can be categorized into a relatively small number of classes.
22 event classes action vs state: actions involve change, such as those described by words like speaking, gave, and skyrocketed.
we designed the 243 algorithms and structures necessary to generate discourse entities from our logical representation of the meaning of utterances, and from pointing gestures, and currently use them in januss <ref>weischedel et al , 1987</ref>, bsn, 1988 pronoun resolution component, which applies centering techniques <tref>grosz et al , 1983</tref>, <ref>sidner 1981, 1983</ref>, <ref>brennan et al 1987</ref> to track and constrain references.
2 meaninq representation for de generation webber found that appropriate discourse entities could be generated from the meaning representation of a sentence by applying rules to the representation that are strictly structural in nature, as long as the representation reflects certain crucial aspects of the sentence.
depending on the properties of a particular grammar, it may, for example, be worthwhile to restrict a given category to its syntactic features before attempting to solve the parse-goal of that category.
for example, the category xa, b, f a, b, ga,hb, i c   may be weakened into: xa,b,f ,,g, if we assume that the predicate weaken/2 relates a term t to a weakened version tw, such that tw subsumes t, then 15 is the improved version of the parse predicate: parsewithweakening cat, p0, p, e0, e  15 weakencat,weakenedcat, parseweakenedcat,p0,p,e0,e, catweakenedcat.
2 there are important differences between the technique for limited prediction in this parser, and other techniques for limited prediction such as shiebers notion of restriction <tref>shieber, 1985</tref> which we also use.
in methods such as shiebers, predictions are weakened in ways that can result in an overall gain in efficiency, but predictions nevertheless must be dynamically generated for every phrase that is built bottom-up.
in addition, the parser maintains a skeletal copy of the chart in which edges are labeled only by the nonterminal symbols contained in their context-free backbone, which gives us more efficient indexing of the full grammar rules.
comparison with other parsers table 1 compares the average number of edges, average number of predictions, and average parse times 1 in seconds per utterance for the limited 1all parse times given in this paper were produced on a sun sparcstation 10/51, running quintus pro111 for grammar with start symbol , phrase structure rules p, lexicon l, context-independent categories ci, and context-dependent categories cd; and for word string w  wlwn: variant edges preds secs bottom-up 1191 0 146 limited left-context 203 25 10 left-corner 112 78 40 table h comparison of syntax-only parsers if  e cd, predictt, 0; addemptycategories 0 ; for i from i to n do foreach c such that c--wi el do addedgetochartc, i-i, i ; makenewpredictionsc, ii, i ; findnew-reductionsc, il,i end addemptycategories i ; end sub findmew-reductionsb, j, k  foreach a and a such that a- b 6 p do foreach i such that i  match, j do if a 6 cd and predicteda,i or a 6 ci addedgetocharta, i, k; makenewpredictionsa, i, k ; findnewreductionsa, i, k ; end end  sub addemptycategoriesi  foreach a such that a - e e p do if a 6 cd and predicteda,/ or a 6 ci addedgetocharta, i, i ; makenewpredictionsa, i, i ; findnewreductionsa, i, i ; end  sub makenewpredictionsa, i, j  foreach aft e predictionsi do predict fl, j end foreach h - abfl 6 p such that h 6 ci and b e cd and fl 6 ci do predict b, j end foreach h -- ab 6 p such that h e cd and b e cd and fl e ci and predictedh, i or h left-corner-of c and predictedc, i do predict b, j end figure 1: limited left-context algorithm left-context parser with those for a variant equivalent to a bottom-up parser when all categories are context independent and for a variant equivalent to a left-corner parser when all categories are context dependent.
the situation becomes more complicated when we move to unification-based grammars, since there may be an unbounded number of different categories appearing in the accessible stack states.
in the system implemented here we used restriction <tref>shieber, 1985</tref> on the stack states to restrict attention to a finite number of distinct stack states for any given stack depth.
since the restriction operation maps a stack state to a more general one, it produces a finite-state approximation which accepts a superset of the language generated by the original unification grammar.
yet these grammars apparently have enough formal power to describe natural language at least, they can describe the crossed-serial dependencies of dutch and swiss german, which are presently the most widely accepted example of a construction that goes beyond context-free grammar <tref>shieber 1985a</tref>.
though theoretically very attractive, codescription has its price: i the grammar is difficult to modularize due to the fact that the levels constrain each other mutually and ii there is a computational overhead when parsers use the complete descriptions.
problems of these kinds which were already noted by <tref>shieber, 1985</tref> motivated tile research described here.
the goal was to develop more flexible ways of using codescriptive grammars than having them applied by a parser with full informational power.
the underlying observation is that constraints in such grammars can play different roles:  genuine constraints which relate directly to tile grammaticality wellformedness of the input.
furthermore, the need to perform nondestructive unification means that a large proportion of the processing time is spent copying feature structures.
one approach to this problem is to refine parsing algorithms by developing techniques such as restrictions, structure-sharing, and lazy unification that reduce the amount of structure that is stored and hence the need for copying of features structures <tref>shieber, 1985</tref>; <ref>pereira, 1985</ref>; <ref>karttunen and kay, 1985</ref>; <ref>wroblewski, 1987</ref>; <ref>gerdemann, 1989</ref>; <ref>godden, 1990</ref>; <ref>kogure, 1990</ref>; <ref>emele, 1991</ref>; <ref>tomabechi, 1991</ref>; <ref>harrison and ellison, 1992</ref>.
while these techniques can yield significant improvements in performance, the generality of unification-based grammar formalisms means that there are still cases where expensive processing is unavoidable.
original earley prediction works on category symbols.
since categorial grammars exhibit an extensive embedding of categories within other categories, it is useful to unify templates not only with the whole lexical dag but also with its categorial subgraphs.
fhese ,lasses may be established in a number of ways; the one we have adopted in that presented by harrison and ellison,  992 which builds on l;he work of <tref>shieber, 1985</tref>: it introduces the nol;ion of a negative restrictor to define equivalence classes.
for instance, in the above example we could define the negative restrictor to be orth.
contrary to bottomup parsing, however, the adaptation of a top-down algorithm for ug requires some special care.
top-down prediction with a restrictor r where r is a finite set of paths through a feature-structure amounts to the following: restriction the restriction of a feature-structure f relative to a restrictor r is the most specific feature-structure f  e f, such that every path in f j has either an atomic value or is an element of r predictor step for each item, end, lhs, parsed, next i toparse such that rjve, is the restriction of next relative to r, and each rule rne:t  rhs, add itemi,i, rge:t, , rhs.
in particular, in order to derive a finite cf grammar, we will need to consider only those features that have a finite number of possible values, or at least consider only finitely many of the possible values for infinitely valued features.
removing these features may give us a more permissive language model, but it will still be a sound approximation.
the situation becomes more complicated when we move to unification-based grammars, since there may be an unbounded number of different categories appearing in the accessible stack states.
in the system implemented here we used restriction <tref>shieber, 1985</tref> on the stack states to restrict attention to a finite number of distinct stack states for any given stack depth.
since the restriction operation maps a stack state to a more general one, it produces a finite-state approximation which accepts a superset of the language generated by the original unification grammar.
more specifically, magic generation falls prey to non-termination in the face of head recursion, ie, the generation analog of left recursion in parsing.
this necessitates a dynamic processing strategy, ie, memoization, extended with an abstraction function like, eg, restriction <tref>shieber, 1985</tref>, to weaken filtering and a subsumption check to discard redundant results.
it is shown that for a large class of grammars the subsumption check which often influences processing efficiency rather dramatically can be eliminated through fine-tuning of the magic predicates derived for a particular grammar after applying an abstraction function in an off-line fashion.
we view the linking relation not simply as a filter to increase efficiency within the domain of syntactic analysis--this aspect is stressed by <tref>shieber 1985</tref> and other investigators such as <ref>bouma 1991</ref>--but rather as a device for the top-down predictive instantiation of information, as shieber et al.
in this paper we are concerned especially with morphosyntactic information and illustrate the relevance of predictive linking for morphological analysis and for the analysis of unknown or new lexical items.
in particular, declaring certain category-valued features so that they cannot take variable values may lead to nontermination in the backbone construction for some grammars.
building lr parse tables for large nl grammars the backbone grammar generated from the anlt grammar is large: it contains almost 500 distinct categories and more than 1600 productions.
in contrast to the symbols in context-free grammars, feature structures in unification-based grammars often include information encoding part of the derivation history, most notably semantics.
in order to achieve successful packing rates, feature restriction <tref>shieber, 1985</tref> is used to remove this information during creation of the packed parse forest.
during the unpacking phase, which operates only on successful parse trees, these features are unified back in again.
for their experiments with efficient subsumptionbased packing, <ref>oepen and carroll, 2000</ref> experimented with different settings of the packing restrictor for the english resource grammar erg <ref>copestake and flickinger, 2000</ref>: they found that good packing rates, and overall good performance during forest creation and unpacking were achieved, for the erg, with partial restriction of the semantics, eg keeping index features unrestricted, since they have an impact on external combinatorial potential, but restricting most of the internal mrs representation, including the list of elementary predications and scope constraints.
attention is restricted here to approximations of context-free grammars because context-free languages are the smallest class of formal language that can realistically be applied to the analysis of natural language.
that is, instantiation of productions introduces the nontermination problem of left-recursive productions to the procedure, as well as to the predictor step of earleys algorithm.
the situation is different for active chart items since daughters can affect their siblings.
to be independent from a-certain grammatical theory or implementation, we use restrictors similar to <tref>shieber, 1985</tref> as a flexible and easyto-use specification to perform this deletion.
a positive restrictor is an automaton describing the paths in a feature structure that will remain after restriction the deletion operation, 3there are refinements of the technique which we have implemented and which in practice produce additional benefits; we will report these in a subsequent paper.
briefly, they involve an improvement to th e path collection method, and the storage of other information besides types in the vectors.
that is, for the reference determination, the subject roles of the candidates referent within a discourse segment will be checked intheflrstplace.
candi pron and candi noantecedent are to be examined in the cases when the subject-role checking fails, which conflrms the hypothesis in the s-list model by <ref>strube 1998</ref> that co-refereing candidates would have higher preference than other candidates in the pronoun resolution.
pragmatic level working together, surface patterns and possessive relationships can deal with many ppas found in our corpus, but we still have two problems to be solved: semantic ambiguity among two or more acceptable candidates and abstract anaphors/antecedents, which cannot be solved by simply applying possessive relationship rules.
for these cases, and possibly for some other cases not included in previous rules, we suggest a pragmatic factor, adapted from s brennans et al 1987 centering algorithm.
although sentence center plays a crucial role in many works in anaphor resolution, usually limiting the number of candidates to be considered, we notice that, because ppas can refer to almost any np in the sentence rather than, for example, personal pronouns, which are often related to the sentence center, pragmatic knowledge plays only a secondary but still important role in our approach.
we have adapted basic aspects of center algorithm, considering subject/object preference, and domain concepts preference, 1012 suggested by r <ref>mitkov 1996</ref>, aiming to estimate the most probable center for intrasentential ppas.
even though we are not following here the distinction between constraints and rules introduced in <ref>brennan, friedman, and pollard 1987</ref>, we will use for these three claims the names brennan et al gave them, by which they are now best known: constraint 1 strong: all utterances of a segment except for the first have exactly one cb.
231 constraint 1, topic uniqueness, and entity coherence.
given the above possible sources of informar tion, we arrive at the following equation, where fp denotes a function from pronouns to their antecedents: fp  argmaxp ap  alp, h, l, t, l, so, d a where ap is a random variable denoting the referent of the pronoun p and a is a proposed antecedent.
in the conditioning events, h is the head constituent above p, l r is the list of candidate antecedents to be considered, t is the type of phrase of the proposed antecedent always a noun-phrase in this study, i is the type of the head constituent, sp describes the syntactic structure in which p appears, dspecifies the distance of each antecedent from p and m is the number of times the referent is mentioned.
it is often claimed in current work on in natural language generation that the constraints on felicitous text proposed by the theory are useful to guide text structuring, in combination with other factors see <ref>karamanis, 2003</ref> for an overview.
however, how successful centerings constraints are on their own in generating a felicitous text structure is an open question, already raised by the seminal papers of the theory <tref>brennan et al , 1987</tref>; <ref>grosz et al , 1995</ref>.
in accordance with recent work in the emerging field of text-to-text generation <ref>barzilay et al , 2002</ref>; <ref>lapata, 2003</ref>, we assume that the input to text structuring is a set of clauses.
identify transition with the cb and cf resolved, use the criteria from <tref>brennan et al , 1987</tref> to assign the transition.
cheapness is satisfied by a transition pair un-1, un, un, unl if the preferred center of un is the cb of unl for example, this test is satisfied by a retain-shift sequence but not by continue-shift, so it is predicted that the former pattern will be used to introduce a new center.
this claim is consistent with the findings of <ref>brennan 1998</ref>, <tref>brennan et al 1987</tref>.
if we consider examples la-e below, the sequence cd-e , including a retain-shift sequence, reads more fluently than c-d-e even though the latter scores better according to the canonical ranking.
pfnocb, a second baseline, which enhances mnocb with a global constraint on coherence that <ref>karamanis, 2003</ref> calls the pagefocus pf.
pfbfp which is based on pf as well as the original formulation of ct in <tref>brennan et al , 1987</tref>.
pfkp which makes use of pf as well as the recent reformulation of ct in <ref>kibble and power, 2000</ref>.
one more filtering criterion is mutual information mi, which reflects the relatedness of two terms in their combination , kj ww  to keep a relation  kji wwwp, we require , kj ww be a meaningful combination.
we use the following pointwise mi <tref>church and hanks 1989</tref>:  ,log, kj kj kj wpwp wwpwwmi  we only keep meaningful combinations such that 0, >kj wwmi  by these filtering criteria, we are able to reduce considerably the number of biterms and triterms.
for example, on a collection of about 200mb, with a vocabulary size of about 148k, we selected only about 27m useful biterms and about 137m triterms, which remain tractable.
afterward, if a target adjective sense was not resolved, semantic indicator attributes were applied; no individual indicator nouns were used.
the semantic attributes that were applied were animate, body part, color, concrete, human, and text type; <tref>church and hanks 1989</tref> had pointed to two of these attributes, person and body part also time, previously mentioned above in a seemingly casual listing of just five attributes potentially useful for describing the lexico-syntactic regularities of noun-verb relations.
disambiguation by these syntactic and semantic attributes is effectively as reliable as disambiguation using significant indicator nouns: having three apparent errors in disambiguation is not significantly worse than the errorless performance of the significant indicator nouns in the 100-sentence samples.
content words that have a close syntactic relation to one another are useful candidates for examination and are intuitively more likely to bear a close semantic relation than words that are near one another but are not related syntactically.
determining the potential of this line of evidence is the focus of this paper.
like path coreference, semantic compatibility can be considered a form of world knowledge needed for more challenging pronoun resolution instances.
suppose we are determining whether ham is a suitable antecedent for the pronoun it in eat it.
we calculate the mi as: mieat:obj, ham  log preat:obj:hampreat:objprham although semantic compatibility is usually only computed for possessive-noun, subject-verb, and verb-object relationships, we include 121 different kinds of syntactic relationships as parsed in our news corpus3 we collected 488 billion parent:rel:node triples, including over 327 million possessive-noun values, 129 billion subject-verb and 877 million verb-direct object.
this line of research was motivated by a series of successful applications of mutual information statistics to other problems in natural language processing.
in the last decade, research in speech recognition <ref>jelinek 1985</ref>, noun classification <ref>hindle 1988</ref>, predicate argument relations <tref>church  hanks 1989</tref>, and other areas have shown that mutual information statistics provide a wealth of information for solving these problems.
it is a function of the probabilities of the two events: mz, u  log u xzpvy in this paper, the events x and y will be part-of-speech n-grams instead of single parts-of-speech, as in some earlier work.
for evaluation of the association measures, a6 -best strategies section 41 are supplemented with precision and recall graphs section 42 over the complete data sets.
41 eiir: expected independent information ranking model baseline model recall the task definition from section 3.
expression 1 calculates the score between two neighboring letters; uki  e e mwj,wid;d  x,qd 1 dl j-i--d--1 where wl as an even;, d as the distance between two even;s, dmax as the maximum distance used in the processing we set drnax - 5, and gd as the weight fimction on distance for this system gd  d-2<ref>sano et al , 1996</ref>, to decrease tile influence of tile d-bigrams when the distance get longer <tref>church and hanks, 1989</tref>.
in this paper, we define novel measures both collocation based and context based measures to measure the relative compositionality of mwes of v-n type see section 6 for details.
integrating these statistical measures should provide better evidence for ranking the expressions.
mutual information is attractive because it is not only easy to compute, but also takes into consideration corpus statistics and semantics.
these features have largely been evaluated by the correlation of the compositionality value predicted by these measures with the gold standard value suggested by human judges.
it has been shown that the correlation of these measures is higher than simple baseline measures suggesting that these measures represent compositionality quite well.
even though, these measures have been shown to represent compositionality quite well, compositionality itself has not been shown to be useful in any application yet.
more is to be learned from the fact that you can drink wine than from the fact that you can drink it even though there are more clauses in our sample with  as an object of drink than with wine.
the mutual information of two events lx y is defined as follows: px y lxy  log2 px py where px y is the joint probability of events x and y, and px and py axe the respective independent probabilities.
when the joint probability px y is high relative to the product of the independent probabilities, i is positive; when the joint probability is relatively low, i is negative.
mutual information has been positively used in many nlp tasks such as collocation analysis <tref>church and hanks, 1989</tref>, terminology extraction <ref>damerau, 1993</ref>, and word sense disambiguation <ref>brown et al , 1991</ref>.
only a positive combination of both these two ingredients can give good results when applying and evaluating the model.
however, the statistics we use provide more information and allow us to have more precision in our output.
since we use grammatical context, the feature set is considerably larger than the simple word based proximity feature set for the newspaper corpus.
43 calculating feature vectors having collected all nouns and their features, we now proceed to construct feature vectors and values for nouns from both corpora using mutual information <tref>church and hanks, 1989</tref>.
the so of a phrase is determined based upon the phrases pointwise mutual information pmi with the words excellent and poor.
the so for a a28a37a36a39a38a41a40a29a42a44a43 is the difference between its pmi with the word excellent and its pmi with the word poor the method used to derive these values takes advantage of the possibility of using the world wide web as a corpus, similarly to work such as <ref>keller and lapata, 2003</ref>.
the probabilities are estimated by querying the altavista advanced search engine1 for counts.
to solve the problem, we make use of a kind of cooperative evolution strategy to design an evolutionary algorithm.
word compositions have long been a concern in lexicography<ref>benson et al 1986</ref></ref>; <ref>miller et al 1995</ref>, and now as a specific kind of lexical knowledge, it has been shown that they have an important role in many areas in natural language processing, eg, parsing, generation, lexicon building, word sense disambiguation, and information retrieving, etceg , <ref>abney 1989, 1990</ref>; <ref>benson et al 1986</ref></ref>; <ref>yarowsky 1995</ref>; <tref>church and hanks 1989</tref>; church, <ref>gale, hans, and hindle 1989</ref>.
dictionaries produced by hand always substantially lag real language use.
the last two points do not argue against the use of existing dictionaries, but show that the incomplete information that they provide needs to be supplemented with further knowledge that is best collected automatically the desire to combine hand-coded and automatically learned knowledge 1a point made by <tref>church and hanks 1989</tref>.
for example, among the 27 verbs that most commonly cooccurred with from, church and hanks found 7 for which this 235 suggests that we should aim for a high precision learner even at some cost in coverage, and that is the approach adopted here.
c8b4crcyd4d3d7b5 bp cub4crbnd4d3d7b5 cub4crbnd4d3d7b5b7cub4bmcrbnd4d3d7b5 c8b4crcyd2ctcvb5 bp cub4crbnd2ctcvb5 cub4crbnd2ctcvb5b7cub4bmcrbnd2ctcvb5 pmi based polarity value using pmi, the strength of association between cr and positive sentences and negative sentences is defined as follows <tref>church and hanks, 1989</tref>.
c8c5c1b4crbnd4d3d7b5 bp d0d3cv be c8b4crbnd4d3d7b5 c8b4crb5c8b4d4d3d7b5 c8c5c1b4crbnd2ctcvb5 bp d0d3cv be c8b4crbnd2ctcvb5 c8b4crb5c8b4d2ctcvb5 pmi based polarity value is defined as their difference.
if the absolute value of the relative distance in a sentence for a feature and an opinion word is less than minimum-offset, they are considered contextdependent.
many efficient techniques exist to extract multiword expressions, collocations, lexical units and idioms <tref>church and hanks, 1989</tref>; <ref>smadja, 1993</ref>; <ref>dias et al , 2000</ref>; <ref>dias, 2003</ref>.
the definition can be easily extended to a set of expressions t given a pair vt and vh we define the following entailment strength indicator svt,vh.
counts are considered useful when they are greater or equal to 3.
we have already used this algorithm successfully as a part of a system to assign senses to english and french words on the basis of the context in which they appear <ref>brown et al 1991a, 1991b</ref>.
it can resolve the alignment problem on real bilingual text.
on clean inputs, such as the canadian hansards and the hong kang hansards, these methods have been very successful.
church, kenneth w, 1993; <ref>chen, stanley, 1993</ref> proposed some methods to resolve the problem in noisy bilingual texts.
the result shows that the use of dependency relation helps to acquire interesting translation patterns.
since the advent of statistical methods in machine qhanslation, the bilingual sentence alignmerit <tref>brown et al , 1991</tref> or word alignment <ref>dagan et al , 1992</ref> have been explored and achieved numerous success over the last decade.
as word sequences are not translated literally a word for a word, acquiring phraseqevel correspondence still remains an important problem to be exploited.
4 conclusions compared with other word alignment algorithms <ref>brown et al , 1993</ref>; <ref>gale and church, 1991a</ref>, wordalign does not require sentence alignment as input, and was shown to produce useful alignments for small and noisy corpora.
taking the output of charalign as input, wordalign produces significantly better, word7 offset from correct alignment 0 1 2 3 4 percentage 605 108 75 52 16 accumulative percentage 605 713 788 84 856 table 2: wordaligns precision on noisy input, scanned by an ocr device.
588 ido dagan and alon itai word sense disambiguation vides a useful source of a sense tagged corpus.
<ref>gale, church, and yarowsky 1992a</ref> have also exploited this resource for achieving large amounts of testing and training materials.
clearly, statistics on lexical relations can also be useful for target word selection.
some of our initial data suggest that the hypothesis of deep semantic selection may in fact be correct, as well as indicating what the nature of the coercion rules may be.
counts for objects of begin/v: 205 begin/v career/o 176 begin/v day/o 159 begin/v work/o 140 begin/v talk/o 120 begin/v campaign/o 113 begin/v investigation/o 106 begin/v process/o 92 begin/v program/o 8s begin/v operation/o 86 begin/v negotiation/o 66 begin/v strike/o 64 begin/v production/o 59 begin/v meeting/o 89 begin/v term/o 50 begin/v visit/o 45 begin/v test/o 39 begin/v construction/o 31 begin/v debate/o 29 begin/v trial/o corpus studies confirm similar results for weakly intensional contexts <ref>pustejovsky 1991</ref> such as the complement of coercive verbs such as veto.
these are interesting because regardless of the noun type appearing as complement, it is embedded within a semantic interpretation of the proposal to, thereby clothing the complement within an intensional context.
some of our initial data suggest that the hypothesis of deep semantic selection may in fact be correct, as well as indicating what the nature of the coercion rules may be.
corpus studies confirm similar results for weakly intensional contexts such as the complement of coercive verbs such as veto.
these are interesting because regardless of the noun type appearing as complement, it is embedded within a semantic interpretation of the proposal to, thereby clothing the complement within an intensional context.
methods of resolving ambiguities have been based, for example, on the assumption that case slots are mutually independent <tref>hindle and rooth 1991</tref>, or at most two case slots are dependent <ref>collins and brooks 1995</ref>.
in this article, we propose an efficient and general method of learning dependencies between case frame slots.
in general, these words will not be adjacent in the text, so it will not be possible to use existing approaches unmodified eg <ref>church and hanks 1989</ref>, because these apply to adjacent words in unanalyzed text.
<tref>hindle and rooth 1991</tref> report good results using a mutual information measure of collocation applied within such a structurally defined context, and their approach should carry over to our framework straightforwardly.
one way of integrating structural collocational information into the system presented above would be to make use of the semantic component of the anlt grammar this component pairs logical forms with each distinct syntactic analysis that represent, among other things, the predicate-argument structure of the input.
in the resolution of pp attachment and similar ambiguities, it is collocation at this level of representation that appears to be most relevant.
the novel aspect of our study is that we collect not only operational pairs, but triples, such as nprep n, vprepn etc in fact, the preposition convey important information on the nature of the semantic link between syntactically related content words.
by looking at the preposition, it is possible to restrict the set of semantic relations underlying a syntactic relation eg forpurpose,beneficiary.
partial parsing techniques have been used with a considerable success in processing large volumes of text, for example atts fidditch <tref>hindle and rooth, 1991</tref> parsed 13 million words of associated press news messages, while mits parser de <ref>marcken, 1990</ref> was used to process the 1 million word lancaster/oslo/bergen lob corpus.
this kind of partial analysis may be sufficient in some applications because of a relatively high precision of identifying correct syntactic dependencies.
future directions this paper presented one method of learning subcategorizations, but there are other approaches one might try.
this method could be usefully incorporated into my parser, but it remains a special-purpose technique for one particular ease.
another research direction would be making the parser stochastic as well, rather than it being a categorical finite state device that runs on the output of a stochastic tagger.
the generalization step is needed in order to represent the input case frame instances more compactly as well as to judge the degree of acceptability of unseen case frame instances.
for the extraction problem, there have been various methods proposed to date, which are quite adequate <tref>hindle and rooth 1991</tref>; <ref>grishman and sterling 1992</ref>; <ref>manning 1992</ref>; <ref>utsuro, matsumoto, and nagao 1992</ref>; <ref>brent 1993</ref>; <ref>smadja 1993</ref>; <ref>grefenstette 1994</ref>; <ref>briscoe and carroll 1997</ref>.
a number of methods for generalizing values of a case frame slot for a verb have been  cc media res.
figure 10 shows the result as lat, where the threshold for t-score is set to 128 significance level of 90 percent.
from figure 10 we see that with respect to accuracy-coverage curves, mdl outperforms both sa and la throughout, while sa is better than la next, we tested the method of applying a default rule after applying each method.
sense disambiguation thus resembles many other decision tasks, and not surprisingly, several common decision algorithms were employed in different works.
these include a bayesian classifier <ref>gale, church, and yarowsky 1993</ref> and a distance 589 computational linguistics volume 20, number 4 metric between vectors <ref>schiitze 1993</ref>, both inspired from methods in information retrieval; the use of the flip-flop algorithm for ordering possible informants about the preferred sense, trying to maximize the mutual information between the informant and the ambiguous word <tref>brown et al 1991</tref>; and the use of confidence intervals to establish the degree of confidence in a certain preference, combined with a constraint propagation algorithm the current paper.
at the present stage of research on sense disambiguation, it is difficult to judge whether a certain decision algorithm is significantly superior to others.
a third difference concerns the granularity of wsd attempted, which one can illustrate in terms of the two levels of semantic distinctions found in many dictionaries: homograph and sense see section 31.
like <ref>cowie, guthrie, and guthrie 1992</ref>, we shall give results at both levels, but it is worth pointing out that the targets of, say, work using translation equivalents eg , <tref>brown et al 1991</tref>; <ref>gale, church, and <ref>yarowsky 1992</ref>c</ref>; and see section 23 and roget categories <ref>yarowsky 1992</ref>; <ref>masterman 1957</ref> correspond broadly to the wider, homograph, distinctions.
in this paper we attempt to show that the high level of results more typical of systems trained on many instances of a restricted vocabulary can also be obtained by large vocabulary systems, and that the best results are to be obtained from an optimization of a combination of types of lexical knowledge see section 2.
11 lexical knowledge and wsd syntactic, semantic, and pragmatic information are all potentially useful for wsd, as can be demonstrated by considering the following sentences: 1 2 3 4 john did not feel well.
semi-supervised methods for wsd are characterized in terms of exploiting unlabeled data in learning procedure with the requirement of predefined sense inventory for target words.
they roughly fall into three categories according to what is used for supervision in learning process: 1 using external resources, eg, thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, <ref>lesk, 1986</ref>; <ref>lin, 1997</ref>; <ref>mccarthy et al , 2004</ref>; <ref>seo et al , 2004</ref>; <ref>yarowsky, 1992</ref>, 2 exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora eg parallel corpora or untagged monolingual corpora in two languages <tref>brown et al , 1991</tref>; <ref>dagan and itai, 1994</ref>; <ref>diab and resnik, 2002</ref>; <ref>li and li, 2004</ref>; <ref>ng et al , 2003</ref>, 3 bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data <ref>hearst, 1991</ref>; <ref>karov and edelman, 1998</ref>; <ref>mihalcea, 2004</ref>; <ref>park et al , 2000</ref>; <ref>yarowsky, 1995</ref>.
as a commonly used semi-supervised learning method for wsd, bootstrapping algorithm works by iteratively classifying unlabeled examples and adding confidently classified examples into labeled dataset using a model learned from augmented labeled dataset in previous iteration.
it can be found that the affinity information among unlabeled examples is not fully explored in this bootstrapping process.
for example, the correct alignment of the bilingual corpus in figure 2 consists of the sentence bead el; f1 followed by the sentence bead e2; ;2, f3.
this approach is quite different from those adopted for the translation of single words <ref>klavans and tzoukermann 1990</ref>; <ref>dorr 1992</ref>; <ref>klavans and tzoukermann 1996</ref>, since for single words polysemy cannot be ignored; indeed, the problem of sense disambiguation has been linked to the problem of translating ambiguous words <tref>brown et al 1991</tref>; <ref>dagan, itai, and schwall 1991</ref>; <ref>dagan and itai 1994</ref>.
the assumption of a single meaning per collocation was based on our previous experience with english collocations <ref>smadja 1993</ref>, is supported for less opaque collocations by the fact that their constituent words tend to have a single sense when they appear in the collocation <ref>yarowsky 1993</ref>, and was verified during our evaluation of champollion section 7.
we construct a mathematical model of the events we want to correlate, namely, the appearance of any word or group of words in the sentences of our corpus, as follows: to each group of words g, in either the source or the target language, we map a binary random variable xc that takes the value 1 if g appears in a particular sentence and 0 if not.
semi-supervised methods for wsd are characterized in terms of exploiting unlabeled data in the learning procedure with the need of predened sense inventories for target words.
the information for semi-supervised sense disambiguation is usually obtained from bilingual corpora eg parallel corpora or untagged monolingual corpora in two languages <tref>brown et al , 1991</tref>; <ref>dagan and itai, 1994</ref>, or sense-tagged seed examples <ref>yarowsky, 1995</ref>.
they always rely on hand-crafted lexicons eg , wordnet as sense inventories.
by repeating the above processes, it can create an accurate classifier for word translation disambiguation.
to deal with these robustness issues, <ref>church 1993</ref> developed a character-based alignment method called charalign.
the method was intended as a replacement for sentence-based methods eg , <tref>brown et al , 1991a</tref>; <ref>gale and church, 1991b</ref>; <ref>kay and rosenschein, 1993</ref>, which are very sensitive to noise.
this paper describes a new program, called wordalign, that starts with an initial rough alignment eg , the output of charalign or a sentence-based alignment method, and produces improved alignments by exploiting constraints at the word-level.
more importantly, because wordalign and charalign were designed to work robustly on texts that are smaller and more noisy than the hansards, it has been possible to successfully deploy the programs at att language line services, a commercial translation service, to help them with difficult terminology.
the information retrieval application may be of particular relevance to this audience.
the information retrieval application may be of particular relevance to this audience.
it would be highly desirable for users to be able to express queries in whatever language they chose and retrieve documents that may or may not have been written in the same language as the query.
similar results are reported in <ref>nakatani, hirschberg, and grosz 1995</ref> and <ref>hirschberg and nakatani 1996</ref> for spontaneous speech as well.
<ref>grosz and hirschberg 1992</ref> also use the classification and regression tree system cart <ref>brieman et al 1984</ref> to automatically construct and evaluate decision trees for classifying aspects of discourse structure from intonational feature values.
<ref>in swerts 1995</ref>, paragraph boundaries are empirically obtained as described above.
semcor contains 91,808 words tagged with wordnet synsets, 6,071 of which are proper names, which we ignored, leaving 85,737 words which could potentially be translated.
the translation contains only 36,869 words tagged with ldoce senses; however, this is a reasonable size for an evaluation corpus for the task, and it is several orders of magnitude larger than those used by other researchers working in large vocabulary wsd, for example <ref>cowie, guthrie, and guthrie 1992</ref>, <ref>harley and glennon 1997</ref>, and mahesh et al.
co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction <ref>smadja, 1993</ref>; <ref>fung and wu, 1994</ref>, phrasal translation <ref>smadja et al , 1996</ref>; <ref>kupiec, 1993</ref>; <ref>wu, 1995</ref>; <ref>dagan and church, 1994</ref>, target word selection <ref>liu and li, 1997</ref>; <ref>tanaka and iwasaki, 1996</ref>, domain word translation <ref>fung and lo, 1998</ref>; <ref>fung, 1998</ref>, sense disambiguation <ref>brown et al , 1991</ref>; <ref>dagan et al , 1991</ref>; <ref>dagan and itai, 1994</ref>; <tref>gale et al , 1992a</tref>; <tref>gale et al , 1992b</tref>; <tref>gale et al , 1992c</tref>; <ref>shiitze, 1992</ref>; <ref>gale et al , 1993</ref>; <ref>yarowsky, 1995</ref>, and even recently for query translation in cross-language ir as well <ref>ballesteros and croft, 1998</ref>.
we want to devise a method that uses only monolingual data in the primary language to train co-occurrence information.
co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction <ref>smadja, 1993</ref>; <ref>fung and wu, 1994</ref>, phrasal translation <ref>smadja et al , 1996</ref>; <ref>kupiec, 1993</ref>; <ref>wu, 1995</ref>; <ref>dagan and church, 1994</ref>, target word selection <ref>liu and li, 1997</ref>; <ref>tanaka and iwasaki, 1996</ref>, domain word translation <ref>fung and lo, 1998</ref>; <ref>fung, 1998</ref>, sense disambiguation <ref>brown et al , 1991</ref>; <ref>dagan et al , 1991</ref>; <ref>dagan and itai, 1994</ref>; <tref>gale et al , 1992a</tref>; <tref>gale et al , 1992b</tref>; <tref>gale et al , 1992c</tref>; <ref>shiitze, 1992</ref>; <ref>gale et al , 1993</ref>; <ref>yarowsky, 1995</ref>, and even recently for query translation in cross-language ir as well <ref>ballesteros and croft, 1998</ref>.
7 subjects a, b, c, d, e, f, g 161 i0 hei falls over, figure 2: portion of segntentation from narrative 6 subject annotation of narratrs httention digression to describe suml track no verbal communication ie , speaker describes lack thereof describes that it is a silent movie with only nature sounds speaker describes sound techniques used in tnovie explain that there is no speaking ill nlovie figure 3: segment spanning 1,12 through 154 3 discourse segment boundaries in <ref>passonneau and litman, 1993</ref>, we show that our subjects agree with one another at levels that are statistically significant, thus demonstrating the reliability of intention as a segmentation criterion.
percent agreement is defined in <tref>gale et al , 1992</tref> as the ratio of observed agreements with the majority opinion to possible agreements with the majority opinion.
we use percent agreement to measure the ability of subjects to agree with one anotlter on whether there is  segment boundary between two adjacent prosodic phrases.
we find that the average agreement across the 20 narratives on the status of all potential boundary locations is 89 with a range from 82-92.
this process is similar to the use of general pseudowords <tref>gale et al , 1992b</tref>; <ref>gaustad, 2001</ref>; <ref>nakov and hearst, 2003</ref>, but has some essential differences.
this artificial ambiguous word need to simulate the function of the real ambiguous word, and to acquire semantic knowledge as the real ambiguous word does.
thus, we call it an equivalent pseudoword ep for its equivalence with the real ambiguous word.
making such an assumption is reasonable since pos taggers that can achieve accuracy of 96 are readily available to assign pos to unrestricted english sentences <ref>brill, 1992</ref>; <ref>cutting et al , 1992</ref>.
in addition, sense definitions are only available for root words in a dictionary.
these are words that are not morphologically inflected, such as interest as opposed to the plural form interests, fall as opposed to the other inflected forms like fell, fallen, falling, falls, etc the sense of a morphologically inflected content word is the sense of its uninflected form.
table 5: tion out score 0008421 0007895 0007669 0007588 0007283 0006812 0006430 0006218 0005921 0005527 0005335 0005335 0005221 0004731 0004470 0004275 0003878 0003859 0003859 0003784 0003686 0003550 0003519 0003481 0003407 0003407 0003338 0003324 some chinese ut english teng-hui sar flu lei poultry sar hijack poultry tung diaoyu primeminister president china lien poultry china flu primeminister president poultry kalkanov poultry sar zhuhai primeminister president flu apologise unknown word translachinese  weng-hui  u lei j poultry  chee-hwa  teng-hui  sar  chee-hwa : teng-hui  weng-hui w weng-hui clam  teng-hui - chee-hwa  teng-hui lei  chee-hwa  chee-hwa  leung  zhuhai i lei j yeltsin - chee-hwa  lam lam j poultry w teng-hui 0003250 dpp 0003206 tang 0003202 tung 0003040 leung 0003033 china 0002888 zhuhai 0002886 tung  teng-hui tang leung leung  sar  lunar tung 1994 for sense disambiguation between multiple usages of the same word.
in the years since the appearance of the first papers on using statistical models for bilingual lexicon compilation and machine translation<ref>brown et al , 1993</ref>; <ref>brown et al , 1991</ref>; <ref>gale and <ref>church, 1993</ref></ref>; <ref>church, 1993</ref>; <ref>simard et al , 1992</ref>, large amount of human effort and time has been invested in collecting parallel corpora of translated texts.
our goal is to alleviate this effort and enlarge the scope of corpus resources by looking into monolingual, comparable texts.
word sense disambiguation wsd is wellknown as one of the more difficult problems in the field of natural language processing, as noted in <tref>gale et al, 1992</tref>; <ref>kilgarriff, 1997</ref>; <ref>ide and vronis, 1998</ref>, and others.
the difficulties stem from several sources, including the lack of means to formalize the properties of context that characterize the use of an ambiguous word in a given sense, lack of a standard and possibly exhaustive sense inventory, and the subjectivity of the human evaluation of such algorithms.
to address the last problem, <tref>gale et al, 1992</tref> argue for upper and lower bounds of precision when comparing automatically assigned sense labels with those assigned by human judges.
the difficulties stem from several sources, including the lack of means to formalize the properties of context that characterize the use of an ambiguous word in a given sense, lack of a standard and possibly exhaustive sense inventory, and the subjectivity of the human evaluation of such algorithms.
to address the last problem, <tref>gale et al, 1992</tref> argue for upper and lower bounds of precision when comparing automatically assigned sense labels with those assigned by human judges.
the lower bound should not drop below the baseline usage of the algorithm in which every word that is disambiguated is assigned the most frequent sense whereas the upper bound should not be too restrictive when the word in question is hard to disambiguate even for human judges a measure of this difficulty is the computation of the agreement rates between human annotators.
identification and formalization of the determining contextual parameters for a word used in a given sense is the focus of wsd work that treats texts in a monolingual settingthat is, a setting where translations of the texts in other languages either do not exist or are not considered.
the summed deviation for perfect performance is thus 0.
researchers have begun to investigate the ability of humans to agree with one another on segmen108 tation, and to propose methodologies for quantifying their findings.
several studies have used expert coders to locally and globally structure spoken discourse according to the model of <ref>grosz and sidnet 1986</ref>, including <ref>grosz and hirschberg, 1992</ref>; <ref>hirschberg and grosz, 1992</ref>; <ref>nakatani et al , 1995</ref>; <ref>stifleman, 1995</ref>.
<ref>moser and moore 1995</ref> had an expert coder assign segments and various segment features and relations based on rst.
to quantify their findings, these studies use notions of agreement <tref>gale et al , 1992</tref>; <ref>moset and moore, 1995</ref> and/or reliability <ref>passonneau and litman, 1993</ref>; passonneau and litman, to appear; <ref>isard and carletta, 1995</ref>.
by using 10-fold cross validation <ref>kohavi and john, 1995</ref> to automatically pick the best number of nearest neighbors to use, the performance of lsxas has improved.
4 word sense disambiguation in the large in <tref>gale et al , 1992</tref>, it was argued that any wide coverage wsd program must be able to perform significantly better than the most-frequent-sense classifier to be worthy of serious consideration.
the performance of lexas as indicated in table 1 is significantly better than the most-frequent-sense classifier for the set of 191 words collected in our corpus.
figure 1 and 2 also confirm that all the training examples collected in our corpus are effectively utilized by lexas to improve its wsd performance.
word sense disambiguation has long been one of the major concerns in natural language processing area eg , <ref>bruce et al , 1994</ref>; <ref>choueka et al , 1985</ref>; <ref>gale et al , 1993</ref>; <ref>mcroy, 1992</ref>; <ref>yarowsky 1992, 1994, 1995</ref>, whose aim is to identify the correct sense of a word in a particular context, among all of its senses defined in a dictionary or a thesaurus.
undoubtedly, effective disambiguation techniques are of great use in many natural language processing tasks, eg, machine translation and information retrieving <ref>allen, 1995</ref>; <ref>ng and lee, 1996</ref>; <ref>resnik, 1995</ref>, etc previous strategies for word sense disambiguation mainly fall into two categories: statistics-based method and exemplar-based method.
statistics-based method often requires large-scale corpora eg , <ref>hirst, 1987</ref>; <ref>luk, 1995</ref>, sense-tagging or not, monolingual or aligned bilingual, as training data to specify significant clues for each word sense.
23 heuristic 3: sense ordering <tref>gale et al , 1992</tref> reports that word sense disambiguation would be at least 75 correct if a system assigns the most frequently occurring sense.
<ref>miller et al , 1994</ref> found that automatic i we use english wordnet version 16 l 143 assignment of polysemous words in brown corpus to senses in wordnet was 58 correct with a heuristic of most frequently occurring sense.
we adopt these previous results to develop sense ordering heuristic.
in the area of word sense disambiguation, <ref>black 1988</ref> developed a model based on decision trees using a corpus of 22 million tokens, after manually sense-tagging approximately 2,000 concordance lines for five test words.
since then, supervised learning from sense-tagged corpora has since been used by several researchers: zernik 1990, 1991, <ref>hearst 1991</ref>, <ref>leacock, towell, and voorhees 1993</ref>, gale, church, and yarowsky 1992d, 1993, <ref>bruce and wiebe 1994</ref>, miller et al.
however, despite the availability of increasingly large corpora, two major obstacles impede the acquisition of lexical knowledge from corpora: the difficulties of manually sense-tagging a training corpus, and data sparseness.
<ref>sutcliffe and slater 1995</ref> replicated this method on full text samples from orwells animal farm and found similar results 72 correct sense assignment, compared with a 33 chance baseline, and 40 using lesks method.
several authors for example, krovetz and croft 1989, guthrie et al 1991, slator 1992, cowie, guthrie, and guthrie 1992, janssen 1992, braden-harder 1993, liddy and paik 1993 have attempted to improve results by using supplementary fields of information in the electronic version of the longman dictionary of contemporary english ldoce, in particular, the box codes and subject codes provided for each sense.
<ref>atkins 1987</ref> and kilgarriff forthcoming also implicitly adopt the view of <ref>harris 1954</ref>, according to which each sense distinction is reflected in a distinct context.
in this volume, schiitze continues in this vein and proposes a technique that avoids the problem of sense distinction altogether: he creates sense clusters from a corpus rather than relying on a pre-established sense list.
furthermore, our percent agreement figures are comparable with the results of other segmentation studies discussed above.
while studies of other tasks have achieved stronger results eg , 968 in a word-sense disambiguation study <tref>gale et al , 1992</tref>, the meaning of percent agreement in isolation is unclear.
for example, a percent agreement figure of less than 90 could still be very meaningful if the probability of obtaining such a figure is low.
in the next section we demonstrate the significance of our findings.
agreement among subjects we measure the ability of subjects to agree with one another, using a figure called percent agreement.
percent agreement, defined in <tref>gale et al , 1992</tref>, is the ratio of observed agreements with the majority opinion to possible agreements with the majority opinion.
here, agreement among four, five, six, or seven subjects on whether or not there is a segment boundary between two adjacent prosodic phrases constitutes a majority opinion.
reliability the correspondence between discourse segments and more abstract units of meaning is poorly understood see <ref>moore and pollack, 1992</ref>.
we present initial results of an investigation of whether naive subjects can reliably segment discourse using speaker intention as a criterion.
the default strategy of picking the most frequent sense has been advocgted as the baseline performance for evaluating wsd programs <tref>gale et al , 1992b</tref>; <ref>miller et al , 1994</ref>.
since wordnet orders its senses such that sense 1 is the most frequent sense, one possibility is to always pick sense 1 as the best sense assignment.
this is in spite of the conditional independence assumption made by the naive-bayes algorithm, which may be unjustified in the domains tested.
gale, church and yarowsky <tref>gale et al , 1992a</tref>; <ref>gale et al , 1995</ref>; <ref>yarowsky, 1992</ref> have also successfully used the naive-bayes algorithm and several extensions and variations for word sense disambiguation.
on the other hand, our past work on wsd <ref>ng and lee, 1996</ref> used an exemplar-based or nearest neighbor learning approach.
much recent research on word sense disambiguation wsd has adopted a corpus-based, learning approach.
many different learning approaches have been used, including neural networks <ref>leacock et al , 1993</ref></ref>, probabilistic algorithms <ref>bruce and wiebe, 1994</ref>; <tref>gale et al , 1992a</tref>; <ref>gale et al , 1995</ref>; <ref>leacock et al , 1993</ref></ref>; <ref>yarowsky, 1992</ref>, decision lists <ref>yarowsky, 1994</ref>, exemplar-based learning algorithms <ref>cardie, 1993</ref>; <ref>ng and lee, 1996</ref>, etc in particular, <ref>mooney 1996</ref> evaluated seven state-of-the-art machine learning algorithms on a common data set for disambiguating six senses of the word line.
his results indicate that the simple naive-bayes algorithm gives the highest accuracy on the line corpus tested.
table 5: some chinese unknown word translation output score 0008421 0007895 0007669 0007588 0007283 0006812 0006430 0006218 0005921 0005527 0005335 0005335 0005221 0004731 0004470 0004275 0003878 0003859 0003859 0003784 0003686 0003550 0003519 0003481 0003407 0003407 0oo3338 0003324 0003250 0003206 0003202 0003040 0003033 0002888 0002886 english chinese teng-hui  teng-hui sar , sar flu n m, lei  lei poultry j poultry sar  chee-hwa hijack  teng-hui poultry  sar ,ng  chee-hw diaoyu  teng-hui primeminister  teng-hui president  teng-hui china  lava lien  teng-hui poultry  chee-hwa china  teng-hui flu  lei privaeminister -i chee-hwa president 1; chee-hwa poultry  leung kalkanov i zhuhai poultry i lei sar 1 j l yeltsin zhuhai -1 l chee-hwa primeminister  lain president  lava flu  poultry apologise  teng-hui dee  teng-hui tang j tang islng  leung leung : leung china tn sar zhuhai  lunar ttulg  tung 1994 for sense disambiguation between multiple usages of the same word.
in the years since the appearance of the first papers on using statistical models for bilingual lexicon compilation and machine translation<ref>brown et al, 1993</ref>; <ref>brown et al, 1991</ref>; <ref>gale and <ref>church, 1993</ref></ref>; <ref>church, 1993</ref>; <ref>simard et al, 1992</ref>, large amount of human effort and time has been invested in collecting parallel corpora of translated texts.
our goal is to alleviate this effort and enlarge the scope of corpus resources by looking into monolingual, comparable texts.
the paradise model posits that performance can be correlated with a meaningful external criterion such as usability, and thus that the overall goal of a spoken dialogue agent is to maximize an objective related to usability.
user satisfaction ratings <ref>kamm, 1995</ref>; <ref>shriberg, wade, and price, 1992</ref>; <ref>polifroni et al , 1992</ref> have been frequently used in the literature as an external indicator of the usability of a dialogue agent.
the model further posits that two types of factors are potential relevant contributors to user satisfaction namely task success and dialogue costs, and that two types of factors are potential relevant contributors to costs <ref>walker, 1996</ref>.
in addition to the use of decision theory to create this objective structure, other novel aspects of paradise include the use of the kappa coefficient <ref>carletta, 1996</ref>; <ref>siegel and castellan, 1988</ref> to operationalize task success, and the use of linear regression to quantify the relative contribution of the success and cost factors to user satisfaction.
this approach has many widely acknowledged limitations <ref>hirschman and pao, 1993</ref>; <ref>danieli et al , 1992</ref>; <ref>bates and ayuso, 1993</ref>, eg, although there may be many potential dialogue strategies for carrying out a task, the key is tied to one particular dialogue strategy.
in contrast, agents using different dialogue strategies can be compared with measures such as inappropriate utterance ratio, turn correction ratio, concept accuracy, implicit recovery and transaction success danieli lwe use the term agent to emphasize the fact that we are evaluating a speaking entity that may have a personality.
readers who wish to may substitute the word system wherever agent is used.
the problems arise because most sense distinctions are not as clear as the distinction between river bank and money bnk, so it is not always straightforward for a person to say what the correct answer is thus we do not always know what it would mean to say that a computer program got the right answer.
if people can only agree on the correct answer x of the time, a claim that a program achieves more than x accuracy is hard to interpret, and x is the upper bound for what the program can meaningfully achieve.
although this methodology could be valid for certain nlp problems, such as english part-of-speech tagging, we think that there exists reasonable evidence to say that, in wsd, accuracy results cannot be simply extrapolated to other domains contrary to the opinion of other authors <ref>ng, 1997b</ref>: on the asupervised approaches, also known as data-driven or corpus-dmven, are those that learn from a previously semantically annotated corpus.
oi1 the other hand, it does not seem reasonable to think that the training material is large and representative enough to cover all potential types of examples.
to date, a thorough study of the domain dependence of wsd --in the style of other studies devoted to parsing <ref>sekine, 1997</ref>-has not been carried out.
<ref>ng, 1997b</ref> estimates that the manual annotation effort necessary to build a broad coverage semantically annotated english corpus is about 16 personyears.
word sense disambiguation is a potentially crucial task in many nlp applications, such as machine translation brown, della pietra, and <ref>della pietra 1991</ref>, parsing <ref>lytinen 1986</ref>; <ref>nagao 1994</ref> and text retrieval <ref>krovets and croft 1992</ref>; <ref>voorhees 1993</ref>.
various corpus-based approaches to word sense disambiguation have been proposed <ref>bruce and wiebe 1994</ref>; <ref>charniak 1993</ref>; <ref>dagan and itai 1994</ref>; <ref>fujii et al 1996</ref>; <ref>hearst 1991</ref>; <ref>karov and edelman 1996</ref>; <ref>kurohashi and nagao 1994</ref>; <ref>li, szpakowicz, and matwin 1995</ref>; <ref>ng and lee 1996</ref>; <ref>niwa and nitta 1994</ref>; schitze 1992; <ref>uramoto 1994b</ref>; <ref>yarowsky 1995</ref>.
our verb sense disambiguation system is based on such an approach, that is, an example-based approach.
alignment at other levels of resolution is obviously useful.
other logical approaches involve aligning parse trees of a sentence and its translation <ref>matsumoto, ishimoto, and utsuro 1993</ref>; <ref>meyers, yangarber, and grishman 1996</ref>, or simultaneously generating parse trees and alignment arrangements <ref>wu 1995</ref>.
that is, an assumption of full statistical dependence <tref>yarowsky, 1994</tref>, rather than the more common full independence, is made3 when llf events el, e2,, e, are fully independent, then the joint probability pe1 a a en is the product of peipen, but if they are maximally dependent, it is the minimum of these values.
of course, neither assumption is any more than an approximation to the truth; but assuming dependence has the advantage that the estimate of the joint probability depends much less strongly on n, and so estimates for alternative joint events can be directly compared, without any possibly tricky normalization, even if they are composed of different numbers of atomic events.
this property is desirable: different sub-paths through a chart may span different numbers of edges, and one can imagine evaluation criteria which are only defined for some kinds of edge, or which often duplicate information supplied by other criteria.
we improve the original decision list by using written words in the default evidence.
the improved decision list can raise the f-measure of error detection.
we also use the special evidence default, frqwl, default is defined as the frequency of wl.
step5 pick the highest strength estwh,ej among 5as in this paper, the addition of a small value is an easy and effective way to avoid the unsatisfactory case, as shown in <tref>yarowsky, 1994</tref>.
<tref>yarowsky, 1994</tref>, and using the mxpost1 part of speech tagger and wordnets lemmatization, the following feature set was used: bag of word lemmas for the context words in the preceding, current and following sentence; unigrams of lemmas and parts of speech in a window of /three words, where each position provides a distinct feature; and bigrams of lemmas in the same window.
to obtain a multi-class classifier we used a standard one-vs-all approach of training a binary svm for each possible sense and then selecting the highest scoring sense for a test example.
it has been amply demonstrated that a wide assortment of machine learning algorithms are quite effective at extracting linguistic information from manually annotated corpora.
among the machine learning algorithms studied, rule based systems have proven effective on many natural language processing tasks, including part-of-speech tagging <ref>brill, 1995</ref>; <ref>ramshaw and marcus, 1994</ref>, spelling correction <ref>mangu and brill, 1997</ref>, word-sense disambiguation <ref>gale et al , 1992</ref>, message understanding <ref>day et al , 1997</ref>, discourse tagging <ref>samuel et al , 1998</ref>, accent restoration <tref>yarowsky, 1994</tref>, prepositional-phrase attachment <ref>brill and resnik, 1994</ref> and base noun phrase identification ramshaw and marcus, in press; <ref>cardie and pierce, 1998</ref>; <ref>veenstra, 1998</ref>; <ref>argamon et al , 1998</ref>.
many of these rule based systems learn a short list of simple rules typically on the order of 50-300 which are easily understood by humans.
since these rule-based systems achieve good performance while learning a small list of simple rules, it raises the question of whether peoand woman.
in contrast, lexas uses supervised learning from tagged sentences, which is also the approach taken by most recent work on wsd, including <ref>bruce and wiebe, 1994</ref>; <ref>miller et al , 1994</ref>; <ref>leacock et al , 1993</ref>; <tref>yarowsky, 1994</tref>; <ref>yarowsky, 1993</ref>; <ref>yarowsky, 1992</ref>.
in contrast, lexas uses supervised learning from tagged sentences, which is also the approach taken by most recent work on wsd, including <ref>bruce and wiebe, 1994</ref>; <ref>miller et al , 1994</ref>; <ref>leacock et al , 1993</ref>; <tref>yarowsky, 1994</tref>; <ref>yarowsky, 1993</ref>; <ref>yarowsky, 1992</ref>.
that local collocation knowledge provides important clues to wsd is pointed out in <ref>yarowsky, 1993</ref>, although it was demonstrated only on performing binary or very coarse sense disambiguation.
however, his work used decision list to perform classification, in which only the single best disambiguating evidence that matched a target context is used.
in contrast, we used exemplar-based learning, where the contributions of all features are summed up and taken into account in coming up with a classification.
1995, we formalize the problem of deciding dependency preference of subordinate clauses by utilizing the correlation of scope embedding preference and dependency preference of japanese subordinate clauses.
then, as a statistical learning method, we employ the decision list learning method of <tref>yarowsky 1994</tref>, where optimal combination of those features are selected and sorted in the form of decision rules, according to the strength of correlation between those features and the dependency preference of the two subordinate clauses.
we evaluate the proposed method through the experiment on learning dependency preference of japanese subordinate clauses from the edr bracketed corpus section 4.
roughly speaking, the first corresponds to the case where clause2 inherently has a scope of the same or a broader breadth compared with that of clause1, while the second corresponds to the case where clause2 inherently has a narrower scope compared with that of clause17 32 decision list learning a decision list <tref>yarowsky, 1994</tref> is a sorted list of the decision rules each of which decides the value of a decision d given some evidence e each decision rule in a decision list is sorted tour modeling is slightly different from those of other standard approaches to statistical dependency analysis <ref>collins, 1996</ref>; <ref>fujio and matsumoto, 1998</ref>; <ref>haruno et al , 1998</ref> which simply distinguish the two cases: the case where dependency relation holds between the given two vp chunks or clauses, and the case where dependency relation does not hold.
this is because we assume that this case is more loosely related to the scope embedding preference of subordinate clauses.
we formalize the problem of deciding scope embedding preference as a classification problem, in which various types of linguistic information of each subordinate clause are encoded as features and used for deciding which one of given two subordinate clauses has a broader scope than the other.
as a statistical learning method, we employ the decision list learning method of <tref>yarowsky 1994</tref>.
113 table 2: features of japanese subordinate clauses feature type  of feat  each binary feature punctuation 2 with-comma, without-comma grammatical adverb, adverbial-noun, formal-noun, temporal-noun, some features have distinction 17 quoting-particle, copula, predicate-conjunctive-particle, of chunk-final/middle topic-marking-particle, sentence-final-particle 12 conjugation form of chunk-final conjugative word lexical lexicalized forms of grammatical features, with more than 9 occurrences in edr corpus 235 stem, base, mizen, renyou, rental, conditional, imperative, ta, tari, re, conjecture, volitional adverb eg , ippou-de, irai, adverbial-noun eg , tame, baai topic-marking-particle eg , ha, mo, quoting-particle to, predicate-conjunctive-particle eg , ga, kara, temporal-noun eg , ima, shunkan, formal-noun eg , koto, copula dearu, sentence-final-particle eg , ka, yo 31 the task definition considering the dependency preference of japanese subordinate clauses described in section 24, the following gives the definition of our task of deciding the dependency of japanese subordinate clauses.
for each piece of evidence, calculate the likelihood ratio of the conditional probability of a decision d  xl given the presence of that piece of evidence to the conditional probability of the rest of the decisions d -,xl: pdxl i ei lg2 pdxl ei then, a decision list is constructed with pieces of evidence sorted in descending order with respect to their likelihood ratios, s 2.
the provider is ixa and they also applied smoothing to generate more robust decision lists.
while the basic idea of our model is similar to trigger models, they handle co-occurrences of word pairs independently and do not use a representation of the whole context.
this omission is also done in applications such as word sense dismnbiguation yarowsky: 1994; fung et al , 1999.
the decision-list algorithm used here <tref>yarowsky, 1994</tref> identifies other collocations that reliably partition the seed training data, ranked by the purity of the distribution.
9 initial decision list for plant abbreviated logl 810 758 739 720 627 470 439 430 410 352 348 345 collocation sense plant life  a manufacturing plant  b life within 4-2-10 words  a manufacturing in 4-2-10 words  b animal within -i-2-10 words  a equipment within -1-2-10 words , b employee within 4-2-10 words  b assembly plant  b plant closure  b plant species  a automate within 4-2-10 words :: b microscopic plant  a 9note that a given collocate such as life may appear multiple times in the list in different collocations1 relationships, including left-adjacent, right-adjacent, cooccurrence at other positions in a k-word window and various other syntactic associations.
our probabilistic decision lists can thus be thought of as a competitive way to probabilize tbls, with the advantage of preserving the list-structure and simplicity of tbl, and the possible disadvantage of losing the dependency on the current state.
his technique involves estimating both a probability based on the global probability distribution for a question, and a local probability, given that no questions higher in the list were true, and then interpolating between the two probabilities.
lemmatization allows for more compact and generalizable data by clustering all inflected forms of an ambiguous word together, an effect already commented on by <tref>yarowsky 1994</tref>.
the more inflection in a language, the more lemmatization will help to compress and generalize the data.
we make use of the advantage of clustering all instances of eg one verb in a single classifier instead of several classifiers one for each inflected form found in the data.
4 adaptation of decision lists to n-way ambiguities decision lists as defined in <ref>yarowsky, 1993</ref>; 1994 are simple means to solve ambiguity problems.
they have been successfully applied to accent restoration, word sense disambiguation 209 and homograph disambiguation <tref>yarowsky, 1994</tref>; 1995; 1996.
in order to build decision lists the training examples are processed to extract the features each feature corresponds to a kind of collocation, which are weighted with a log-likelihood measure.
4 adaptation of decision lists to n-way ambiguities decision lists as defined in <ref>yarowsky, 1993</ref>; 1994 are simple means to solve ambiguity problems.
they have been successfully applied to accent restoration, word sense disambiguation 209 and homograph disambiguation <tref>yarowsky, 1994</tref>; 1995; 1996.
in order to build decision lists the training examples are processed to extract the features each feature corresponds to a kind of collocation, which are weighted with a log-likelihood measure.
the list of all features ordered by log-likelihood values constitutes the decision list.
the context within which the ambiguous word occurs is typically represented by a set of linguistically motivated features from which a learning algorithm induces a representative model that performs the disambiguation.
a variety of classifiers have been employed for this task see mooney 1996 and ide and veronis 1998 for overviews, the most popular being decision lists <ref>yarowsky 1994, 1995</ref> and naive bayesian classifiers <ref>pedersen 2000</ref>; <ref>ng 1997</ref>; <ref>pedersen and bruce 1998</ref>; <ref>mooney 1996</ref>; <ref>cucerzan and yarowsky 2002</ref>.
we employed a naive bayesian classifier <ref>duda and hart 1973</ref> for our experiments, as it is a very convenient framework for incorporating prior knowledge and studying its influence on the classification task.
in section 51 we describe a basic naive bayesian classifier and show how it can be extended with informative priors.
despite their simplicity, decision lists dlist for short as defined in <tref>yarowsky 1994</tref> have been shown to be very effective for wsd <ref>kilgarriff  palmer, 2000</ref>.
machine learning methods have become the most popular technique in a variety of classification problems of these sort, and have shown significant success.
based methods <ref>zavrel et al , 1997</ref>, linear classifiers <ref>roth, 1998</ref>; <ref>roth, 1999</ref> and transformationbased learning <ref>brill, 1995</ref>.
in many of these classification problems a significant source of difficulty is the fact that the number of candidates is very large  all words in words selection problems, all possible tags in tagging problems etc since general purpose learning algorithms do not handle these multi-class classification problems well see below, most of the studies do not address the whole problem; rather, a small set of candidates typically two is first selected, and the classifier is trained to choose among these.
obviously, how to measure the confidence of features is a very important issue for the decision list.
provided that 1  0ps f > for all i :  max    i i confidence f p s f 1 this value measures the extent to which the context is unambiguously correlated with one particular slot i s  24 slot-value merging and semantic reclassification the slot-value merger is to combine the slots assigned to the concepts in an input sentence.
in particular, they may involve performing speech recognition on speech data, parsing on text data, application of hand-coded rules to the results of parsing, or some combination of these.
note that this is in sharp contrast with the naive bayes classifier <ref>duda et al , 2000</ref>, which assumes complete independence.
of course, neither assumption can be true in practice; however, as argued in <ref>carter, 2000</ref>, there are good reasons for preferring the dependence alternative as the better option in a situation where there are many features extracted in ways that are likely to overlap.
there is typically no corpus data available at the start of a project, but considerable amounts at the end: the intention behind alterf is to allow us to shift smoothly from an initial version of the system which is entirely rule-based, to a final version which is largely data-driven.
for example, in the procedure assistant domain we represent the utterances please speak up show me the sample syringe set an alarm for five minutes from now no i said go to the next step respectively as fincrease volumeg fshow, sample syringeg fset alarm, 5, minutesg fcorrection, next stepg where increase volume, show, sample syringe, set alarm, 5, minutes, correction and next step are semantic atoms.
left  context   ml2mii ll,ight named entity  context  m   mm<3  1 2 current position 4 supervised learning for japanese named entity recognition this section describes how to apply tile decision list learning method to chunking/tagging named entities.
41 decision list learning a decision list <ref>rivest, 1987</ref>; <tref>yarowsky, 1994</tref> is a sorted list of decision rules, each of which decides the wflue of a decision d given some evidence e each decision rule in a decision list is sorted in descending order with respect to some preference value, and rules with higher preference values are applied first when applying the decision list to some new test; data.
first, the random variable d representing a decision w, ries over several possible values, and the random wriable e representing some evidence varies over 1 and 0 where 1 denotes the presence of the corresponding piece of evidence, 0 its absence.
then, given some training data in which the correct value of the decision d is annotated to each instance, the conditional probabilities pd  x i e  1 of observing the decision d  x under the condition of the presence of the evidence e e  1 are calculated and the decision list is constructed by the tbllowing procedure.
in general, creating training data tbr supervised learning is somewhat easier than creating pattern matching rules by hand.
next, we apply yarowskys method tbr supervised decision list learning i <tref>yarowsky, 1994</tref> to 1vve choose tile decision list learning method as the 705 table 1: statistics of ne types of irex ne type organization person location artifact date time money percent total frequency  training 3676 197 3840 206 5463 292 747 40 3567 191 502 27 390 21 492 26 18677 test 361 239 338 224 413 274 48 32 260 172 54 35 15 10 21 14 1510 japanese named entity recognition, into which we incorporate several noun phrase chunking techniques sections 3 and 4 and experimentally evaluate their performance on the irex, workshops training and test data section 5.
as one of those noun phrase chunking techniques, we propose a method for incorporating richer contextual information as well as patterns of constituent morphemes within a named entity, compared with those considered in tire previous research <ref>sekine et al , 1998</ref>; <ref>borthwick, 1999</ref>, and show that the proposed method outperlbrms these approaches.
2 japanese named entity recognition 21 task of the irex workshop the task of named entity recognition of the irex workshop is to recognize eight named entity types in table 1 irex <ref>conmfittee, 1999</ref>.
this small number has almost no effect on more frequent words, but boosts the score of less common, yet potentially equally informative, words.
222 decision list the decision list classifier uses the log-likelihood of correspondence between each context feature and each sense, using additive smoothing <tref>yarowsky, 1994</tref>.
instances that did not match any rule in the decision list were assigned the most frequent sense, as calculated from the training data.
other research, such as yarowskys into accent restoration in <ref>spanish and french 1994</ref>, which reports accuracy levels of 9099, is again at a more rough-grained level, in this case that of distinguished unaccented and accented word forms.
while the sense tagging results are fairly encouraging, the part of speech tagging results arc at present relatively poor.
it thus secrns sensible, especially noting wilks and stevensons analysis mentioned above, to first run a sentence through a traditional part of speech tagger before trying to disambiguate the senses.
from this perspective, either accent identification can be extended to truecasing or truecasing can be extended to incorporate accent restoration.
<tref>yarowsky, 1994</tref> reports good results with statistical methods for spanish and french accent restoration.
there is a vast literature on spelling correction <ref>jones and martin, 1997</ref>; <ref>golding and roth, 1996</ref> using both linguistic and statistical approaches.
f         any subset of braceleftbig mlength, netag,pos bracerightbig braceleftbig class sys no outputs bracerightbig in the training and testing phases, within each segment segev j of event expression, a class is assigned to each system, where each class class i sys for the i-th system is represented as a list of the classes of the named entities output by the system: class i sys  braceleftbigg /,  , / no output i 1,,n 34 learning algorithm we apply a simple decision list learning method to the task of learning a classifier for combining outputs of named entity chunkers 4  a decision list <tref>yarowsky, 1994</tref> is a sorted list of decision rules, each of which decides the value of class given some features f of an event.
each decision rule in a decision list is sorted in descending order with respect to some preference value, and rules with higher preference values are applied first when applying the decision list to some new test data.
32 nave bayes the second system used was a nave bayes classifier where the similarity between an instance, i, and a sense class, sj, is defined as: simi,sj  pi,sj  psjpisj we then choose the sense class, sj, which maximized the similarity function above, making standard independence assumptions.
the ordering of rules employed in a decision list in order to simplify the representation and perform conflict resolution apparently gives it an advantage over other symbolic methods on this task.
with respect to training time, the symbolic methods are significantly slower since they are searching for a simple declarative representation of the concept.
a number of effective concept-learning systems have employed decision lists clark 84 <ref>niblett, 1989</ref>; <ref>quinlan, 1993</ref>; <ref>mooney  califf, 1995</ref> and they have already been successfully applied to lexical disambiguation <tref>yarowsky, 1994</tref>.
all of the logic-based methods are variations of the foil algorithm for induction of first-order function-free horn clauses <ref>quinlan, 1990</ref>, appropriately simplified for the propositional case.
suppose, for example, that pattern a noun: normal noun; particle: case-particle: none: wo; verb: normal form: 217; symhol: punctuatioif occurs 13 times in a learlfing set and that tell of the occurrences include the inserted partition inal:k suppose also thai; pattern b noun; particle; verb; symbol occurs 12a times in a learning set and that 90 of the occurrences include the mark.
over the past decade, there has been tremendous progress on learning parsing models from treebank data <tref>magerman, 1995</tref>; <ref>collins, 1999</ref>; <ref>charniak, 1997</ref>; <ref>ratnaparkhi, 1999</ref>; <ref>charniak, 2000</ref>; <ref>wang et al , 2005</ref>; <ref>mcdonald et al , 2005</ref>.
learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation tricks to cope with the sparse data problems <ref>collins, 1997</ref>; <ref>bikel, 2004</ref>.
over the past decade, there has been tremendous progress on learning parsing models from treebank data <tref>magerman, 1995</tref>; <ref>collins, 1999</ref>; <ref>charniak, 1997</ref>; <ref>ratnaparkhi, 1999</ref>; <ref>charniak, 2000</ref>; <ref>wang et al , 2005</ref>; <ref>mcdonald et al , 2005</ref>.
learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation tricks to cope with the sparse data problems <ref>collins, 1997</ref>; <ref>bikel, 2004</ref>.
a recent trend in natural language processing has been toward a greater emphasis on statistical approaches, beginning with the success of statistical part-of-speech tagging programs <ref>church 1988</ref>, and continuing with other work using statistical part-of-speech tagging programs, such as bbn plum <ref>weischedel et al 1993</ref> and nyu proteus <ref>grishman and sterling 1993</ref>.
nevertheless, most natural language systems remain primarily rule based, and even systems that do use statistical techniques, such as att chronus <ref>levin and pieraccini 1995</ref>, continue to require a significant rule based component.
development of a complete end-to-end statistical understanding system has been the focus of several ongoing research efforts, including <ref>miller et al 1995</ref> and <ref>koppelman et al 1995</ref>.
in empirical approaches to parsing, lexical/semantic collocation extracted from corpus has been proved to be quite useful for ranking parses in syntactic analysis.
a broader range of information, in particular lexical information, was found to be essential in disambiguating the syntactic structures of real-world sentences.
spatter <tref>magerman, 1995</tref> augmented the pure pcfg by introducing a number of lexical attributes.
the spatter parser attained 87 accuracy and first made stochastic parsers a practical choice.
after training, head tagging is performed according to equation 1, where 15 is the estimated probability and hi is a characteristic function which is true iff word i is a head word.
n h  argmaxh hwilhihilhi-1hi-2 i1 1 the second pass then takes the words with this head information and supertags them according to equation 2, where thio is the supertag of the epart of speech tagging models have not used heads in this manner to achieve variable length contexts.
there have been two main robust parsing paradigms: finite state grammar-based approaches such as <ref>abney 1990</ref>, <ref>grishman 1995</ref>, and hobbs et al.
<ref>srinivas 1997a</ref> has presented a different approach called supertagging that integrates linguistically motivated lexical descriptions with the robustness of statistical techniques.
the idea underlying the approach is that the computation of linguistic structure can be localized if lexical items are associated with rich descriptions supertags that impose complex constraints in a local context.
second, one might propagate lexical information upward through the productions.
a more linguistically motivated approach is to expand the domain of productions downward to incorporate more tree structures.
standard symbolic machine learning techniques have been successfully applied to a number of tasks in natural language processing nlp.
examples include the use of decision trees for syntactic analysis <tref>magerman, 1995</tref>, coreference <ref>aone and bennett, 1995</ref>; <ref>mccarthy and lehnert, 1995</ref>, and cue phrase identification <ref>litman, 1994</ref>; the use of inductive logic programming for learning semantic grammars and building prolog parsers 113 <ref>zelle and mooney, 1994</ref>; <ref>zelle and mooney, 1993</ref>; the use of conceptual clustering algorithms for relative pronoun resolution <ref>cardie, 1992a</ref>; <ref>cardie 1992b</ref>, and the use of case-based learning techniques for lexical tagging tasks <ref>cardie, 1993a</ref>; daelemans et al , submitted.
in theory, both statistical and machine learning techniques can significantly reduce the knowledge-engineering effort for building large-scale nlp systems: they offer an automatic means for acquiring robust heuristics for a host of lexical and structural disambiguation tasks.
it is well-known in the machine learning community, however, that the success of a learning algorithm depends critically on the representation used to describe the training and test instances <ref>almuallim and dietterich, 1991</ref>, langley and sage, in press.
a strength of these models is undoubtedly the powerful estimation techniques that they use: maximum-entropy modeling in <ref>ratnaparkhi 1997</ref> or decision trees in <ref>jelinek et al 1994</ref> and <tref>magerman 1995</tref>.
a weakness, we will argue in this section, is the method of associating parameters with transitions taken by bottom-up, shift-reduce-style parsers.
we give examples in which this method leads to the parameters unnecessarily fragmenting the training data in some cases or ignoring important context in other cases.
593 collins head-driven statistical models for nl parsing internal rules lexical rules top  s jj  last s  np np vp nn  week np  jj nn nnp  ibm np  nnp vbd  bought vp  vbd np nnp  lotus np  nnp figure 1 a nonlexicalized parse tree and a list of the rules it contains.
the precision and recall of the traces found by model 3 were 938 and 901, respectively out of 437 cases in section 23 of the treebank, where three criteria must be met for a trace to be correct: 1 it must be an argument to the correct headword; 2 it must be in the correct position in relation to that headword preceding or following; 15 <tref>magerman 1995</tref> collapses advp and prt into the same label; for comparison, we also removed this distinction when calculating scores.
22 statistical parsers pioneered by the ibm natural language group <ref>fujisaki et al 1989</ref> and later pursued by, for example, <ref>schabes, roth, and osborne 1993</ref>, jelinek et al.
the rules to assign a structure to an input are extracted automatically from hand-annotated parses of large corpora, which are then subjected to smoothing to obtain reasonable coverage of the language.
during the last few years large treebanks have become available to many researchers, which has resulted in researches applying a range of new techniques for parsing systems.
most of the methods that are being suggested include some kind of machine learning, such as history based grammars and decision tree models <ref>black et al , 1993</ref>; <tref>magerman, 1995</tref>, training or inducing statistical grammars <ref>black, garside and leech, 1993</ref>; <ref>pereira and schabes, 1992</ref>; <ref>schabes et al , 1993</ref>, or other techniques <ref>bod, 1993</ref>.
consequently, syntactical analysis has become an area with a wide variety of a algorithms and methods for learning and parsing, and b type of information used for learning and parsing sometimes referred to as feature set.
these methods only could become popular through evaluation methods for parsing systems, such as bracket accuracy, bracket recall, sentence accuracy and viterbi score.
although we report promising results, parse selection that is sufficiently accurate for many practical applications will require a more lexicalised system.
however, the massively increased coverage obtained here by relaxing subcategorisation constraints underlines the need to acquire accurate and complete subcategorisation frames in a corpus-driven fashion, before such constraints can be exploited robustly and effectively with free text.
in our experiments, the window starts at the sentence prior to that containing the token and extends back w the window size sentences.
the choice to use sentences as the unit of distance is motivated by our intention to incorporate triggers of this form into a probabilistie treebank-based parser and tagger, such as <ref>black et al , 1998</ref>; <ref>black et al , 1997</ref>; <ref>brill, 1994</ref>; <ref>collins, 1996</ref>; <ref>jelinek et al , 1994</ref>; <tref>magerman, 1995</tref>; <ref>ratnaparkhi, 1997</ref>.
all such parsers and taggers of which we are aware use only intrasentential information in predicting parses or tags, and we wish to remove this information, as far as possible, from our results 7 the window was not allowed to cross a document boundary.
this approach was congruent with the great success of word n-gram models in speech recognition, and drew strength from a broader interest in lexicalized grammars, as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such as pp attachments <ref>ford et al , 1982</ref>; <ref>hindle and rooth, 1993</ref>.
in the following decade, great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized pcfg models <tref>magerman, 1995</tref>; <ref>charniak, 1997</ref>; <ref>collins, 1999</ref>; <ref>charniak, 2000</ref>; <ref>charniak, 2001</ref>.
however, several results have brought into question how large a role lexicalization plays in such parsers.
<ref>johnson 1998</ref> showed that the performance of an unlexicalized pcfg over the penn treebank could be improved enormously simply by annotating each node by its parent category.
however, perhaps even more significant has been the lexicalization of the grammar formalisms being probabilistically modeled: crucially, all the recent, successful statistical parsers have in some way made use of bilexical dependencies.
155 even more crucially, the bilexical dependencies involve head-modifier relations hereafter referred to simply as head relations.
the intuition behind the lexicalization of a grammar formalism is to capture lexical items idiosyncratic parsing preferences.
in those research, extracted lexical/semantic collocation is especially useful in terms of ranking parses in syntactic analysis as well as automatic construction of lexicon for nlp.
for example, in the context of syntactic disambiguation, <ref>black 1993</ref> and <tref>magerman 1995</tref> proposed statistical parsing models based-on decisiontree learning techniques, which incorporated not only syntactic but also lexical/semantic information in the decision-trees.
for example, in the context of syntactic disambiguation, <ref>black 1993</ref> and <tref>magerman 1995</tref> proposed statistical parsing models based-on decisiontree learning techniques, which incorporated not only syntactic but also lexical/semantic information in the decision-trees.
other researchers have proposed automatically selecting the conditioning information for various states of the model, thus potentially increasing greatly the space of possible features and selectively choosing the best predictors for each situation.
another example of automatic feature selection for parsing is in the context of a deterministic parsing model that chooses parse actions based on automatically induced decision structures over a very rich feature set <ref>hermjakob and mooney, 1997</ref>.
we grew the trees fully and we calculated final expansion probabilities at the leaves by linear interpolation with estimates one level above.
the node direction features indicate whether a node is a left child, a right child, or a single child.
32 word alignment because most of the 700 languages in odin are low-density languages with no on-line bilingual dictionariesorlargeparallelcorpora, aligning the source sentence and its english translation directly would not work well.
the final step is to use the training examples to learn an effective search policy so that our run-time generation component can find good output sentences in a reasonable time frame.
in particular, we use variants of existing search optimization <ref>daum and marcu, 2005</ref> and ranking algorithms <ref>collins and koo, 2005</ref> to train our run-time component to find good outputs within a specified time window; see also <ref>stent et al, 2004</ref>; <ref>walker et al, 2001</ref>.
in addition, we deterministically add features to improve several grammatical aspects, including 1 enforcing verb inflectional agreement in derived trees, 2 enforcing consistency in the finiteness of vp and s complements, and 3 restricting subject/direct object/indirect object complements to play the same grammatical role in derived trees.
in the second stage, the complements and adjuncts in the decorated trees are incrementally re80 syntax: cat: sa fin:other,  cat: s cat: np,  apr: vbp, apn: other pos: prp we fin:yes,  cat: vp apn: other,  pos: vbp do pos: rb nt fin: yes,  cat: vp, gra: obj1 fin: yes,  cat: vp, gra: obj1 pos: vbp have cat: np,  gra: obj1 operations: initial tree comp semantics: speech-actaction  assert speech-actcontentpolarity  negative speech-actcontentattribute  resourceattribute syntax: cat: np,  apr: vbp, gra: obj1,  apn: other pos: jj medical pos: nns supplies cat: advp,  gra: adj pos: rb here cat: np,  apr: vbz, gra: adj,  apn: 3ps pos: nn captain operations: comp left/right adjunction left/right adjunction semantics: speech-actcontentvalue  medical-supplies speech-actcontentobject-id  market addressee  captain-kirk dialogue-actaddressee  captain-kirk speech-actaddressee  captain-kirk figure 2: the linguistic resources inferred from the training example in figure 1.
whats more, on top of the large undertaking of designing and implementing a statistical parsing model, the use of heuristics has required a further e ort, forcing the researcher to bring both linguistic intuition and, more often, engineering savvy to bear whenever moving to a new treebank.
the apparently haphazard placement of these rules that pick out fw and the rarity of fw nodes in the data strongly suggest these rules are the result of engineering e ort.
furthermore, it is not at all apparent that tree-transforming heuristics that are useful for one parsing model will be useful for another.
311 enriching igt in a previous study <ref>xia and lewis, 2007</ref>, we proposed a three-step process to enrich igt data: 1 parse the english translation with an english parser and convert english phrase structures ps into dependency structures ds with a head percolation table <tref>magerman, 1995</tref>, 2 align the target line and the english translation using the gloss line, and 3 project the syntactic structures both ps and ds from english onto the target line.
for instance, given the igt example in ex 1, the enrichment algorithm will produce the word alignment in figure 1 and the syntactic structures in figure 2.
the  teacher  gave  a  book  to    the    boy   yesterday rhoddodd  yr   athro     lyfr     ir     bachgen  ddoe  gloss line:  translation: target line: gave-3sg  the  teacher book  to-the  boy   yesterday figure 1: aligning the target line and the english translation with the help of the gloss line 533 gave a projecting ds athro bachgen lyfr yr ddoeir  rhoddodd s np1 vp nn teacher vbd   gave np2 dt a np4pp nn the in np3 yesterday nn dt book nn boy dt to s np nn vbd np nppp nn indt nn nndt   rhoddodd   gave yrthe    athro teacher lyfr book     ir to-the bachogen boy ddoe yesterday teacher a boy the book the yesterdayto the b projecting ps figure 2: projecting syntactic structure from english to the target language we evaluated the algorithm on a small set of 538 igt instances for several languages.
also, we treat a propbank argument arg0 : : : arg9 as a complement and a propbank adjunct argms as an adjunct when such annotation is available1 otherwise, we basically follow the approach of <ref>chen, 2001</ref>2 besides introducing one kind of tag extraction 1the version of the propbank we are using is not fully annotated with semantic role information, although the most common predicates are.
thus maxent has at least one advantage over each of the reviewed pos tagging techniques.
it is better able to use diverse information than markov models, requires less supporting techniques than sdt, and unlike tbl, can be used in a probabilistic framework.
since most realistic natural language applications must process words that were never seen before in training data, all experiments in this paper are conducted on test data that include unknown words.
the experiments in this paper test the hypothesis that better use of context will improve the accuracy.
a maximum entropy model is well-suited for such experiments since it cornbines diverse forms of contextual information in a principled manner, and does not impose any distributional assumptions on the training data.
in contrast, the maxent model combines diverse and non-local information sources without making any independence assumptions.
however, the aforementioned sdt techniques require word classes<ref>brown et al , 1992</ref> to help prevent data fragmentation, and a sophisticated smoothing algorithm to mitigate the effects of any fragmentation that occurs.
6 conclusion it is worth noting that while we have presented the use of edge-based best-first chart parsing in the service of a rather pure form of pcfg parsing, there is no particular reason to assume that the technique is so limited in its domain of applicability.
one can imagine the same techniques coupled with more informative probability distributions, such as lexicalized pcfgs <ref>charniak, 1997</ref>, or even grammars not based upon literal rules, but probability distributions that describe how rules are built up from smaller components <tref>magerman, 1995</tref>; <ref>collins, 1997</ref>.
clearly further research is warranted.
be this as it may, the take-home lesson from this paper is simple: combining an edge-based agenda with the figure of merit from cc  is easy to do by simply binarizing the grammar  provides a factor of 20 or so reduction in the number of edges required to find a first parse, and  improves parsing precision and recall over exhaustive parsing.
uk black,eubank,kashiokaatritlcojp gleechocentllancsacuk 1 introduction a treebank is a body of natural language text which has been grammatically annotated by hand, in terms of some previously-established scheme of grammatical analysis.
treebanks have been used within the field of natural language processing as a source of training data for statistical part og speech taggers <ref>black et al , 1992</ref>; <ref>brill, 1994</ref>; <ref>merialdo, 1994</ref>; <ref>weischedel et al , 1993</ref> and for statistical parsers <ref>black et al , 1993</ref>; <ref>brill, 1993</ref>; aelinek et al , 1994; <tref>magerman, 1995</tref>; <ref>magerman and marcus, 1991</ref>.
in this article, we present the atr/lancaster 7reebauk of american english, a new resource tbr natural-language-, processing research, which has been prepared by lancaster university uks unit for computer research on the english language, according to specifications provided by atr japans statistical parsing group.
moreover, the results of a less-than-optimal version of dop on the wall street journal corpus suggest that the approach can be succesfully extended to larger domains.
as future research, we will apply the full dop model on wsj word strings in order to compare our results with the best known parsers on this domain <tref>magerman, 1995</tref>; <ref>collins, 1996</ref>.
acknowledgements i am grateful to remko scha for many useful comments and additions.
i also thank three anonymous reviewers for their comments.
the latter approach has become increasingly popular eg <ref>schabes et al , 1993</ref>; <ref>weischedel et al , 1993</ref>; <ref>briscoe, 1994</ref>; <tref>magerman, 1995</tref>; <ref>collins, 1996</ref>.
we calculate the precision, recall, and fscore; however for brevitys sake we only report the f-score for most experiments in this section.
in addition to antecedent recovery, we also report parsing accuracy, using the bracketing f-score, the combined measure of parseval-style labeled bracketing precision and recall <tref>magerman, 1995</tref>.
figure 9 shows that the perfect scheme would achieve roughly 93 precision and recall, which is a dramatic increase over the top 1 accuracy of 87 precision and 86 recall.
figure 10 shows that the exact match, which counts the percentage of times 2results for spatter on section 23 are reported in <ref>collins, 1996</ref></ref> parser precision maximum entropy  868 maximum entropy 875 <ref>collins, 1996</ref></ref> 857 <tref>magerman, 1995</tref> 843 recall 856 863 853 840 table 5: results on 2416 sentences of section 23 0 to 100 words in length of the wsj treebank.
evaluations marked with  collapse the distinction between advp and prt, and ignore all punctuation.
the parseval black and others, 1991 measures compare a proposed parse p with the corresponding correct treebank parse t as follows:  correct constituents in p recall   constituents in t  correct constituents in p precision   constituents in p a constituent in p is correct if there exists a constituent in t of the same label that spans the same words.
table 5 shows results using the parseval measures, as well as results using the slightly more forgiving measures of <ref>collins, 1996</ref> and <tref>magerman, 1995</tref>.
table 5 shows that the maximum entropy parser performs better than the parsers presented in <ref>collins, 1996</ref> and <tref>magerman, 1995</tref> , which have the best previously published parsing accuracies on the wall st journal domain.
it is often advantageous to produce the top n parses instead of just the top 1, since additional information can be used in a secondary model that reorders the top n and hopefully improves the quality of the top ranked parse.
table 5 shows results using the parseval measures, as well as results using the slightly more forgiving measures of <ref>collins, 1996</ref> and <tref>magerman, 1995</tref>.
table 5 shows that the maximum entropy parser performs better than the parsers presented in <ref>collins, 1996</ref> and <tref>magerman, 1995</tref> , which have the best previously published parsing accuracies on the wall st journal domain.
it is often advantageous to produce the top n parses instead of just the top 1, since additional information can be used in a secondary model that reorders the top n and hopefully improves the quality of the top ranked parse.
suppose there exists a perfect reranking scheme that, for each sentence, magically picks the best parse from the top n parses produced by the maximum entropy parser, where the best parse has the highest average precision and recall when compared to the treebank parse.
the algorithm exploits robust lexical, syntactic, and semantic knowledge sources.
i introduction the application of decision-based learning techniques over rich sets of linguistic features has improved significantly the coverage and performance of syntactic and to various degrees semantic parsers <ref>simmons and yu, 1992</ref>; <tref>magerman, 1995</tref>; <ref>hermjakob and mooney, 1997</ref>.
in this paper, we apply a similar paradigm to developing a rhetorical parser that derives the discourse structure of unrestricted texts.
crucial to our approach is the reliance on a corpus of 90 texts which were manually annotated with discourse trees and the adoption of a shift-reduce parsing model that is well-suited for learning.
if an erroneous string is extracted, its errors will propagate through the rest of the input :trings.
:3 our approach 31 the c45 learning algorithm decision tree induction algorithms have been successfully applied for nlp problems such as sentence boundary dismnbiguation <ref>pahner et al 1997</ref>, parsing <tref>magerman 1995</tref> and word segmentation <ref>mekuavin et al 1997</ref>.
we employ the c45 <ref>quinhln 1993</ref> decision tree induction program as the learning algorithm for word extraction.
the induction algorithm proceeds by evaluating content of a series of attributes and iteratively building a tree fiom the attribute values with the leaves of the decision tree being the value of the goal attribute.
by implementing our own version of the publicly available collins parser <ref>collins, 1996</ref>, we also learned a dependency model that enables the mapping of parse trees into sets of binary relations between the head-word of each constituent and its sibling-words.
dogs iditarodcount pull sled in the structure above, count represents the expected answer type, replacing the question stem how many.
one is a full parser of english, using a statistically learned decision procedure; spatter has achieved the highest scores yet reported on parsing english text <tref>magerman, 1995</tref>.
because the measurable improvement in parsing is so great compared to manually constructed parsers, it appears to offer a qualitatively better parser.
we are looking ormat de scription message message reader i m orphologieal analyzer  lexieai pattern matcher  fast partial parser  semantic interpreter  entene e-level pattern matcher discourse  format  s gml handling initial iden tification of entities grouping words into meaningful phrases establish relationships within sentences establish relationships overall ----template/annotation generator i output entities and relationships  output figure 2: plum system architecture: rectangles represent domain-independent, language-independent algorithms; ovals represent knowledge bases.
for example, statistical techniques may have suggested the importance of hire, a verb which many groups did not happen to define.
second, since there has been a marked improvement in the quality of full parsers, now achieving an f in the high 80s <tref>magerman, 1995</tref>, we believe it is now feasible to consider using full parsers again.
the rationale is straightforward: for full templates eg , st scores have been mired with an f in the 50s ever since muc-3 in 1991.
pattern matching has given us very robust, very portable technology, but has not broken the performance barrier all systems have run up against.
however, unlike the models of <ref>black 1993</ref>, <tref>magerman 1995</tref>, and <ref>collins 1996</ref>, we put an assumption that syntactic and lexical/semantie features are independent.
then, we focus on extracting lexical/semantic collocational knowledge of verbs which is useful in syntactic analysis.
in those research, extracted lexical/semantic collocation is especially useful in terms of ranking parses in syntactic analysis as well as automatic construction of lexicon for nlp.
for example, in the context of syntactic disambiguation, <ref>black 1993</ref> and <tref>magerman 1995</tref> proposed statistical parsing models based-on decision-tree learning techniques, which incorporated not only syntactic but also lexical/semantic information in the decision-trees.
the key to extraction of the relations is that any phrase can be substituted by the corresponding tree head-word links marked bold in figure 1.
because we assumed that the relations within the same phrase are independent, all the relations are between the modifier constituents and the head of a phrase only.
when inspecting manually, the binary word tree representation appears to be the most easy to understand.
in this case it is necessary to use a hard-clustering method, such that a binary word tree can be constructed by the clustering process, as we did in the example in the previous sections.
we present a hard clustering algorithm, in the sense that every word belongs to exactly one cluster or is one leaf in the binary word-tree of a particular part of speech.
these results have important implications for crosslinguistic parsing research, as they allow us to tease apart language-specific and annotationspecific effects.
previous work for english eg , <tref>magerman, 1995</tref>; <ref>collins, 1997</ref> has shown that lexicalization leads to a sizable improvement in parsing performance.
research on german <ref>dubey and keller, 2003</ref> showed that lexicalization leads to no sizable improvement in parsing performance for this language.
algorithms for decision tree induction <ref>quinlan 1986</ref>; <ref>bahl et al 1989</ref> have been successfully applied to nlp problems such as parsing <ref>resnik 1993</ref>; <tref>magerman 1995</tref> and discourse analysis <ref>siegel and mckeown 1994</ref>; <ref>soderland and lehnert 1994</ref>.
we tested the satz system using the c45 <ref>quinlan 1993</ref> decision tree induction program as the learning algorithm and compared the results to those obtained previously with the neural network.
in order to construct the etrees, which make such distinction, lextract requires its user to provide additional information in the form of three tables: a head percolation table, an argument table, and a tagset table.
they consider systematically a number of alternative probao bilistic formulations, including those of <ref>resnik 1992</ref> and <ref>schabes 1992</ref> and implemented systems based on other underlying grammatical frameworks, evaluating their adequacy from both a theoretical and empirical perspective in terms of their ability to model particular distributions of data that occur in existing treebanks.
<tref>magerman 1995</tref>, <ref>collins 1996</ref>, <ref>ratnaparkhi 1997</ref>, <ref>charniak 1997</ref> and others describe implemented systems with impressive accuracy on parsing unseen data from the penn treebank <ref>marcus, santorini  marcinkiewicz, 1993</ref>.
the accuracies reported for these systems are substantially better than their non-lexicalised probabilistic context-free grammar analogues, demonstrating clearly the value of lexico-statistical information.
in our experiments, the window starts al the sentence prior to that containing the token and extends back w the window size sentences.
the choice to use sentences as the unit of distance is motivated by our intention to incorporale triggers of this form into a probabilistie treebank based parser and tagger, sneh as <ref>black et al, 1998</ref>; <ref>black et al, 1997</ref>; <ref>brill, 1994</ref>; <ref>collins, 1996</ref>: aelinek et al, 1994; <tref>magerman, 1995</tref>; blatnaparkhi, 1997.
all su<h parsers and taggers of which we are aware use only intrasentential information in predicting parses or tags, and we wish to remove this information, as far as possible, from our results ; the window was not allowed to cross a document bmndary.
second, one might propagate lexical information upward through the productions.
a more linguistically motivated approach is to expand the domain of productions downward to incorporate more tree structures.
for each word a45, the upper-most node with lexical head a45 which has a right sibling node determines the features on the basis of which we decide whether to insert a discourse boundary.
coming to this problem from the standpoint of tree transformation, we naturally view our work as a descendent of <ref>johnson 1998</ref> and <ref>klein and manning 2003</ref>.
in retrospect, however, there are perhaps even greater similarities to that of <tref>magerman, 1995</tref>; <ref>henderson, 2003</ref>; <ref>matsuzaki et al , 2005</ref>.
we would like to infer the number of annotations for each nonterminal automatically.
however, again in retrospect, it is in the work of <tref>magerman 1995</tref> that we see the greatest similarity.
perhaps a more substantial difference is that by not casting his problem as one of learning phrasal categories magerman loses all of the free pcfg technology that we can leverage.
our experiment quantitatively analyzes several feature types power for syntactic structure prediction and draws a series of interesting conclusions.
the paper proposes an information-theory-based feature types analysis model, which uses the measures of predictive information quantity, predictive information gain, predictive information redundancy and predictive information summation to quantitatively analyse the different contextual feature types or feature types combinations predictive power for syntactic structure.
pf model describes the probability of each feature in feature set fs taking on specific values when a cfg rule a ->  is given.
to make the model more practical in parameter estimation, we assume the features in feature set fs are independent from each other, thus:    fsfi afipafsp ,,  5 under this pcfgpf model, the goal of a parser is to choose a parse that maximizes the following score: ,maxarg 1 afs i i i n i t pstscore    6 our model is thus a simplification of more sophisticated models which integrate pcfgs with features, such as those in <tref>magerman1995</tref>, <ref>collins1997</ref> and <ref>goodman1997</ref>.
compared with these models, our model is more practical when only small training data is available, since we assume the independence between features.
other studies have shown that when both speech and text are available to labelers, segmentation is clearer <ref>swerts 1995</ref> and reliability improves <tref>hirschberg and nakatani 1996</tref>.
recall precision fallout error summed deviation pause 92 18 54 49 193 cue 72 15 53 50 216 np 50 31 15 19 153 humans 74 55 09 11 91 if cue1  true then boundary else nonboundary figure 11 cue word algorithm.
<ref>in grosz and hirschberg 1992</ref>, percent agreement see section 32 among 7 coders on 3 texts under two conditions--text plus speech or text alone--is reported at levels ranging from 743 to 951.
<ref>in hirschberg and nakatani 1996</ref>, average reliability measured using the kappa coefficient discussed in carletta 1996 of segmentinitial labels among 3 coders on 9 monologues produced by the same speaker, labeled using text and speech, is8 or above for both read and spontaneous speech; values of at least 8 are typically viewed as representing high reliability see section 32.
<ref>wilson and wiebe 2005</ref> extend this basic annotation scheme to include different types of subjectivity, including positive sentiment, negative sentiment, positive arguing, and negative arguing.
speech was found to improve inter-annotator agreement in discourse segmentation of monologs <tref>hirschberg and nakatani 1996</tref>.
acoustic clues have been successfully employed for the reliable detection of the speakers emotions, including frustration, annoyance, anger, happiness, sadness, and boredom <ref>liscombe et al 2003</ref>.
61 reader judgments there is a growing concern surrounding issues of intercoder reliability when using human judgments to evaluate discourse-processing algorithms <ref>carletta 1996</ref>; <ref>condon and cech 1995</ref>.
proposals have recently been made for protocols for the collection of human discourse segmentation data <ref>nakatani et al 1995</ref> and for how to evaluate the validity of judgments so obtained <ref>carletta 1996</ref>; <ref>isard and carletta 1995</ref>; ros6 1995; <ref>passonneau and litman 1993</ref>; <ref>litman and passonneau 1995</ref>.
recently, hirschberg 52 <ref>hearst texttiling and nakatani 1996</ref> have reported promising results for obtaining higher interjudge agreement using their collection protocols.
proposals have recently been made for protocols for the collection of human discourse segmentation data <ref>nakatani et al 1995</ref> and for how to evaluate the validity of judgments so obtained <ref>carletta 1996</ref>; <ref>isard and carletta 1995</ref>; ros6 1995; <ref>passonneau and litman 1993</ref>; <ref>litman and passonneau 1995</ref>.
recently, hirschberg 52 <ref>hearst texttiling and nakatani 1996</ref> have reported promising results for obtaining higher interjudge agreement using their collection protocols.
the judges were asked simply to mark the paragraph boundaries at which the topic changed; they were not given more explicit instructions about the granularity of the segmentation.
metric f nm s nonm p  user turns 218 53 228 65 065  correct turns 72 18 67 22 059 asrmis 37 27 46 28 046 semmis 5 6 12 14 009 table 2.
average standard deviation for objective metrics in the first problem 6 related work discourse structure has been successfully used in non-interactive settings eg understanding specific lexical and prosodic phenomena <tref>hirschberg and nakatani, 1996</tref>, natural language generation <ref>hovy, 1993</ref>, essay scoring <ref>higgins et al , 2004</ref> as well as in interactive settings eg predictive/generative models of postural shifts <ref>cassell et al , 2001</ref>, generation/interpretation of anaphoric expressions <ref>allen et al , 2001</ref>, performance modeling <ref>rotaru and litman, 2006</ref>.
one related study is that of <ref>rich and sidner, 1998</ref>.
prosody labeling on spontaneous speech corpora like boston directions corpus bdc, switchboard swbd has garnered attention in <tref>hirschberg and nakatani, 1996</tref>; <ref>gregory and altun, 2004</ref>.
automatic prosody labeling has been achieved through various machine learning techniques, such as decision trees <ref>hirschberg, 1993</ref>; <ref>wightman and ostendorf, 1994</ref>; <ref>ma et al , 2003</ref>, rule-based systems <ref>shimei and mckeown, 1999</ref>, bagging and boosting on cart <ref>sun, 2002</ref>, hidden markov models <ref>conkie et al , 1999</ref>, neural networks hasegawa-<ref>johnson et al , 2005</ref>,maximum-entropy models <ref>brenier et al , 2005</ref> and conditional random fields <ref>gregory and altun, 2004</ref>.
for example, in figure 1, if the student would have answered tutor 2 correctly, the next tutor turn would have had the same content as tutor 5 but the advance label.
also, while a human annotation of the discourse structure will be more complex but more time consuming <tref>hirschberg and nakatani, 1996</tref>; <ref>levow, 2004</ref>, its advantages are outweighed by the automatic nature of our discourse structure annotation.
we would like to highlight that our transition annotation is domain independent and automatic.
our transition labels capture behavior like starting a new dialogue newtoplevel, crossing discourse segment boundaries push, popup, popupadv and local phenomena inside a discourse segment advance, samegoal.
information status has generated large interest among researchers because of its complex interaction with other linguistic phenomena, thus affecting several natural language processing tasks.
since it correlates with word order and pitch accent <ref>lambrecht, 1994</ref>; <tref>hirschberg and nakatani, 1996</tref>, for instance, incorporating knowledge on information status would be helpful for natural language generation, and in particular text-tospeech systems.
another area where information status can play an important role is anaphora resolution.
following the methodology in ttirschberg and <ref>nakatani, 1996</ref>, we measured the reliability of coding for a linearized version of the iu tree, by calculating the reliability of coding of iu beginnings using the kappa metric.
we calculated the observed pairwise agreement of cgus marked as the beginnings of ius, and factored out the expected agreement estimated from the actual data, giving the pairwise kappa score.
other attempts have had more success using improved annotation tools and more precise instructions <ref>grosz and hirschberg, 1992</ref>; <tref>hirschberg and nakatani, 1996</tref>.
while agreement among annotators regarding linear segmentation has been found to be higher than 80 <ref>hearst, 1997</ref>, with respect to hierarchical segmentation it has been observed to be as low as 60 <ref>flammia and zue, 1995</ref>.
let a28a30a40a30a11a14a28a21a40a30a28a16a31a36a3a41a34 be the set of senses of a3  for each sense of a3 a3a42a28a44a43a46a45a47a28a30a40a30a11a14a28a21a40a30a28a16a31a36a3a41a34  we obtain a ranking score by summing over the a27a48a28a21a28a16a31a32a3a6a15a33a11a50a49a30a34 of each neighbour a11a50a49a51a45a52a4a6a5  multiplied by a weight.
this weight is the wordnet similarity score a3a42a11a14a28a30a28  between the target sense a3a53a28a35a43  and the sense of a11a54a49 a11a14a28a35a55a56a45a57a28a30a40a30a11a14a28a21a40a30a28a16a31a36a11a54a49a16a34  that maximises this score, divided by the sum of all such wordnet similarity scores for a28a30a40a21a11a19a28a21a40a30a28a26a31a32a3a41a34 and a11a50a49  thus we rank each sense a3a42a28 a43 a45a10a28a30a40a21a11a19a28a21a40a30a28a26a31a32a3a41a34 using:a58a41a59 a11a14a2a54a60a61a11a50a62a64a63a66a65a35a67a16a68a12a40a26a31a32a3a53a28 a43 a34a69a7 a70 a71a44a72a33a73a16a74a76a75 a27a48a28a21a28a16a31a32a3a6a15a33a11a50a49a30a34a30a77 a3a42a11a14a28a21a28a16a31a32a3a53a28a35a43a36a15a33a11a50a49a30a34 a78 a5a19a79a81a80a39a82 a73 a79a61a83 a71 a79a81a83a61a79a36a84a85a5a19a86 a3a53a11a14a28a30a28a26a31a32a3a42a28 a43 a82 a15a17a11 a49 a34 1 where: a3a42a11a14a28a21a28a16a31a32a3a53a28a35a43a87a15a17a11a54a49a21a34a66a7 a88a46a89a21a90 a71 a79a92a91 a73 a79a81a83 a71 a79a81a83a61a79a93a84 a71a44a72 a86 a31a36a3a42a11a14a28a30a28a26a31a32a3a53a28a35a43a36a15a33a11a14a28a35a55a29a34a87a34 22 acquiring the automatic thesaurus there are many alternative distributional similarity measures proposed in the literature, for this work we used the measure and thesaurus construction method described by <tref>lin 1998</tref>.
importantly, inter-dependence between links can still be accommodated by exploiting dynamic features in training features that take into account the labels of some of the surrounding components when predicting the label of a target component.
4 extensions to large margin parsing the approach presented above has a limitation: it uses a local scoring function instead of a global scoring function to compute the score for a candidate tree.
realword spelling correction is also referred to as context sensitive spelling correction, which tries to detect incorrect usage of valid words in certain contexts <ref>golding and roth, 1996</ref>; <ref>mangu and brill, 1997</ref>.
distributional similarity between words has been investigated and successfully applied in many natural language tasks such as automatic semantic knowledge acquisition <tref>dekang lin, 1998</tref> and language model smoothing <ref>essen and steinbiss, 1992</ref>; <ref>dagan et al , 1997</ref>.
3 distributional similarity-based models for query spelling correction 31 motivation most of the previous work on spelling correction concentrates on the problem of designing better error models based on properties of character strings.
in this light, the contributions of this paper are fourfold.
first, instead of separately addressing the tasks of collecting unlabeled sets of instances <tref>lin, 1998</tref>, assigning appropriate class labels to a given set of instances <ref>pantel and ravichandran, 2004</ref>, and identifying relevant attributes for a given set of classes <ref>pasca, 2007</ref>, our integrated method from section 2 enables the simultaneous extraction of class instances, associated labels and attributes.
second, by exploiting the contents of query logs during the extraction of labeled classes of instances from web documents, we acquire thousands 4,583, to be exact of open-domain classes covering a wide range of topics and domains.
the accuracy reported in section 32 exceeds 80 for both instance sets and class labels, although the extraction of classes requires a remarkably small amount of supervision, in the form of only a few commonly-used is-a extraction patterns.
another approach used the words distribution to cluster the words <ref>pereira, 1993</ref>, and inoue <ref>inoue, 1991</ref> also used the word distributional information in the japanese-english word pairs to resolve the polysemous word problem.
the wmts is well suited to lra, because the wmts scales well to large corpora one terabyte, in our case, it gives exact frequency counts unlike most web search engines, it is designed for passage retrieval rather than document retrieval, and it has a powerful query syntax.
find alternates: for each word pair a:b in the input set, look in <tref>lins 1998a</tref> thesaurus for the top num sim words in the following experiments, num sim is 10 that are most similar to a for each a prime that is similar to a, make a new word pair a prime :b likewise, look for the top num sim words that are most similar to b, and for each b prime , make a new word pair a:b prime  a:b is called the original pair and each a prime :b or a:b prime is an alternate pair.
the intent is that alternates should have almost the same semantic relations as the original.
for each input pair, there will now be 2  num sim alternate pairs.
for each input pair, there will now be 2  num sim alternate pairs.
the first column in table 7 shows the alternate pairs that are generated for the original pair quart:volume.
as a courtesy to other users of lins on-line system, we insert a 20-second delay between each two queries.
the classes of words are computed on the fly over all sequences of terms in the extracted patterns, on top of a large set of pairwise similarities among words <tref>lin, 1998</tref> extracted in advance from around 50 million news articles indexed by the google search engine over three years.
all digits in both patterns and sentences are replaced with a common marker, such 810 that any two numerical values with the same number of digits will overlap during matching.
538 ture for npi whose value is the most likely ne type.
motivated by this observation, we create for each of npis ten most semantically similar nps a neighbor feature whose value is the surface string of the np.
to determine the ten nearest neighbors, we use the semantic similarity values provided by lins dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic de nition of similarity.
an example extraction would be <eastern airlines, the carrier>, where the rst entry is a proper noun labeled with either one of the seven muc-style ne types4 or others5 and the second entry is a common noun.
we then infer the sc of a common noun as follows: 1 we compute the probability that the common noun co-occurs with each of the eight ne types6 based on the extracted appositive relations, and 2 if the most likely ne type has a co-occurrence probability above a certain threshold we set it to 07, we create a induced class fea1this is motivated by <tref>lins 1998c</tref> observation that a coreference resolver that employs only the rst wordnet sense performs slightly better than one that employs more than one sense.
2 subj verb: if npi is involved in a subjectverb relation, we create a subj verb feature whose value is the verb participating in the relation.
our motivation here is to coarsely model subcategorization.
the distributed frequency of an object, which takes an average of the frequency of occurrence with an object over all verbs occurring with the object above a threshold.
the classes over which the probability distribution is calculated are selected according to the minimum description length principle mdl which uses the argument head tokens for nding the best classes for representation.
similarity and association measures can help for the cases of near-synonymy.
however, while similarity measures such as wordnet distance or lins similarity metric only detect cases of semantic similarity, association measures such as the ones used by poesio et al , or by garera and yarowsky also find cases of associative bridg497 lin98 rff they they:g2 pl03 land country/state/land staat staat kemalismus regierung kontinent state state kemalism government continent stadt stadt bauernfamilie prasident region city city agricultural family president region region landesregierung bankgesellschaft dollar stadt region country government banking corporation dollar city bundesrepublik bundesregierung baht albanien staat federal republic federal government baht albania state republik gewerkschaft gasag hauptstadt bundesland republic trade union a gas company capital state medikament medical drug arzneimittel pille ru patient arzneimittel pharmaceutical pill a drug patient pharmaceutical praparat droge abtreibungspille arzt lebensmittel preparation drug non-medical abortion pill doctor foodstuff pille praparat viagra pille praparat pill preparation viagra pill preparation hormon pestizid pharmakonzern behandlung behandlung hormone pesticide pharmaceutical company treatment treatment lebensmittel lebensmittel praparat abtreibungspille arznei foodstuff foodstuff preparation abortion pill drug highest ranked words, with very rare words removed : ru 486, an abortifacient drug lin98: lins distributional similarity measure <tref>lin, 1998</tref> rff: geffet and dagans relative feature focus measure <ref>geffet and dagan, 2004</ref> they: association measure introduced by <ref>garera and yarowsky 2006</ref> they:g2: similar method using a log-likelihood-based statistic see <ref>dunning 1993</ref> this statistic has a preference for higher-frequency terms pl03: semantic space association measure proposed by <ref>pado and lapata 2003</ref> table 1: similarity and association measures: most similar items ing like 1a,b; the result of this can be seen in table 2: while the similarity measures lin98, rff list substitutable terms which behave like synonyms in many contexts, the association measures garera and yarowskys they measure, pado and lapatas association measure also find non-compatible associations such as countrycapital or drugtreatment, which is why they are commonly called relationfree.
for the purpose of coreference resolution, however we do not want to resolve the door to the antecedent the house as the two descriptions do not corefer, and it may be useful to filter out non-similar associations.
12 information sources different resources may be differently suited for the recognition of the various relations.
while none of the information sources can match the precision of the hypernymy information encoded in germanet, or that of using a combination of high-precision patterns with the world wide web as a very large corpus, it is possible to achieve a considerable improvement in terms of recall without sacrificing too much precision by combining these methods.
for the association measures, the fact that they are relation-free also means that they can profit from added semantic filtering.
the novel distance-bounded semantic similarity method where we use the most similar words in the previous discourse together with a semantic classbased filter and a distance limit comes near the precision of using surface patterns, and offers better accuracy than gasperin and vieiras method of using the globally most similar words.
note, however, that <ref>mccarthy et al , 2004</ref> used the information about distributionally similar words to approximate corpus frequencies for word senses, whereas we target the estimation of a property of a given word sense the subjectivity.
next, for each sense wsi of the word w, we determine the similarity with each of the words in the list dsw, using a wordnet-based measure of semantic similarity wnss.
41 weight tuning there are several motivations for learning the graph weights  in this domain.
first, some dependency relations  foremost, subject and object  are in general more salient than others <tref>lin, 1998</tref>; <ref>pado and lapata, 2007</ref>.
in addition, dependency relations may have varying importance per different notions of word similarity eg, noun vs verb similarity <ref>resnik and diab, 2000</ref>.
the learning methods described in this paper can be readily applied to 911 other directed and labelled entity-relation graphs7 the graph representation described in this paper is perhaps most related to syntax-based vector space models, which derive a notion of semantic similarity from statistics associated with a parsed corpus <ref>grefenstette, 1994</ref>; <tref>lin, 1998</tref>; <ref>pado and lapata, 2007</ref>.
in most cases, these models construct vectors to represent each word wi, where each element in the vector for wi corresponds to particular context c, and represents a count or an indication of whether wi occurred in context c a context can refer to simple co-occurrence with another word wj, to a particular syntactic relation to another word eg, a relation of direct object to wj, etc given these word vectors, inter-word similarity is evaluated using some appropriate similarity measure for the vector space, such as cosine vector similarity, or lins similarity <tref>lin, 1998</tref>.
recently, pado and lapata <ref>pado and lapata, 2007</ref> have suggested an extended syntactic vector space model called dependency vectors, in which rather than simple counts, the components of a word vector of contexts consist of weighted scores, which combine both co-occurrence frequency and the importance of a context, based on properties of the connecting dependency paths.
instead, we include learning techniques to optimize the graphwalk based similarity measure.
the learning methods described in this paper can be readily applied to 911 other directed and labelled entity-relation graphs7 the graph representation described in this paper is perhaps most related to syntax-based vector space models, which derive a notion of semantic similarity from statistics associated with a parsed corpus <ref>grefenstette, 1994</ref>; <tref>lin, 1998</tref>; <ref>pado and lapata, 2007</ref>.
in most cases, these models construct vectors to represent each word wi, where each element in the vector for wi corresponds to particular context c, and represents a count or an indication of whether wi occurred in context c a context can refer to simple co-occurrence with another word wj, to a particular syntactic relation to another word eg, a relation of direct object to wj, etc given these word vectors, inter-word similarity is evaluated using some appropriate similarity measure for the vector space, such as cosine vector similarity, or lins similarity <tref>lin, 1998</tref>.
recently, pado and lapata <ref>pado and lapata, 2007</ref> have suggested an extended syntactic vector space model called dependency vectors, in which rather than simple counts, the components of a word vector of contexts consist of weighted scores, which combine both co-occurrence frequency and the importance of a context, based on properties of the connecting dependency paths.
to further enhance the quality of co-occurrence data, we search on the specific phrase a16 is measured in in which a16 is one of the related concepts of a14  this allows for the simultaneous discovery of unknown units and the retrieval of their co-occurrence counts.
this allows us to handle sentential constructions that may intervene between measured and a meaningful unit.
for each unit a17 that is related to measured via in, we increment the co-occurrence count a18a20a19a21a17a23a22a24a16a26a25, thereby collecting frequency counts for each a17 with a16  the patterns precision prevents incidental cooccurrence between a related concept and some unit that may occur simply because of the general topic of the document.
thus, there is strong motivation to expand the list of units obtained from google by automatically considering similar units.
the similar word expansion can add a term like gigs as a unit for size by virtue of its association with gigabytes, which is on the original list.
cluster clustered similar words of duty with similarity score responsibility 016, obligation 0109, task 0101, function 0098, role 0091, post 0087, position 0086, job 0084, chore 008, mission 008, assignment 0079, liability 0077  tariff0091, restriction 0089, tax 0086, regulation 0085, requirement 0081, procedure 0079, penalty 0079, quota 0074, rule 007, levy 0061  fee 0085, salary 0081, pay 0064, fine 0058 personnel 0073, staff0073 training 0072, work 0064, exercise 0061 privilege 0069, right 0057, license 0056 22.
cluster clustered similar words of duty with similarity score responsibility 016, obligation 0109, task 0101, function 0098, role 0091, post 0087, position 0086, job 0084, chore 008, mission 008, assignment 0079, liability 0077  tariff0091, restriction 0089, tax 0086, regulation 0085, requirement 0081, procedure 0079, penalty 0079, quota 0074, rule 007, levy 0061  fee 0085, salary 0081, pay 0064, fine 0058 personnel 0073, staff0073 training 0072, work 0064, exercise 0061 privilege 0069, right 0057, license 0056 22.
43 alignment model <ref>glickman et al , 2006</ref> was among the top scoring systems on the rte-1 challenge and supplies a probabilistically motivated lexical measure based on word co-occurrence statistics.
we are going to extend the set of content bearing words and to include verbs.
we will take advantage of the flexibility provided by our framework and use syntax based measure of similarity in the computation of the verb vectors, following <tref>lin, 1998</tref>.
we are planning to integrate more sophisticated techniques in our framework.
in this paper the sophisticated parser rasp toolkit 2 <ref>briscoe et al, 2006</ref> was utilized to extract this kind of word relations.
we use the following example for illustration purposes: the library has a large collection of classic books by such authors as herrick and shakespeare.
as manning and schu tze 1999 argued, this does not seem to be a good measure of the strength of association between a word and a local position.
it is a statistically based dependency parser which is reported to reach 89 precision and 82 recall on press reportage texts.
this is a syntactic detector, a point missed by <ref>evans 2001</ref> in his criticism: the patterns are robust to intervening words and modi ers eg  it was never thought by the committee that  provided the sentence is parsed correctly7 we automatically parse sentences with minipar, a broad-coverage dependency parser <tref>lin, 1998b</tref>.
our work is part of a trend of extracting other important information from statistical distributions.
unfortunately, none of the corpus-based features improved performance on the development set and are thus excluded from further consideration.
1 thesaurus creation over the last ten years, interest has been growing in distributional thesauruses hereafter simply thesauruses.
we then calculate normalized tuple similarity scores over the tuple pairs using a metric that accounts for similarities in both syntactic structure and content of each tuple.
a thesaurus constructed from corpus statistics <tref>lin, 1998</tref> is utilized for the content similarity.
we term these vectors lexical because they are collected by looking only at the lexicals in the text ie no sense information is used.
we use the term ontological feature vector to refer to a feature vector whose features are for a particular sense of the word.
summationtext lh scorel openclasswordsh 2 2we set the threshold to 001 3the active verbal form with direct modifiers where scorel is 1 if it appears in p, or if it is a derivation of a word in p according to wordnet.
3 proof system like logic-based systems, our proof system consists of propositions t, h, and intermediate premises, and inference entailment rules, which derive new propositions from previously established ones.
as a striking example, the 14 most syntactically similar verbs to believe in order are think, guess, hope, feel, wonder, theorize, fear, reckon, contend, suppose, understand, know, doubt, and suggest  all mental action verbs.
verb pairs instances cosine bind 83 bound 95 0950 plunge 94 tumble 87 0888 dive 36 plunge 94 0867 dive 36 tumble 87 0866 jump 79 tumble 87 0865 fall 84 fell 102 0859 intersperse 99 perch 81 0859 assail 100 chide 98 0859 dip 81 fell 102 0858 buffet 72 embroil 100 0856 embroil 100 lock 73 0856 embroil 100 superimpose 100 0856 fell 102 jump 79 0855 fell 102 tumble 87 0855 embroil 100 whipsaw 63 0850 pluck 100 whisk 99 0849 acquit 100 hospitalize 99 0849 disincline 70 obligate 94 0848 jump 79 plunge 94 0848 dive 36 jump 79 0847 assail 100 lambaste 100 0847 festoon 98 strew 100 0846 mar 78 whipsaw 63 0846 pluck 100 whipsaw 63 0846 ensconce 101 whipsaw 63 0845 table 2.
top 25 most syntactically similar pairs of the 3257 verbs in propbank.
our approach is strictly empirical; the similarity of verbs is determined by examining the syntactic contexts in which they appear in a large text corpus.
our approach is analogous to previous work in extracting collocations from large text corpora using syntactic information <tref>lin, 1998</tref>.
this work proposes using graded word sense relationships rather than fixed groupings clusters.
this is transformed from a distance measure in the wn-similarity package by taking the reciprocal: jcns1,s2  1/djcns1,s2 we use raw bnc data for calculating ic values.
dist we use a distributional similarity measure <tref>lin, 1998</tref> to obtain a fixed number 50 of the top ranked nearest neighbours for the target nouns.
the number of unique collocations in the resulting database 2 is about 11 million.
to measure the compositionality, semantically similar words are more suitable than synomys.
902 614 distributed frequency of object a0  the distributed frequency of object is based on the idea that if an object appears only with one verb or few verbs in a large corpus, the collocation is expected to have idiomatic nature <ref>tapanainen et al , 1998</ref>.
for example, sure in make sure occurs with very few verbs.
the higher the value of a38, the more is the likelihood of the collocation to be a mwe.
we obtained the best results section 8 when we substituted top-5 similar words for both the verb and the object.
to measure the compositionality, semantically similar words are more suitable than synomys.
lins database was created using the particular distributionalsimilaritymeasurein<tref>lin, 1998</tref>, applied to a large corpus of news data 64 million words 4.
the setting allowed us to analyze different types of state of the art models and their behavior with respect to characteristic sub-cases of the problem.
the major conclusion that seems to arise from our experiments is the effectiveness of combining a knowledge based thesaurus such as wordnet with distributional statistical information such as <tref>lin, 1998</tref>, overcoming the known deficiencies of each method alone.
on the other hand, successfully incorporating local and global contextual information, as similar to wsd methods, remains a challenging task for future research.
the method we use to predict the rst sense is that of mccarthy et al.
in the case of sfs, we perform full synset wsd based on one of the above options, and then map the prediction onto the corresponding unique sf.
if a98a56a30a31a4 a33 is the set of co-occurrence types a30 a55 a14a16a95 a33 such that a99a100a30a42a4a43a14 a55 a14a32a95 a33 is positive then the similarity between two nouns, a4 and a10, can be computed as: a26a41a28a15a28a27a30a42a4a43a14a16a10 a33 a8 a75 a83a102a101a42a103a50 a85 a70a11a104 a83a102a6a13a85a106a105 a104 a83 a67 a85 a30a78a99a100a30a31a4a7a14 a55 a14a16a95 a33a41a107 a99a108a30a31a10a109a14 a55 a14a16a95 a33a86a33 a75 a83a102a101a42a103a50 a85 a70a11a104 a83a102a6a13a85 a99a108a30a31a4a7a14 a55 a14a32a95 a33a45a107 a75 a83a84a101a42a103a50 a85 a70a11a104 a83 a67 a85 a99a108a30a31a10a109a14 a55 a14a16a95 a33 where: a99a108a30a31a4a7a14 a55 a14a32a95 a33 a8a111a110a21a112a114a113 a54 a30a31a95a73a115a116a4a118a117 a55 a33 a54 a30a42a95a73a115 a55 a33 a thesaurus entry of size a3 for a target noun a4 is then defined as the a3 most similar nouns to a4  22 the wordnet similarity package we use the wordnet similarity package 005 and wordnet version 16.
in order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of <tref>lin 1998</tref>.
we then use the wordnet similarity package <ref>patwardhan and pedersen, 2003</ref> to give us a semantic similarity measure hereafter referred to as the wordnet similarity measure to weight the contribution that each neighbour makes to the various senses of the target word.
let a28a15a34a20a10a18a28a20a34a15a28a25a30a31a4 a33 be the set of senses of a4  for each sense of a4 a4a35a28a37a36a39a38a40a28a20a34a15a10a13a28a15a34a20a28a27a30a31a4 a33  we obtain a ranking score by summing over the a26a41a28a15a28a27a30a42a4a43a14a16a10a45a44 a33 of each neighbour a10a46a44a47a38a48a5 a6  multiplied by a weight.
this weight is the wordnet similarity score a4a49a10a13a28a15a28  between the target sense a4a35a28a37a36  and the sense of a10a45a44 a10a13a28a37a50a51a38a52a28a15a34a15a10a13a28a20a34a15a28a27a30a42a10a45a44 a33  that maximises this score, divided by the sum of all such wordnet similarity scores for a28a20a34a15a10a13a28a15a34a20a28a27a30a31a4 a33 and a10a46a44  thus we rank each sense a4a49a28 a36 a38a53a28a15a34a20a10a18a28a20a34a15a28a25a30a31a4 a33 using: a54a56a55 a34a15a57a41a58a41a59a60a34a15a10a13a61a37a34a63a62a64a61a37a65 a55 a34a27a30a31a4a35a28a37a36 a33 a8 a66 a67a69a68a32a70a27a71a73a72 a26a29a28a20a28a27a30a31a4a7a14a32a10 a44 a33a15a74 a4a49a10a13a28a20a28a27a30a31a4a35a28a37a36a42a14a32a10a46a44 a33 a75 a6a18a76a78a77a80a79 a70 a76a82a81 a67 a76a78a81a82a76a42a83a84a6a18a85 a4a35a10a13a28a15a28a25a30a31a4a49a28 a36 a79 a14a16a10a45a44 a33 1 where: a4a49a10a13a28a20a28a27a30a31a4a35a28a37a36a86a14a16a10a45a44 a33 a8 a87a89a88a20a90 a67 a76a92a91 a70 a76a78a81 a67 a76a78a81a82a76a93a83 a67a69a68 a85 a30a42a4a49a10a13a28a15a28a25a30a31a4a35a28a37a36a42a14a32a10a13a28a37a50 a33a86a33 21 acquiring the automatic thesaurus the thesaurus was acquired using the method described by <tref>lin 1998</tref>.
<ref>like lin 1999</ref>, we generate lexical variants of the target automatically by replacing either the verb or the noun constituent by a semantically similar word from the automatically-built thesaurus of <tref>lin 1998</tref>.
we use wordnet wn as our sense inventory.
the senses of a worda2 are each assigned a ranking score which sums over the distributional similarity scores of the neighbours and weights each neighbours score by a wn similarity score <ref>patwardhan and pedersen, 2003</ref> between the sense of a2 and the sense of the neighbour that maximises the wn similarity score.
early experiments in thesaurus extraction <ref>grefenstette, 1994</ref> suffered from the limited size of available corpora, but more recent experiments have used much larger corpora with greater success <tref>lin, 1998a</tref>.
the list of measure and weight functions we compared against is not complete, and we hope to add other functions to provide a general framework for thesaurus extraction experimentation.
we would also like to expand our evaluation to include direct methods used by others <tref>lin, 1998a</tref> and using the extracted thesaurus in nlp tasks.
our proposed weight functions are motivated by our intuition that highly predictive attributes are strong collocations with their terms.
we describe the functions evaluated in these experiments using an extension of the asterisk notation used by <tref>lin 1998a</tref>, where an asterisk indicates a set ranging over all existing values of that variable.
432 the brandeis semantic ontology as a second source of lexical coherence, we used the brandeis semantic ontology or bso <ref>pustejovsky et al , 2006</ref>.
the bso is a lexicallybased ontology in the generative lexicon tradition <ref>pustejovsky, 2001</ref>; <ref>pustejovsky, 1995</ref>.
3 learning to merge word senses 31 wordnet-based features here we describe the feature space we construct for classifying whether or not a pair of synsets should be merged; first, we employ a wide variety of linguistic features based on information derived from wordnet.
we use eight similarity measures implemented within the wordnet::similarity package5, described in <ref>pedersen et al , 2004</ref>; these include three measures derived from the paths between the synsets in wordnet: hso hirst and st-<ref>onge, 1998</ref>, lch <ref>leacock and chodorow, 1998</ref>, and wup <ref>wu and palmer, 1994</ref>; three measures based on information content: res <ref>resnik, 1995</ref>, lin <tref>lin, 1998</tref>, and jcn <ref>jiang and conrath, 1997</ref>; the gloss-based extended lesk measure lesk, <ref>banerjee and pedersen, 2003</ref>, and finally the gloss vector similarity measure vector <ref>patwardan, 2003</ref>.
in this paper we focus on the task of automatically selecting the best near-synonym that should be used in a particular context.
the natural way to validate an algorithm for this task would be to ask human readers to evaluate the quality of the algorithms output, but this kind of evaluation would be very laborious.
word-similarity classes <tref>lin, 1998</tref> derived from clustering are also used to expand the pool of potential collocations; this type of semantic relatedness among words is expressed in the similarcoll feature.
when formulating the features similarcoll and dictcoll, the words related to each context word are considered as potential collocations <ref>wiebe et al , 1998</ref>.
co-occurrence fresense distinctions precision recall fine-grained 566 565 course-grained 660 658 table 1: results for senseval-3 test data.
if the precision of s i c i,n is greater than a threshold t, then the words in this set are retained as pses.
this is important for subjectivity recognition, because pses are not limited to verb-noun relationships.
the method is then used to identify an unusual form of collocation: one or more positions in the collocation may be filled by any word of an appropriate part of speech that is unique in the test data.
we hypothesized that two words may be distributionally similar because they are both potentially subjective eg , tragic, sad, and poignant are identified from bizarre.
in addition, we use distributional similarity to improve estimates of unseen events: a word is selected or discarded based on the precision of it together with its n most similar neighbors.
we also presented a procedure for automatically identifying potentially subjective collocations, including fixed collocations and collocations with placeholders for unique words.
table 9 summarizes the results of testing all of the above types of pses.
all show increased precision in the evaluations.
we picked the widely cited and competitive eg.
<ref>weeds and weir, 2003</ref> measure of <tref>lin 1998</tref> as a representative case, and utilized it for our analysis and as a starting point for improvement.
21 lins 98 similarity measure lins similarity measure between two words, w and v, is defined as follows:,,, ,, ,           fvweightfwweight fvweightfwweight vwsim vffwff vfwff where fw and fv are the active features of the two words and the weight function is defined as mi.
the target text is used for this purpose, provided it is large enough to learn a thesaurus from.
otherwise a large corpus with sense distribution similar to the target text text pertaining to the specified domain must be used.
below are contrived, but plausible, examples of each for the word pulse; the numbers are conditional probabilities.
relation-free dp pulse: beat 28, racing 2, grow 13, beans 09, heart 04,   .
jsd cp is another relative entropybased measure like asd cp  but it is symmetric.
simjami is a variant <tref>lin, 1998</tref> in which the features of a word are those contexts for which the pointwise mutual information mi between the word and the context is positive, where mi can be calculated using ic;w  log pcjwpc.
let seenrp be the set of seen headwords for an argument rp of a predicate p then we model the selectional preference s of rp for a possible headword w0 as a weighted sum of the similarities between w0 and the seen headwords: srpw0  summationdisplay wseenrp simw0,wwtrpw simw0,w is the similarity between the seen and the potential headword, and wtrpw is the weight of seen headword w similarity simw0,w will be computed on the generalization corpus, again on the basis of extracted tuples p,rp,w.
we will be using the similarity metrics shown in table 1: cosine, the dice and jaccard coefficients, and <ref>hindles 1990</ref> and <tref>lins 1998</tref> mutual information-based metrics.
in this paper we propose a new, simple model for selectional preference induction that uses corpus-based semantic similarity metrics, such as cosine or <tref>lins 1998</tref> mutual informationbased metric, for the generalization step.
the use of synonyms is another way of increasing the coverage of question terminology;; while semantic features try to achieve it by generalization, synonyms do it by lexical expansion.
our plan is to use the synonyms obtained from very large corpora reported in <tref>lin, 1998</tref>.
our approach can be distinguished from classical distributional approach by different points.
first, we use triple occurrences to build a distributional space one triple implies two contexts and two lexical units, but we use the transpose of the classical space: each point x i of this space is a syntactical context with the form rw, each dimension j is a lexical units, and each value x i j is the frequency of corresponding triple occurrences.
it extends prior work on syntax-based models <ref>grefenstette, 1994</ref>; <tref>lin, 1998</tref>, by providing a general framework for defining context so that a large number of syntactic relations can be used in the construction of the semantic space.
our approach differs from <tref>lin 1998</tref> in three important ways: a by introducing dependency paths we can capture non-immediate relationships between words ie , between subjects and objects, whereas lin considers only local context dependency edges in our terminology; the semantic space is therefore constructed solely from isolated head/modifier pairs and their inter-dependencies are not taken into account; b lin creates the semantic space from the set of dependency edges that are relevant for a given word; by introducing dependency labels and the path value function we can selectively weight the importance of different labels eg , subject, object, modifier and parametrize the space accordingly for different tasks; c considerable flexibility is allowed in our formulation for selecting the dimensions of the semantic space; the latter can be words see the leaves in figure 1, parts of speech or dependency edges; in lins approach, it is only dependency edges features in his terminology that form the dimensions of the semantic space.
write a for the lexical association function which computes the value of a cell of the matrix from a co-occurrence frequency: ki j  a f bi;t j 3 evaluation 31 parameter settings all our experiments were conducted on the british national corpus bnc, a 100 million word collection of samples of written and spoken language <ref>burnard, 1995</ref>.
we used <tref>lins 1998</tref> broad coverage dependency parser minipar to obtain a parsed version of the corpus.
minipar employs a manually constructed grammar and a lexicon derived from wordnet with the addition of proper names 130,000 entries in total.
it extends prior work on syntax-based models <ref>grefenstette, 1994</ref>; <tref>lin, 1998</tref>, by providing a general framework for defining context so that a large number of syntactic relations can be used in the construction of the semantic space.
our approach differs from <tref>lin 1998</tref> in three important ways: a by introducing dependency paths we can capture non-immediate relationships between words ie , between subjects and objects, whereas lin considers only local context dependency edges in our terminology; the semantic space is therefore constructed solely from isolated head/modifier pairs and their inter-dependencies are not taken into account; b lin creates the semantic space from the set of dependency edges that are relevant for a given word; by introducing dependency labels and the path value function we can selectively weight the importance of different labels eg , subject, object, modifier and parametrize the space accordingly for different tasks; c considerable flexibility is allowed in our formulation for selecting the dimensions of the semantic space; the latter can be words see the leaves in figure 1, parts of speech or dependency edges; in lins approach, it is only dependency edges features in his terminology that form the dimensions of the semantic space.
experiment 2 showed that a model that relies on rich context specifications can reliably distinguish between different types of lexical relations.
however, at structural level, the concept-based seeds share the same or similar linguistic patterns eg subject-verb-object patterns with the corresponding types of proper names.
in fact, the anaphoric function of pronouns and common nouns to represent antecedent nes indicates the substitutability of proper names by the corresponding common nouns or pronouns.
for the distributional similarity component we employ the similarity scheme of <ref>geffet and dagan, 2004</ref>, which was shown to yield improved predictions of non-directional lexical entailment pairs.
this scheme utilizes the symmetric similarity measure of <tref>lin, 1998</tref> to induce improved feature weights via bootstrapping.
these weights identify the most characteristic features of each word, yielding cleaner feature vector representations and better similarity assessments.
distributional similarity measures are typically computed through exhaustive processing of a corpus, and are therefore applicable to corpora of bounded size.
the method uses a thesaurus obtained from the text by parsing, extracting grammatical relations and then listing each word w with its top k nearest neighbours, where k is a constant.
like <ref>mccarthy et al , 2004</ref> we use k  50 and obtain our thesaurus using the distributional similarity metric described by <tref>lin, 1998</tref> and we use wordnet wn as our sense inventory.
the senses of a word w are each assigned a ranking score which sums over the distributional similarity scores of the neighbours and weights each neighbours score by a wn similarity score <ref>patwardhan and pedersen, 2003</ref> between the sense of w and the sense of the neighbour that maximises the wn similarity score.
those words that obtain the best values are considered to be most similar.
practical implementations of algorithms based on this principle have led to excellent results as documented in papers by <ref>ruge 1992</ref>, <ref>grefenstette 1994</ref>, <ref>agarwal 1995</ref>, <ref>landauer  dumais 1997</ref>, <ref>schtze 1997</ref>, and <tref>lin 1998</tref>.
fortunately, we did not need to conduct our own experiment to obtain the humans similarity estimates.
there are two parameters of this process, neither of whichwas varied in <ref>wiebe, 2000</ref>: c, the cluster size considered, andft, a lteringthreshold, such that, if the seed word and the words in its cluster have, as a set, lower precision than the ltering threshold on the training data, the entire cluster, including the seed word, is ltered out.
the use of wordnet measures is intended to simulate the mental connections that visitors make between exhibit content, given that each visit can interpret content in a number of different ways.
22 vocabulary support synonyms for lower frequency more difficult words are output using a statistically-generated word similarity matrix <tref>lin, 1998</tref>.
this shows that we have extracted a reasonable number of features for each phrase, since distributional similarity techniques have been shown to work well for words which occur more than 100 times in a given corpus <tref>lin, 1998</tref>; <ref>weeds and weir, 2003</ref>.
w ord s imilar w ords  with similarity score  eat cook 0127, drink 0108, consume 0101, feed 0094, taste 0093, like 0092, serve 0089, bake 0087, sleep 0086, pick 0085, fry 0084, freeze 0081, enjoy 0079, smoke 0078, harvest 0076, love 0076, chop 0074, sprinkle 0072, toss 0072, chew 0072 salad soup 0172, sandwich 0169, sauce 0152, pasta 0149, dish 0135, vegetable 0135, cheese 0132, dessert 013, entree 0121, bread 0116, meat 0116, chicken 0115, pizza 0114, rice 0112, seafood 011, dressing 0109, cake 0107, steak 0105, noodle 0105, bean 0102 the collocation database for the words eat and salad  the database contains a total of 11 million unique dependency relationships.
21 collocation database given a word w in a dependency relationship such as subject or object , the collocation database is used to retrieve the words that occurred in that relationship with w, in a large corpus, along with their frequencies <tref>lin, 1998a</tref>.
eat : object: almond 1, a pple 25, bean 5, beam 1, binge 1, bread 13, cake 17, cheese 8, dish 14, disorder 20, egg 31, grape 12, grub 2, hay 3, junk 1, meat 70, poultry 3, rabbit 4, soup 5, sandwich 18, pasta 7, vegetable 35,  subject: adult 3, animal 8, beetle 1, cat 3, child 41, decrease 1, dog 24, family 29, guest 7, kid 22, patient 7, refugee 2, rider 1, russian 1, shark 2, something 19, we 239, wolf 5,  salad : adj-modifier: assorted 1, crisp 4, fresh 13, good 3, grilled 5, leftover 3, mixed 4, olive 3, prepared 3, side 4, small 6, special 5, vegetable 3,  object-of: add 3, consume 1, dress 1, grow 1, harvest 2, have 20, like 5, love 1, mix 1, pick 1, place 3, prepare 4, return 3, rinse 1, season 1, serve 8, sprinkle 1, taste 1, test 1, toss 8, try 3,  figure 1.
eat : object: almond 1, a pple 25, bean 5, beam 1, binge 1, bread 13, cake 17, cheese 8, dish 14, disorder 20, egg 31, grape 12, grub 2, hay 3, junk 1, meat 70, poultry 3, rabbit 4, soup 5, sandwich 18, pasta 7, vegetable 35,  subject: adult 3, animal 8, beetle 1, cat 3, child 41, decrease 1, dog 24, family 29, guest 7, kid 22, patient 7, refugee 2, rider 1, russian 1, shark 2, something 19, we 239, wolf 5,  salad : adj-modifier: assorted 1, crisp 4, fresh 13, good 3, grilled 5, leftover 3, mixed 4, olive 3, prepared 3, side 4, small 6, special 5, vegetable 3,  object-of: add 3, consume 1, dress 1, grow 1, harvest 2, have 20, like 5, love 1, mix 1, pick 1, place 3, prepare 4, return 3, rinse 1, season 1, serve 8, sprinkle 1, taste 1, test 1, toss 8, try 3,  figure 1.
excepts of entries in the collocation database for eat and salad  table 1  the top 20 most similar words of eat and salad as given by <tref>lin, 1998b</tref>.
w ord s imilar w ords  with similarity score  eat cook 0127, drink 0108, consume 0101, feed 0094, taste 0093, like 0092, serve 0089, bake 0087, sleep 0086, pick 0085, fry 0084, freeze 0081, enjoy 0079, smoke 0078, harvest 0076, love 0076, chop 0074, sprinkle 0072, toss 0072, chew 0072 salad soup 0172, sandwich 0169, sauce 0152, pasta 0149, dish 0135, vegetable 0135, cheese 0132, dessert 013, entree 0121, bread 0116, meat 0116, chicken 0115, pizza 0114, rice 0112, seafood 011, dressing 0109, cake 0107, steak 0105, noodle 0105, bean 0102 the collocation database for the words eat and salad  the database contains a total of 11 million unique dependency relationships.
each cluster corresponds to a sense of the headword.
thus, correct match of an argument corresponds to correct role identification.
the contextual preferences for h were constructed manually: the named-entity types for cpv:nh were set by adapting the entity types given in the guidelines to the types supported by the lingpipe ner described in section 32.
as a more natural ranking method, we also utilize scbc directly, denoted rankedcbc, having mv:er,t  scbcr,t.
first of all, it allows us to provide both positive and negative examples, avoiding the use of one-class classification algorithms that in practice perform poorly <ref>dagan et al , 2006</ref>.
second, the large availability of manually constructed substitution lexica, such as wordnet <ref>fellbaum, 1998</ref>, or the use of repositories based on statistical word similarities, such as the database constructed by <tref>lin 1998</tref>, allows us to find an adequate substitution lexicon for each target word in most of the cases.
2006 adapted the classical supervised wsd setting to approach the sense matching problem ie , the binary lexical entailment problem of deciding whether a word, such as position, entails a different word, such as job, in a given context by defining a one-class learning algorithm based on support vector machines svm.
however, in other research <ref>budanitsky and hirst 2001</ref>; <ref>patwardhan, banerjee, and pedersen 2003</ref>; <ref>mccarthy, koeling, and weeds 2004</ref>, it has been shown that the distance measure of <ref>jiang and conrath 1997</ref> referred to herein as the jc measure is a superior wordnet-based semantic similarity measure: wn dist jc w 1, w 2   max c 1 sw 1 c 2 sw 2  parenleftbigg max csupc 1 supc 2  2logc  log pc 1   log pc 2  parenrightbigg 50 in our work, we make an empirical comparison of neighbors derived using a wordnet-based measure and each of the distributional similarity measures using the technique discussed in section 3.
45 hindles <ref>measure hindle 1990</ref> proposed an mi-based measure, which he used to show that nouns could be reliably clustered based on their verb co-occurrences.
this expression is the same as the numerator in the expressions for precision and recall in the difference-weighted mi-based crm: p dw mi w 1, w 2   summationtext tp iw 1, c  miniw 1, c,iw 2, c iw 1, c summationtext fw 1  iw 1, c  summationtext tp miniw 1, c, iw 2, c summationtext fw 1  iw 1, c 39 r dw mi w 1, w 2   summationtext tp iw 2, c  miniw 2, c,iw 1, c iw 2, c summationtext fw 2  iw 2, c  summationtext tp miniw 2, c, iw 1, c summationtext fw 2  iw 2, c 40 since tp  tw 1   tw 2 .
further, the noun hyponymy hierarchy in wordnet, which will be used as a pseudo-gold standard for comparison, is widely recognized in this area of research.
we consider only a single grammatical relation because we believe that it is important to evaluate the usefulness of each grammatical relation in calculating similarity before deciding how to combine information from 5 this results in a single 80:20 split of the complete data set, in which we are guaranteed that the original relative frequencies of the target nouns are maintained.
33 evaluation of class attributes extraction parameters: given a target class specified as a set of instances and a set of five seed attributes for a class eg, quality, speed, number of users, market share, reliability for searchengine, the method described in section 22 extracts ranked lists of class attributes from the input query logs.
each attribute of the merged list is  0  02  04  06  08  1  0  10  20  30  40  50 precision rank class: holiday manually assembled instances automatically extracted instances  0  02  04  06  08  1  0  10  20  30  40  50 precision rank class: average-class manually assembled instances automatically extracted instances  0  02  04  06  08  1  0  10  20  30  40  50 precision rank class: mountain manually assembled instances automatically extracted instances  0  02  04  06  08  1  0  10  20  30  40  50 precision rank class: average-class manually assembled instances automatically extracted instances figure 3: accuracy of attributes extracted based on manually assembled, gold standard m vs automatically extracted e instance sets, for a few target classes leftmost graphs and as an average over all 37 target classes rightmost graphs.
distance-weighted averaging differs from distributional clustering in that it does not explicitly cluster words.
the choice of these two measures was motivated by work described in <ref>dagan, lee, and pereira 1999</ref>, in which the jensenshannon divergence outperforms related similarity measures such as the confusion probability or the l 1 norm on a pseudodisambiguation task that uses verb-object pairs.
the confusion probability has been used by several authors to smooth word co367 lapata the disambiguation of nominalizations occurrence probabilities <ref>essen and steinbiss 1992</ref>; <ref>grishman and sterling 1994</ref> and shown to give promising performance.
<ref>briefly, clark and weir 2002</ref> populate the wordnet hierarchy based on corpus frequencies of all nouns for a verb/slot pair, and then determine the appropriate probability estimate at each node in the hierarchy by using a24 a102 to determine whether to generalize an estimate to a parent node in the hierarchy.
we compare spd to other measures applied directly to the unpropagated probability profiles given by the clark-weir method: the probability distribution distance given by skew divergence skew <tref>lee, 1999</tref>, as well as the general vector distance given by cosine cos.
these are the measures aside from spd that performed best in our pilot experiments.
it is worth noting that the method of <ref>clark and weir 2002</ref> does not yield a tree cut, but instead generally populates the wordnet hierarchy with non-zero probabilities.
due to the original kl distance is asymmetric and is not defined when zero frequency occurs.
some enhanced kl models were developed to prevent these problems such as jensen-shannon <ref>jianhua, 1991</ref>, which introducing a probabilistic variable m, or  -skew divergence <tref>lee, 1999</tref>, by adopting adjustable variable .
research shows that skew divergence achieves better performance than other measures.
<ref>lee, 2001</ref> 1yxs rgencedskewdive yxxkl aaa  2/,2/yx,js shannon-djensen yxm myklmxkl   to convert distance to similarity value, we adopt the formula inspired by <ref>mochihashi, and matsumoto 2002</ref>.
the  constant is a value between 0 and 1 we also experimented with euclidian distance, the l1 norm, and cosine measures.
this choice of similarity measures was motivated by results of studies by <ref>levy et al 1998</ref> and <tref>lee 1999</tref> which compared several well known measures on similar tasks and found these three to be superior to many others.
another reason for this choice is that there are different ideas underlying these measures: while the jaccards coefficient is a binary measure, l1 and the skew divergence are probabilistic, the former being geometrically motivated and the latter being a version of the information theoretic kullback leibler divergence cf.
the optimal configuration varies by the divergence measure with d  50 and c  14 for kl divergence, d  200 and c  4 for symmetrised kl, and d  150 and c  2 for js divergence.
for a48 a49 a1a51a50a23a52a54a53a19a5 and word-conditional context distributions a55 and a56, we have the so-called a48 -divergences <ref>zhu and rohwer, 1998</ref>: a57 a58 a1a59a55a60a52a61a56a4a5a63a62a59a64 a53a65a14 a7 a55 a58 a56 a11a38a66 a58 a48a67a1a45a53a18a14a16a48a42a5 1 divergences a57 a68 and a57 a11 are defined as limits as a48a6a69 a50 and a48a6a69a70a53 :a57 a11 a1a59a55a60a52a61a56a4a5a71a64 a57 a68 a1a51a56a67a52a51a55a72a5a71a64a74a73 a55a76a75a78a77a47a79 a55 a56 in other words, a57 a11a19a1a59a55a60a52a61a56a4a5 is the kl-divergence of a55 from a56  members of this divergence family are in some sense preferred by theory to alternative measures.
it can be shown that the a48 -divergences or divergences defined by combinations of them, such as the jensen-shannon or skew divergences <tref>lee, 1999</tref> are the only ones that are robust to redundant contexts ie , only divergences in this family are invariant <ref>csiszar, 1975</ref>.
note that if any a56 a8 a64a80a50, then a57 a11a81a1a59a55a60a52a61a56a4a5 is infinite; in general, the kldivergence is very sensitive to small probabilities, and careful attention must be paid to smoothing if it is to be used with text co-occurrence data.
we do not know whether or to what extent this particular parameter setting is universally best, best only for english, best for newswire english, or best only for the specific test we have devised.
we have restricted our attention to a relatively small space of similarity measures, excluding many previously proposed measures of lexical affinity but see weeds, et al 2004, and <tref>lee 1999</tref> for some empirical comparisons.
lee observed that measures from the space of invariant divergences particularly the js and skew divergences perform at least as well as any of a wide variety of alternatives.
jensen-shannon is well defined for all distributions becausetheaverageofpi andqi isnon-zerowhenevereither number is these measures and others are surveyed in <ref>lee, 2001</ref>, who finds that jensen-shannon is outperformed by the skew divergence measure introduced by lee in 1999.
the skew divergence2 accounts for zeros in q by mixing in a small amount of p sp,q  dp bardbl q  1p  summationtexti pi log piqi1pi lee found that as   1, the performance of skew divergence on natural language tasks improves.
in particular, it outperforms most other models and even beats pure kl divergence modified to avoid zeros with sophisticated smoothing models.
one-noun of-prep his-det worst-adj of-prep all-det quality-noun of-prep the-det to-prep do-verb so-adverb in-prep the-det company-noun you-pronoun and-conj your-pronoun have-verb taken-verb the-det rest-noun of-prep us-pronoun are-verb at-prep least-adj but-conj if-prep you-pronoun as-prep a-det weapon-noun continue-verb to-to do-verb purpose-noun of-prep the-det could-modal have-verb be-verb it-pronoun seem-verb to-prep to-pronoun continue-verb to-prep have-verb be-verb the-det do-verb something-noun about-prep cause-verb you-pronoun to-to evidence-noun to-to back-adverb that-prep you-pronoun are-verb i-pronoun be-verb not-adverb of-prep the-det century-noun of-prep money-noun be-prep 291 wiebe, wilson, bruce, bell, and martin learning subjective language table 6 random sample of unique generalized collocations in op1.
however, due to the lack of a tight de nition for the concept of distributional similarity and the broad range of potential applications, a large number of measures of distributional similarity have been proposed or adopted see section 2.
the rst approach is not ideal since it assumes that the goal of distributional similarity methods is to predict semantic similarity and that the semantic resource used is a valid gold standard.
further, the second approach is clearly advantageous when one wishes to apply distributional similarity methods in a particular application area.
23 distributional kernels given the effectiveness of distributional similarity measures for numerous tasks in nlp and the interpretation of kernels as similarity functions, it seems natural to consider the use of kernels tailored for co-occurrence distributions when performing semantic classification.
650 distance definition derived linear kernel l2 distance2 summationtextcpcw1pcw22 summationtextc pcw1pcw2 l1 distance summationtextcpcw1pcw2 summationtextc minpcw1,pcw2 jensen-shannon summationtextc pcw1log2 2pcw1pcw1pcw2  summationtextc pcw1log2 pcw1pcw1pcw2  divergence pcw2log2 2pcw2pcw1pcw2 pcw2log2 pcw2pcw1pcw2 hellinger distance summationtextcradicalbigpcw1radicalbigpcw22 summationtextcradicalbigpcw1pcw2 table 1: squared metric distances on co-occurrence distributions and corresponding linear kernels were shown by lee to give better similarity estimates than the l2 distance.
those two measures have been previously shown to give promising performance for the task of estimating the frequencies of unseen verb-argument pairs <ref>dagan et al , 1999</ref>; <ref>grishman and sterling, 1994</ref>; <ref>lapata, 2000</ref>; <tref>lee, 1999</tref>.
those two measures have been previously shown to give promising performance for the task of estimating the frequencies of unseen verb-argument pairs <ref>dagan et al , 1999</ref>; <ref>grishman and sterling, 1994</ref>; <ref>lapata, 2000</ref>; <tref>lee, 1999</tref>.
the skew divergence represents a generalisation of the kullback-leibler divergence and was proposed by <tref>lee 1999</tref> as a linguistically motivated distance measure.
we use a value of   :99.
we explored in detail the influence of different types and sizes of context by varying the context specification and path value functions.
there are a number of studies that, starting from this hypothesis, have built automatic or semi-automatic procedures for clustering words <ref>brill and marcus, 1992</ref>; <ref>pereira et al , 1993</ref>; <ref>martin et al , 1998</ref>, especially in the field of cognitive sciences <ref>redington et al , 1998</ref>; <ref>gobet and pine, 1997</ref>; <ref>clark, 2000</ref>.
in <ref>brill and marcus, 1992</ref> it is given a semiautomatic procedure that, starting from lexical statistical data collected from a large corpus, aims to arrange target words in a tree more precisely a dendrogram, instead of clustering them automatically.
this procedure requires a linguistic examination of the resulting tree, in order to identify the word classes that are most appropriate to describe the phenomenon under investigation.
the formula is symmetric but does not satisfy the triangle inequality.
this shows that we have extracted a reasonable number of features for each phrase, since distributional similarity techniques have been shown to work well for words which occur more than 100 times in a given corpus <ref>lin, 1998</ref>; <ref>weeds and weir, 2003</ref>.
word similarity-based smoothing approach is used in our system to make advantage of the huge unlabeled corpus.
this is advantageous in the computation of similarity, since computing the sums over all co-occurrence types rather than just those co-occurring with at least one of the words is 1 very computationally expensive and 2 due to their vast number, the effect of these zero frequency co-occurrence types tends to outweigh the effect of those co-occurrence types that have actually occurred.
24 difference-weighted models in additive models, no distinction is made between features that have occurred to the same extent with each word and features that have occurred to different extents with each word.
further, the noun hyponymy hierarchy in wordnet, which will be used as a pseudo-gold standard for comparison, is widely recognized in this area of research.
we consider only a single grammatical relation because we believe that it is important to evaluate the usefulness of each grammatical relation in calculating similarity before deciding how to combine information from 5 this results in a single 80:20 split of the complete data set, in which we are guaranteed that the original relative frequencies of the target nouns are maintained.
a statistical technique using a language model that assigns a zero probability to these previously unseen events will rule the correct parse or interpretation of the utterance impossible.
similarity-based smoothing <ref>hindle 1990</ref>; <ref>brown et al 1992</ref>; <ref>dagan, marcus, and markovitch 1993</ref>; <ref>pereira, tishby, and lee 1993</ref>; <ref>dagan, lee, and pereira 1999</ref> provides an intuitively appealing approach to language modeling.
for example, in a speech recognition task, we might predict that cat is a more likely subject of growl than the word cap, even though neither co-occurrence has been seen before, based on the fact that cat is similar to words that do occur as the subject of growl eg , dog and tiger, whereas cap is not.
by using japanese html documents, we empirically show that our proposed method can obtain a significant number of hyponymy relations which would otherwise be missed by alternative methods.
most of these techniques have relied on particular linguistic patterns, such as np such as np the frequencies of use for such linguistic patterns are relatively low, though, and there can be many expressions that do not appear in such patterns even if we look at large corpora.
the effort of searching for other clues indicating hyponymy relations is thus significant.
another line of research, which is more closely related to the current study, is to extend existing thesauri by classifying new words with respect to their given structures eg <ref>tokunaga et al, 1997</ref>; <ref>pekar, 2004</ref>.
an early effort along this line is <ref>hearst 1992</ref>, who attempted to identify hyponyms from large text corpora, based on a set of lexico-syntactic patterns, to augment and critique the content of wordnet.
in contrast, in this paper we focus on the problem of determining the categories of interest.
another thread of work is on finding synonymous terms and word associations, as well as automatic acquisition of is-a or genus-head relations from dictionary definitions and free text <ref>hearst, 1992</ref>; <tref>caraballo, 1999</tref>.
that work focuses on finding the right position for a word within a lexicon, rather than building up comprehensible and coherent faceted hierarchies.
one way to overcome this problem might be to give judges information about a sequence of higher ancestors, in order to make the judgement easier.
<ref>charniak  roark 1998</ref>, evaluating the semantic lexicon against gold standard resources the muc-4 and the wsj corpus, reports that the ratio of valid to total entries for their system lies between 20 and 40.
such hierarchical examples are quite sparse, and greater coverage was later attained by <ref>riloff and shepherd 1997</ref> and <ref>roark and charniak 1998</ref> in extracting relations not of hierarchy but of similarity, by finding conjunctions or co-ordinations such as cloves, cinammon, and nutmeg and cars and trucks this work was extended by <tref>caraballo 1999</tref>, who built classes of related words in this fashion and then reasoned that if a hierarchical relationship could be extracted for any member of this class, it could be applied to all members of the class.
this technique can often mistakenly reason across an ambiguous middle-term, a situation that was improved upon by <ref>cederberg and widdows 2003</ref>, by combining pattern-based extraction with contextual filtering using latent semantic analysis.
semantic word learning is different from subjective word learning, but we have shown that metabootstrapping and basilisk could be successfully applied to subjectivity learning.
this includes the extraction of hyponymy and synonymy relations <ref>hearst 1992</ref>; <tref>caraballo 1999</tref>, among others as well as meronymy <ref>berland and charniak 1999</ref>; <ref>meyer 2001</ref>.
10 one approach to the extraction of instances of a particular lexical relation is the use of patterns that express lexical relations structurally explicitly in a corpus <ref>hearst 1992</ref>; <ref>berland and charniak 1999</ref>; <tref>caraballo 1999</tref>; <ref>meyer 2001</ref>, and this is the approach we focus on here.
this includes the extraction of hyponymy and synonymy relations <ref>hearst 1992</ref>; <tref>caraballo 1999</tref>, among others as well as meronymy <ref>berland and charniak 1999</ref>; <ref>meyer 2001</ref>.
10 one approach to the extraction of instances of a particular lexical relation is the use of patterns that express lexical relations structurally explicitly in a corpus <ref>hearst 1992</ref>; <ref>berland and charniak 1999</ref>; <tref>caraballo 1999</tref>; <ref>meyer 2001</ref>, and this is the approach we focus on here.
the literature on automated text categorization is enormous, but assumes that a set of categories has already been created, whereas the problem here is to determine the categories of interest.
there has also been extensive work on finding synonymous terms and word associations, as well as automatic acquisition of is-a or genus-head relations from dictionary definitions and glosses <ref>klavans and whitman, 2001</ref> and from free text <ref>hearst, 1992</ref>; <tref>caraballo, 1999</tref>.
in this paper, we evaluate some existing similarity metrics and propose and motivate a new metric which outperforms the existing metrics.
the advantage of clustering approaches is that they permit algorithms to identify is-a relations that do not explicitly appear in text, however they generally fail to produce coherent clusters from fewer than 100 million words; hence they are unreliable for small corpora.
in section 3, we show how latent semantic analysis can be used to filter potential relationships according to their semantic plausibility.
in section 4, we show how correctly extracted relationships can be used as seed-cases to extract several more relationships, thus improving recall; this work shares some similarities with that of <tref>caraballo 1999</tref>.
in section 5 we show that combining the techniques of section 3 and section 4 improves both precision and recall.
4 improving recall using coordination information one of the main challenges facing hyponymy extraction is that comparatively few of the correct relations that might be found in text are expressed overtly by the simple lexicosyntactic patterns used in section 2, as was apparent in the results presented in that section.
first of all, it would be interesting to apply lsa to a system for building an entire hypernym-labelled ontology in roughly the way described in <tref>caraballo, 1999</tref>, perhaps by using an lsa-weighted voting method to determine which hypernym would be used to label each node.
also, systematic comparison of the lexicosyntactic patterns used for extraction to determine the relative productiveness and accuracy of each pattern might prove illuminating, as would comparison across different corpora to determine the impact of the topic area and medium/format of documents on the effectiveness of hyponymy extraction.
this project is meant to provide a tool to support other methods.
2 previous work to the best of our knowledge, this is the first attempt to automatically rank nouns based on specificity.
our disposal, wordnet <ref>fellbaum, 1998</ref> contains very little information that would be considered as being about attributesonly information about parts, not about qualities such as height, or even to the values of such attributes in the adjective networkand this information is still very sparse.
2 the work discussed here could be perhaps best described as an example of empirical ontology: using linguistics and philosophical ideas to improve the results of empirical work on lexical / ontology acquisition, and vice versa, using findings from empirical analysis to question some of the assumptions of theoretical work on ontology and the lexicon.
my analysis of the sinica corpus shows that contrary to expectation, most of unknown words in chinese are common nouns, adjectives, and verbs rather than proper nouns.
while context is clearly an important feature, this paper focuses on non-contextual features, which may play a key role for unknown words that occur only once and hence have limited context.
another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes eg, <tref>caraballo, 1999</tref>; <ref>cimiano and volker, 2005</ref>; <ref>mann, 2002</ref>, and learning semantic relations such as meronymy <ref>berland and charniak, 1999</ref>; <ref>girju et al, 2003</ref>.
weakly supervised learning methods for semantic lexicon generation have utilized co-occurrence statistics <ref>riloff and shepherd, 1997</ref>; <ref>roark and charniak, 1998</ref>, syntactic information <ref>tanev and magnini, 2006</ref>; <ref>pantel and ravichandran, 2004</ref>; <ref>phillips and riloff, 2002</ref>, lexico-syntactic contextual patterns eg, resides in <location> or moved to <location> <ref>riloff and jones, 1999</ref>; <ref>thelen and riloff, 2002</ref>, and local and global contexts <ref>fleischman and hovy, 2002</ref>.
the features are shown with hidden variables corresponding to wordspecific hidden values, such as shares1 or bought3.
however, we did not find a significant difference between the performance of either method.
mainstream approaches in statistical parsing are based on nondeterministic parsing techniques, usually employing some kind of dynamic programming, in combination with generative probabilistic models that provide an n-best ranking of the set of candidate analyses derived by the parser <ref>collins, 1997</ref>; <ref>collins, 1999</ref>; <ref>charniak, 2000</ref>.
these parsers can be enhanced by using a discriminative model, which reranks the analyses output by the parser <tref>johnson et al , 1999</tref>; <ref>collins and duffy, 2005</ref>; <ref>charniak and johnson, 2005</ref>.
a radically different approach is to perform disambiguation deterministically, using a greedy parsing algorithm that approximates a globally optimal solution by making a sequence of locally optimal choices, guided by a classifier trained on gold standard derivations from a treebank.
maximum-entropy markov models <ref>mccallum et al , 2000</ref> and stochastic unification-based grammars <tref>johnson et al , 1999</tref> are standardly estimated with conditional estimators, and it would be interesting to know whether conditional estimation affects the quality of the estimated model.
it should be noted that in practice, the mcle of a model with a large number of features with complex dependencies may yield far better performance than the mle of the much smaller model that could be estimated with the same computational effort.
nevertheless, as this paper shows, conditional estimators can be used with other kinds of models besides maxent models, and in any event it is interesting to ask whether the mle differs from the mcle in actual applications, and if so, how.
therefore there are a large number of features available that could be used by stochastic models for disambiguation.
other researchers have worked on extracting features useful for disambiguation from unification grammar analyses and have built log linear models aka stochastic unification based grammars <tref>johnson et al , 1999</tref>; <ref>riezler et al , 2000</ref>.
one is for a simple model with a relatively small number of features, and the other is for a model with a large number of features.
the usefulness of priors in maximum entropy models is not new to this work: gaussian prior smoothing is advocated in <ref>chen and rosenfeld 2000</ref>, and used in all the stochastic lfg work <tref>johnson et al , 1999</tref>.
however, until recently, its role and importance have not been widely understood.
for example, <ref>zhang and oles 2001</ref> attribute the perceived limited success of logistic regression for text categorization to a lack of use of regularization.
the data for estimation consists of pairs of original sentences y and goldstandard summarized f-structures s which were manually selected from the transfer output for each sentence.
for training data sj,yjmj1 and a set of possible summarized structures sy for each sentence y, the objective was to maximize a discriminative criterion, namely the conditional likelihood l of a summarized f-structure given the sentence.
in global linear models glms for structured prediction, eg, <tref>johnson et al, 1999</tref>; <ref>lafferty et al, 2001</ref>; <ref>collins, 2002</ref>; <ref>altun et al, 2003</ref>; <ref>taskar et al, 2004</ref>, the optimal label y for an input x is y  arg max yyx w fx,y 1 where yx is the set of possible labels for the input x; fx,y  rd is a feature vector that represents the pair x,y; and w is a parameter vector.
this paper describes a glm for natural language parsing, trained using the averaged perceptron.
a crucial advantage of our approach is that it considers a very large set of alternatives in yx, and can thereby avoid search errors that may be made in the first-pass parser1 another approach that allows efficient training of glms is to use simpler syntactic representations, in particular dependency structures mcdon1some features used within reranking approaches may be difficult to incorporate within dynamic programming, but it is nevertheless useful to make use of glms in the dynamicprogramming stage of parsing.
moreover, property design can be carried out in a targeted way, ie properties can be designed in order to improve the disambiguation of grammatical relations that, so far, are disambiguated particularly poorly or that are of special interest for the task that the systems output is used for.
by demonstrating that property design is the key to good log-linear models for deepsyntactic disambiguation, our work confirms that specifying the features of a subg stochastic unification-based grammar is as much an empirical matter as specifying the grammar itself<tref>johnson et al , 1999</tref>.
furthermore, i thank the audiences at several pargram meetings, at the research workshop of the israel science foundation on large-scale grammar development and grammar engineering at the university of haifa and at the sfb 732 opening colloquium in stuttgart for their important feedback on earlier versions of this work.
firstly, the rudimentary character of functional annotations in standard treebanks has hindered the direct use of such data for statistical estimation of linguistically fine-grained statistical parsing systems.
furthermore, the effort involved in coding broadcoverage grammars by hand has often led to the specialization of grammars to relatively small domains, thus sacrificing grammar coverage ie the percentage of sentences for which at least one analysis is found on free text.
one appeal of these methods is their flexibility in incorporating features into a model: essentially any features which might be useful in discriminating good from bad structures can be included.
a second appeal of these methods is that their training criterion is often discriminative, attempting to explicitly push the score or probability of the correct structure for each training sentence above the score of competing structures.
as expected, we observed that the regularization term increases the accuracy, especially when the training data is small; but we did not observe much difference when we used different regularization terms.
our goal in this paper is not to build the best tagger or recognizer, but to compare different loss functions and optimization methods.
since we did not spend much effort on designing the most useful features, our results are slightly worse than, but comparable to the best performing models.
this method generates 50-best lists that are of substantially higher quality than previously obtainable.
we used these parses as the input to a maxent reranker <tref>johnson et al , 1999</tref>; <ref>riezler et al , 2002</ref> that selects the best parse from the set of parses for each sentence, obtaining an f-score of 910 on sentences of length 100 or less.
we describe a reranking parser which uses a regularized maxent reranker to select the best parse from the 50-best parses returned by a generative parsing model.