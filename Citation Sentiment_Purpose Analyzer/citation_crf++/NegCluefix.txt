in practice, computational limitations do not allow the enumeration of all possible assignments for long sentences, and smoothing is required for infrequent events.
although more sophisticated algorithms for unsupervised learning which can be trained on plain text instead on manually tagged corpora are well established see eg <ref>merialdo, 1994</ref>, we decided not to use them.
the main reason is that with large tag sets, the sparse-data-problem can become so severe that unsupervised training easily ends up in local minima, which can lead to poor results without any indication to the user.
the problem with this framework, however, is that such reliable corpora are hardly awdlable duc to a huge amount of the labor-intensive work required.
in case of the acquisition of non-core knowledge, such as specific, lexically or dolnain dependent knowledge, preparation of annotated corpora becomes more serious problem.
simplex or complex nps eg , <tref>church 1988</tref>; <ref>hindle and rooth 1991</ref>; <ref>wacholder 1998</ref> identify simplex or base nps  nps which do not have any component nps -at least in part because this bypasses the need to solve the quite difficult attachment problem, ie, to determine which simpler nps should be combined to output a more complex np.
but if people find complex nps more useful than simpler ones, it is important to focus on improvement of techniques to reliably identify more complex terms.
research efforts at ibm chodorow, et al 1988; neff, et al 1989, bell labs church, et al 1989, new mexico state university <ref>wilks 1987</ref>, and elsewhere have used mechanical processing of on-line dictionaries to infer at least minimal syntactic and semantic information from dictionary definitions.
in order to make these improvements, we need access to word-class information pos information <ref>johansson et al 1986</ref>; <ref>black, garside, and leech 1993</ref> or semantic information <ref>beckwith et al 1991</ref>, which is usually obtained in three main ways: firstly, we can use corpora that have been manually tagged by linguistically informed experts <ref>derouault and merialdo 1986</ref>.
secondly, we can construct automatic part-ofspeech taggers and process untagged corpora <ref>kupiec 1992</ref>; <ref>black, garside, and leech 1993</ref>; this method boasts a high degree of accuracy, although often the construction of the automatic tagger involves a bootstrapping process based on a core corpus which has been manually tagged <tref>church 1988</tref>.
if an external resource is used in the form of a morphological analyzer ma, this will almost always overgenerate, yielding false ambiguity.
but even if the ma is tight, a considerable proportion of ambiguous tokens will come from legitimate but rare analyses of frequent types <tref>church, 1988</tref>.
for example the word nem, can mean both not and gender, so both adv and noun are valid analyses, but the adverbial reading is about five orders of magnitude more frequent than the noun reading, 12596 vs 4 tokens in the 1 m word manually annotated szeged korpusz <ref>csendes et al , 2004</ref>.
thus the difficulty of the task is better measured by the average information required for disambiguating a token.
as far as coverage is concerned, our parser can handle recursive structures, which is an advantage compared to simpler techniques such as that described by <tref>church 1988</tref>.
on the other hand, the markov assumption underlying our approach means that only strictly local dependencies are recognised.
for full parsing, one would probably need non-local contextual information, such as the long-range trigrams in link grammar della <ref>pietra et al , 1994</ref>.
we rewrite this term as follows: prw1,nd1,n n  i-iprwidilwl,ildl,il i1 n  l-i prwilwl,i-ldl,i prdilwl,i-ldl,i-1 i1 7 equation 7 involves two probability distributions that need to be estimated.
prwiiwl,ildl,i  prwildi 8 prdiiwuldl,il  prdiidul 9 however, to successfully incorporate pos information, we need to account for the full richness of the probability distributions, as will be demonstrated in section 344.
in practice, computational limitations do not allow the enumeration of all possible assignments for long sentences, and smoothing is required for infrequent events.
although more sophisticated algorithms for unsupervised learning which can be trained on plain text instead on manually tagged corpora are well established see eg <ref>merialdo, 1994</ref>, we decided not to use them.
the main reason is that with large tag sets, the sparse-data-problem can become so severe that unsupervised training easily ends up in local minima, which call lead to poor results without any indication to the user.
we redistribute the probability mass of low count sequences to unseen sequences.
generalized forward backward reestimation generalization of the forward and viterbi algorithm in english part of speech taggers, the maximization of equation 1 to get the most likely tag sequence, is accomplished by the viterbi algorithm <tref>church, 1988</tref>, and the maximum likelihood estimates of the parameters of equation 2 are obtained from untagged corpus by the forwardbackward algorithm <ref>cutting et al , 1992</ref>.
however, it is impossible to apply the viterbi algorithm and the forward-backward algorithm for word segmentation of those languages that have no delimiter between words, such as japanese and chinese, because word segmentation hypotheses overlap one another.
the computational tools available for studying machinereadable corpora are at present still rather primitive.
there is very little interactive software.
this method is not applicable to arabic and hebrew, which lack typographical marking of proper nouns.
the lexical distribution of grammatical knowledge one finds in many lexiealized grammar formalisms eg , ltags <tref>schabes et al , 1988</tref> or hpsg <ref>pollard  sag, 1994</ref> is still constrained to declarative notions.
given that the control flow of text understanding is globally unpredictable and, also, needs to be purposefully adapted to critical states of the analysis eg , cases of severe extragrammaticality, we drive lexicalization to its limits in that we also incorporate procedural control knowledge at the lexical gr,unmar level.
which cannot be felicitously uttered except in a context where there is something in the discourse that a restriction could apply to.
handling all the possible variations in complement distribution in such formalisms inevitably leads to an explosion in the number of such frames, and a correspondingly more difficult task in porting to a new domain.
other lexicalized grammars collapse syntactic and ordering information and are forced to represent ordering alternatives by lexical ambiguity, most notable l-tag <tref>schabes et al , 1988</tref> and some versions of cg <ref>hepple, 1994</ref>.
this is not necessary in our approach, which drastically reduces the search space for parsing.
various parsing techniques have been developed for lexicalized grammars such as lexicalized tree adjoining grammar ltag <tref>schabes et al , 1988</tref>, and head-driven phrase structure grammar hpsg <ref>pollard and sag, 1994</ref>.
54 29 3 43 40 4 97 69 7 these results are remarkably good, in spite of the fact that many other systems are reported to reach an accuracy of 9697.
those systems, however, all use heavier artillery than morp, that has been deliberately restricted in accordance with the hypotheses presented above.
this restrictiveness concerns both the size of the lexicon and the ways of carrying out disambiguation.
2 <tref>moens and steedman 1988</tref> also use this term, but they restrict it to momentaneous events.
unfortunately, the terminology used in the literature for these kinds of categories varies so much that a standardization seems out of reach.
although there have been quite a few studies on individual aspects of sentence planning, little attention has been paid to the interaction between the various tasks--exceptions are <ref>rambow and korelsky 1992</ref> and <ref>wanner and hovy 1996</ref>--and in particular to the role of marker choice in the overall sentence planning process.
there exists a large body of research in nlu on analysing the temporal structure of texts, including the role of temporal markers, though again restricted to english <tref>moens and steedman 1988</tref>; <ref>lascarides and oberlander 1993</ref>; <ref>hitzeman et al 1995</ref>.
in order to solve the nontermination problem, <tref>shieber 1985</tref> proposes restrictor, a statically predefined set of features to consider in propagation, and restriction, a filtering function which removes the features not in restrictor from top-down expectation.
however, not only does this approach fail to provide a method to automatically generate the restrictor set, it may weaken the predicative power of top-down expectation more than necessary: a globally defined restrictor can only specify the least common features for all propagation paths.
manual detection is also problematic: when a grammar is large, particularly if semantic features are included, complete detection is nearly impossible.
however, this severely restricts the range of possible instantiations of shiebers algorithm.
7 another possibility is to filter out problematic features in the prediction step by using the function p however, automatic detection of such features ie , automatic derivation of p is undecidable for the same reason as the prediction nontermination problem caused by left recursion for unification grammars <tref>shieber 1985</tref>.
manual detection is also problematic: when a grammar is large, particularly if semantic features are included, complete detection is nearly impossible.
it is more so because 1 there is no restriction such as that there should be only one zero morpheme within an s clause, and 2 the stack is useless because zero morphemes are independent morphemes and are not bound to other morphemes comparable to wh-words.
<tref>shieber 1985</tref> proposes a more efficient approach to gaps in the patr-ii formalism, extending earleys algorithm by using restriction to do top-down filtering.
even if goal weakening is reminiscent of shiebers 1985 restriction operator, the rules of the game are quite different: in the case of goal weakening, as much information as possible is removed without risking nontermination of the parser, whereas in the case of shiebers restriction operator, information is removed until the resulting parser terminates.
for the current version of the grammar of ovis, weakening the goal category in such a way that all information below a depth of 6 is replaced by fresh variables eliminates the problem caused by the absence of the occur check; moreover, this goal-weakening operator reduces parsing times substantially.
however, their particular realization of the technique is severely restricted for nlp applications, since it uses a deterministic one-path lr algorithm, applicable only to semantically unambiguous grammars.
this does not deny that compilation methods may be able to convert a grammar into a program that generates without termination problems.
but even this ad hoc solution is problematic, as there may be no principled bound on the size of the subcategorization list.
for instance, in analyses of dutch cross-serial verb constructions <ref>evers 1975</ref>; <ref>huybrechts 1984</ref>, subcategorization lists may be concatenated by syntactic rules moort32 computational linguistics volume 16, number 1, <ref>march 1990</ref> shieber et al semantic head-driven grammar gat 1984; <ref>fodor et al 1985</ref>; <ref>pollard 1988</ref>, resulting in arbitrarily long lists.
subcategorization lists under this analysis can have any length, and it is impossible to predict from a semantic structure the size of its corresponding subcategorization list merely by examining the lexicon.
<ref>whereas haas 1989</ref> found that top-down filtering never helps to actually decrease parse times in a bottom-up parser, we have found at least one example german where top-down filtering is useful.
24 top-down predictive linking the aim of our proposal is to define equivalence relations that keep the linking relation finite while also preventing it from being too restrictive; this turns the linking relation into a weakpredietion table in the sense of haas 1989: 227ff.
like shieber 1985, 1992 with the notion of restriction, we confine our attention to a subset of specifications; in particular, we can define a feature structure that subsumes all vp-type feature structures of shiebers recursive subcategorization rules.
whatever term one takes, an important aspect of the relation is that it can be used to reduce the search space of possible syntactic analyses at an earlier point in parsing and thus serves to improve the efficiency of a parser.
his central notion of restriction, whereby a restrictor is a finite subset of the paths specified in a feature structure, is related to the technique we introduce here, since both guarantee the finiteness of an otherwise possibly infinite domain of complex categories, but shiebers restrictors are specified manually.
this requirement places a greater load on the grammar writer and is inconsistent with most recent unification-based grammar formalisms, which represent grammatical categories entirely as feature bundles eg <ref>gazdar et al 1985</ref>; <ref>pollard and sag 1987</ref>; <ref>zeevat, calder, and klein 1987</ref>.
in addition, it violates the principle that grammatical formalisms should be declarative and defined independently of parsing procedure, since different definitions of the cf portion of the grammar will, at least, effect the efficiency of the resulting parser and might, in principle, lead to nontermination on certain inputs in a manner similar to that described by <tref>shieber 1985</tref>.
in what follows, we will assume that the unification-based grammars we are considering are represented in the anlt object grammar formalism <ref>briscoe et al 1987</ref>.
problems in the prediction step of the earley parser used for unification-based formalisms no longer exist.
the use of restrictors as proposed by <tref>shieber 1985</tref> is no longer necessary and the difficulties caused by treating subcategorization as a feature is no longer a problem.
by assuming that the number of structures associated with a lexical item is finite, since each structure has a lexical item attached to it, we implicitly make the assumption that an input string of finite length cannot be syntactically infinitely ambiguous.
however, these models are inadequate for language interpretation, since they cannot express the relevant syntactic and semantic regularities.
augmented phrase structure grammar apsg formalisms, such as unification-based grammars <tref>shieber, 1985a</tref>, can express many of those regularities, but they are computationally less suitable for language modeling, because of the inherent cost of computing state transitions in apsg parsers.
the above problems might be circumvented by using separate grammars for language modeling and language interpretation.
ideally, the recognition grammar should not reject sentences acceptable by the interpretation grammar and it should contain as much as reasonable of the constraints built into the interpretation grammar.
one of the most unusual features of centering theory is that the notions of utterance, previous utterance, ranking, and realization used in the definitions above are left unspecified, to be appropriately defined on the basis of empirical evidence, and possibly in a different way for each language.
as a result, centering theory is best viewed as a cluster of theories, each of which specifies the parameters in a different ways: eg, ranking has been claimed to depend on grammatical function <ref>kameyama, 1985</ref>; <tref>brennan et al , 1987</tref>, on thematic roles <ref>cote, 1998</ref>, and on the discourse status of the cfs <ref>strube and hahn, 1999</ref>; there are at least two definitions of what counts as previous utterance <ref>kameyama, 1998</ref>; <ref>suri and mccoy, 1994</ref>; and realization can be interpreted either in a strict sense, ie, by taking a cf to be realized in an utterance only if an np in that utterance denotes that cf, or in a looser sense, by also counting a cf as realized if it is referred to indirectly by means of a bridging reference <ref>clark, 1977</ref>, ie, an anaphoric expression that refers to an object which wasnt mentioned before but is somehow related to an object that already has, as in the vase the handle see, eg, the discussion in <ref>grosz et al , 1995</ref>; <ref>walker et al , 1998b</ref>.
3 methods the fact that so many basic notions of centering theory do not have a completely specified definition makes empirical verification of the theory rather difficult.
because any attempt at directly annotating a corpus for utterances and their cbs is bound to force the annotators to adopt some specification of the basic notions of the theory, previous studies have tended to study a particular variant of the theory <ref>di eugenio, 1998</ref>; <ref>kameyama, 1998</ref>; <ref>passonneau, 1993</ref>; <ref>strube and hahn, 1999</ref>; <ref>walker, 1989</ref>.
performance is particularly low with possessive pronouns which often only have antecedents in the current sentence.
although they report that their method estimates over 90 of zero subjects correctly, there are several difficulties including the fact that the test corpus is identical with the corpus from which the pragmatic constraints are extracted, and the fact that there are so many rules46 rules to estimate 175 sentences.
this distinction underlies my proposals about the attentional consequences of pitch accents when applied to pronominals, in particular, that while most pitch accents may weaken or reinforce a cospecifiers status as the center of attention, a contrastively stressed pronominal may force a shift, even when contraindicated by textual features.
processing complex sentences: a reason for extending focusing algorithms although complex sentences are prevalent in written english, most other local focusing research focusing: sidner 1979 and carter 1987; centering: grosz, joshi, and weinstein 1983, 1995, brennan, friedman, and pollard 1987, walker 1989, 1993, kameyama 1986 2, walker, iida, and cote 1994, brennan 1998, kameyama, passonneau, and poesio 1993, linson 1993 and hoffman 1998; and pundit: dahl 1986, palmer et al 1986, and dahl and ball 1990 did not explicitly and/or adequately address how to process complex sentences.
thus, there is a need to extend focusing algorithms.
however, many of these models are restricted by their simplifying assumption of communication via a command language.
since the constraints are eflective in the lifferent target from ours, the accuracy of identifying the referents of zero pronouns would be improved much more by using both of his constraints and the constraint we proposed.
although this kind of theory has a good point that it is independent of the type o17 discourse, the linguistic constraints specitic to expressions like the pragmatic constraints l/roposed by dohsaka or us are more accurate than theirs when the speeitlc constraints are applicable.
the advantages of this new model, the left-right centering algorithm lrc, lie in its incremental processing of utterances and in its low computational overhead.
a second motivation for the project is to remedy the dearth of empirical results on pronoun resolution methods.
the fundamental problem is that any of the factorial number of permutations of the sentences could be the optimal discourse, which makes for a formidable search space for nontrivial discourses.
in preliminary experiments it also led to slightly better results than the conditional probability measure.
as in <tref>church and hanks 1989</tref>, the words can appear in any order and they can be separated by an arbitrary number of other words.
this limitation is intrinsic to the technique used since mutual information scores are defined for two items.
such collocations are not of interest for our purpose, although they could be useful for disambiguation or other semantic purposes.
these methods are more appropriate for retrieving bound compositions, while less appropriate for retrieving free ones.
in fact, corpus linguistics became a popular research field because of the claim that shallow techniques could overcome the lexical coverage bottleneck of traditional nlp techniques.
among the applications of collocational analysis for lexical acquisition are: the derivation of syntactic disambiguation cues <ref>basili et al 1991, 1993a</ref>; <ref>hindle and rooths 1991</ref>,1993; <ref>sekine 1992</ref> <ref>bogges et al 1992</ref>, sense preference <ref>yarowski 1992</ref>, acquisition of selectional restrictions <ref>basili et al 1992b, 1993b</ref>; <ref>utsuro et al 1993</ref>, lexical preference in generation <ref>smadjia 1991</ref>, word clustering <ref>pereira 1993</ref>; <tref>hindle 1990</tref>; <ref>basili et al 1993c</ref>, etc in the majority of these papers, even though the precedent or subsequent statistical processing reduces the number of accidental associations, very large corpora 10,000,000 words are necessary to obtain reliable data on a large enough number of words.
in addition, most papers produce a performance evaluation of their methods but do not provide a measure of the coverage, ie the percentage of cases for which their method actually provides a right or wrong solution.
or the cooccurrence of two words within a limited distance in the context.
a major problem for the above applications is how to estimate the probability of cooccurrences that were not observed in the training corpus.
however, dealing with already linguistic filtered data, <ref>assadi, 1997</ref> aims at statistically build rough clusters supposing that similar candidate terms have similar expansions.
although they have been successful in acquiring related words, various parameters such as similarity measures and weighting are involved.
most of these approaches, however, need large or even very large corpora in order for word classes to be discovered 1 whereas it is often the case that the data to be processed are insufficient to provide reliable lexical intbrmation.
probably the most widely used association weight function is point-wise mutual information mi <ref>church et al , 1990</ref>, <tref>hindle, 1990</tref>, <ref>lin, 1998</ref>, <ref>dagan, 2000</ref>, defined by:  ,log, 2 fpwp fwpfwmi  a known weakness of mi is its tendency to assign high weights for rare features.
since a handmade thesaurus is not slfitahle for machine use, and expensive to compile, automatical construction ofa thesaurus has been attempted using corpora <tref>hindle, 1990</tref>.
more recent papers <tref>hindle 1990</tref>, <ref>pereira and tishby 1992</ref> proposed to cluster nouns on the basis of a metric derived from the distribution of subject, verb and object in the texts.
both papers use as a source of information large corpora, but differ in the type of statistical approach used to determine word similarity.
these studies, though valuable, leave several open problems: 70 1 a metric of conceptual closeness based on mere syntactic similarity is questionable, particularly if applied to verbs.
the problem of the methods is that the precision of the extracted synonymous words is low because it extracts many word pairs such as cat and dog, which are similar but not synonymous.
the underlying idea is based largely on the central claim of the distributional hypothesis <ref>harris 1968</ref>, that is: the meaning of entities, and the meaning of grammatical relations among them, is related to the restriction of combinations of these entities relative to other entities.
there are inherent problems in evaluating automatic thesaurus extraction techniques, and much research assumes a gold standard that does not exist see kilgarriff 2003 and weeds 2003 for more discussion of this.
a further problem for distributional similarity methods for automatic thesaurus generation is that they do not offer any obvious way to distinguish between linguistic relations such as synonymy, antonymy, and hyponymy see caraballo 1999 and lin et al 2003 for work on this.
although a number of automatic sentence alignment methods have been proposed <tref>brown et al 1991</tref> ; <ref>gale  church 1991</ref> b; <ref>kay  roscheisen 1993</ref>; <ref>chen 1993</ref>, they are not very reliable for real noisy bilingual texts.
alignment algorithms assume the availability of text unit boundary information and their output has less expressive power than a general bitext map.
the only published solution to the more difficult general bitext mapping problem <ref>church 1993</ref> can err by several typeset pages.
however, these algorithms can fumble in bitext sections that contain many sentences of very similar length, like this vote record: english french mr mcinnis.
due to the noisy nature of the web documents, parallel web pages may consist of non-translational content and many out-of-vocabulary words, both of which reduce sentence alignment accuracy.
on clean inputs, such as the canadian hansards, these methods have been very successful at least 96 correct by sentence.
unfortunately, if the input is noisy due to ocr and/or unknown markup conventions, then these methods tend to break down because the noise can make it difficult to find paragraph boundaries, let alone sentences.
<ref>as chen 1993</ref> points out, dynamic programming is particularly susceptible to deletions occurring in one of the two languages.
although it has been suggested that lengthbased methods are language-independent <ref>gale  church 1991</ref>; <tref>brown et al 1991</tref>, they may in fact rely to some extent on length correlations arising from the historical relationships of the languages being aligned.
although it has been suggested that lengthbased methods are language-independent <ref>gale  church 1991</ref>; <tref>brown et al 1991</tref>, they may in fact rely to some extent on length correlations arising from the historical relationships of the languages being aligned.
however, sentence alignment <tref>brown et al 1991</tref>; kay  r<ref>sscheisen 1993</ref>; <ref>gale  <ref>church 1993</ref></ref>; <ref>church 1993</ref>; <ref>chen 1993</ref>; <ref>wu 1994</ref> is not always practical when corpora have unclear sentence boundaries or with noisy text segments present in only one language.
therefore, sentence mapping algorithms need not worry about crossing correspondences.
soon thereafter, <ref>church 1993</ref> found that bitext mapping at the sentence level is not an option for noisy bitexts found in the real world.
sentences are often difficult to detect, especially where punctuation is missing due to ocr errors.
sent fair although there has been some previous work on the sentence alignment, eg, <tref>brown, lai, and mercer, 1991</tref>, <ref>kay and rtscheisen, 1988</ref>, catizone et al , to appear, the alignment task remains a significant obstacle preventing many potential users from reaping many of the benefits of bilingual corpora, because the proposed solutions are often unavailable, unreliable, and/or computationally prohibitive.
past statistical methods for non-parallel corpora <ref>fung and yee, 1998</ref> are not valid for finding translations of words or expressions with low frequency.
this paper describes a new program, called wordalign, that starts with an initial rough alignment eg , the output of charalign or a sentence-based alignment method, and produces improved alignments by exploiting constraints at the word-level.
or the cooccurrence of two words within a limited distance in the context.
a major problem for the above applications is how to estimate the probability of cooccurrences that were not observed in the training corpus.
we use the definition of lexical likelihood described above to avoid this problem.
the attachment sites would be determined to be the same, however, if we were to use two-word probabilities c:f<ref>resnik, 1993</ref>, and thus the ambiguity of only one of the sentences can be resolved.
there have been a number of methods proposed to perform structural disambiguation using probability models, many of which have proved to be quite effective <ref>alshawi and carter, 1995</ref>; <ref>black et al , 1992</ref>; briscoe and carroll.
although each of the disambiguation methods proposed to date has its merits, none resolves the disambiguation problem completely satisfactorily.
thus our problem involves the following three subproblems: a resolving structural ambiguities based on lpr in terms of probabilistic representations, b resolving structural ambiguities based on rap and alpp in terms of probabilistic representations, and c combining the two.
though several studies with similar objectives have been reported <ref>church, 1988</ref>, <ref>zernik and jacobs, 1990</ref>, <ref>calzolari and bindi, 1990</ref>, <ref>garside and leech, 1985</ref>, <tref>hindle and rooth, 1991</tref>, <ref>brown et al , 1990</ref>, they require that sample corpora be correctly analyzed or tagged in advance.
because their preparation needs a lot of manual assistance or an unerring tagger or parser, this requirement makes their algorithm, troublesome in actual application environments.
in our experiments, described below, we compare the performance of our proposed method, which we refer to as mdl, against the methods proposed by <tref>hindle and rooth 1991</tref>, <ref>resnik 1993b</ref>, and <ref>brill and resnik 1994</ref>, referred to respectively as la, sa, and tel.
it thus demonstrates a possible trade-off between these two types of probabilities: using more informative statistics of the target language may compensate for the lack of translation probabilities.
previous corpus-based sense disambiguation methods require substantial amounts of sense-tagged training data <ref>kelly and stone, 1975</ref>; <ref>black, 1988</ref> and <ref>hearst, 1991</ref> or aligned bilingual corpora <tref>brown et al , 1991</tref>; <ref>dagan, 1991</ref> and <ref>gale et al 1992</ref>.
<ref>yarowsky 1992</ref> introduces a thesaurus-based approach to statistical sense disambiguation which works on monolingual corpora without the need for sense-tagged training data.
later work from the ai community relied heavily upon selectional restrictions for verbs, although primarily in terms of features exhibited by their arguments such as drinkable rather than in terms of individual words or word classes.
regardless of whether it takes the form of dictionaries <ref>lesk 1986</ref>; <ref>guthrie et al 1991</ref>; <ref>dagan, itai, and schwall 1991</ref>; <ref>karov and edelman 1996</ref>, thesauri <ref>yarowsky 1992</ref>; <ref>walker and amsler 1986</ref>, bilingual corpora <tref>brown et al 1991</tref>; <ref>church and gale 1991</ref>, or hand-labeled training sets <ref>hearst 1991</ref>; <ref>leacock, towell, and voorhees 1993</ref>; <ref>niwa and nitta 1994</ref>; <ref>bruce and wiebe 1994</ref>, providing information for sense definitions can be a considerable burden.
what makes our approach unique is that, since we narrow the problem to sense discrimination, we can dispense of an outside source of knowledge for defining senses.
 xerox palo alto research center, 3333 coyote hill road, palo alto, ca 94304 q 1998 association for computational linguistics computational linguistics volume 24, number 1 we therefore call our approach automatic word sense discrimination, since we do not require manually constructed sources of knowledge.
although there is some hope from using aligned bilingual corpora as training data for supervised algorithms <tref>brown et al , 1991</tref>, this approach suffers from both the limited availability of such corpora, and the frequent failure of bilingual translation differences to model monolingual sense differences.
the use of dictionary definitions as an optional seed for the unsupervised algorithm stems from a long history of dictionary-based approaches, including <ref>lesk 1986</ref>, guthrie et al.
a problem with the first two approaches is that it is not easy to obtain sufficiently large parallel or manually tagged corpora for the pair of languages targeted.
although the third approach eases the problem of preparing corpora, it suffers from a lack of useful information in the source language.
although this requirement may seem trivial, most corpus-based methods do not, in fact, allow such flexibility.
traditionally, the problem of sparse data is approached by estimating the probability of unobserved co-occurrences using the actual co-occurrences in the training set.
in comparison to these approaches, we use similarity information throughout training, and not merely for estimating co-occurrence statistics.
traditionally, the problem of sparse data is approached by estimating the probability of unobserved cooccurrences using the actual cooccurrences in the training set.
in comparison to these approaches, we use similarity information throughout training, and not merely for estimating cooccurrence statistics.
a second problem with the fidditch parser is poor performances: tilt recall and precision at detecting word collocations are declared to be as low as 50, i-iowever it is unclear if this value applies only to svo triples, and how it has been derived.
the recall is low because tile fidditch parser, as other partial parsers <ref>sekine et al, 1992</ref>; resnik and hearst, i993, only detect links between adjacent or near-adjacent words.
in fact, corpus linguistics became a popular research field because of the claim that shallow techniques could overcome the lexical coverage bottleneck of traditional nlp techniques.
in fact, corpus linguistics became a popular research field because of the claim that shallow techniques could overcome the lexical coverage bottleneck of traditional nlp techniques.
among the applications of collocational analysis for lexical acquisition are: the derivation of syntactic disambiguation cues <ref>basili et al 1991, 1993a</ref>; <ref>hindle and rooths 1991</ref>,1993; <ref>sekine 1992</ref> <ref>bogges et al 1992</ref>, sense preference <ref>yarowski 1992</ref>, acquisition of selectional restrictions <ref>basili et al 1992b, 1993b</ref>; <ref>utsuro et al 1993</ref>, lexical preference in generation <ref>smadjia 1991</ref>, word clustering <ref>pereira 1993</ref>; <ref>hindle 1990</ref>; <ref>basili et al 1993c</ref>, etc in the majority of these papers, even though the precedent or subsequent statistical processing reduces the number of accidental associations, very large corpora 10,000,000 words are necessary to obtain reliable data on a large enough number of words.
in addition, most papers produce a performance evaluation of their methods but do not provide a measure of the coverage, ie the percentage of cases for which their method actually provides a right or wrong solution.
unfortunately, this is not always the case, and the above methodology suffers flom the weaknesses pointed out by <ref>wu, 1997</ref> concerning parse-parse-match procedures.
although these approaches achieved good results for the task considered, most of them aim to extract fixed collocations, mainly noun phrases, and require the information which is dependent on each language such as dictionaries and parts of speech.
this highlights the need for finding multi-word translation correspondences.
previous works that focus on multi-word translation correspondences from parallel corpora include noun phrase correspondences <tref>kupiec, 1993</tref>, fixed/flexible collocations <ref>smadja et al , 1996</ref>, n-gram word sequences of arbitrary length <ref>kitamura and matsumoto, 1996</ref>, non-compositional compounds <ref>melamed, 2001</ref>, captoids <ref>moore, 2001</ref>, and named entities 1.
in all of these approaches, a common problem seems to be an identification of meaningful multi-word translation units.
there are a number of factors which make handling of multi-word units more complicated than it appears.
however, the identification of subsentential, nested, phrasal translations within the parallel texts remains a nontrivial problem, due to the added complexity of dealing with constituent structure.
manual phrasal matching is feasible only for small corpora, either for toy-prototype testing or for narrowly restricted applications.
4 generating translation candidates 41 extraction of japanese terms errors in the extraction of terms and phrases from parallel texts eventually lead to a failure in acquiring the correct term/phrase correspondences.
japanese nps can be recognized more accurately than english nps because japanese has considerably less multi-category words.
as noted in <ref>tsuji 2002</ref>, many previous methods <ref>dagan et al , 1993</ref>; <tref>kupiec, 1993</tref>; <ref>wu and xia, 1994</ref>; <ref>melamed, 1996</ref>; <ref>smadja et al , 1996</ref> deal with this problem based on frequency of words appearing in the corpora, which can not be effectively applied to lowfrequency words, such as transliterated words.
a common problem is the delimitation and spotting of the units to be matched.
this is not a real problem for methods aiming at alignments at a high level of granularity paragraphs, sentences where unit delimiters are clear.
however, sentence alignment <ref>brown et al 1991</ref>; kay  r<ref>sscheisen 1993</ref>; <ref>gale  <ref>church 1993</ref></ref>; <ref>church 1993</ref>; <ref>chen 1993</ref>; <ref>wu 1994</ref> is not always practical when corpora have unclear sentence boundaries or with noisy text segments present in only one language.
however, such methods encounter two fundamental problems: translation of regional variations and the lack of up-to-date and high-lexical-coverage corpus source, which are worthy of further investigation.
the first problem is resulted from the fact that the translations of a term may have variations in different dialectal regions.
unfortunately, this is not always the case, and the above methodology suffers from the weaknesses pointed out by <ref>wu, 1997</ref> concerning parse-parse-match procedures.
we have previously shown an algorithm which extracts a bilingual lexicon from noisy parallel corpus without sentence alignment <ref>fung  mckeown 1994</ref>; <ref>fung 1995</ref>.
although bilingual parallel corpora have been available in recent years, they are still relatively few in comparison to the large amount of monolingual text.
however, despite efforts in compiling parallel corpora, sufficient amounts of such corpora are still unavailable.
in cases like <ref>yarowsky, 1995</ref>, unsupervised methods offer accuracy results than rival supervised methods <tref>yarowsky, 1994</tref> while requiring only a fraction of the data preparation effort.
such methods have also been a key driver of progress in statistical machine translation, which depends heavily on unsupervised word alignments <ref>brown et al , 1993</ref>.
there are also interesting problems for which supervised learning is not an option.
although the work of yarowsky, i994 can be applied to wsd, the results reported in <tref>yarowsky, 1994</tref> only dealt with accent restoration, which is a much simpler problem.
it is unclear how yarowskys method will fare on wsd of a common test data set like the one we used, nor has his method been tested on a large data set with highly ambiguous words tagged with the refined senses of wordnet.
although the work of yarowsky, i994 can be applied to wsd, the results reported in <tref>yarowsky, 1994</tref> only dealt with accent restoration, which is a much simpler problem.
it is unclear how yarowskys method will fare on wsd of a common test data set like the one we used, nor has his method been tested on a large data set with highly ambiguous words tagged with the refined senses of wordnet.
attempts to alleviate this tagbottleneck ilude tmotstrias te ot ill,, 1996; <ref>hearst, 1991</ref> and unsupervised algorith yarowsky, 199s dictionary-based approaches rely on linguistic knowledge sources such as mali,e-readable dictionaries <ref>luk, 1995</ref>; <ref>veronis and ide, 1990</ref> and wordnet <ref>agirre and rigau, 1996</ref>; <ref>resnik, 1995</ref> and e0ploit these for word sense disaznbiguation.
moreover, word trigrams are ineffective at capturing longdistance properties such as discourse topic and tense.
feature-based approaches, such as bayesian classifters <ref>gale, church, and yarowsky, 1993</ref>, decision lists <tref>yarowsky, 1994</tref>, and bayesian hybrids <ref>golding, 1995</ref>, have had varying degrees of success for the problem of context-sensitive spelling correction.
however, we report experiments that show that these methods are of limited effectiveness for cases such as their, there, theyre and than, then, where the predominant distinction to be made among the words is syntactic.
71 confusion set train test most freq.
syncretism and related morphological ambiguities present a problem for many nl applications where lexical disambiguation is important; cases where the orthographic form is identical but the pronunciations of the various functions differ are particularly important for speech applications, such as text-to-speech, since appropriate word pronunciations must be computed from orthographic forms that underspecify the necessary information.
ideally one would like to build models that use contextual information to perform lexical disambiguation <ref>yarowsky 1992, 1994</ref>, but such models must be trained on specialized tagged corpora either hand-generated or semi-automatically generated and such training corpora are often not available, at least in the early phases of constructing a particular application.
lacking good contextual models, one is forced to fall back on estimates of the lexical prior probabilities for the various functions of a form.
following standard terminology, a lexical prior can be defined as follows: imagine that a given form is n-ways ambiguous; the lexical prior probability of sense i of this form is simply the probability of sense i independent of the context in which the particular instantiation of the form occurs.
the main drawback of this approach is that non contiguous or shifted collocations cannot be identified, decreasing the generalization power of the learning algorithm.
compared to recent stochastic english parsers that yield 86 to 87 accuracy <ref>collins, 1996</ref>; <tref>magerman, 1995</tref>, 8433 seems unsatisfactory at the first glance.
the main reason behind this lies in the difference between the two corpora used: penn treebank <ref>marcus et al , 1993</ref> and edr corpus edr, 1995.
these grammars are based on magermans headpercolation scheme to determine the headword of each nonterminal <tref>magerman 1995</tref>.
unfortunately this means that head-lexicalized stochastic grammars are not able to capture dependency relations between words that according to magermans head-percolation scheme are nonheadwords -eg between more and than in the wsj construction carry more people than cargo where neither more nor than are headwords of the np constituent more people than cargo.
1994, and <tref>magerman 1995</tref> can suffer from very similar problems to the label bias or observation bias problem observed in tagging models, as described in <ref>lafferty, mccallum, and pereira 2001</ref> and <ref>klein and manning 2002</ref>.
<ref>charniaks 1997</ref> models will most likely perform quite differently with binarybranching trees for example, his current models will learn that rules such as vp  vsgppare very rare, but with binary-branching structures, this context sensitivity will be lost.
the models of <tref>magerman 1995</tref> and <ref>ratnaparkhi 1997</ref> use contextual predicates that would most likely need to be modified given a different annotation style.
we describe several simple, linguistically motivated annotations which do much to close the gap between a vanilla pcfg and state-of-the-art lexicalized models.
it is a bit worse than the unlexicalized pcfgs of <ref>klein and manning 2003</ref> and matsuzaki et al.
2005, and of course, it is also worse than state-of-the-art lexicalized parsers experience shows that evaluation results on sections 22 and 23 do not differ much.
unfortunately, the state of knowledge in this regard is very limited.
many probabilistic evaluation models have been published inspired by one or more of these feature types <ref>black, 1992</ref> <ref>briscoe, 1993</ref> <ref>charniak, 1997</ref> <ref>collins, 1996</ref> <ref>collins, 1997</ref> <tref>magerman, 1995</tref> <ref>eisner, 1996</ref>, but discrepancies between training sets, algorithms, and hardware environments make it difficult, if not impossible, to compare the models objectively.
we should expect to see, in grouping together paragraph-sized units instead of utterances, a decrease in the complexity of the feature set and algorithm needed.
the work described here makes use only of lexical distribution information, in lieu of prosodic cues such as intonational pitch, pause, and duration <tref>hirschberg and nakatani 1996</tref>, discourse markers such as oh, well, ok, however <ref>schiffrin 1987</ref>; <ref>litman and passonneau 1995</ref>, pronoun reference resolution <ref>passonneau and litman 1993</ref>; <ref>webber 1988</ref> and tense and aspect <ref>webber 1987</ref>; <ref>hwang and schubert 1992</ref>.
from a computational viewpoint, deducing textual topic structure from lexical occurrence information alone is appealing, both because it is easy to compute, and because discourse cues are sometimes misleading with respect to the topic structure <ref>brown and yule 1983</ref>, section 3.
up to now, there have been few researches which directly address the problem of extracting synonymous collocations.
the problem of the methods is that the precision of the extracted synonymous words is low because it extracts many word pairs such as cat and dog, which are similar but not synonymous.
for example, while dog and cat are recognized as the most similar concepts by the method described in <tref>lin, 1998</tref>, it is hard to imagine a context in which these words would be interchangeable.
for example, while dog and cat are recognized as the most similar concepts by the method described in <tref>lin, 1998</tref>, it is hard to imagine a context in which these words would be interchangeable.
distributional techniques, however, are poorly applicable to rare words, ie, those words for which a corpus does not contain enough cooccurrence data to judge about their meaning.
such words are the primary concern of many practical nlp applications: as a rule, they are semantically focused words and carry a lot of important information.
22 compound similarity as a critical technique, word similarity is generally used in the example-based models of semantic classification.
the measure of word similarity can be divided into two major approaches: taxonomy-based lexical approach <ref>resnik 1995</ref>, <tref>lin 1998a</tref>, <ref>chen and chen 1998</ref> and context-based syntactic approach <tref>lin 1998b</tref>,<ref>chen and you 2002</ref>, which is not the concern in this context-free model.
however, two problems arise here for the taxonomy-based lexical approach.
first, such similarity measures risk the failure to capture the similarity among some semantically highly related words, if they happen to be put under classes distant from each other according to a specific ontology4.
although the 24 msr corpus used strict means of resolving interrater disagreements during its construction, the annotators agreed with the msr corpus labels only 935 187/200 of the time.
one weakness of our system is that we rely on a thesaurus <tref>lin, 1998</tref> for word similarity information for predicate argument tuple pairing.
if a predicate argument tuples target or one argument is realized as a phrase borrow  check out, for instance, the thesaurus is unable to provide an accurate similarity score.
we seek to overcome these difficulties by generating toefl-like tests automatically from wordnet <ref>fellbaum, 1998</ref>.
given a corpus, we first derive a list of words occurring with sufficient marginal frequency to support a distributional comparison.
a large body of work has attempted to capture corpus-based estimates of word similarity <ref>pereira et al , 1993</ref>; <tref>lin, 1998</tref>; however, the lack of large sense-tagged corpora prevent most such techniques from being used effectively to compare different senses of the same word.
probably the most widely used association weight function is point-wise mutual information mi <ref>church et al , 1990</ref>, <ref>hindle, 1990</ref>, <tref>lin, 1998</tref>, <ref>dagan, 2000</ref>, defined by:  ,log, 2 fpwp fwpfwmi  a known weakness of mi is its tendency to assign high weights for rare features.
by contrast, in this paper we present a method that accurately determines sense dominance even in relatively small amounts of target text a few hundred sentences; although it does use a corpus, it does not require a similarly-sense-distributed corpus.
however, due to the lack of a tight de nition for the concept of distributional similarity and the broad range of potential applications, a large number of measures of distributional similarity have been proposed or adopted see section 2.
however, due to the lack of a tight de nition for the concept of distributional similarity and the broad range of potential applications, a large number of measures of distributional similarity have been proposed or adopted see section 2.
other approaches usually consider either given sets of synonyms among which one is to be chosen for a translation for instance <ref>edmonds and hirst, 2002</ref> or must choose a synonym word against unrelated terms in the context of a synonymy test <ref>freitag et al , 2005</ref>, a seemingly easier task than actually proposing synonyms.
this makes for very complex evaluation procedures without an intuitive interpretation, and there is no assessment of the quality of the automated thesaurus.
we 583 also attempted adding lins 1998 similarity scores but they appeared to be redundant.
fully unsupervised semantic clustering eg, <tref>lin, 1998</tref>; <ref>lin and pantel, 2002</ref>; <ref>davidov and rappoport, 2006</ref> has the disadvantage that it may or may not produce the types and granularities of semantic classes desired by a user.
we use it instead of point-wise mutual information in <tref>lin 1998</tref> because the latter tends to overestimate the association between two parts with low frequencies.
in our opinion, such analyses act at best as a coarse filter in the comparison of a set of measures and an even coarser one in the assessment of a single measure.
it is worthwhile to remark that, due to the ambiguity of the entailed words eg , position could also entail either perspective or place, not every occurrence of them should be taken into account, in order to avoid misleading predictions caused by the irrelevant senses.
therefore, approaches based on a more classical contextual similarity technique <tref>lin, 1998</tref>; <ref>dagan, 2000</ref>, where words are described globally by context vectors, are doomed to fail.
and third, although corpus-based methods eg , lins 1998 do compute different similarity values for different pairs of near-synonyms of the same cluster, church et al.
recent research in computational linguistics has focused more on developing methods to compute the degree of semantic similarity between any two words, or, more precisely, between the simple or primitive concepts 15 denoted by any two words.
unfortunately, these methods are generally unhelpful in computing the similarity of near-synonyms because the measures lack the required precision.
interestingly, the performance of the asymmetric kl divergence and the symmetric js divergence is very close, which makes it difficult to conclude whether the relation discovery domain is a symmetric domain or an asymmetric domain like <tref>lees 1999</tref> task of improving probability estimates for unseen word co-occurrences.
a shortcoming of all the models we will describe here is that they are derived from the basic bag-of-words models and as such do not account for word order or other notions of syntax.
negative semi-definite kernels are bydefinitionsymmetric, whichrulesthekullbackleibler divergence and <tref>lees 1999</tref>-skew divergence out of consideration.
we use the cosine similarity measure for windowbased contexts and the following commonly used similarity measures for the syntactic vector space: <ref>hindles 1990</ref> measure, the weighted lin measure <ref>wu and zhou, 2003</ref>, the -skew divergence measure <tref>lee, 1999</tref>, the jensen-shannon js divergence measure <ref>lin, 1991</ref>, jaccards coef cient van <ref>rijsbergen, 1979</ref> and the confusion probability <ref>essen and steinbiss, 1992</ref>.
we use verb-object relations in both active and passive voice constructions as did pereira et al.
we use the cosine similarity measure for windowbased contexts and the following commonly used similarity measures for the syntactic vector space: <ref>hindles 1990</ref> measure, the weighted lin measure <ref>wu and zhou, 2003</ref>, the -skew divergence measure <tref>lee, 1999</tref>, the jensen-shannon js divergence measure <ref>lin, 1991</ref>, jaccards coef cient van <ref>rijsbergen, 1979</ref> and the confusion probability <ref>essen and steinbiss, 1992</ref>.
although -skew outperforms the simpler measures in ranking nouns, its performance on verbs is worse than the performance of weighted lin.
but this is simply not an adequate model of semantic relatedness, for which substitutability is far too strict a requirement: window and house are semantically related, but they are not plausibly substitutable in most contexts.
however, it is well known that any knowledge-based system suffers from the so-called knowledge acquisition bottleneck, ie the difficulty to actually model the domain in question.
as stated in <tref>caraballo, 1999</tref>, wordnet has been an important lexical knowledge base, but it is insufficient for domain specific texts.
so, many attempts have been made to automatically produce taxonomies <ref>grefenstette, 1994</ref>, but <tref>caraballo, 1999</tref> is certainly the first work which proposes a complete overview of the problem by 1 automatically building a hierarchical structure of nouns based on bottom-up clustering methods and 2 labeling the internal nodes of the resulting tree with hypernyms from the nouns clustered underneath by using patterns such as b is a kind of a.
as stated in <tref>caraballo, 1999</tref>, wordnet has been an important lexical knowledge base, but it is insufficient for domain specific texts.
so, many attempts have been made to automatically produce taxonomies <ref>grefenstette, 1994</ref>, but <tref>caraballo, 1999</tref> is certainly the first work which proposes a complete overview of the problem by 1 automatically building a hierarchical structure of nouns based on bottom-up clustering methods and 2 labeling the internal nodes of the resulting tree with hypernyms from the nouns clustered underneath by using patterns such as b is a kind of a.
however, such clustering algorithms fail to name their classes.
these methods are very interesting but of limited applicability, because nouns that do not appear in known lexico-syntactic patterns cannot be learned.
7 conclusion all the approaches cited above focus on some aspect of the problem of lexical acquisition.
the sparseness of these patterns prevents this from being an effective approach to the problem we address here.
the hypernyms used to label the internal nodes of that hierarchy are chosen in a simple fashion; pattern-matching as in <ref>hearst 1992</ref> is used to identify candidate hypernyms of the words dominated by a particular node, and a simple voting scheme selects the hypernyms to be used.
likewise, in the trec domain, air toxics such as benzene can suggest that benzene is a kind of air toxic.
in our view, these previous systems used weak syntactic models because the syntactic structures sometimes identified desirable semantic associations and sometimes did not.
to compensate, statistical models were used to separate the meaningful semantic associations from the spurious ones.
there are inherent problems in evaluating automatic thesaurus extraction techniques, and much research assumes a gold standard that does not exist see kilgarriff 2003 and weeds 2003 for more discussion of this.
a further problem for distributional similarity methods for automatic thesaurus generation is that they do not offer any obvious way to distinguish between linguistic relations such as synonymy, antonymy, and hyponymy see caraballo 1999 and lin et al 2003 for work on this.
3 the probability model the dag-like nature of the dependency structures makes it difficult to apply generative modelling techniques <ref>abney, 1997</ref>; <tref>johnson et al , 1999</tref>, so we have defined a conditional model, similar to the model of <ref>collins 1996</ref> see also the conditional model in <ref>eisner 1996b</ref>.
while the model of <ref>collins 1996</ref> is technically unsound <ref>collins, 1999</ref>, our aim at this stage is to demonstrate that accurate, efficient wide-coverage parsing is possible with ccg, even with an over-simplified statistical model.
future work will look at alternative models4 4the reentrancies creating the dag-like structures are fairly limited, and moreover determined by the lexical categories.
a significant difference is that we apply maximum entropy estimation for feature forests to avoid the inherent problem with estimation: the exponential explosion of parsing results given by the grammar.
unfortunately, large parsed ubg corpora are not yet available.