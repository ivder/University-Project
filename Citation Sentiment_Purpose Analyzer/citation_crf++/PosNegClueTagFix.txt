given that each token has on the average more than 2 possible tags, the procedural description <pos>above</pos> is very inefficient for all but very short sentences.	however, the observation that our constraints are localized to a window of a small number of tokens say at most 5 tokens in a sequence, suggests a more <pos>efficient</pos> scheme originally used by <tref>church 1988</tref>.	assume our constraint windows are allowed to look at a window of at most size k sequential parses.	let us take the first k tokens of a sentence and generate all possible paths of k arcs spanning k  1 nodes, and apply all constraints to these short paths.
the experimental results show the proposed method significantly outperforms both hand-crafted and conventional statistical methods.	the last few years have seen the <pos>great</pos> <pos>success</pos> of stochastic part-of-speech pos taggers <tref>church, 1988</tref>: <ref>kupiec, 1992</ref>; charniak et m , 1993; <ref>brill, 1992</ref>; <ref>nagata, 1994</ref>.	the stochastic approach generally attains 94 to 96 accuracy and replaces the labor-intensive compilation of linguistics rules by using an automated <pos>learning</pos> algorithm.	however, 1ntt is an abbreviation of nippon telegraph and telephone corporation.
1.	a recent trend in <pos>natural</pos> language processing has been toward a greater emphasis on statistical approaches, beginning with the <pos>success</pos> of statistical part-of-speech tagging programs <tref>church 1988</tref>, and continuing with other work using statistical part-of-speech tagging programs, such as bbn plum <ref>weischedel et al 1993</ref> and nyu proteus <ref>grishman and sterling 1993</ref>.	more recently, statistical methods have been applied to domain-specific semantic parsing <ref>miller et al 1994</ref>, and to the more difficult problem of wide-coverage syntactic parsing <ref>magerman 1995</ref>.	nevertheless, most <pos>natural</pos> language systems remain primarily rule based, and <pos>even</pos> systems that do use statistical techniques, such as att chronus <ref>levin and pieraccini 1995</ref>, continue to require a <pos>significant</pos> rule based component.
disambiguation of capitalized words is usually handled by pos taggers, which <pos>treat</pos> capitalized words in the same way as other categories, that is, by accounting for the immediate syntactic context and using estimates collected from a training corpus.	<ref>as church 1988</ref> <pos>rightly</pos> pointed out, however, <pos><pos>proper</pos></pos> nouns and capitalized words are particularly problematic: some capitalized words are proper nouns and some are not.	estimates from the brown corpus can be misleading.	for example, the capitalized word acts is found twice in the brown corpus, both times as a <pos>proper</pos> noun in a title.
the morphological ambiguity <pos>will</pos> differ depending on the level of tagging used in each case, as shown in table 2.	there are two kinds of methods for morphological disambiguation: on one hand, statistical methods need little effort and obtain very <pos><pos>good</pos></pos> results <tref>church, 1988</tref>; cutting el al, 1992, at least when applied to english, but when we try to apply them to basque we encounter additional problems; on the other hand, some rule-based systems <ref>brill, 1992</ref>; <ref>voutilainen et al, 1992</ref> are at least as good as statistical systems and are <pos>better</pos> adapted to free-order languages and agglutinative languages.	so, we 381 have selected one of each group: constraint grammar formalism <ref>karlsson et al, 1995</ref> and the hmm based tatoo tagger <ref>armstrong et al, 1995</ref>, which has been designed to be applied it to the output of a morphological analyser and the tagset can be switched <pos>easily</pos> without changing the input text.	second  third 70 ks i m m mcg mcg figure 1-initial ambiguity3.
we are tagging this material with a much simpler tagset than used by previous projects, as discussed at the oct 1989 darpa workshop.	the material is first processed using ken churchs tagger <tref>church 1988</tref>, which labels it as if it were brown corpus material, and then is mapped to our tagset by a sedscript.	because of fundamental differences in tagging strategy between the penn treebank project and the brown project, the resulting mapping is about 9 inaccurate, given the tagging guidelines of the penn treebank project as given in 40 pages of <pos>explicit</pos> tagging guidelines.	this material is then hand-corrected by our annotators; the result is <pos>consistent</pos> within annotators to about 3 cf.
h90-1055:17.	deducing linguistic structure from the statistics of <pos>large</pos> corpora eric brill david magerman mitchell marcus beatrice santorini department of computer and information science university of pennsylvania philadelphia, pa 19104 1 introduction within the last two years, approaches using both stochastic and symbolic techniques have proved <pos>adequate</pos> to deduce lexical ambiguity resolution rules with less than 3-4 error rate, when trained on <pos>moderate</pos> sized 500k word corpora of english text eg <tref>church, 1988</tref>; <ref>hindle, 1989</ref>.	the <pos>success</pos> of these techniques suggests that much of the grammatical structure of language may be derived automatically through distributional analysis, an approach attempted and abandoned in the 1950s.	we describe here two experiments to see how far purely distributional techniques can be pushed to automatically provide both a set of part of speech tags for english, and a grammatical analysis of <pos>free</pos> english text.
this line of research was <pos>motivated</pos> by a series of <pos>successful</pos> applications of mutual information statistics to other problems in <pos>natural</pos> language processing.	in the last decade, research in speech <pos>recognition</pos> <ref>jelinek 1985</ref>, noun classification <ref>hindle 1988</ref>, predicate argument relations <ref>church  hanks 1989</ref>, and other areas have shown that mutual information statistics provide a wealth of information for solving these problems.	22 mutual information statistics the mutual information statistic <ref>fano 1961</ref> is a measure of the interdependence of two signals in a message.	it is a function of the probabilities of the two events: mz, u  log u xzpvy in this paper, the events x and y <pos>will</pos> be part-of-speech n-grams instead of single parts-of-speech, as in some earlier work.
4 concluding remarks though there can be little doubt that the ruling system of bakeoffs actively encourages a degree of oneupmanship, our paper and our software are not offered in a <pos>competitive</pos> <pos>spirit</pos>.	as we said at the out211 set, we dont <pos>necessarily</pos> believe hunpos to be in any way <pos>better</pos> than tnt, and certainly the main ideas have been pioneered by <ref>derose 1988</ref>, <tref>church 1988</tref>, and others long before this generation of hmm work.	but to <pos>improve</pos> the results beyond what a <pos>basic</pos> hmm can <pos>achieve</pos> one needs to tune the system, and <pos>progress</pos> can only be made if the experiments are end to end replicable.	there is no doubt many other systems could be tweaked further and <pos><pos>improve</pos></pos> on our results what matters is that anybody could now also tweak hunpos without any restriction to improve the state of the art.
in practice, computational limitations do not allow the enumeration of all possible assignments for <neg>long</neg> sentences, and smoothing is required for infrequent events.	this is described in more detail in the original publication <tref>church, 1988</tref>.	<neg>although</neg> more sophisticated algorithms for unsupervised learning which can be trained on plain text instead on manually tagged corpora are well established see eg <ref>merialdo, 1994</ref>, we decided not to use them.	the main reason is that with large tag sets, the sparse-data-problem can become so <neg>severe</neg> that unsupervised training easily ends up in local minima, which can lead to <neg>poor</neg> results without any indication to the user.
much research has been donc oll knowledge acquisition fiom large-scalc annotated corpora as a rich source of linguistic knowledge.	mtior works done to create english pos taggers henceforth, taggers, for example, include <tref>church 1988</tref>, <ref>kupicc 1992</ref>, <ref>brill 1992</ref>and <ref>voutilaincn et al 1992</ref>.	the <neg>problem</neg> with this framework, however, is that such reliable corpora are <neg>hardly</neg> awdlable duc to a huge amount of the labor-intensive work required.	in case of the acquisition of non-core knowledge, such as specific, lexically or dolnain dependent knowledge, preparation of annotated corpora becomes more <neg>serious</neg> <neg>problem</neg>.
but dictionaries of technical terminology have many one-word terms.	simplex or <neg><neg>complex</neg></neg> nps eg , <tref>church 1988</tref>; <ref>hindle and rooth 1991</ref>; <ref>wacholder 1998</ref> identify simplex or base nps  nps which do not have any component nps -at <neg>least</neg> in part because this bypasses the <neg>need</neg> to solve the quite <neg>difficult</neg> attachment <neg>problem</neg>, ie, to determine which simpler nps should be combined to output a more complex np.	but if people find <neg>complex</neg> nps more useful than simpler ones, it is important to focus on improvement of techniques to reliably identify more complex terms.	semantic and syntactic terms variants.
thus, examples 3-5 illustrate how the syntactic context of a word can <pos><pos>help</pos></pos> determine its meaning.	22 <pos>motivation</pos> from previous work 221 parsing in recent years, the <pos><pos>success</pos></pos> of statistical parsing techniques can be attributed to several factors, such as the increasing size of computing machinery to accommodate larger models, the availability of resources such as the penn treebank <ref>marcus et al , 1993</ref> and the success of machine <pos>learning</pos> techniques for lowerlevel nlp problems, such as part-of-speech tagging <tref>church, 1988</tref>; <ref>brill, 1995</ref>, and ppattachment <ref>brill and resnik, 1994</ref>; <ref>collins and brooks, 1995</ref>.	however, perhaps <pos>even</pos> more <pos>significant</pos> has been the lexicalization of the grammar formalisms being probabilistically modeled: crucially, all the recent, <pos>successful</pos> statistical parsers have in some way made use of bilexical dependencies.	this includes both the parsers that attach probabilities to parser moves <ref>magerman, 1995</ref>; <ref>ratnaparkhi, 1997</ref>, but also those of the lexicalized pcfg variety <ref>collins, 1997</ref>; <ref>charniak, 1997</ref>.
recent research advances may lead to the development of <pos>viable</pos> book indexing methods for chinese books.	these include the availability of <pos>efficient</pos> and high <pos>precision</pos> word segmentation methods for chinese text <ref>chang et al , 1991</ref>; <ref>sproat and shih, 1990</ref>; <ref>wang et al , 1990</ref>, the availability of statistical analysis of a chinese corpus <ref>liu et al , 1975</ref> and large-scale electronic chinese dictionaries with partof-speech information <ref>chang et al , 1988</ref>; bdc, 1992, the corpus-based statistical part-of-speech tagger <tref>church, 1988</tref>; <ref>derose, 1988</ref>; <ref>beale, 1988</ref>, as <pos>well</pos> as phrasal and clausal analyzers <tref>church 1988</tref>; <ref>ejerhed 1990</ref> 2.	problem description as being pointed out in <ref>salton, 1988</ref>, back-of-book indexes may consist of more than one word that are derived from a noun phrase.	given the text of a book, an indexing system, must perform some <pos>kind</pos> of phrasal and statistical analysis in order to produce a list of candidate indexes and their occurrence statistics in order to generate indexes as shown in figure 1 which is an excerpt from the reconstruction of indexes of a book on transformational grammar for mandarin chinese <ref>tang, 1977</ref>.
much recent research in the field of <pos>natural</pos> language processing nlp has focused on an empirical, corpus-based approach <ref>church and mercer, 1993</ref>.	the high accuracy achieved by a corpus-based approach to part-of-speech tagging and noun phrase parsing, as demonstrated by <tref>church, 1988</tref>, has inspired similar approaches to other problems in <pos>natural</pos> language processing, including syntactic parsing and word <pos><pos>sense</pos></pos> disambiguation wsd.	the availability of <pos>large</pos> quantities of part-ofspeech tagged and syntactically parsed sentences <pos>like</pos> the penn treebank corpus <ref>marcus, santorini, and marcinkiewicz, 1993</ref> has contributed greatly to the development of <pos>robust</pos>, <pos>broad</pos> coverage partof-speech taggers and syntactic parsers.	the penn treebank corpus contains a <pos>sufficient</pos> number of partof-speech tagged and syntactically parsed sentences to serve as <pos>adequate</pos> training material for building <pos>broad</pos> coverage part-of-speech taggers and parsers.
one method of handling large vocabularies is simply increasing the size of the lexicon.	research efforts at ibm chodorow, et al 1988; neff, et al 1989, bell labs church, et al 1989, new mexico state university <ref>wilks 1987</ref>, and elsewhere have used mechanical processing of on-line dictionaries to infer at <neg>least</neg> minimal syntactic and semantic information from dictionary definitions.	however, even assuming a very large lexicon already exists, it can never be complete.	systems aiming for coverage of unrestricted language in broad domains must continually deal with new words and novel word senses.
in order to make these improvements, we <neg>need</neg> access to word-class information pos information <ref>johansson et al 1986</ref>; <ref><neg>black</neg>, garside, and <neg><neg>leech</neg></neg> 1993</ref> or semantic information <ref>beckwith et al 1991</ref>, which is usually obtained in three main ways: firstly, we can use corpora that have been manually tagged by linguistically informed experts <ref>derouault and merialdo 1986</ref>.	secondly, we can construct automatic part-ofspeech taggers and process untagged corpora <ref>kupiec 1992</ref>; <ref><neg>black</neg>, garside, and <neg><neg>leech</neg></neg> 1993</ref>; this method boasts a high degree of accuracy, <neg>although</neg> often the construction of the automatic tagger involves a bootstrapping process based on a core corpus which has been manually tagged <tref>church 1988</tref>.	the third option is to derive a fully automatic word-classification system from untagged corpora.	some advantages of this last approach include its applicability to any natural language for which some corpus exists, independent of the degree of development of its grammar, and its parsimonious commitment to the machinery of modern linguistics.
if an external resource is used in the form of a morphological analyzer ma, this will almost always overgenerate, yielding <neg>false</neg> <neg>ambiguity</neg>.	but even if the ma is tight, a considerable proportion of <neg>ambiguous</neg> tokens will come from legitimate but rare analyses of frequent types <tref>church, 1988</tref>.	for example the word nem, can <neg>mean</neg> both not and gender, so both adv and noun are valid analyses, but the adverbial reading is about five orders of magnitude more frequent than the noun reading, 12596 vs 4 tokens in the 1 m word manually annotated szeged korpusz <ref>csendes et al , 2004</ref>.	thus the <neg>difficulty</neg> of the task is better measured by the average information required for disambiguating a token.
part-of-speech tagging is required to detect new terms formed through conversion.	this is quite <pos>feasible</pos> using statistical taggers <pos>like</pos> those of <ref>garside 1987</ref>, <tref>church 1988</tref> or <ref>foster 1991</ref> which <pos>achieve</pos> performance upwards of 97 on <pos>unrestricted</pos> text.	terms formed through semantic drift are the wolves in sheeps clothing stealing through terminological pastures.	they are <pos>well</pos> enough concemcd to allude at times <pos>even</pos> the human reader and no automatic term-recognition system has attempted to <pos>distinguish</pos> such terms, despite the prevalence ofpolysemy in such fields as the social sciences riggs, 1993 and the <pos>importance</pos> for purposes of terminological standardization that deviant usage be tracked.
on sentences with <40 words, the former model performs at 69 <pos>precision</pos>, 75 recall, and the latter at 77 precision and 78 recall.	ever since the <pos>success</pos> of hmms application to part-of-speech tagging in <tref>church, 1988</tref>, machine <pos>learning</pos> approaches to <pos>natural</pos> language processing have steadily become more widespread.	this increase has of course been due to their proven efficacy in many tasks, but also to their engineering efficacy.	many machine <pos>learning</pos> approaches let the data speak for itself data ipsa loquuntur, as it were, allowing the modeler to focus on what features of the data are <pos>important</pos>, rather than on the complicated interaction of such features, as had often been the case with hand-crafted nlp systems.
the program can be trained even with a relatively small amount of treebank data; then it can be j used for parsing unrestricted pre-tagged text.	as far as coverage is <neg>concerned</neg>, our parser can handle recursive structures, which is an advantage compared to simpler techniques such as that described by <tref>church 1988</tref>.	on the other hand, the markov assumption underlying our approach means that only <neg>strictly</neg> local dependencies are recognised.	for full parsing, one would probably <neg>need</neg> non-local contextual information, such as the long-range trigrams in link grammar della <ref>pietra et al , 1994</ref>.
purely rule-based techniques seemed too brittle for dealing with the variety of constructions, the long sentences averaging 29 words per sentence, and the degree of unexpected input.	statistical models based on local information eg , <ref>derose 1988</ref>; <tref>church 1988</tref> might operate effectively in spite of sentence length and unexpected input.	to see whether our four hypotheses in italics <pos><pos>above</pos></pos> effectively addressed the four concerns above, we chose to test the hypotheses on two well-known problems: ambiguity both at the structural level and at the part-of-speech level and inferring syntactic and semantic information about unknown words.	guided by the past <pos>success</pos> of probabilistic models in speech processing, we have integrated probabilistic models into our language processing systems.
we report in section 2 on our experiments on the assignment of part of speech to words in text.	the <pos>effectiveness</pos> of such models is <pos>well</pos> known <ref>derose 1988</ref>; <tref>church 1988</tref>; <ref>kupiec 1989</ref>; <ref>jelinek 1985</ref>, and they are currently in use in parsers eg de <ref>marcken 1990</ref>.	our work is an incremental <pos>improvement</pos> on these models in three ways: 1 much less training data than theoretically required proved <pos>adequate</pos>; 2 we integrated a probabilistic model of word features to handle unknown words <pos>uniformly</pos> within the probabilistic model and measured its <pos>contribution</pos>; and 3 we have applied the forward-backward algorithm to <pos>accurately</pos> compute the most likely tag set.	in section 3, we demonstrate that probability models can <pos>improve</pos> the performance of knowledge-based syntactic and semantic processing in dealing with structural ambiguity and with unknown words.
this was expanded upon by <ref>gale et al , 1992</ref>, and in a class-based variant by <ref>yarowsky, 1992</ref>.	decision trees <ref>brown, 1991</ref> have been usefully applied to word-sense ambiguities, and hmm part-of-speech taggers <ref>jelinek 1985</ref>, <tref>church 1988</tref>, <ref>merialdo 1990</ref> have addressed the syntactic ambiguities presented here.	<ref>hearst 1991</ref> presented an <pos>effective</pos> approach to modeling local contextual evidence, while <ref>resnik 1993</ref> gave a <pos>classic</pos> treatment of the use of word classes in selectional constraints.	an algorithm for combining syntactic and semantic evidence in lexical ambiguity resolution has been realized in <ref>chang et al , 1992</ref>.
preprocessing the corpus with a part of speech tagger phrasal verbs involving the preposition to raise an <pos>interesting</pos> problem because of the possible confusion with the infinitive marker to.	we have found that if we first tag every word in the corpus with a part of speech using a method such as <tref>church 1988</tref> or <ref>derose 1988</ref>, and then measure associations between tagged words, we can identify <pos>interesting</pos> contrasts between verbs associated with a following preposition toin and verbs associated with a following infinitive marker toto.	part of speech notation is borrowed from <ref>francis and kucera 1982</ref>; m  preposition; to  infinitive marker; vb  bare verb; vbg  verb  ing; vbd  verb  ed; vbz  verb  s; vbn  verb  en.	the score identifies quite a number of verbs associated in an <pos>interesting</pos> way with to; restricting our attention to pairs with a score of 30 or more, there are 768 verbs associated with the preposition toin and 551 verbs with the infinitive marker toto.
in the partof-speech tagging field, the disambiguation of capitalized words is treated similarly to the disambiguation of common words.	however, as <tref>church 1988</tref> <pos>rightly</pos> pointed out <pos><pos>proper</pos></pos> nouns and capitalized words are particularly problematic: some capitalized words are proper nouns and some are not.	estimates from the brown corpus can be misleading.	for example, the capitalized word acts is found twice in brown corpus, both times as a <pos>proper</pos> noun in a title.
given that each token has on the average more than 2 possible tags, the procedural description <pos>above</pos> is very inefficient for m1 but very short sentences.	however, the observation that our constraints are localized to a window of a small number of tokens say at most 5 tokens in a sequence, suggests a more <pos>efficient</pos> scheme originally used by <tref>church 1988</tref>.	assume our constraint windows are allowed to look at a window of at most size k sequential parses.	let us take the first k tokens of a sentence and generate all possible paths of k arcs spanning k  1 nodes, and apply all constraints to these short paths.
we rewrite this term as follows: prw1,nd1,n n  i-iprwidilwl,ildl,il i1 n  l-i prwilwl,i-ldl,i prdilwl,i-ldl,i-1 i1 7 equation 7 involves two probability distributions that <neg>need</neg> to be estimated.	these are the same distributions that are needed by previous pos-based language models equation 5 and pos taggers <tref>church 1988</tref>; <ref>charniak et al 1993</ref>.	however, these approaches simplify the context so that the lexical probability is just conditioned on the pos category of the word, and the pos probability is conditioned on just the preceding pos tags, which leads to the following two approximations.	prwiiwl,ildl,i  prwildi 8 prdiiwuldl,il  prdiidul 9 however, to successfully incorporate pos information, we <neg>need</neg> to account for the full richness of the probability distributions, as will be demonstrated in section 344.
1992 circumvent this problem by training their taggers on untagged data using tile itaum-welch algorithm also know as the forward-backward algorithm.	they report rates of <pos>correctly</pos> tagged words which are comparable to that presented by <tref>church 1988</tref> and <ref>kempe 1993</ref>.	a third and rather new approach is tagging with artificial neural networks.	in the area of speech <pos>recognition</pos> neural networks have been used for a decade r, ow.
in practice, computational limitations do not allow the enumeration of all possible assignments for <neg>long</neg> sentences, and smoothing is required for infrequent events.	this is described in more detail in the original publication <tref>church, 1988</tref>.	<neg>although</neg> more sophisticated algorithms for unsupervised learning which can be trained on plain text instead on manually tagged corpora are well established see eg <ref>merialdo, 1994</ref>, we decided not to use them.	the main reason is that with large tag sets, the sparse-data-problem can become so <neg>severe</neg> that unsupervised training easily ends up in local minima, which call lead to <neg>poor</neg> results without any indication to the user.
indeed, recent increased <pos><pos>interest</pos></pos> in the problem of disambiguating lexical category in english has led to <pos>significant</pos> <pos>progress</pos> in developing <pos>effective</pos> programs for assigning lexical category in <pos>unrestricted</pos> text.	the most <pos>successful</pos> and <pos>comprehensive</pos> of these are based on probabilistic modeling of category sequence and word category <ref>church 1987</ref>; <ref>garside, leech and sampson 1987</ref>; <ref>derose 1988</ref>.	these stochastic methods show <pos>impressive</pos> performance: church reports a <pos>success</pos> rate of 95 to 99, and shows a sample text with an error rate of less than one percent.	what may seem particularly surprising is that these methods <pos>succeed</pos> essentially without reference to syntactic structure; purely surface lexical patterns are involved.
we redistribute the probability mass of <neg>low</neg> count sequences to unseen sequences.	generalized forward <neg>backward</neg> reestimation generalization of the forward and viterbi algorithm in english part of speech taggers, the maximization of equation 1 to get the most likely tag sequence, is accomplished by the viterbi algorithm <tref>church, 1988</tref>, and the maximum likelihood estimates of the parameters of equation 2 are obtained from untagged corpus by the forwardbackward algorithm <ref>cutting et al , 1992</ref>.	however, it is <neg>impossible</neg> to apply the viterbi algorithm and the forward-backward algorithm for word segmentation of those languages that have no delimiter between words, such as japanese and chinese, because word segmentation hypotheses overlap one another.	figure 3 shows an example of overlapping word hypotheses and possible word segmentations for the string ntig-f all prefectures in the nation.
problem.	<pos>excellent</pos> methods have been developed for part-of-speech pos tagging using stochastic models trained on partially tagged corpora <tref>church, 1988</tref>; cutting, <ref>kupiec, pedersen  sibun, 1992</ref>.	semantic issues have been addressed, particularly for <pos><pos>sense</pos></pos> disambiguation, by using <pos>large</pos> contexts, eg, 50 nearby words <ref>gale, church  yarowsky, 1992</ref> or by reference to on-line dictionaries <ref>krovetz, 1991</ref>; <ref>lesk, 1986</ref>; <ref>liddy  paik, 1992</ref>; <ref>zernik, 1991</ref>.	more recently, methods to work with entirely untagged corpora have been developed which show <pos>great</pos> <pos>promise</pos> <ref>brill  marcus, 1992</ref>; <ref>finch  chater, 1992</ref>; <ref>myaeng  li, 1992</ref>; <ref>schutze, 1992</ref>.
to appear, <ref>hearst 1991</ref>, <ref>lesk 1986</ref>, <ref>smadja and mckeown 1990</ref>, <ref>walker 1987</ref>, <ref>veronis and ide 1990</ref>, <ref>yarowsky 1992</ref>, zemik 1990, 1991.	much of this work offers the <pos>prospect</pos> that a disambiguation system might be <pos>able</pos> to input <pos><pos>unrestricted</pos></pos> text and tag each word with the most likely <pos><pos>sense</pos></pos> with <pos><pos>fairly</pos></pos> <pos><pos>reasonable</pos></pos> accuracy and <pos><pos>efficiency</pos></pos>, <pos><pos>just</pos></pos> as part of speech taggers eg , <tref>church 1988</tref> can now input unrestricted text and assign each word with the most likely part of speech with fairly reasonable accuracy and efficiency.	the availability of massive lexicographic databases offers a <pos>promising</pos> route to overcoming the knowledge acquisition bottleneck.	more than thirty years ago, bari-<ref>iillel 1960</ref> predicted that it would be futile to write expert-system-like rules by-hand as they had been doing at georgetown at the time because there would be no way to scale up such rules to cope with <pos>unrestricted</pos> input.
it shows the descriptive power of low-level morphology-based constraints.	the most <pos>successful</pos> <pos>achievements</pos> so far in the domain of large-scale morphological disambiguation of running text have been those for english reported by <ref>garside, leech, and sampson 1987</ref>, on tagging the lob corpus, and <tref>church 1988</tref>, on assigning part-of-speech labels and parsing noun phrases.	<pos><pos>success</pos></pos> rates ranging between 95-99 are reported, depending on how success is defined.	these approaches are probabilistic and based on transitional probabilities calculated from extensive pretagged corpora.
additionally, there is a slight but not <pos>significant</pos> <pos>improvement</pos> of tagging accuracy.	statistical part-of-speech disambiguation can be efficiently done with n-gram models <tref>church, 1988</tref>; <ref>cutting et al , 1992</ref>.	these models are equivalent to hidden markov models hmms <ref>rabiner, 1989</ref> of order n 1.	the states represent parts of speech categories, tags, there is exactly one state for each category, and each state outputs words of a particular category.
in the past decade, the speech <pos>recognition</pos> community has had huge successes in applying hidden markov models, or hmms to their problems.	more recently, the <pos>natural</pos> language processing community has effectively employed these models for part-ofspeech tagging, as in the seminal <tref>church, 1988</tref> and other, more recent efforts <ref>weischedel et al , 1993</ref>.	we would now propose that hmms have <pos>successfully</pos> been applied to the problem of name-finding.	we have built a named-entity ne <pos>recognition</pos> system using a slightly-modified version of an hmm; we call our system nymble.
the computational tools available for studying machinereadable corpora are at present still rather <neg>primitive</neg>.	these are concordancing programs see figure 1, which are basically kwic key word in context; <ref>aho et al 1988</ref> indexes with additional features such as the ability to extend the context, sort leftward as well as rightward, and so on.	there is very <neg>little</neg> interactive software.	in a typical situation in the lexicography of the 1980s, a lexicographer is giwen the concordances for a word, marks up the printout with colored pens to identify the salient senses, and then writes syntactic descriptions and definitions.
2 relation to previous works quite a few works have dealt with extending a given pos tagger, mainly by smoothing it using extra-information about untreated words.	for example, <tref>church, 1988</tref> uses the simple heuristic of predicting proper nouns from capitalization.	this method is not applicable to arabic and hebrew, which <neg>lack</neg> typographical marking of proper nouns.	more advanced methods like those described by weischedel et al.
work on the use of synchronous tags to capture quantifier scoping possibilities makes use of so-called multicomponent tags.	finally, the base tags may be lexicalized <tref>schabes et al , 1988</tref> or not.	once the base formalism has been decided upon we currently are using lexicalized multi-component tags with substitution and adjunction, a <pos>simple</pos> translation strategy from a source string to a target is to parse the string using an <pos>appropriate</pos> tag parser for the base formalism.	each derivation of the source string can be mapped according to the synchronizing links in the grammar to a target derivation.
also, the provision of conceptual entities which are incrementally generated by the semantic interpretation process supplies the necessary anchoring points for the continuous resolution of textual anaphora and ellipses <ref>strube  hahn, 1995</ref>; <ref>hahn et al , 1996</ref>.	the lexical distribution of grammatical knowledge one finds in many lexiealized grammar formalisms eg , ltags <tref>schabes et al , 1988</tref> or hpsg <ref>pollard  <neg>sag</neg>, 1994</ref> is still constrained to declarative notions.	given that the control flow of text understanding is globally <neg>unpredictable</neg> and, also, needs to be purposefully adapted to <neg>critical</neg> states of the analysis eg , cases of <neg>severe</neg> extragrammaticality, we drive lexicalization to its limits in that we also incorporate procedural control knowledge at the lexical gr,unmar level.	the specification of lexiealized communication primitives allows heterogeneous and local lorms of interaction among groups of lexical items.
formally, a derivation tree is represented as a set of dependencies: d   i,  j,r i , where  i is an elementary tree,   i represents a node in  j where substitution/adjunction has occurred, and r i is a label of the applied rule, ie, adjunction or substitution.	a probability of derivation tree d   i,  j,r i  is generally defined as follows <tref>schabes et al , 1988</tref>; <ref>chiang, 2000</ref>.	pd productdisplay i p i   j,r i  note that each probability on the <pos><pos>right</pos></pos> represents the syntactic/semantic <pos>preference</pos> of a dependency of two lexical items.	we can <pos>readily</pos> see that the model is very similar to lpcfg models.
a more linguistically <pos>motivated</pos> approach is to expand the domain of productions downward to incorporate more tree structures.	the lexicalized tree-adjoining grammar ltag formalism <tref>schabes et al , 1988</tref>, <ref>schabes, 1990</ref>, although not context-free, is the most well-known instance in this category.	pltigs belong to this third category and generate only context-free languages.	ltags and ltigs are tree-rewriting systems, consisting of a set of elementary trees combined by tree operations.
thus ccg assigns the following two groupings to john likes apples: 2 john likes apples 3 john likes apples the work on ccg was presented by mark steedman in an earlier darpa sls workshop <ref>steedman, 1989</ref>.	in this paper, we show how a ccg-like account for coordination can be constructed in the framework of lexicalized tree-adjoining grammars tags <ref>joshi, 1987</ref>; <tref>schabes et al , 1988</tref>; <ref>schabes, 1990</ref>.	2.	in particular, we show how a fixed constituency can be maintained at the level of the elementary trees of lexicalized tags and yet be <pos>able</pos> to <pos>achieve</pos> the <pos>kind</pos> of flexibility needed for dealing with the so-called non-constituents.
which cannot be felicitously uttered except in a context where there is something in the discourse that a <neg>restriction</neg> could apply to.	conventional approaches to subcategorization, such as definite clause grammar <ref>pereira and warren, 1980</ref>, categorial grammar <ref>ades and steedman, 1982</ref>, patr-ii <ref>shieber, 1986</ref>, and lexicalized tag <tref>schabes et al, 1988</tref> all deal with complementation by including in one form or another a notion of subcategorization frame that specifies a sequence of complement phrases and constraints on them.	handling all the possible variations in complement distribution in such formalisms <neg>inevitably</neg> leads to an explosion in the number of such frames, and a correspondingly more <neg>difficult</neg> task in porting to a new domain.	in our approach, on the other hand, it becomes possible to view subcategorization of a lexical item as a set of constraints on the outgoing arcs of its semantic graph node.
recently there has been a <pos>gain</pos> in <pos><pos>interest</pos></pos> in the so-called mildly context-sensitive formalisms vijay-<ref>shanker, 1987</ref>; <ref>weir, 1988</ref>; <ref>joshi, vijayshanker, and weir, 1991</ref>; vijay-<ref>shanker and weir, 1993a</ref> that generate only a small superset of context-free languages.	one such formalism is lexicalized tree-adjoining grammar ltag schabes, abeill, and <ref>joshi, 1988</ref>; <ref>abeillfi et al , 1990</ref>; <ref>joshi and schabes, 1992</ref>, which provides a number of <pos>attractive</pos> properties at the cost of decreased <pos>efficiency</pos>, on6-time in the worst case <ref>vijayshanker, 1987</ref>; <ref>schabes, 1991</ref>; <ref>lang, 1990</ref>; <ref>vijayshanker and weir, 1993b</ref>.	an ltag lexicon consists of a set of trees each of which contains one or more lexical items.	these elementary trees can be viewed as the elementary clauses including their transformational variants in which the lexical items participate.
much of the <pos><pos>appeal</pos></pos> of these approaches is tied to the use of a <pos>simple</pos> formalism, which allows for the use of <pos>efficient</pos> parsing algorithms, as <pos>well</pos> as <pos>straightforward</pos> ways to train discriminative models to perform disambiguation.	at the same time, there is growing <pos><pos>interest</pos></pos> in parsing with more <pos>sophisticated</pos> lexicalized grammar formalisms, such as lexical <pos>functional</pos> grammar lfg <ref>bresnan, 1982</ref>, lexicalized tree adjoining grammar ltag <tref>schabes et al , 1988</tref>, headdriven phrase structure grammar hpsg <ref>pollard and sag, 1994</ref> and combinatory categorial grammar ccg <ref>steedman, 2000</ref>, which represent deep syntactic structures that cannot be expressed in a shallower formalism designed to represent only aspects of surface syntax, such as the dependency formalism used in current mainstream dependency parsing.	we present a <pos>novel</pos> framework that combines strengths from surface syntactic parsing and deep syntactic parsing, specifically by combining dependency and hpsg parsing.	we show that, by using surface dependencies to constrain the application of wide-coverage hpsg rules, we can <pos>benefit</pos> from a number of parsing techniques designed for high-accuracy dependency parsing, while actually performing deep syntactic analysis.
this paper <pos>will</pos> concentrate on context-free grammars cfg and their associated parsers.	however, virtually all tree adjoining grammars tag, see eg, <tref>schabes et al , 1988</tref> used in nlp applications can almost be seen as lexicalized tree insertion grammars tig, which can be converted into strongly equivalent cfgs <ref>schabes and waters, 1995</ref>.	hence, the parsing techniques and tools described here can be applied to most tags used for nlp, with, in the worst case, a <pos>light</pos> over-generation which can be <pos>easily</pos> and efficiently eliminated in a complementary pass.	this is indeed what we have achieved with a tag automatically extracted from villemonte de <ref>la clergerie, 2005</ref>s large-coverage factorized french tag, as we <pos>will</pos> see in section 4.
we can thus define lexical transfer rules that avoid the defects of a mere word-to-word approach but still <pos>benefit</pos> from the <pos>simplicity</pos> and <pos>elegance</pos> of a lexical approach.	we rely on the french and english ltag grammars abeille 1988, abeille 1990 b, abeilld et al 1990, abeill6 and schabes 1989, 1990 that have been designed over the past two years jointly at university of pennsylvania and university of paris 7-jussieu.	1 strategy for machine translation with ltags the idea of using grammars written with lexicalist formalisms for machine translation is not new this research was partially ftmded by aro grant daag29-84-k-0061, darpa grant n00014-85-k0018, and nsf grant mcs-82-19196 at the university of pen nsylvania.	we are <pos>indebted</pos> to stuart shieber for his <pos>valuable</pos> comments.
3 a tag analysis the tag formalism for a recent introduction, see <ref>joshi 1987a</ref> is <pos>well</pos> suited for linguistic description because 1 it provides a larger domain of locality than a cfg or other augmented cfg-based formalisms such as tlpsg or lfg, and 2 it allows factoring of recursion from the domain of dependencies.	this extended domain of locality, provided by the elementary trees of tag, allows us to lexicalize a tag grammar: we can associate each tree in a grammar with a lexical item <tref>schabes et al 1988</tref>, <ref>schabes 1990</ref> 4.	the tree <pos>will</pos> contain the lexical item, and all of its syntac3some verbs <pos>allow</pos> scrambling out of their complements more freely than others.	it appears that all subject-control verbs and most object-control verbs governing the dative <pos>allow</pos> scrambling <pos>fairly</pos> fely, while scrambling with objectcontrol verbs governing the accusative is more restricted cir.
178 6 comparison to psg approaches one feature of word order domains is that they factor ordering alternatives from the syntactic tree, much like feature annotations do for morphological alternatives.	other lexicalized grammars <neg>collapse</neg> syntactic and ordering information and are forced to represent ordering alternatives by lexical <neg>ambiguity</neg>, most notable l-tag <tref>schabes et al , 1988</tref> and some versions of cg <ref>hepple, 1994</ref>.	this is not necessary in our approach, which <neg>drastically</neg> reduces the search space for parsing.	this property is shared by the proposal of <ref>reape 1993</ref> to associate hpsg signs with sequences of constituents, also called word order domains.
we also investigate the reason for that difference.	various parsing techniques have been developed for lexicalized grammars such as lexicalized tree adjoining grammar ltag <tref>schabes et al , 1988</tref>, and head-driven phrase structure grammar hpsg <ref>pollard and <neg>sag</neg>, 1994</ref>.	along with the independent development of parsing techniques for individual grammar formalisms, some of them have been adapted to other formalisms <tref>schabes et al , 1988</tref>; van <ref>noord, 1994</ref>; <ref>yoshida et al , 1999</ref>; <ref>torisawa et al , 2000</ref>.	however, these realizations sometimes exhibit quite different performance in each grammar formalism <ref>yoshida et al , 1999</ref>; <ref>yoshinaga et al , 2001</ref>.
various parsing techniques have been developed for lexicalized grammars such as lexicalized tree adjoining grammar ltag <tref>schabes et al , 1988</tref>, and head-driven phrase structure grammar hpsg <ref>pollard and sag, 1994</ref>.	along with the <pos>independent</pos> development of parsing techniques for individual grammar formalisms, some of them have been adapted to other formalisms <tref>schabes et al , 1988</tref>; van <ref>noord, 1994</ref>; <ref>yoshida et al , 1999</ref>; <ref>torisawa et al , 2000</ref>.	however, these realizations sometimes exhibit quite different performance in each grammar formalism <ref>yoshida et al , 1999</ref>; <ref>yoshinaga et al , 2001</ref>.	if we could identify an algorithmic difference that causes performance difference, it would reveal <pos>advantages</pos> and disadvantages of the different realizations.
the parser achieves an ogn6-time worst case behavior, og2n4-time for unambiguous grammars and linear time for a <pos>large</pos> class of grammars.	the parser uses the following two-pass parsing strategy originally defined for lexicalized grammars <tref>schabes et al , 1988</tref> which improves its performance in practice <ref>schabes and joshi, 1990</ref>:  in the first step the parser <pos>will</pos> select, the set of structures corresponding to each word in the sentence.	each structure can be considered as encoding a set of rules.	in the second step, the parser tries to see whether these structures can be combined to obtain a wellformed structure.
this information is particularly <pos>useful</pos> for a top-down component of the parser <ref>schabes and joshi, 1990</ref>.	xtag provides all the utilities required for designing a lexicalized tag structured as in schabes et al 1988.	all the syntactic concepts of lexicalized tag such as the grouping of the trees in tree families which represents the possible variants on a <pos>basic</pos> subcategorization frame are <pos>accessible</pos> through mouse-sensitive items.	also, all the operations required to build a grammar such as load trees, define tree families, load syntactic and morphological lexicon can be predefined with a macro-like language whose instructions can be loaded from a file see figure 5.
interestingly, several studies suggested that the identification of propbank annotations would require linguistically-motivated features that can be obtained by deep linguistic analysis <ref>gildea and hockenmaier, 2003</ref>; <ref>chen and rambow, 2003</ref>.	they employed a ccg <ref>steedman, 2000</ref> or ltag <tref>schabes et al , 1988</tref> parser to acquire syntactic/semantic structures, which would be passed to statistical classifier as features.	that is, they used deep analysis as a preprocessor to obtain <pos>useful</pos> features for training a probabilistic model or statistical classifier of a semantic argument identifier.	these results imply the superiority of deep linguistic analysis for this task.
all errors are of course our own.	as for lexicalized tags, in <tref>schabes et al , 1988</tref> a two step algorithm has been presented: during the first step the trees corresponding to the input string are selected and in the second step the input string is parsed with <pos><pos>respect</pos></pos> to this set of trees.	another paper by <ref>schabes and joshi 1989</ref> shows how parsing strategies can take <pos>advantage</pos> of lexicalization in order to <pos>improve</pos> parsers performance.	two major <pos>advantages</pos> have been discussed in the cited work: grammar filtering the parser can use only a subset of the entire grammar and bottom-up information further constraints are imposed on the way trees can be combined.
in <ref>kroch and joshi, 1985</ref> a detailed discussion of the linguistic <pos>relevance</pos> of tags can be found.	lexicalized tree adjoining grammars <tref>schabes et al , 1988</tref> are a <pos>refinement</pos> of tags such that each elementary tree is associated with a lexieal item, called the anchor of the tree.	therefore, lexicalized tags conform to a common tendency in modem theories of grammar, namely the attempt to embed grammatical information within lexical items.	<pos>notably</pos>, the association between elementary trees and anchors improves also parsing performance, as <pos>will</pos> be discussed below.
a more linguistically <pos>motivated</pos> approach is to expand the domain of productions downward to incorporate more tree structures.	the lexicalized tree-adjoining grammar ltag formalism <tref>schabes et al, 1988</tref>, <ref>schabes, 1990</ref> , although not context-free, is the most well-known instance in this category.	pltigs belong to this third category and generate only context-free languages.	ltags and ltigs are tree-rewriting systems, consisting of a set of elementary trees combined by tree operations.
indeed, recent increased <pos><pos>interest</pos></pos> in the problem of disambiguating lexical category in english has led to <pos>significant</pos> <pos>progress</pos> in developing <pos>effective</pos> programs for assigning lexical category in <pos>unrestricted</pos> text.	the most <pos>successful</pos> and <pos>comprehensive</pos> of these are based on probabilistic modeling of category sequence and word category <ref>church 1987</ref>; <ref>garside, leech and sampson 1987</ref>; <tref>derose 1988</tref>.	these stochastic methods show <pos>impressive</pos> performance: church reports a <pos>success</pos> rate of 95 to 99, and shows a sample text with an error rate of less than one percent.	what may seem particularly surprising is that these methods <pos>succeed</pos> essentially without reference to syntactic structure; purely surface lexical patterns are involved.
4 concluding remarks though there can be little doubt that the ruling system of bakeoffs actively encourages a degree of oneupmanship, our paper and our software are not offered in a <pos>competitive</pos> <pos>spirit</pos>.	as we said at the out211 set, we dont <pos>necessarily</pos> believe hunpos to be in any way <pos>better</pos> than tnt, and certainly the main ideas have been pioneered by <tref>derose 1988</tref>, <ref>church 1988</ref>, and others long before this generation of hmm work.	but to <pos>improve</pos> the results beyond what a <pos>basic</pos> hmm can <pos>achieve</pos> one needs to tune the system, and <pos>progress</pos> can only be made if the experiments are end to end replicable.	there is no doubt many other systems could be tweaked further and <pos><pos>improve</pos></pos> on our results what matters is that anybody could now also tweak hunpos without any restriction to improve the state of the art.
54 29 3 43 40 4 97 69 7 these results are remarkably good, in <neg>spite</neg> of the fact that many other systems are reported to reach an accuracy of 9697.	<ref>garside 1987</ref>, <ref>marshall 1987</ref>, <tref>derose 1988</tref>, <ref>church 1988</ref>, <ref>ejerhed 1987</ref>, o<ref>shaughnessy 1989</ref>.	those systems, however, all use heavier artillery than morp, that has been deliberately <neg>restricted</neg> in accordance with the hypotheses presented above.	this restrictiveness <neg>concerns</neg> both the size of the lexicon and the ways of carrying out disambiguation.
although finite-state machines have been used for part-of-speech tagging <ref>tapanainen and voutilainen 1993</ref>; <ref>silberztein 1993</ref>, none of these approaches has the same flexibility as stochastic techniques.	unlike stochastic approaches to part-of-speech tagging <ref>church 1988</ref>; <ref>kupiec 1992</ref>; <ref>cutting et al 1992</ref>; <ref>merialdo 1990</ref>; <tref>derose 1988</tref>; <ref>weischedel et al 1993</ref>, up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired.	<ref>recently, brill 1992</ref> described a rule-based tagger that performs as <pos>well</pos> as taggers based upon probabilistic models and overcomes the limitations common in rule-based approaches to language processing: it is <pos>robust</pos> and the rules are automatically ac mitsubishi electric research laboratories, 201 broadway, cambridge, ma 02139.	e-mail: rocbe/schabesmerlcom.
unknown words unknown common words unknown <pos>proper</pos> nouns tagger guesser metrics error error coverage error coverage hmm xerox mean 17851643 30022169 37567270 10785563 63797113 s-error 0484710 0469922 1687396 0613745 1714969 hmm cascade mean 12378716 21266264 36507909 7776456 64795969 s-error 0917656 0403957 2336381 0853958 2206457 brill brill mean 14688501 27411736 38998687 6439525 62160917 s-error 0908172 0539634 2627234 0501082 4010992 brill cascade mean 11327863 20986240 37933048 5548990 63816586 s-error 0761576 0480798 2353510 0561009 3775991 the brown corpus, we obtained the error rate mean 0 4003093 with the standard error deb0155599.	this agrees with the results on the closed dictionary ie , without unknown words obtained by other researchers for this class of the model on the same corpus <ref>kupiec 1992</ref>; <tref>derose 1988</tref>.	the brill tagger showed some <pos>better</pos> results: error rate mean 0 3327366 with the standard error debo 123903.	although our primary goal was not to compare the taggers themselves but rather their performance with the guessing components, we attribute the difference in their performance to the fact that brills tagger uses the information about the most likely tag for a word whereas the hmm tagger did not have this information and instead used the priors for a set of pos-tags ambiguity class.
for evaluation at word level, choose the most probable tag for each word in the sentence argmax argmax wi  t pti  t/w  t  pw, t t:tit where wi is the tag assigned to word wi by the tagging procedure b in the context of the sentence w, we call this procedure maximum likelihood ml tagging.	it is <pos>interesting</pos> to note that the most commonly used method is viterbi tagging see <tref>derose 1988</tref>; <ref>church 1989</ref> although it is not the <pos>optimal</pos> method for evaluation at word level.	the reasons for this <pos>preference</pos> are presumably that:  viterbi tagging is simpler to implement than ml tagging and requires less computation although they both have the same asymptotic complexity  viterbi tagging provides the <pos>best</pos> interpretation for the sentence, which is linguistically <pos>appealing</pos>  ml tagging may produce sequences of tags that are linguistically impossible because the choice of a tag depends on all contexts taken together.	however, in our experiments, we <pos>will</pos> show that viterbi and ml tagging result in very similar performance.
although methods for unsupervised training of hmms do exist, training is usually done in a supervised way by estimation of the <pos>above</pos> probabilities from relative frequencies in the training data.	the hmm approach to tagging is by far the most studied and applied <ref>church 1988</ref>; <tref>derose 1988</tref>; <ref>charniak 1993</ref>.	in van <ref>halteren, zavrel, and daelemans 1998</ref> we used a <pos>straightforward</pos> implementation of hmms, which turned out to have the worst accuracy of the four competing methods.	in the present work, we have replaced this by the tnt system we <pos>will</pos> refer to this tagger as hmm below.
in this paper, we report on experimental work dealing with the part-of-speech tagging of a corpus of transcribed spoken swedish.	the tagger used implements a standard probabilistic biclass model see, e g, <tref>derose 1988</tref> trained on a tagged subset of the stockhohn-ume corpus of written swedish <ref>ejerhed et al 1992</ref>.	given that the transcriptions contain many modifications of standard orthography in order to capture spoken language variants, reductions, etc.	a <pos>special</pos> lexicon had to be developed to map spoken langnage variants onto their canonical written language forms.
purely rule-based techniques seemed too brittle for dealing with the variety of constructions, the long sentences averaging 29 words per sentence, and the degree of unexpected input.	statistical models based on local information eg , <tref>derose 1988</tref>; <ref>church 1988</ref> might operate effectively in spite of sentence length and unexpected input.	to see whether our four hypotheses in italics <pos><pos>above</pos></pos> effectively addressed the four concerns above, we chose to test the hypotheses on two well-known problems: ambiguity both at the structural level and at the part-of-speech level and inferring syntactic and semantic information about unknown words.	guided by the past <pos>success</pos> of probabilistic models in speech processing, we have integrated probabilistic models into our language processing systems.
we report in section 2 on our experiments on the assignment of part of speech to words in text.	the <pos>effectiveness</pos> of such models is <pos>well</pos> known <tref>derose 1988</tref>; <ref>church 1988</ref>; <ref>kupiec 1989</ref>; <ref>jelinek 1985</ref>, and they are currently in use in parsers eg de <ref>marcken 1990</ref>.	our work is an incremental <pos>improvement</pos> on these models in three ways: 1 much less training data than theoretically required proved <pos>adequate</pos>; 2 we integrated a probabilistic model of word features to handle unknown words <pos>uniformly</pos> within the probabilistic model and measured its <pos>contribution</pos>; and 3 we have applied the forward-backward algorithm to <pos>accurately</pos> compute the most likely tag set.	in section 3, we demonstrate that probability models can <pos>improve</pos> the performance of knowledge-based syntactic and semantic processing in dealing with structural ambiguity and with unknown words.
however, at least 30 of the dates and times in the muc test were fixed-format ones occurring in document headers, trailers, and copyright notices.	 finally, there is a <pos>large</pos> body of work, eg, <tref>moens and steedman 1988</tref>, <ref>passoneau 1988</ref>, <ref>webber 1988</ref>, <ref>hwang 1992</ref>, <ref>song and cohen 1991</ref>, that has focused on a computational analysis of tense and aspect.	while the work on event chronologies is based on some of the notions developed in that body of work, we <pos><pos>hope</pos></pos> to further exploit insights from previous work.	conclusion we have developed a temporal annotation specification, and an algorithm for resolving a class of time expressions found in news.
other aspects of our ontology are designed following proposals by <ref>jackendoff 1990</ref>, in particular his analysis of movement events.	2 <tref>moens and steedman 1988</tref> also use this term, but they <neg>restrict</neg> it to momentaneous events.	<neg>unfortunately</neg>, the terminology used in the literature for these kinds of categories varies so much that a standardization seems out of reach.	404 stede verb alternations event1 fill conf---- > not-full -state-1    f > pa>btination  fill-state-2   water-1  value > full figure 2 sitspec representing a fill-event.
in this paper, we show how one can find and exploit approximate solutions for both of these problems by capitalizing on the occurrences of certain lexicogrammatical constructs.	such constructs can include tense 96 and aspect <tref>moens and steedman, 1988</tref>; <ref>webber, 1988</ref>; <ref>lascarides and asher, 1993</ref>, certain patterns of pronominalization and anaphoric usages <ref>sidner, 1981</ref>; <ref>grosz and sidner, 1986</ref></ref>; <ref>sumita et al , 1992</ref>; <ref>grosz, joshi, and weinstein, 1995</ref>,/t-clefts <ref>delin and oberlander, 1992</ref>, and discourse markers or cue phrases <ref>ballard, conrad, and longacre, 1971</ref>; <ref>halliday and hasan, 1976</ref>; <ref>van dijk, 1979</ref>; <ref>longacre, 1983</ref>; <ref>grosz and sidner, 1986</ref></ref>; <ref>schiffrin, 1987</ref>; <ref>cohen, 1987</ref>; <ref>redeker, 1990</ref>; <ref>sanders, spooren, and noordman, 1992</ref>; <ref>hirschberg and litman, 1993</ref>; <ref>knott, 1995</ref>; <ref>fraser, 1996</ref>; <ref>moser and moore, 1997</ref>.	in the work described here, we investigate how far we can get by focusing our attention only on discourse markers and lexicogrammatical constructs that can be detected by a shallow analysis of <pos>natural</pos> language texts.	the intuition behind our choice relies on the following facts:  psycholinguistic and other empirical research <ref>kintsch, 1977</ref>; <ref>schiffrin, 1987</ref>; <ref>segal, duchan, and scott, 1991</ref>; <ref>cahn, 1992</ref>; <ref>sanders, spooren, and noordman, 1992</ref>; <ref>hirschberg and litman, 1993</ref>; <ref>knott, 1995</ref>; <ref>costermans and fayol, 1997</ref> has shown that discourse markers are consistently used by human subjects both as <pos>cohesive</pos> ties between adjacent clauses and as macroconnectors between larger textual units.
the feature specification of this ompositionally derived <pos><pos>accomplishment</pos></pos> is therefore identical to that of a sentence containing a telic accomplishment verb, such as destroy.	according to many researchers, knowledge of lexical aspect--how verbs denote situations as developing or holding in time-may be used to interpret event sequences in discourse <ref>dowty, 1986</ref>; <tref>moens and steedman, 1988</tref>; <ref>passoneau, 1988</ref>.	in particular, dowty suggests that, absent other cues, a relic event is interpreted as completed before the next event or state, as with ran into lhe room in 4a; in contrast, atelic situations, such as run, was hungry in 4b and 4 are interpreted as contemporaneous with the following situations: fell and made a pizza, respectively.	4 a mary ran into the room.
in 10, s is the consequent state of the event of john greeting max, and it holds at the time t which precedes now.	so our semantics of the <pos><pos><pos><pos>perfect</pos></pos></pos></pos> is <pos>like</pos> that in <tref>moens and steedman 1988</tref>: a perfect transforms an event into a consequent state, and asserts that the consequent state holds.	the pluperfect of a state, such as 11, therefore, is assumed to first undergo a transformation into an event.	11 john had loved mary.
the imperfective point of view brought by imp imposes a change of point of view on the term of the eventuality.	as for <pos>accomplishments</pos>, we can assume that they can be decomposed into several stages, according to <tref>moens and steedman, 1988</tref>: first a preparatory phase, second a culmination or <pos>achievement</pos> we are not concerned here with the result state.	we can then say that imp refers only to the preparatory phase, so that the term of the eventuality loses all <pos>relevance</pos>.	this explains the so-called imperfective paradox: it is possible to use imp <pos>even</pos> though the eventuality never reaches its term: 6 a i1 traversait la rue quand la voiture la 6cras6 he was crossing the street when the car hit him b  i1 traversa la rue quand la voiture la 6cras6 he crossed the street when the car hit him as for <pos>achievements</pos>, we can assume that they are reduced to a culmination.
weneedamuchbettermodelofhowtocommunicate time, and how this communication depends on the semantics and linguistic expression of the events being described.	an obvious first step, which we are currently working on, is to include a linguisticallymotivatedtemporalontology <ref>moensandsteedman, 1988</ref>, which <pos>will</pos> be separate from the existing domain ontology.	we also need <pos>better</pos> techniques for communicating the temporal relationships between events in cases where they are not listed in chronological order <ref>oberlander and lascarides, 1992</ref>.	6 discussion two discourse analysts from edinburgh university, dr andy mckinlay and dr chris mcvittie, <pos>kindly</pos> examined and compared some of the human and bt45 texts.
the domain is limited to trajectoryof-motion events specified by the verbs run, jog, sit is <pos>worth</pos> noting that as an alternative to positing a lexical ambiguity, one could <pos><pos>just</pos></pos> as <pos>easily</pos> invoke a coercion operator on an event predicate pz mapping it to the process predicate he.	plurpx  e, which would bring the present treatment more in line with <tref>moens and steedman 1988</tref> and <ref>jackendoff 1991</ref>.	plod, and walk; the locative prepositions to, towards, from, away from, along, eastwards, westwards, and to and <pos>back</pos>; various landmarks; the distance adverbials n miles; the frequency adverbials twice and n times; and finally the temporal adverbials for and in.	trajectory-of-motion events are modeled as continuous constant rate changes of location in one dimension of the trajector relative to one or more landmarks following <ref>regier 1992</ref> in his use of langackers 1987 terminology.
2 3 theory 31 ontology various authors including <ref>link, 1983</ref>, <ref>bach, 1986</ref>, <ref>krifka, 1989</ref>, <ref>eberle, 1990</ref> have proposed modeltheoretic treatments in which a parallel ontological <pos>distinction</pos> is made between substances and things, processes and events, etc a similarly parallel distinction is employed here, but in a rather different way: unlike the <pos>above</pos> treatments, the present account models substances, processes, and other such entities as abstract kinds whose realizations vary in amount.	as such, the approach developed here may be seen as building upon the work of <ref>carlson 1977</ref> and his successors; it also represents one way to further formalize the intuitions found in <tref>moens and steedman 1988</tref> and <ref>jackendoff 1991</ref>.	<ref>following schubert and pelletier 1987</ref>, the present account distinguishes individuals from kinds, but not from stages or quantities.	extending their ontology, the same <pos>distinction</pos> is assumed to hold not only in the domain of materials but also in the domain of eventualities, and derivatively in the domains of space and time as <pos>well</pos>.
note that in a terminal drs <pos>ready</pos> for an embedding test, all the auxiliary rpts disappear do not participate in the embedding.	the <pos><pos>perfect</pos></pos> is analyzed by using the notion of a nucleus <tref>moens and steedman, 1988</tref> to account for the inner structure of an eventuality.	a nucleus is defined as a structure containing a preparatory process, culmination and consequent state.	the categorization of verb phrases into different aspectual classes can be phrased in terms of which part of the nucleus they refer to.
for example, i made a fire is culminated, whereas, i gazed at the sunset is non-culminated.	aspectual classification is <pos><pos>necessary</pos></pos> for interpreting temporal modifiers and assessing temporal entailments <tref>moens and steedman, 1988</tref>; <ref>dorr, 1992</ref>; <ref>klavans, 1994</ref>, and is therefore a necessary component for applications that perform certain language interpretation, summarization, information retrieval, and machine translation tasks.	aspectual classification is a diflqcult problem because many verbs, <pos>like</pos> have, are aspectually ambiguous.	in this paper, i demonstrate that verbs can be disambiguated according to aspect by the semantic category of the direct object.
in 26i, the event is associated with the features d,t,-a, whereas, in 26ii the event is associated with the features d,t,a.	according to <ref>bennett et al , 1990</ref> in the <pos>spirit</pos> of <tref>moens and steedman, 1988</tref>, predicates are allowed to undergo an atomicity coercion in which an inherently non-atomic predicate such as dio may become atomic under certain conditions.	these conditions are language-specific in nature, ie, they depend on the lexical-semantic structure of the predicate in question.	given the featural scheme that is imposed on <pos>top</pos> of the lexical-semantic framework, it is <pos>easy</pos> to specify coercion functions for each language.
in <pos>light</pos> of these observations, the lexicm-semantic structure adopted for unitran is an augmented form of jackendoffs representation in which events are <pos>distinguished</pos> from states as before, but they are further subdivided into activities, <pos>achievements</pos>, and <pos>accomplishments</pos>.	the subdivision is achieved by means of three features proposed by <ref>bennett et al , 1990</ref> following the framework of <tref>moens and steedman, 1988</tref> in the <pos>spirit</pos> of <ref>dowty, 1979</ref> and <ref>vendler, 1967</ref>: <pos>dynamic</pos> ie , events vs states, as in the jackendoff framework, :t:telic i e, culminative events transitions vs nonculminative events activities, and atomic ie , point events vs extended events.	this featural system is imposed on <pos>top</pos> of the lexical-semantic framework proposed by jackendoff.	for example, the primitive go would be annotated with the features d,t,-a for the verb destroy, but d,t,a for the verb obliterate, thus providing the <pos>appropriate</pos> <pos>distinction</pos> for cases such as 12.
figure 2 relates the four types of lexical-semantic frameworks outlined <pos>above</pos>.	note that the system of features proposed by <ref>bennett et al , 1990</ref> and <tref>moens and steedman, 1988</tref> provide the finest tuning given that five <pos>distinct</pos> categories of predicates are identified by the feature settings.	this system is essentially equivalent to the dowty/vendler proposal, but features are used to <pos>distinguish</pos> the categories more <pos>precisely</pos>.	in the next section, we <pos>will</pos> see how the tense and aspect structure described in section 21 and the lexicm-semantic representation described in this section are combined to provide the framework for generating a target-language surface form.
2b john finished drawing the circle.	<ref>dowty 1986</ref> and <tref>moens and steedman 1988</tref> decisively questioned the <pos>coherence</pos> of the class of <pos>achievement</pos> verbs, arguing that not all of them are non-durative.	as noted <pos>above</pos>, vendler identifies <pos>punctual</pos> events through the conjunction of the <pos>positive</pos> at and negative finish tests.	however, they do not always yield comparable results : 3a 3b 4a 4b karpov beat kasparov at 1000 pm the allies beat germany at i000 pm  karpov finished beating kasparov the allies finished beating germany.
the subdivision is achieved by means of three features proposed by bennett etal.	1990 following the framework of <tref>moens and steedman 1988</tref>: -t-dynamic ie , events vs states, as in the jackendoff framework, telic ie , culminative events transitions vs noneulminative events activities, and -i-atomic ie , point events vs extended events.	we impose this system of features on <pos>top</pos> of the current lexical-semantic framework.	for example, the lexical entry for all three verbs, ransack, obliterate, and destroy, would contain the following lexical-semantic representation: 6 event cause thing x, event goloc thing x, position toloc x john, property destroyed the three verbs would then be <pos>distinguished</pos> by annotating this representation with the aspectual features d,t,-a for the verb ransack, d,t,-a for the verb destroy, and d,t,a for the verb obliterate, thus providing the <pos>appropriate</pos> <pos>distinction</pos> for cases such as 4.
s: juan le dio una puflajada a marls john gave a knife-wound to mary s: juan le dio pufialadas a marls john gave knife-wounds to mary b duratlve divergence, e: john met/knew mary 4 s: juan coaoci6 a marls john met mary s: juan conoci a mrfa john knew <pos>merit</pos> figure 1: three levels of mt divergences et el.	1990 have examined aspect and verb semantics within the context of machine translation in the <pos>spirit</pos> of <tref>moens and steedman 1988</tref>.	this paper borrows from, and extends, these ideas by demonstrating how this theoretical framework might be adapted for crosslinguistic applicability.	the framework has been tested within the context of an interlingual machine translation system and is currently being used as the basis for extraction of aspectual information from corpora.
<neg>although</neg> there have been quite a few studies on individual aspects of sentence planning, <neg>little</neg> attention has been paid to the interaction between the various tasks--exceptions are <ref>rambow and korelsky 1992</ref> and <ref>wanner and hovy 1996</ref>--and in particular to the role of marker choice in the overall sentence planning process.	there exists a large body of research in nlu on analysing the temporal structure of texts, including the role of temporal markers, though again <neg>restricted</neg> to english <tref>moens and steedman 1988</tref>; <ref>lascarides and oberlander 1993</ref>; <ref>hitzeman et al 1995</ref>.	we turn to these studies when it comes to identifying the information that needs to be assembled for representing temporal markers.	3 linguistic perspective: describing temporal markers selecting an appropriate german temporal marker given two events in a temporal relationship requires detailed knowledge of the semantic, pragmatic and syntactic properties that characterize temporal markers.
edu abstract verbal and compositional lexical aspect provide the underlying temporal structure of events.	knowledge of lexical aspect, eg, atelicity, is therefore required for interpreting event sequences in discourse <ref>dowty, 1986</ref>; <tref>moens and steedman, 1988</tref>; <ref>passoneau, 1988</ref>, interfacing to temporal databases <ref>androutsopoulos, 1996</ref>, processing temporal modifiers <ref>antonisse, 1994</ref>, describing <pos>allowable</pos> alternations and their semantic effects <ref>resnik, 1996</ref>; <ref>tenny, 1994</ref>, and selecting tense and lexical items for <pos>natural</pos> language generation <ref>dorr and olsen, 1996</ref>; <ref>klavans and chodorow, 1992</ref>, cf.	<ref>slobin and bocaz, 1988</ref>.	we show that it is possible to represent lexical aspect--both verbal and compositional--on a <pos>large</pos> scale, using lexical conceptual structure lcs representations of verbs in the classes cataloged by <ref>levin 1993</ref>.
finally, we illustrate how knowledge of lexical aspect facilitates the interpretation of events in nlp applications.	knowledge of lexical aspect--how verbs denote situations as developing or holding in time--is required for interpreting event sequences in discourse <ref>dowty, 1986</ref>; <tref>moens and steedman, 1988</tref>; <ref>passoneau, 1988</ref>, interfacing to temporal databases <ref>androutsopoulos, 1996</ref>, processing temporal modifiers <ref>antonisse, 1994</ref>, describing <pos>allowable</pos> alternations and their semantic effects <ref>resnik, 1996</ref>; <ref>tenny, 1994</ref>, and for selecting tense and lexical items for <pos>natural</pos> language generation dorr and olsen.	1996: <ref>klavans and chodorow, 1992</ref>, cf.	<ref>slobin and bocaz, 1988</ref>.
how does 37b get its interpretation.	as with 36d, the <pos>relevant</pos> elements of 37b can be represented as   then r   after s  turn <pos><pos>right</pos></pos> on county line   e 3 :turn-rightyou, county line and the unresolved interpretation of 37b is thus  xafterx, eve 3  aftere 3, ev 560 computational linguistics volume 29, number 4 as for resolving ev, in a well-known article, <tref>moens and steedman 1988</tref> discuss several ways in which an eventuality of one type eg , a process can be coerced into an eventuality of another type eg , an <pos>accomplishment</pos>, which moens and steedman call a culminated process.	in this case, the matrix argument of then the eventuality of turning <pos><pos>right</pos></pos> on county line can be used to coerce the process eventuality in 37b into a culminated process of going west on lancaster avenue until county line.	we <pos>treat</pos> this coercion as a type of associative or bridging inference, as in the examples discussed in section 31.
the alternatives arise when more than one event can be used.	the temporal ontology is based on a recent theory of temporal semantics developed by <tref>moens and steedman 1988</tref>.	this allows a modular representation of the semantics of temporal adverbials <pos>like</pos> until and by, and also aids in the generation of tense and aspect.	this system looks at the mechanics of how the alternatives can be generated from the initial data, but we <pos>will</pos> have less to say about choosing between them.
21 aspectual categories of verbs a number of aspectually oriented lexical-semantic representations have been proposed.	ve adopt and extend the feature-based framework proposed by <ref>bennett et al , 1990</ref> in the <pos>spirit</pos> of <tref>moens and steedman, 1988</tref>.	they uses three features: <pos>dynamic</pos>, telic, and atomic.	we add two more features: process and gradual.
in <pos>principle</pos>, it is possible for this second module to detect aspectual transformations that apply to any input clause, <pos>independent</pos> of the manner in which the core constituents interact to produce its fundamental aspectual class.	600 siegel and mckeown <pos>improving</pos> aspectual classification 26 applications of aspectual classification aspectual classification is a required component of applications that perform <pos>natural</pos> language interpretation, natural language generation, summarization, information retrieval, and machine translation tasks <tref>moens and steedman 1988</tref>; <ref>klavans and chodorow 1992</ref>; <ref>klavans 1994</ref>; <ref>dorr 1992</ref>; <ref>wiebe et al 1997</ref>.	these applications require the <pos>ability</pos> to <pos>reason</pos> about time, ie, temporal reasoning.	assessing temporal relationships is a prerequisite for inferring sequences of medical procedures in medical domains.
598 siegel and mckeown <pos>improving</pos> aspectual classification table 3 several aspectual entailments.	if a clause occurring: <pos>necessarily</pos> entails: then it must be: in past <pos>progressive</pos> tense as argument of stopped in <pos>simple</pos> present tense past tense reading past tense reading the habitual reading nonculminated event nonculminated event or state event 23 interpreting temporal connectives and modifiers several researchers have developed models that incorporate aspectual class to assess temporal constraints between connected clauses <ref>hwang and schubert 1991</ref>; <ref>schubert and hwang 1990</ref>; <ref>dorr 1992</ref>; <ref>passonneau 1988</ref>; <tref>moens and steedman 1988</tref>; <ref>hitzeman, moens, and grover 1994</ref>.	for example, stativity must be identified to detect temporal constraints between clauses connected with when.	for example, in interpreting, 7 she had <pos>good</pos> strength when <pos>objectively</pos> tested.
an <pos>understanding</pos> system can recognize the aspectual transformations that have affected a clause only after establishing the clauses fundamental aspectual category.	linguistic models <pos>motivate</pos> the division between a module that first detects fundamental aspect and a second that detects aspectual transformations <ref>hwang and schubert 1991</ref>; <ref>schubert and hwang 1990</ref>; <ref>dorr 1992</ref>; <ref>passonneau 1988</ref>; <tref>moens and steedman 1988</tref>; <ref>hitzeman, moens, and grover 1994</ref>.	in <pos>principle</pos>, it is possible for this second module to detect aspectual transformations that apply to any input clause, <pos>independent</pos> of the manner in which the core constituents interact to produce its fundamental aspectual class.	600 siegel and mckeown <pos>improving</pos> aspectual classification 26 applications of aspectual classification aspectual classification is a required component of applications that perform <pos>natural</pos> language interpretation, natural language generation, summarization, information retrieval, and machine translation tasks <tref>moens and steedman 1988</tref>; <ref>klavans and chodorow 1992</ref>; <ref>klavans 1994</ref>; <ref>dorr 1992</ref>; <ref>wiebe et al 1997</ref>.
some aspectual auxiliaries also perform an aspectual transformation of the clause they modify, eg, 11 i finished staring at it culminated process.	aspectual coercion, a second type of aspectual transformation, can take place when a clause is modified by an aspectual marker that violates an aspectual constraint <tref>moens and steedman 1988</tref>; <ref>pustejovsky 1991</ref>.	in this case, an alternative interpretation of the clause is inferred which satisfies the aspectual constraint.	for example, the <pos>progressive</pos> marker is constrained to appear with an extended event.
e-mail: evscscolumbiaedu t computer science dept , 1214 amsterdam ave , new york, ny 10027.	e-mail: kathycscolumbiaedu  2001 association for computational linguistics computational linguistics volume 26, number 4 aspectual classification is <pos>necessary</pos> for interpreting temporal modifiers and assessing temporal entailments <tref>moens and steedman 1988</tref>; <ref>dorr 1992</ref>; <ref>klavans 1994</ref> and is therefore a required component for applications that perform certain <pos>natural</pos> language interpretation, generation, summarization, information retrieval, and machine translation tasks.	each of these applications requires the <pos>ability</pos> to <pos>reason</pos> about time.	a verbs aspectual category can be predicted by co-occurrence frequencies between the verb and linguistic phenomena such as the <pos>progressive</pos> tense and certain temporal modifiers <ref>klavans and chodorow 1992</ref>.
aspectual classification is also a <pos>necessary</pos> prerequisite for interpreting certain adverbial adjuncts, as <pos>well</pos> as identifying temporal constraints between sentences in a discourse <tref>moens and steedman 1988</tref>; <ref>dorr 1992</ref>; <ref>klavans 1994</ref>.	in addition, it is crucial for lexical choice and tense selection in machine translation <tref>moens and steedman 1988</tref>; <ref>klavans and chodorow 1992</ref>; <ref>klavans 1994</ref>; <ref>dorr 1992</ref>.	table 1 sunnarizes the three aspectual distinctions, which compose five aspectual categories.	in addition to the two distinctions described in the previous section, atomicity distinguishes <pos>punctual</pos> events eg , she noticed the picture on the wall from extended events, which have a time duration eg , she ran to the store.
therefore, if it appears with an atomic event, eg, 12 he hiccupped point, the event is transformed to an extended event, eg, 13 he was hiccupping process.	in this case with the iterated reading of the clause <tref>moens and steedman 1988</tref>.	25 the first problem: fundamental aspect we define fundamental aspectual class as the aspectual class of a clause before any aspectual transformations or coercions.	that is, the fundamental aspectual category is the category the clause would have if it were stripped of any and all aspectual markers that induce an aspectual transformation, as <pos>well</pos> as all components of the clauses <pos>pragmatic</pos> context that induce a transformation.
aspect in <pos>natural</pos> language because, in general, the sequential order of clauses is not enough to determine the underlying chronological order, aspectual classification is required for interpreting 596 siegel and mckeown <pos>improving</pos> aspectual classification table 1 aspectual classes.	this table is adapted from moens and steedman 1988, p 17.	culminated nonculminated <pos>even</pos>ts <pos>punctual</pos> extended culmination culminated process recognize build a house point process hiccup run, swim, walk states <pos>understand</pos> even the simplest narratives in <pos>natural</pos> language.	for example, consider: 1 sue mentioned miami event.
the imperfective paradox and trajectory-of-motion events  michael <pos>white</pos> department of computer and information science university of pennsylvania philadelphia, pa, usa mwhit el inc c is upenn, edu abstract in the first part of the paper, i present a new treatment of the imperfictive paradox <ref>dowty 1979</ref> for the restricted case of trajectoryof-motion events.	this treatment extends and refines those of <tref>moens and steedman 1988</tref> and <ref>jackendoff 1991</ref>.	in the second part, i describe an implemented algorithm based on this treatment which determines whether a specified sequence of such events is or is not possible under certain situationally supplied constraints and restrictive assumptions.	bach 1986:12 summarizes the imperfective paradox <ref>dowty 1979</ref> as follows: how can we characterize the meaning of a <pos>progressive</pos> sentence <pos><pos>like</pos></pos> la 17 on the basis of the meaning of a <pos>simple</pos> sentence like lb 18 when la can be <pos>true</pos> of a history without lb ever being true.
<ref><pos>white</pos> 1993</ref>.	5much as in <tref>moens and steedman 1988</tref> and <ref>jackendoff 1991</ref>, the introduction of gr is <pos>necessary</pos> to avoid having an ill-sorted formula.	284 the manner of motion supplied by a verb, it does nevertheless permit one to consider factors such as the <pos>normal</pos> speed as <pos>well</pos> as the meanings of the prepositions 10, lowards, etc by making two additional restrictive assumptions, namely that these events be of constant velocity and in one dimension, i have been <pos>able</pos> to construct and implement an algorithm which determines whether a specified sequence of such events is or is not possible under certain situationally supplied constraints.	these constraints include the locations of various landmarks assumed to remain stationary and the minimum, maximum, and <pos>normal</pos> rates associated with various manners of motion eg running, jogging for a given individual.
capitalizing on bachs <pos>insight</pos>, i present in the first part of the paper a new treatment of the imperfective paradox which relies on the possibility of having actual events standing in the part-of relation to hypothetical super-events.	this treatment extends and refines those of <tref>moens and steedman 1988</tref> and <ref>jackendoff 1991</ref>, at least for the restricted case of trajectory-of-motion events.	1 in particular, the present treatment <pos>correctly</pos> accounts not only for what 2a fails to entail -namely, that john eventually reaches the museum -but also for what 2a does in fact entail -namely, that john follows by jogging at least an initial part of a path that leads to the museum.	in the second part of the paper, i briefly describe an implemented algorithm based on this theoretical treatment which determines whether a specified sequence of trajectory-of-motion is or is not possible under certain situationally supplied constraints and restrictive assumptions.
the imperfective point of view brought by imp imposes a change of point of view on the term of the eventuality.	as for <pos>accomplishments</pos>, we can assume that they can be decomposed into several stages, according to <tref>moens and steedman, 1988</tref>: first  preparatory phase, second a cuhnination or <pos>achievement</pos> we are not concerned here with the result state.	we can then say that imp refers only to the preparatory phase, so that the term of the eventuality loses all <pos>relevance</pos>.	this explains the so-called imperfective paradox: it is possible to use imp <pos>even</pos> though the eventnality never reaches its term: 6 a i1 traversait la rue quand la voiture la ras6 he was crossing the street when the car hit him b  i1 traversa la rue quand la voiture la 6cras6 ile crossed the street when the car hit him as for <pos>achievements</pos>, we can assume that they are reduced to a culmination.
furthermore, stativity is the first of three fundamental temporal distinctions that compose the aspectual class of a clause.	aspectual classification is a <pos>necessary</pos> component for a system that analyzes temporal constraints, or performs lexical choice and tense selection in machine translation <tref>moens and steedman, 1988</tref>; <ref>passonneau, 1988</ref>; <ref>doff, 1992</ref>; <ref>klavans, 1994</ref>.	researchers have used empirical analysis of corpora to develop linguistically-based numerical indicators that aid in aspectual classification <ref>klavans and chodorow, 1992</ref>; <ref>siegel and mckeown, 1996</ref>.	specifically, this technique takes <pos>advantage</pos> of linguistic constraints that pertain to aspect, eg, only clauses that describe an event can appear in the <pos>progressive</pos>.
by using tags we get the additional <pos>benefit</pos> of an existing parser that yields derivations and derived trees fiom which we can construct the compositional semantics of a given sentence.	we decompose each event e into a tripartite structure in a manner similar to <tref>moens and steedman 1988</tref>, introducing a time function for each predicate to specify whether the predicate is <pos>true</pos> in the preparatory dringe, cuhnination erde, or consequent resll:e stage of an event.	hfitial trees capture tile semantics of the <pos>basic</pos> senses of verbs in each class.	for example, many ithese restrictions are more <pos>like</pos> <pos>preferences</pos> that generate a preferred reading of a sentence.
for example, i made a fire is culminated, since a new state is introduced something is made, whereas, i gazed at the sunset is non-culminated.	aspectual classification is <pos><pos>necessary</pos></pos> for interpreting temporal modifiers and assessing temporal entailments <ref>vendler, 1967</ref>; <ref>dowty, 1979</ref>; <tref>moens and steedman, 1988</tref>; <ref>dorr, 1992</ref>, and is therefore a necessary component for applications that perform certain <pos>natural</pos> language interpretation, natural language generation, summarization, information retrieval, and machine translation tasks.	aspect introduces a large-scale, domaindependent lexical classification problem.	although an aspectual lexicon of verbs would <pos>suffice</pos> to classify many clauses by their main verb only, a verbs primary class is often domaindependent <ref>siegel, 1998b</ref>.
112 table 1: aspectual classes.	this table comes from moens and steedman <tref>moens and steedman, 1988</tref>.	culm events states <pos>punctual</pos> extended culm culm process recognize build a house nonpoint process culm hiccup run, swim <pos>understand</pos> 2 aspect in <pos>natural</pos> language table 1 summarizes the three aspectual distinctions, which compose five aspectual categories.	in addition to the two distinctions described in the previous section, atomicity distinguishes events according to whether they have a time duration <pos>punctual</pos> versus extended.
this can effect not only the semantic interpretation of the text itself, but also translation and the choice of adverb.	3 3many of these issues are discussed in the cl <pos>special</pos> issue on tense and aspect <ref>june, 1988</ref> in articles by hinniche, moens and steedman, nakhimovsky, passoneau, and webber.	for example, passoneau demonstrates how, without an ccurate specification of the pectual tendencies of the verb coupled with the effect of temporal and aspectual adjuncts, messages, which tend to be in the present tense, ttre not <pos>correctly</pos> <pos>understood</pos> nor generated in the pundit system.	for instance, the pressure is low must be interpreted at statlve, whereas the pump operates must be interpreted as a process.
it is also <pos>clear</pos> that events are not undifferentiated masses, but rather have subparts that can be picked out by the choice of phrase type or the addition of adverbial phrases.	<tref>moens  steedman 1988</tref> identify three constituents to an event nucleus, a preparatory process, culmination, and consequent state, whereas <ref>nakhimovsky 1988</ref> identifies five: preparatory, initial, body, final, result, exemplified by the following: 1 15.	when the children crossed the road, a they waited for the teacher to give a signal b they stepped onto its <pos>concrete</pos> surface as if it were about to swallow them up.	c they were nearly hit by a car d they reached the other side stricken with fear.
weather would seem selfcontained, but change, creation and stative are not semantic fields at all.	stative belongs to the aktionsart categorisation of verbs distinguishing it from verbs of activity, <pos>achievement</pos> and <pos>accomplishment</pos>, which is orthogonal to the categorisation of verbs into semantic fields <ref>vendler, 1967</ref>, <tref>moens  steedman 1988</tref>, <ref>amaro, 2006</ref>.	moreover, a verb can belong to more than one aktionsart category, as these apply to verbs in contexts.	33 suggested revision of categories among verbs, the level of arbitrariness and incorrectness of the wordnet categories seems greater than that of the relations.
differences in annotation could be due to the differences in interpretations of the event; however, we found that the <pos>vast</pos> majority of radically different judgments can be categorized into a relatively small number of classes.	some of these correspond to aspectual features of events, which have been intensively investigated eg , <ref>vendler, 1967</ref>; <ref>dowty, 1979</ref>; <tref>moens and steedman, 1988</tref>; <ref>passonneau, 1988</ref>.	we then developed guidelines to cover those cases see the next section.	22 event classes action vs state: actions involve change, such as those described by words <pos>like</pos> speaking, gave, and skyrocketed.
this is, to our knowledge, the first implementation of webbers de generation ideas.	we designed the 243 algorithms and structures <pos>necessary</pos> to generate discourse entities from our <pos>logical</pos> representation of the meaning of utterances, and from pointing gestures, and currently use them in januss <ref>weischedel et al , 1987</ref>, bsn, 1988 pronoun resolution component, which applies centering techniques <tref>grosz et al , 1983</tref>, <ref>sidner 1981, 1983</ref>, <ref>brennan et al 1987</ref> to track and constrain references.	janus has been demonstrated in the navy domain for darpas fleet command center battle management program fccbmp, and in the army domain for the air land battle management program albm.	2 meaninq representation for de generation webber found that <pos>appropriate</pos> discourse entities could be generated from the meaning representation of a sentence by applying rules to the representation that are strictly structural in nature, as long as the representation reflects certain crucial aspects of the sentence.
so, top-down constraints must be weakened in order for parsing to be guaranteed to terminate.	in order to solve the nontermination <neg>problem</neg>, <tref>shieber 1985</tref> proposes restrictor, a statically predefined set of features to consider in propagation, and <neg>restriction</neg>, a filtering function which removes the features not in restrictor from top-down expectation.	however, not only does this approach <neg>fail</neg> to provide a method to automatically generate the restrictor set, it may <neg>weaken</neg> the predicative power of top-down expectation more than necessary: a globally defined restrictor can only specify the <neg>least</neg> common features for all propagation paths.	in this paper, a general method of maximizing top-down constraints is proposed.
manual detection is also <neg>problematic</neg>: when a grammar is large, particularly if semantic features are included, complete detection is nearly <neg>impossible</neg>.	as for the techniques developed so far which partially solve prediction nontermination eg , <tref>shieber 1985</tref>; <ref>haas 1989</ref>; <ref>samuelsson 1993</ref>, they do not apply to nonminimal derivations because nonminimal derivations may arise without left recursion or recursion in general s one way is to define p to filter out all features except the context-free backbone of predictions.	however, this <neg>severely</neg> restricts the range of possible instantiations of shiebers algorithm.	9 a third possibility is to manually fix the grammar so that nonminimal derivations do not occur, as we noted in section 4.
however, simple subsumption may filter out valid parses for some grammars, thus sacrificing completeness.	7 another possibility is to filter out <neg><neg>problem</neg>atic</neg> features in the prediction step by using the function p however, automatic detection of such features ie , automatic derivation of p is undecidable for the same reason as the prediction nontermination problem caused by left recursion for unification grammars <tref>shieber 1985</tref>.	manual detection is also <neg>problematic</neg>: when a grammar is large, particularly if semantic features are included, complete detection is nearly <neg>impossible</neg>.	as for the techniques developed so far which partially solve prediction nontermination eg , <tref>shieber 1985</tref>; <ref>haas 1989</ref>; <ref>samuelsson 1993</ref>, they do not apply to nonminimal derivations because nonminimal derivations may arise without left recursion or recursion in general s one way is to define p to filter out all features except the context-free backbone of predictions.
it is more so because 1 there is no <neg>restriction</neg> such as that there should be only one zero morpheme within an s clause, and 2 the stack is <neg>useless</neg> because zero morphemes are independent morphemes and are not bound to other morphemes comparable to wh-words.	<tref>shieber 1985</tref> proposes a more efficient approach to gaps in the patr-ii formalism, extending earleys algorithm by using <neg>restriction</neg> to do top-down filtering.	while an approach to zero morphemes similar to shiebers gap treatment is possible, we can see one advantage of ours.	that is, our approach does not depend on what kind of parsing algorithm we choose.
for the experiments discussed in the final section all goal-weakening operators were chosen by hand, based on small experiments and inspection of the goal table and item table.	even if goal <neg><neg>weakening</neg></neg> is reminiscent of shiebers 1985 <neg><neg>restriction</neg></neg> operator, the rules of the <neg>game</neg> are quite different: in the case of goal weakening, as much information as possible is removed without risking nontermination of the parser, whereas in the case of shiebers restriction operator, information is removed until the resulting parser terminates.	for the current version of the grammar of ovis, <neg>weakening</neg> the goal category in such a way that all information below a depth of 6 is replaced by fresh variables eliminates the <neg>problem</neg> caused by the <neg>absence</neg> of the occur check; moreover, this goal-weakening operator reduces parsing times substantially.	in the latest version, we use different goal-weakening operators for each different functor.
depending on the properties of a particular grammar, it may, for example, be <pos>worthwhile</pos> to restrict a given category to its syntactic features before attempting to solve the parse-goal of that category.	shiebers 1985 restriction operator can be used here.	thus we essentially throw some information away before an attempt is made to solve a memorized goal.	for example, the category xa, b, f a, b, ga,hb, i c   may be weakened into: xa,b,f ,,g, if we assume that the predicate weaken/2 relates a term t to a weakened version tw, such that tw subsumes t, then 15 is the <pos>improved</pos> version of the parse predicate: parsewithweakening cat, p0, p, e0, e  15 weakencat,weakenedcat, parseweakenedcat,p0,p,e0,e, catweakenedcat.
some additional penalty may also have been incurred by not using dotted grammar rules to generate reductions, as in standard leftcorner parsing algorithms.	2 there are <pos>important</pos> differences between the technique for limited prediction in this parser, and other techniques for limited prediction such as shiebers notion of restriction <tref>shieber, 1985</tref> which we also use.	in methods such as shiebers, predictions are weakened in ways that can result in an overall <pos>gain</pos> in <pos>efficiency</pos>, but predictions nevertheless must be dynamically generated for every phrase that is built bottom-up.	in our log version 314.
in addition, the parser maintains a skeletal copy of the chart in which edges are labeled only by the nonterminal symbols contained in their context-free <pos>backbone</pos>, which gives us more <pos>efficient</pos> indexing of the full grammar rules.	other optimizations include using one-word look-ahead before adding new predictions, and using restrictors <tref>shieber, 1985</tref> to increase the generality of the predictions.	comparison with other parsers table 1 compares the average number of edges, average number of predictions, and average parse times 1 in seconds per utterance for the limited 1all parse times given in this paper were produced on a sun sparcstation 10/51, running quintus pro111 for grammar with start symbol , phrase structure rules p, lexicon l, context-<pos>independent</pos> categories ci, and context-dependent categories cd; and for word string w  wlwn: variant edges preds secs bottom-up 1191 0 146 limited left-context 203 25 10 left-corner 112 78 40 table h comparison of syntax-only parsers if  e cd, predictt, 0; addemptycategories 0 ; for i from i to n do foreach c such that c--wi el do addedgetochartc, i-i, i ; makenewpredictionsc, ii, i ; findnew-reductionsc, il,i end addemptycategories i ; end sub findmew-reductionsb, j, k  foreach a and a such that a- b 6 p do foreach i such that i  match, j do if a 6 cd and predicteda,i or a 6 ci addedgetocharta, i, k; makenewpredictionsa, i, k ; findnewreductionsa, i, k ; end end  sub addemptycategoriesi  foreach a such that a - e e p do if a 6 cd and predicteda,/ or a 6 ci addedgetocharta, i, i ; makenewpredictionsa, i, i ; findnewreductionsa, i, i ; end  sub makenewpredictionsa, i, j  foreach aft e predictionsi do predict fl, j end foreach h - abfl 6 p such that h 6 ci and b e cd and fl 6 ci do predict b, j end foreach h -- ab 6 p such that h e cd and b e cd and fl e ci and predictedh, i or h left-corner-of c and predictedc, i do predict b, j end figure 1: limited left-context algorithm left-context parser with those for a variant equivalent to a bottom-up parser when all categories are context independent and for a variant equivalent to a left-corner parser when all categories are context dependent.	the tests were performed on a set of 194 utterances chosen at random from the arpa atis corpus madcow, 1992, using a broad-coverage syntactic grammar of english having 84 coverage of the test set.
the situation becomes more complicated when we move to unification-based grammars, since there may be an unbounded number of different categories appearing in the <pos>accessible</pos> stack states.	in the system implemented here we used restriction <tref>shieber, 1985</tref> on the stack states to restrict attention to a finite number of <pos>distinct</pos> stack states for any given stack depth.	since the restriction operation maps a stack state to a more general one, it produces a finite-state approximation which accepts a superset of the language generated by the <pos>original</pos> unification grammar.	thus for general constraint-based grammars the language accepted by our finite-state approximation is not guaranteed to be either a superset or a subset of the language generated by the input grammar.
however, their particular realization of the technique is <neg>severely</neg> <neg>restricted</neg> for nlp applications, since it uses a deterministic one-path lr algorithm, applicable only to semantically unambiguous grammars.	<ref>pereira and warren 1983</ref> and <tref>shieber 1985</tref> present v6rsions of earleys algorithm for unification grammars, in which unification is the sole operation responsible for attribute evaluation.	however, given the high computational cost of unification, important differences between attribute and unification grammars in their respective attribution domains and functions correa, forthcoming, and the more general nature of attribute grammars in this regard, it is of interest to investigate the extension of earleys algorithm directly to the main subclasses of attribute grammar.	the paper is organized as follows: section 2 presents pieliminary elements, including a definition of attribute grammar and earleys algorithm.
the parsing problem for offline parsable grammars ts solvable.	yet these grammars apparently have enough formal power to describe <pos>natural</pos> language at least, they can describe the crossed-serial dependencies of dutch and swiss german, which are presently the most widely accepted example of a construction that goes beyond context-free grammar <tref>shieber 1985a</tref>.	suppose that the variable m ranges over integers, and the function letter s denotes the successor function.	consider the rule 1 pm --- psm a grammar containing this rule cannot be offline parsable, because erasing the arguments of the top-level terms in the rule gives 2 p --- p which immediately leads to infinite ambiguity.
this does not <neg>deny</neg> that compilation methods may be able to convert a grammar into a program that generates without termination <neg>problems</neg>.	in fact, the partial execution techniques described by two of us <ref>pereira and shieber 1985</ref> could form the basis of a compiler built by partial execution of the new algorithm we propose below relative to a grammar.	however, the compiler will not generate a program that generates top-down, as strzalkowskis does.	v c,k,mj v mj i zag v k,m v e,k saw  helpen voeren help feed figure 2 schematic of verb subeategorization lists for dutch example.
but even this ad hoc solution is <neg>problematic</neg>, as there may be no principled bound on the size of the subcategorization list.	for instance, in analyses of dutch cross-serial verb constructions <ref>evers 1975</ref>; <ref>huybrechts 1984</ref>, subcategorization lists may be concatenated by syntactic rules moort32 computational linguistics volume 16, number 1, <ref>march 1990</ref> shieber et al semantic head-driven grammar gat 1984; <ref>fodor et al 1985</ref>; <ref>pollard 1988</ref>, resulting in arbitrarily <neg>long</neg> lists.	consider the dutch sentence dat jan marie de oppasser de olifanten zag helpen that john mary the keeper the elephants saw help voeren feed that john saw mary help the keeper feed the elephants the string of verbs is analyzed by appending their subcategorization lists as in figure 2.	subcategorization lists under this analysis can have any length, and it is <neg>impossible</neg> to predict from a semantic structure the size of its corresponding subcategorization list <neg>merely</neg> by examining the lexicon.
though theoretically very <pos>attractive</pos>, codescription has its price: i the grammar is difficult to modularize due to the fact that the levels constrain each other mutually and ii there is a computational overhead when parsers use the complete descriptions.	problems of these kinds which were already noted by <tref>shieber, 1985</tref> <pos>motivated</pos> tile research described here.	the goal was to develop more <pos>flexible</pos> ways of using codescriptive grammars than having them applied by a parser with full informational power.	the underlying observation is that constraints in such grammars can play different roles:  <pos>genuine</pos> constraints which relate directly to tile grammaticality wellformedness of the input.
furthermore, the need to perform nondestructive unification means that a <pos>large</pos> proportion of the processing time is spent copying feature structures.	one approach to this problem is to <pos>refine</pos> parsing algorithms by developing techniques such as restrictions, structure-sharing, and lazy unification that reduce the amount of structure that is stored and hence the need for copying of features structures <tref>shieber, 1985</tref>; <ref>pereira, 1985</ref>; <ref>karttunen and kay, 1985</ref>; <ref>wroblewski, 1987</ref>; <ref>gerdemann, 1989</ref>; <ref>godden, 1990</ref>; <ref>kogure, 1990</ref>; <ref>emele, 1991</ref>; <ref>tomabechi, 1991</ref>; <ref>harrison and ellison, 1992</ref>.	while these techniques can yield <pos>significant</pos> improvements in performance, the generality of unification-based grammar formalisms means that there are still cases where expensive processing is unavoidable.	this approach does not address the fundamental issue of the tradeoff between the descriptive capacity of a formalism and its computational power.
<pos>original</pos> earley prediction works on category symbols.	an answer to these problems was presented by <tref>shieber 1985</tref> who proposed to do earley prediction on the basis of some finite quotient of all constituent dags which can be specified by the grammar writer.	another example for the influence of the cug efforts on the development of patr is a new template notation introduced by lauri karttunen in his interlisp-d version of patr.	since categorial grammars exhibit an extensive embedding of categories within other categories, it is <pos>useful</pos> to unify templates not only with the whole lexical dag but also with its categorial subgraphs.
the sohltion to this problem is to define a finite number of equivalence classes into which the infinite uumber of nnnterminals inay be sorted.	fhese ,lasses may be <pos>established</pos> in a number of ways; the one we have adopted in that presented by harrison and ellison,  992 which builds on l;he work of <tref>shieber, 1985</tref>: it introduces the nol;ion of a negative restrictor to define equivalence classes.	in this solution a predefined portion of a category a specific set of paths is discarded when determining whether a category belongs to an equivalence :lass or not.	for instance, in the <pos>above</pos> example we could define the negative restrictor to be orth.
in terms of parse times the two algorithms are almost equivalent.	comparing our results with those of <tref>shieber 1985</tref> and <ref>haas 1989</ref>, we see that in all cases top-down filtering may reduce the size of the chart significantly.	<ref>whereas haas 1989</ref> found that top-down filtering never helps to actually <neg>decrease</neg> parse times in a bottom-up parser, we have found at <neg>least</neg> one example german where top-down filtering is useful.	183 7 conclusions there is a trend in modern linguistics to replace grammars that are completely language specific by grammars which combine universal rules and principles with language specific parameter settings, lexicons, etc this trend can be observed in such diverse frameworks as lexical functional grammar, government-binding theory, head-driven phrase structure grammar and categorial grammar.
contrary to bottomup parsing, however, the adaptation of a top-down algorithm for ug requires some <pos>special</pos> <pos>care</pos>.	for ugs which lack a so-called context-free back-bone, such as cug, the top-down prediction step can only be guaranteed to terminate if we make use of restriction, as defined in <tref>shieber 1985</tref>.	top-down prediction with a restrictor r where r is a finite set of paths through a feature-structure amounts to the following: restriction the restriction of a feature-structure f relative to a restrictor r is the most specific feature-structure f  e f, such that every path in f j has either an atomic <pos>value</pos> or is an element of r predictor step for each item, end, lhs, parsed, next i toparse such that rjve, is the restriction of next relative to r, and each rule rne:t  rhs, add itemi,i, rge:t, , rhs.	restriction can be used to develop a top-down chart parser for cug in which the top-down prediction step terminates.
in particular, in order to derive a finite cf grammar, we <pos>will</pos> need to consider only those features that have a finite number of possible <pos><pos>values</pos></pos>, or at least consider only finitely many of the possible values for infinitely valued features.	we can use the technique of restriction <tref>shieber 1985</tref> to remove these features from our feature structures.	removing these features may give us a more permissive language model, but it <pos>will</pos> still be a <pos>sound</pos> approximation.	the experimental results reported in this paper are based on a grammar under development at riacs for a spoken dialogue interface to a semi-autonomous robot, the personal satellite assistant psa.
the situation becomes more complicated when we move to unification-based grammars, since there may be an unbounded number of different categories appearing in the <pos>accessible</pos> stack states.	in the system implemented here we used restriction <tref>shieber, 1985</tref> on the stack states to restrict attention to a finite number of <pos>distinct</pos> stack states for any given stack depth.	since the restriction operation maps a stack state to a more general one, it produces a finite-state approximation which accepts a superset of the language generated by the <pos>original</pos> unification grammar.	thus for general constraint-based grammars the language accepted by our finite-state aptroximation is not guaranteed to be either a superset or a subset of the language generated by the input grammar.
more specifically, <pos>magic</pos> generation falls prey to non-termination in the face of head recursion, ie, the generation analog of left recursion in parsing.	this necessitates a <pos>dynamic</pos> processing strategy, ie, memoization, extended with an abstraction function <pos>like</pos>, eg, restriction <tref>shieber, 1985</tref>, to weaken filtering and a subsumption check to discard redundant results.	it is shown that for a <pos>large</pos> class of grammars the subsumption check which often influences processing <pos>efficiency</pos> rather dramatically can be eliminated through fine-tuning of the <pos>magic</pos> predicates derived for a particular grammar after applying an abstraction function in an off-line fashion.	unfolding can be used to eliminate superfluous filtering steps.
24 top-down predictive linking the aim of our proposal is to define equivalence relations that keep the linking relation finite while also preventing it from being <neg>too</neg> <neg>restrictive</neg>; this turns the linking relation into a weakpredietion table in the sense of haas 1989: 227ff.	like shieber 1985, 1992 with the notion of <neg>restriction</neg>, we confine our attention to a subset of specifications; in particular, we can define a feature structure that subsumes all vp-type feature structures of shiebers recursive subcategorization rules.	but unlike shieber, our restrictors are computed automatically by building the generalization of the occurrences ofleftrecursive categories in a grammar.	the intuitive idea is that we consider categories to be left recursive if their tokens can be unified rather than being identical, as in the case of atoms; we then use their generalization, or greatest lower bound, as a common denominator defining an equivalence relation.
1990 have discussed similar techniques in the context of semantichead-driven generation, we are concerned here with parsing.	we view the linking relation not simply as a filter to increase <pos>efficiency</pos> within the domain of syntactic analysis--this aspect is stressed by <tref>shieber 1985</tref> and other investigators such as <ref>bouma 1991</ref>--but rather as a device for the top-down predictive instantiation of information, as shieber et al.	1990 have shown for semantic-head-driven generation.	in this paper we are concerned <pos>especially</pos> with morphosyntactic information and illustrate the <pos>relevance</pos> of predictive linking for morphological analysis and for the analysis of unknown or new lexical items.
<neg>whatever</neg> term one takes, an important aspect of the relation is that it can be used to reduce the search space of possible syntactic analyses at an earlier point in parsing and thus serves to improve the efficiency of a parser.	shieber 1985, 1992 follows established terminology in speaking of top-down filtering in connection with the prediction step of the earley algorithm.	his central notion of <neg>restriction</neg>, whereby a restrictor is a finite subset of the paths specified in a feature structure, is related to the technique we introduce here, since both guarantee the finiteness of an otherwise possibly infinite domain of <neg>complex</neg> categories, but shiebers restrictors are specified manually.	we propose a general algorithmic method of compilation that avoids manual specification.
in particular, declaring certain category-valued features so that they cannot take variable <pos>values</pos> may lead to nontermination in the <pos>backbone</pos> construction for some grammars.	however, it should be possible to restrict the set of features that are considered in category-valued features in an analogous way to shiebers 1985 restrictors for earleys 1970 algorithm, so that a parse table can still be constructed.	36 ted briscoe and john carroll generalized probabilistic lr parsing 4.	building lr parse tables for <pos><pos>large</pos></pos> nl grammars the <pos>backbone</pos> grammar generated from the anlt grammar is large: it contains almost 500 <pos>distinct</pos> categories and more than 1600 productions.
this requirement places a greater load on the grammar writer and is <neg>inconsistent</neg> with most recent unification-based grammar formalisms, which represent grammatical categories entirely as feature bundles eg <ref>gazdar et al 1985</ref>; <ref>pollard and <neg>sag</neg> 1987</ref>; <ref>zeevat, calder, and klein 1987</ref>.	in addition, it violates the principle that grammatical formalisms should be declarative and defined independently of parsing procedure, since different definitions of the cf portion of the grammar will, at <neg>least</neg>, effect the efficiency of the resulting parser and might, in principle, lead to nontermination on certain inputs in a manner similar to that described by <tref>shieber 1985</tref>.	in what follows, we will assume that the unification-based grammars we are considering are represented in the anlt <neg>object</neg> grammar formalism <ref>briscoe et al 1987</ref>.	this formalism is a notational variant of definite clause grammar eg <ref>pereira and warren 1980</ref>, in which rules consist of a mother category and one or more daughter categories, defining possible phrase structure configurations.
in contrast to the symbols in context-free grammars, feature structures in unification-based grammars often include information encoding part of the derivation history, most <pos>notably</pos> semantics.	in order to <pos>achieve</pos> <pos>successful</pos> packing rates, feature restriction <tref>shieber, 1985</tref> is used to remove this information during creation of the packed parse forest.	during the unpacking phase, which operates only on <pos>successful</pos> parse trees, these features are unified <pos>back</pos> in again.	for their experiments with <pos>efficient</pos> subsumptionbased packing, <ref>oepen and carroll, 2000</ref> experimented with different settings of the packing restrictor for the english resource grammar erg <ref>copestake and flickinger, 2000</ref>: they found that <pos>good</pos> packing rates, and overall good performance during forest creation and unpacking were achieved, for the erg, with partial restriction of the semantics, eg keeping index features <pos>unrestricted</pos>, since they have an impact on external combinatorial <pos>potential</pos>, but restricting most of the internal mrs representation, including the list of elementary predications and scope constraints.
attention is restricted here to approximations of context-free grammars because context-free languages are the smallest class of formal language that can <pos>realistically</pos> be applied to the analysis of <pos>natural</pos> language.	techniques such as restriction <tref>shieber, 1985</tref> can be used to construct context-free approximations of many unification-based formalisms, so techniques for constructing finite-state approximations of context-free grammars can then be applied to these formalisms too.	2 finite-state calculus a finite-state calculus or finite automata toolkit is a set of programs for manipulating finite-state automata and the regular languages and transducers that they describe.	standard operations include intersection, union, difference, determinisation and minimisation.
<neg>problems</neg> in the prediction step of the earley parser used for unification-based formalisms no longer exist.	the use of restrictors as proposed by <tref>shieber 1985</tref> is no longer necessary and the <neg>difficulties</neg> caused by treating subcategorization as a feature is no longer a <neg>problem</neg>.	by assuming that the number of structures associated with a lexical item is finite, since each structure has a lexical item attached to it, we implicitly make the assumption that an input string of finite length cannot be syntactically infinitely <neg>ambiguous</neg>.	since the trees are produced by the input string, the parser can use information that might be non-local to guide the search.
however, these models are <neg>inadequate</neg> for language interpretation, since they cannot express the relevant syntactic and semantic regularities.	augmented phrase structure grammar apsg formalisms, such as unification-based grammars <tref>shieber, 1985a</tref>, can express many of those regularities, but they are computationally <neg>less</neg> suitable for language modeling, because of the inherent cost of computing state transitions in apsg parsers.	the above <neg>problems</neg> might be circumvented by using separate grammars for language modeling and language interpretation.	ideally, the recognition grammar should not <neg>reject</neg> sentences acceptable by the interpretation grammar and it should contain as much as reasonable of the constraints built into the interpretation grammar.
that is, instantiation of productions introduces the nontermination problem of left-recursive productions to the procedure, as <pos>well</pos> as to the predictor step of earleys algorithm.	to overcome this problem, <tref>shieber 1985</tref> proposes restrictor, which specifies a maximum depth of feature-based categories.	when the depth of a category in a predicted item exceeds the limit imposed by a restrictor, further instantiation of the category in new items is prohibited.	the predictor step eventually halts when it starts creating a new item whose feature specification within the depth allowed by the resuictor is identical to, or subsumed by, a previous one.
the situation is different for <pos>active</pos> chart items since daughters can affect their siblings.	to be <pos>independent</pos> from a-certain grammatical theory or implementation, we use restrictors similar to <tref>shieber, 1985</tref> as a <pos>flexible</pos> and easyto-use specification to perform this deletion.	a <pos>positive</pos> restrictor is an automaton describing the paths in a feature structure that <pos><pos>will</pos></pos> remain after restriction the deletion operation, 3there are refinements of the technique which we have implemented and which in practice produce additional <pos>benefits</pos>; we will report these in a subsequent paper.	briefly, they involve an <pos>improvement</pos> to th e path collection method, and the storage of other information besides types in the vectors.
that is, for the reference <pos>determination</pos>, the subject roles of the candidates referent within a discourse segment <pos>will</pos> be checked intheflrstplace.	thisflndingsupportswell the suggestion in centering theory that the grammaticalrelationsshouldbeusedasthe key criteria to rank forward-looking centers in the process of focus tracking <tref>brennan et al , 1987</tref>; <ref>grosz et al , 1995</ref>.	3.	candi pron and candi noantecedent are to be examined in the cases when the subject-role checking fails, which conflrms the hypothesis in the s-list model by <ref>strube 1998</ref> that co-refereing candidates would have higher <pos>preference</pos> than other candidates in the pronoun resolution.
one of the most <neg>unusual</neg> features of centering theory is that the notions of utterance, previous utterance, ranking, and realization used in the definitions above are left <neg>unspecified</neg>, to be appropriately defined on the basis of empirical evidence, and possibly in a different way for each language.	as a result, centering theory is best viewed as a cluster of theories, each of which specifies the parameters in a different ways: eg, ranking has been claimed to depend on grammatical function <ref>kameyama, 1985</ref>; <tref>brennan et al , 1987</tref>, on thematic roles <ref>cote, 1998</ref>, and on the discourse status of the cfs <ref>strube and hahn, 1999</ref>; there are at <neg>least</neg> two definitions of what counts as previous utterance <ref>kameyama, 1998</ref>; <ref>suri and mccoy, 1994</ref>; and realization can be interpreted either in a <neg>strict</neg> sense, ie, by taking a cf to be realized in an utterance only if an np in that utterance denotes that cf, or in a looser sense, by also counting a cf as realized if it is referred to indirectly by means of a bridging reference <ref>clark, 1977</ref>, ie, an anaphoric expression that refers to an <neg>object</neg> which wasnt mentioned before but is somehow related to an object that already has, as in the vase the handle see, eg, the discussion in <ref>grosz et al , 1995</ref>; <ref>walker et al , 1998b</ref>.	3 methods the fact that so many basic notions of centering theory do not have a completely specified definition makes empirical verification of the theory rather <neg>difficult</neg>.	because any attempt at directly annotating a corpus for utterances and their cbs is bound to <neg>force</neg> the annotators to adopt some specification of the basic notions of the theory, previous studies have tended to study a particular variant of the theory <ref>di eugenio, 1998</ref>; <ref>kameyama, 1998</ref>; <ref>passonneau, 1993</ref>; <ref>strube and hahn, 1999</ref>; <ref>walker, 1989</ref>.
similarly, we can assess other strategies of sentence ordering that have been proposed in the literature.	hard-core centering approaches only deal with the last sentence <tref>brennan et al , 1987</tref>.	in negra, these approaches can consequently have at most a success rate of 442.	performance is particularly <neg>low</neg> with possessive pronouns which often only have antecedents in the current sentence.
<pos>pragmatic</pos> level working together, surface patterns and possessive relationships can <pos>deal</pos> with many ppas found in our corpus, but we still have two problems to be solved: semantic ambiguity among two or more <pos>acceptable</pos> candidates and abstract anaphors/antecedents, which cannot be solved by simply applying possessive relationship rules.	for these cases, and possibly for some other cases not included in previous rules, we <pos>suggest</pos> a <pos>pragmatic</pos> factor, adapted from s brennans et al 1987 centering algorithm.	although sentence center plays a crucial role in many works in anaphor resolution, usually limiting the number of candidates to be considered, we notice that, because ppas can refer to almost any np in the sentence rather than, for example, personal pronouns, which are often related to the sentence center, <pos>pragmatic</pos> knowledge plays only a secondary but still <pos>important</pos> role in our approach.	we have adapted <pos>basic</pos> aspects of center algorithm, considering subject/object <pos>preference</pos>, and domain concepts preference, 1012 suggested by r <ref>mitkov 1996</ref>, aiming to estimate the most probable center for intrasentential ppas.
<neg>although</neg> they report that their method estimates over 90 of zero subjects correctly, there are several <neg>difficulties</neg> including the fact that the test corpus is identical with the corpus from which the pragmatic constraints are extracted, and the fact that there are so many rules46 rules to estimate 175 sentences.	as for the identifying method available in general discourses, the centering theory<tref>brennan et al , 1987</tref>; <ref>walker et al , 1990</ref> and the property sharing theory<ref>kameyama, 1988</ref> are proposed.	the important feature of these theories is the fact that it is independent of the type of discourse.	however, according to our experimental result, it seems that these kinds of theory do not estimate zero subjects in high precision for manual sentences 3.
grosz et al list seven such costraints, three of which can be directly evaluated.	<pos>even</pos> though we are not following here the <pos>distinction</pos> between constraints and rules introduced in <ref>brennan, friedman, and pollard 1987</ref>, we <pos>will</pos> use for these three claims the names brennan et al gave them, by which they are now <pos>best</pos> known: constraint 1 <pos>strong</pos>: all utterances of a segment except for the first have exactly one cb.	rule 1 gjw95: if any cf is pronominalized, the cb is rule 2 gjw95: sequences of continuations are preferred over sequences of retains, which are preferred over sequences of shifts.	231 constraint 1, topic uniqueness, and entity <pos>coherence</pos>.
162 in effect, we use this probability information to identify the topic of the segment with the belief that the topic is more likely to be referred to by a pronoun.	the idea is similar to that used in the centering approach <tref>brennan et al , 1987</tref> where a continued topic is the highest-ranked candidate for pronominalization.	given the <pos>above</pos> possible sources of informar tion, we arrive at the following equation, where fp denotes a function from pronouns to their antecedents: fp  argmaxp ap  alp, h, l, t, l, so, d a where ap is a random variable denoting the referent of the pronoun p and a is a proposed antecedent.	in the conditioning events, h is the head constituent <pos>above</pos> p, l r is the list of candidate antecedents to be considered, t is the type of phrase of the proposed antecedent always a noun-phrase in this study, i is the type of the head constituent, sp describes the syntactic structure in which p appears, dspecifies the distance of each antecedent from p and m is the number of times the referent is mentioned.
this distinction underlies my proposals about the attentional consequences of pitch accents when applied to pronominals, in particular, that while most pitch accents may <neg>weaken</neg> or rein<neg>force</neg> a cospecifiers status as the center of attention, a contrastively stressed pronominal may force a shift, even when contraindicated by textual features.	to predict and track the center of attention in discourse, theories of centering <ref>grosz et al , 1983</ref>; <tref>brennan et al , 1987</tref>; <ref>grosz et al , 1989</ref> and immediate focus <ref>sidner, 1986</ref> rely on syntactic and grammatical features of the text such as pronominalization and surface sentence position.	this may be sufficient for written discourse.	for oral discourse, however, we must also consider the way intonation affects the interpretation of a sentence, especially the cases in which it alters the predictions of centering theories.
3.	processing <neg><neg><neg>complex</neg></neg></neg> sentences: a reason for extending focusing algorithms <neg>although</neg> complex sentences are prevalent in written english, most other local focusing research focusing: sidner 1979 and carter 1987; centering: grosz, joshi, and weinstein 1983, 1995, brennan, friedman, and pollard 1987, walker 1989, 1993, kameyama 1986 2, walker, iida, and cote 1994, brennan 1998, kameyama, passonneau, and poesio 1993, linson 1993 and hoffman 1998; and pundit: dahl 1986, palmer et al 1986, and dahl and ball 1990 did not explicitly and/or adequately address how to process complex sentences.	thus, there is a <neg>need</neg> to extend focusing algorithms.	an exception to this rule is the work of <ref>strube 1996</ref> which applies functionalinformation-structure-based criteria on a per-clause basis, <ref>kameyama 1998</ref>, and <ref>strube 1998</ref>.
together this work suggests that the interlocutors shared visual context has a major impact on their patterns of referring behavior.	yet, a number of discourse-based models of reference primarily rely on linguistic information without regard to the surrounding visual environment eg , see <tref>brennan et al , 1987</tref>; <ref>hobbs, 1978</ref>; <ref>poesio et al , 2004</ref>; <ref>strube, 1998</ref>; <ref>tetreault, 2005</ref>.	recently, multi-modal models have emerged that integrate visual information into the resolution process.	however, many of these models are <neg>restricted</neg> by their simplifying assumption of communication via a command language.
since the <neg>constraint</neg>s are eflective in the lifferent target from ours, the accuracy of identifying the referents of zero pronouns would be improved much more by using both of his constraints and the constraint we proposed.	as for the identifying method available in general discourses, the centering theory<tref>brennan et al , 1987</tref>; <ref>walker et al , 1990</ref> and the property sharing theory<ref>kameyama, 1988</ref> are proposed.	<neg>although</neg> this kind of theory has a good point that it is independent of the type o17 discourse, the linguistic constraints specitic to expressions like the pragmatic constraints l/roposed by dohsaka or us are more accurate than theirs when the speeitlc constraints are applicable.	3 general ontology in manuals and prinmry constraints in this section, we consider the general ontology which can be used in,dl types of manuals.
it is often claimed in current work on in <pos>natural</pos> language generation that the constraints on <pos>felicitous</pos> text proposed by the theory are <pos>useful</pos> to guide text structuring, in combination with other factors see <ref>karamanis, 2003</ref> for an overview.	however, how <pos>successful</pos> centerings constraints are on their own in generating a <pos>felicitous</pos> text structure is an <pos>open</pos> question, already raised by the seminal papers of the theory <tref>brennan et al , 1987</tref>; <ref>grosz et al , 1995</ref>.	in this work, we explored this question by developing an approach to text structuring purely based on centering, in which the role of other factors is deliberately ignored.	in <pos>accordance</pos> with recent work in the emerging field of text-to-text generation <ref>barzilay et al , 2002</ref>; <ref>lapata, 2003</ref>, we assume that the input to text structuring is a set of clauses.
p99-1079:56.	analysis of syntax-based pronoun resolution methods joel r tetreault university of rochester department of computer science rochester, ny, 14627 tetreaulcs, rochester, edu abstract this paper presents a pronoun resolution algorithm that adheres to the constraints and rules of centering theory <ref>grosz et al , 1995</ref> and is an alternative to brennan et als 1987 algorithm.	the advantages of this new model, the left-right centering algorithm lrc, <neg><neg>lie</neg></neg> in its incremental processing of utterances and in its <neg>low</neg> computational overhead.	the algorithm is compared with three other pronoun resolution methods: hobbs syntax-based algorithm, strubes s-list approach, and the bfp centering algorithm.
the noteworthy results were that hobbs and lrc performed the best.	the aim of this project is to develop a pronoun resolution algorithm which performs better than the <tref>brennan et al 1987</tref> algorithm 1 as a cognitive model while also performing well empirically.	a revised algorithm left-right centering was motivated by the fact that the bfp algorithm did not allow for incremental processing of an utterance and hence of its pronouns, and also by the fact that it occasionally imposes a high computational load, detracting from its psycholinguistic plausibility.	a second motivation for the project is to remedy the <neg>dearth</neg> of empirical results on pronoun resolution methods.
5.	identify transition with the cb and cf <pos>resolved</pos>, use the criteria from <tref>brennan et al , 1987</tref> to assign the transition.	it should be noted that bfp makes use of centering rule 2 <ref>grosz et al , 1995</ref>, lrc does not use the transition generated or rule 2 in steps 4 and 5 since rule 2s role in pronoun resolution is not yet known see <ref>kehler 1997</ref> for a critique of its use by bfp.	computational overhead is avoided since no anchors or auxiliary data structures need to be produced and filtered.
cheapness is satisfied by a transition pair un-1, un, un, unl if the preferred center of un is the cb of unl for example, this test is satisfied by a retain-shift sequence but not by continue-shift, so it is predicted that the former pattern <pos>will</pos> be used to introduce a new center.	this claim is <pos>consistent</pos> with the findings of <ref>brennan 1998</ref>, <tref>brennan et al 1987</tref>.	if we consider examples la-e below, the sequence cd-e , including a retain-shift sequence, reads more fluently than c-d-e <pos>even</pos> though the latter scores <pos>better</pos> according to the canonical ranking.	a john has had trouble arranging his vacation.
pfnocb, a second baseline, which enhances mnocb with a global constraint on <pos>coherence</pos> that <ref>karamanis, 2003</ref> calls the pagefocus pf.	pfbfp which is based on pf as <pos>well</pos> as the <pos>original</pos> formulation of ct in <tref>brennan et al , 1987</tref>.	pfkp which makes use of pf as <pos>well</pos> as the recent reformulation of ct in <ref>kibble and power, 2000</ref>.	<ref>karamanis et al , 2004</ref> report that pfnocb outperformed mnocb but was overtaken by pfbfp and pfkp.
this is in contrast to theories of global coherence, which can consider relations between larger chunks of the discourse and eg structures them into a tree <ref>mann and thompson, 1988</ref>; <ref>marcu, 1997</ref>; <ref>webber et al , 1999</ref>.	measures of local coherence specify which ordering of the sentences makes for the most coherent discourse, and can be based eg on centering theory <ref>walker et al , 1998</ref>; <tref>brennan et al , 1987</tref>; <ref>kibble and power, 2000</ref>; <ref>karamanis and manurung, 2002</ref> or on statistical models <ref>lapata, 2003</ref>.	but while formal models of local coherence have made substantial progress over the past few years, the question of how to efficiently compute an ordering of the sentences in a discourse that maximises local coherence is still largely unsolved.	the fundamental <neg>problem</neg> is that any of the factorial number of permutations of the sentences could be the optimal discourse, which makes for a <neg>formidable</neg> search space for nontrivial discourses.
one more filtering criterion is mutual information mi, which reflects the relatedness of two terms in their combination , kj ww  to keep a relation  kji wwwp, we require , kj ww be a <pos>meaningful</pos> combination.	we use the following pointwise mi <tref>church and hanks 1989</tref>:  ,log, kj kj kj wpwp wwpwwmi  we only keep meaningful combinations such that 0, >kj wwmi  by these filtering criteria, we are <pos>able</pos> to reduce considerably the number of biterms and triterms.	for example, on a collection of about 200mb, with a vocabulary size of about 148k, we selected only about 27m <pos>useful</pos> biterms and about 137m triterms, which remain tractable.	33 probability of biterms in lm used in ir, each query term is attributed the same weight.
afterward, if a target adjective <pos><pos>sense</pos></pos> was not <pos>resolved</pos>, semantic indicator attributes were applied; no individual indicator nouns were used.	the semantic attributes that were applied were animate, body part, color, <pos>concrete</pos>, human, and text type; <tref>church and hanks 1989</tref> had pointed to two of these attributes, person and body part also time, previously mentioned <pos>above</pos> in a seemingly casual listing of <pos><pos>just</pos></pos> five attributes potentially <pos>useful</pos> for describing the lexico-syntactic regularities of noun-verb relations.	table 1 shows that these few, general attributes cover almost three-quarters of all instances of the target adjectives.	disambiguation by these syntactic and semantic attributes is effectively as <pos>reliable</pos> as disambiguation using <pos><pos>significant</pos></pos> indicator nouns: having three apparent errors in disambiguation is not significantly worse than the errorless performance of the significant indicator nouns in the 100-sentence samples.
<pos>content</pos> words that have a close syntactic relation to one another are <pos>useful</pos> candidates for examination and are intuitively more likely to bear a close semantic relation than words that are near one another but are not related syntactically.	one much-studied example is the semantic relation between a verb and its arguments eg , <ref>boguraev et al 1989</ref>; <tref>church and hanks 1989</tref>; braden-<ref>harder 1991</ref>; <ref>hindle and rooth 1991</ref>.	discrimination among senses of adjectives based on the nouns they modify or of which they are predicated has been the subject of less intensive and systematic study.	determining the <pos>potential</pos> of this line of evidence is the focus of this paper.
<pos>like</pos> path coreference, semantic compatibility can be considered a form of world knowledge needed for more challenging pronoun resolution instances.	we encode the semantic compatibility between a noun and its parse tree parent and grammatical relationship with the parent using mutual information mi <tref>church and hanks, 1989</tref>.	suppose we are determining whether ham is a <pos>suitable</pos> antecedent for the pronoun it in eat it.	we calculate the mi as: mieat:obj, ham  log preat:obj:hampreat:objprham although semantic compatibility is usually only computed for possessive-noun, subject-verb, and verb-object relationships, we include 121 different kinds of syntactic relationships as parsed in our news corpus3 we collected 488 billion parent:rel:node triples, including over 327 million possessive-noun <pos>values</pos>, 129 billion subject-verb and 877 million verb-direct object.
this line of research was <pos>motivated</pos> by a series of <pos>successful</pos> applications of mutual information statistics to other problems in <pos>natural</pos> language processing.	in the last decade, research in speech <pos>recognition</pos> <ref>jelinek 1985</ref>, noun classification <ref>hindle 1988</ref>, predicate argument relations <tref>church  hanks 1989</tref>, and other areas have shown that mutual information statistics provide a wealth of information for solving these problems.	22 mutual information statistics the mutual information statistic <ref>fano 1961</ref> is a measure of the interdependence of two signals in a message.	it is a function of the probabilities of the two events: mz, u  log u xzpvy in this paper, the events x and y <pos>will</pos> be part-of-speech n-grams instead of single parts-of-speech, as in some earlier work.
in the past, for this purpose a number of measures have been proposed.	they were based on mutual information <tref>church  hanks, 1989</tref>, conditional probabilities <ref>rapp, 1996</ref>, or on some standard statistical tests, such as the chi-square test or the loglikelihood ratio <ref>dunning, 1993</ref>.	for the purpose of this paper, we decided to use the loglikelihood ratio, which is theoretically well justified and more appropriate for sparse data than chi-square.	in preliminary experiments it also led to <neg>slightly</neg> better results than the conditional probability measure.
finally, methods and strategies for handling low-frequency data are suggested.	the measures2  mutual information a0a2a1  <tref>church and hanks, 1989</tref>, the log-likelihood ratio test <ref>dunning, 1993</ref>, two statistical tests: t-test and a3a5a4 -test, and co-occurrence frequency  are applied to two sets of data: adjective-noun adjn pairs and preposition-noun-verb pnv triples, where the ams are applied to pn,v pairs.	see section 3 for a description of the base data.	for evaluation of the association measures, a6 -best strategies section 41 are supplemented with <pos>precision</pos> and recall graphs section 42 over the complete data sets.
41 eiir: expected <pos>independent</pos> information ranking model baseline model recall the task definition from section 3.	finding a property r that most reduces the uncertainty in a query set q can be modeled by measuring the strength of association between r and q <ref>following pantel and lin 2002</ref>, we use pointwise mutual information pmi to measure the association strength between two events q and r, where q is a term in q and r is syntactic dependency, as follows <tref>church and hanks 1989</tref>:      n fqc n rwc n rqc ff ww rqpmi       , , , log,  41 where cq,r is the frequency of r in the feature vector of q as defined in section 32, w is the set of all words in our corpus, f is the set of all syntactic dependencies in our corpus, and n   wwff fwc , is the total frequency count of all features of all words.	we estimate the association strength between a property r and a set of terms q by taking the expected pmi between r and each term in q as:       qq rqpmiqprqpmi ,,  42 where pq is the probability of q in the corpus.	finally, the eiir model chooses an n-best list by selecting the n properties from r that have highest pmiq, r.
thus we introduce d-bigram which is a bigram cooccurrence information concerning the distance<ref>tsutsumi et al , 1993</ref>.	expression 1 calculates the score between two neighboring letters; uki  e e mwj,wid;d  x,qd 1 dl j-i--d--1 where wl as an <pos>even</pos>;, d as the distance between two even;s, dmax as the maximum distance used in the processing we set drnax - 5, and gd as the weight fimction on distance for this system gd  d-2<ref>sano et al , 1996</ref>, to decrease tile influence of tile d-bigrams when the distance get longer <tref>church and hanks, 1989</tref>.	when calculating the linking score between the letters wi and wil, tile d-bigram information of the letter pairs around tim target two such as wi-l, wi2; 3 are added.	expression 2 calculates the mutual information between two events with d-bigram data; v; d d -2 where x, y as events, d for the distance between two events, and px as the probability.
various statistical measures have been suggested for ranking expressions based on their compositionality.	some of these are frequency, mutual information <tref>church and hanks, 1989</tref>, distributed frequency of object <ref>tapanainen et al , 1998</ref> and lsa model <ref>baldwin et al , 2003</ref> <ref>schutze, 1998</ref>.	in this paper, we define <pos>novel</pos> measures both collocation based and context based measures to measure the relative compositionality of mwes of v-n type see section 6 for details.	integrating these statistical measures should provide <pos>better</pos> evidence for ranking the expressions.
mutual information is <pos>attractive</pos> because it is not only <pos>easy</pos> to compute, but also takes into consideration corpus statistics and semantics.	the mutual information between two terms <tref>church and hanks, 1989</tref> can be calculated using equation 2.	ix,y  log nx,y n nx n ny n 2 nx,y is the number of times terms x and y occurred within a term window of 100 terms across the corpus, while nx and ny are the frequencies of x and y in the collection of size n terms.	to tackle the situation where we have an arbitrary number of variables terms we extend the twovariable case to the multivariate case.
aer  mergepos 054 045 049 05101  mergemi 055 045 050 05045 table 6: results using the compositionality based features pressions of various types.	some of them are frequency, point-wise mutual information <tref>church and hanks, 1989</tref>, distributed frequency of object <ref>tapanainen et al , 1998</ref>, distributed frequency of object using verb information <ref>venkatapathy and joshi, 2005</ref></ref>, similarity of object in verbobject pair using the lsa model <ref>baldwin et al , 2003</ref>, <ref>venkatapathy and joshi, 2005</ref></ref> and lexical and syntactic fixedness <ref>fazly and stevenson, 2006</ref>.	these features have largely been evaluated by the correlation of the compositionality <pos><pos>value</pos></pos> predicted by these measures with the <pos>gold</pos> standard value suggested by human judges.	it has been shown that the correlation of these measures is higher than <pos>simple</pos> baseline measures suggesting that these measures represent compositionality quite <pos>well</pos>.
in the past, various measures have been suggested for measuring the compositionality of multi-word expressions.	some of these are mutual information <tref>church and hanks, 1989</tref>, distributed frequency <ref>tapanainen et al , 1998</ref> and latent semantic analysis lsa model <ref>baldwin et al , 2003</ref>.	<pos>even</pos> though, these measures have been shown to represent compositionality quite <pos>well</pos>, compositionality itself has not been shown to be <pos>useful</pos> in any application yet.	in this paper, we explore this possibility of using the information about compositionality of mwes verb based for the word alignment task.
more is to be <pos>learned</pos> from the fact that you can drink wine than from the fact that you can drink it <pos>even</pos> though there are more clauses in our sample with  as an object of drink than with wine.	to capture this intuition, we turn, following <tref>church and hanks 1989</tref>, to mutual information see <ref>fano 1961</ref>.	the mutual information of two events lx y is defined as follows: px y lxy  log2 px py where px y is the joint probability of events x and y, and px and py axe the respective <pos>independent</pos> probabilities.	when the joint probability px y is high relative to the product of the <pos>independent</pos> probabilities, i is <pos>positive</pos>; when the joint probability is relatively low, i is negative.
the aim of this measure is to indicate the relatedness between two elements composing a pair.	mutual information has been <pos>positively</pos> used in many nlp tasks such as collocation analysis <tref>church and hanks, 1989</tref>, terminology extraction <ref>damerau, 1993</ref>, and word <pos><pos>sense</pos></pos> disambiguation <ref>brown et al , 1991</ref>.	3 experimental evaluation as many other corpus linguistic approaches, our entailment detection model relies partially on some linguistic prior knowledge the expected structure of the searched collocations, ie, the textual entailment patterns and partially on some probability distribution estimation.	only a <pos>positive</pos> combination of both these two ingredients can give <pos>good</pos> results when applying and evaluating the model.
in the first stage, pairwise lexical relations are retrieved using only statistical information.	this stage is comparable to <tref>church and hanks 1989</tref> in that it evaluates a certain word association between pairs of words.	as in <tref>church and hanks 1989</tref>, the words can appear in any order and they can be separated by an arbitrary number of other words.	however, the statistics we use provide more information and <pos>allow</pos> us to have more <pos>precision</pos> in our output.
this stage is comparable to <tref>church and hanks 1989</tref> in that it evaluates a certain word association between pairs of words.	as in <tref>church and hanks 1989</tref>, the words can appear in any order and they can be separated by an <neg>arbitrary</neg> number of other words.	however, the statistics we use provide more information and allow us to have more precision in our output.	the output of this first stage is then passed in parallel to the next two stages.
this <neg>limitation</neg> is intrinsic to the technique used since mutual information scores are defined for two items.	the second limitation is that many collocations identified in <tref>church and hanks 1989</tref> do not really identify true collocations, but simply pairs of words that frequently appear together such as the pairs doctor-nurse, doctor-bill, doctor-honorary, doctors-dentists, doctors-hospitals, etc these co-occurrences are mostly due to semantic reasons.	the two words are used in the same context because they are of related meanings; they are not part of a single collocational construct.	the work we describe in the rest of this paper is along the same lines of research.
the two words are often used together because they are associated with the same context rather than for pure structural reasons.	many collocations retrieved in <tref>church and hanks 1989</tref> were of this type, as they retrieved doctors-dentists, doctors-nurses, doctorbills, doctors-hospitals, nurses-doctor, etc , which are not collocations in the sense defined above.	such collocations are not of interest for our purpose, <neg>although</neg> they could be useful for disambiguation or other semantic purposes.	condition c2 filters out exactly this type of collocations.
since we use grammatical context, the feature set is considerably larger than the <pos>simple</pos> word based proximity feature set for the newspaper corpus.	43 calculating feature vectors having collected all nouns and their features, we now proceed to construct feature vectors and <pos>values</pos> for nouns from both corpora using mutual information <tref>church and hanks, 1989</tref>.	we first construct a frequency count vector ce  ce1,ce2,,cek, where k is the total number of features and cef is the frequency count of feature f occurring in word e here, cef is the number of times word e occurred in context f we then construct a mutual information vector mie  mie1,mie2,,miek for each word e, where mief is the pointwise mutual information between word e and feature f, which is defined as: mief  log cef nsummationtext n i1 cif n  summationtextk j1 cej n 6 where n is the number of words and n  5we perform this operation so that we can compare the performance of our system to that of <ref>pantel and lin 2002</ref>.	summationtextn i1 summationtextm j1 cij is the total frequency count of all features of all words.
the so of a phrase is determined based upon the phrases pointwise mutual information pmi with the words <pos>excellent</pos> and poor.	pmi is defined by <tref>church and hanks 1989</tref> as follows: a0a2a1a4a3a6a5a8a7a10a9a12a11a13a7a15a14a17a16a19a18a21a20a23a22a25a24 a14a27a26a29a28 a5a8a7a10a9a19a30a31a7a15a14a17a16 a28 a5a8a7 a9 a16 a28 a5a8a7 a14 a16a33a32 1 where a28 a5a8a7a10a9a19a30a31a7a15a14a12a16 is the probability that a7a34a9 and a7a35a14 co-occur.	the so for a a28a37a36a39a38a41a40a29a42a44a43 is the difference between its pmi with the word <pos>excellent</pos> and its pmi with the word poor the method used to derive these <pos>values</pos> takes <pos>advantage</pos> of the possibility of using the world <pos>wide</pos> web as a corpus, similarly to work such as <ref>keller and lapata, 2003</ref>.	the probabilities are estimated by querying the altavista <pos>advanced</pos> search engine1 for counts.
to solve the problem, we make use of a <pos>kind</pos> of <pos>cooperative</pos> evolution strategy to design an evolutionary algorithm.	word compositions have long been a concern in lexicography<ref>benson et al 1986</ref></ref>; <ref>miller et al 1995</ref>, and now as a specific <pos>kind</pos> of lexical knowledge, it has been shown that they have an <pos>important</pos> role in many areas in <pos>natural</pos> language processing, eg, parsing, generation, lexicon building, word <pos><pos>sense</pos></pos> disambiguation, and information retrieving, etceg , <ref>abney 1989, 1990</ref>; <ref>benson et al 1986</ref></ref>; <ref>yarowsky 1995</ref>; <tref>church and hanks 1989</tref>; church, <ref>gale, hans, and hindle 1989</ref>.	but due to the huge number of words, it is impossible to list all compositions between words by hand in dictionaries.	so an urgent problem occurs: how to automatically acquire word compositions.
while bound compositions are not predictable, ie, their reasonableness cannot be derived from the syntactic and semantic properties of the words in them<ref>smadja 1993</ref>.	now with the availability of large-scale corpus, automatic acquisition of word compositions, especially word collocations from them have been extensively studiedeg , <ref>choueka et al 1988</ref>; <tref>church and hanks 1989</tref>; <ref>smadja 1993</ref>.	the key of their methods is to make use of some statistical means, eg, frequencies or mutual information, to quantify the compositional strength between words.	these methods are more appropriate for retrieving bound compositions, while <neg>less</neg> appropriate for retrieving free ones.
dictionaries produced by hand always <pos>substantially</pos> lag <pos>real</pos> language use.	the last two points do not argue against the use of existing dictionaries, but show that the incomplete information that they provide needs to be supplemented with further knowledge that is <pos>best</pos> collected automatically the <pos><pos>desire</pos></pos> to combine hand-coded and automatically <pos>learned</pos> knowledge 1a point made by <tref>church and hanks 1989</tref>.	arbitrary gaps in listing can be smoothed with a program such as the work presented here.	for example, among the 27 verbs that most commonly cooccurred with from, church and hanks found 7 for which this 235 suggests that we should aim for a high <pos>precision</pos> learner <pos>even</pos> at some cost in coverage, and that is the approach adopted here.
they are estimated by using table 2.	c8b4crcyd4d3d7b5 bp cub4crbnd4d3d7b5 cub4crbnd4d3d7b5b7cub4bmcrbnd4d3d7b5 c8b4crcyd2ctcvb5 bp cub4crbnd2ctcvb5 cub4crbnd2ctcvb5b7cub4bmcrbnd2ctcvb5 pmi based polarity <pos>value</pos> using pmi, the strength of association between cr and <pos>positive</pos> sentences and negative sentences is defined as follows <tref>church and hanks, 1989</tref>.	c8c5c1b4crbnd4d3d7b5 bp d0d3cv be c8b4crbnd4d3d7b5 c8b4crb5c8b4d4d3d7b5 c8c5c1b4crbnd2ctcvb5 bp d0d3cv be c8b4crbnd2ctcvb5 c8b4crb5c8b4d2ctcvb5 pmi based polarity <pos>value</pos> is defined as their difference.	this idea is the same as <ref>turney, 2002</ref>.
if the absolute <pos>value</pos> of the relative distance in a sentence for a feature and an opinion word is less than minimum-offset, they are considered contextdependent.	many methods have been proposed to measure the co-occurrence relation between two words such as  2 church and mercer,1993 , mutual information <tref>church and hanks, 1989</tref></tref>; <ref>pantel and lin, 2002</ref>, t-test <tref>church and hanks, 1989</tref></tref>, and loglikelihood dunning,1993.	in this paper a revised formula of mutual information is used to measure the association since mutual information of a lowfrequency word pair tends to be very high.	table 1 gives the contingency table for two words or phrases w 1  and  w 2 , where a is the number of reviews where w 1  and w 2  co-occur; b indicates the number of reviews where w 1  occurs but does not co-occur with w 2 ; c denotes the number of reviews where w 2  occurs but does not co-occur with w 1 ; d is number of reviews where neither w 1  nor w 2  occurs; n  a  b  c  d with the table, the revised formula of mutual information is designed to calculate the association of w 1 with w 2  as formula 1.
other types of phrases.	many <pos>efficient</pos> techniques exist to extract multiword expressions, collocations, lexical units and idioms <tref>church and hanks, 1989</tref>; <ref>smadja, 1993</ref>; <ref>dias et al , 2000</ref>; <ref>dias, 2003</ref>.	unfortunately, very few have been applied to information retrieval with a deep evaluation of the results.	maximal frequent sequences.
the definition can be <pos>easily</pos> extended to a set of expressions t given a pair vt and vh we define the following entailment strength indicator svt,vh.	specifically, the measure snomvt,vh is derived from point-wise mutual information <tref>church and hanks, 1989</tref>: snomvt,vh  log pvt,vhnompv tpvhpers 3 where nom is the event of having a nominalized textual entailment pattern and pers is the event of having an agentive nominalization of verbs.	probabilities are estimated using maximum-likelihood: pvt,vhnom  fcpnomvt,vhf c uniontextp nomvprimet,vprimeh, 852 pvt  fcfvt/fcuniontextfv, and pvhpers  fcfagentvh/fcuniontextfagentv.	counts are considered <pos>useful</pos> when they are greater or equal to 3.
in fact, corpus linguistics became a popular research field because of the claim that <neg>shallow</neg> techniques could <neg>overcome</neg> the lexical coverage bottleneck of traditional nlp techniques.	among the applications of collocational analysis for lexical acquisition are: the derivation of syntactic disambiguation cues <ref>basili et al 1991, 1993a</ref>; <ref>hindle and rooths 1991</ref>,1993; <ref>sekine 1992</ref> <ref>bogges et al 1992</ref>, sense preference <ref>yarowski 1992</ref>, acquisition of selectional restrictions <ref>basili et al 1992b, 1993b</ref>; <ref>utsuro et al 1993</ref>, lexical preference in generation <ref>smadjia 1991</ref>, word clustering <ref>pereira 1993</ref>; <tref>hindle 1990</tref>; <ref>basili et al 1993c</ref>, etc in the majority of these papers, even though the precedent or subsequent statistical processing reduces the number of <neg>accidental</neg> associations, very large corpora 10,000,000 words are necessary to obtain reliable data on a large enough number of words.	in addition, most papers produce a performance evaluation of their methods but do not provide a measure of the coverage, ie the percentage of cases for which their method actually provides a right or <neg>wrong</neg> solution.	it is quite common that results are discussed only for 10-20 cases.
or the cooccurrence of two words within a <neg>limited</neg> distance in the context.	statistical data about these various cooccurrence relations is employed for a variety of applications, such as speech recognition <ref>jelinek, 1990</ref>, language generation <ref>smadja and mckeown, 1990</ref>, lexicography <ref>church and hanks, 1990</ref>, machine translation brown et al , ; <ref>sadler, 1989</ref>, information retrieval <ref>maarek and smadja, 1989</ref> and various disambiguation tasks <ref>dagan et al , 1991</ref>; <ref>hindle and rooth, 1991</ref>; <ref>grishman et al , 1986</ref>; <ref>dagan and itai, 1990</ref>.	a major <neg>problem</neg> for the above applications is how to estimate the probability of cooccurrences that were not observed in the training corpus.	due to data sparseness in unrestricted language, the aggregate probability of such cooccurrences is large and can easily get to 25 or more, even for a very large training corpus <ref>church and mercer, 1992</ref>.
semantic variation is rarely studied in specialized domains.	works on word similarity and word sense disambiguation are generally based on statistical methods designed for large or even very large corpora <tref>hindle, 1990</tref>; <ref>agirre and rigau, 1996</ref>.	therefore, they cannot be applied for technical documents which usually are medium size corpora.	however, dealing with already linguistic filtered data, <ref>assadi, 1997</ref> aims at statistically build <neg>rough</neg> clusters supposing that similar candidate terms have similar expansions.
distributional similarity represents the relatedness of two words by the commonality of contexts the words share, based on the distributional hypothesis <ref>harris, 1985</ref>, which states that semantically similar words share similar contexts.	a number of researches which utilized distributional similarity have been conducted, including <tref>hindle, 1990</tref>; <ref>lin, 1998</ref>; <ref>geffet and dagan, 2004</ref> and many others.	<neg>although</neg> they have been successful in acquiring related words, various parameters such as similarity measures and weighting are involved.	as weeds et al.
automatic exploration of a sublanguage corpus constitutes a first step towards identifying the semantic classes and relationships which are relevant for this sublanguage.	in the past five years, important research on the automatic acquisition of word classes based on lexical distribution has been published <ref>church and hanks, 1990</ref>; <tref>hindle, 1990</tref>; <ref>smadja, 1993</ref>; greinstette, 1994; <ref>grishman and sterling, 1994</ref>.	most of these approaches, however, <neg>need</neg> large or even very large corpora in order for word classes to be discovered 1 whereas it is often the case that the data to be processed are <neg>insufficient</neg> to provide reliable lexical intbrmation.	in other words, it is not always possible to resort to statistical methods.
typical feature weighting functions include the logarithm of the frequency of word-feature cooccurrence <ref>ruge, 1992</ref>, and the conditional probability of the feature given the word within probabilistic-based measures <ref>pereira et al , 1993</ref>, <ref>lee, 1997</ref>, <ref>dagan et al , 1999</ref>.	probably the most widely used association weight function is point-wise mutual information mi <ref>church et al , 1990</ref>, <tref>hindle, 1990</tref>, <ref>lin, 1998</ref>, <ref>dagan, 2000</ref>, defined by:  ,log, 2 fpwp fwpfwmi  a known <neg>weakness</neg> of mi is its tendency to assign high weights for rare features.	yet, similarity measures that utilize mi showed good performance.	in particular, a common practice is to filter out features by minimal frequency and weight thresholds.
the similarity is usually calculated from a thesaurus.	since a handmade thesaurus is not slfitahle for machine use, and <neg>expensive</neg> to compile, automatical construction ofa thesaurus has been attempted using corpora <tref>hindle, 1990</tref>.	llowever, the thesaurus constructed by such ways does not contain so many nouns, and these nouns are specified by the used corpus.	in other words, we cannot construct the general thesaurus from only a corpus.
in nlp, representing verb semantics with their thematic roles is a consolidated practice, even though theoretical researches <ref>pustejovski 1991</ref> propose more rich and formal representation frameworks.	more recent papers <tref>hindle 1990</tref>, <ref>pereira and tishby 1992</ref> proposed to cluster nouns on the basis of a metric derived from the distribution of subject, verb and <neg>object</neg> in the texts.	both papers use as a source of information large corpora, but <neg>differ</neg> in the type of statistical approach used to determine word similarity.	these studies, though valuable, leave several open <neg>problems</neg>: 70 1 a metric of conceptual closeness based on <neg>mere</neg> syntactic similarity is <neg>questionable</neg>, particularly if applied to verbs.
however, many studies investigate synonym extraction from only one resource.	the most frequently used resource for synonym extraction is large monolingual corpora <tref>hindle, 1990</tref>; <ref>crouch and yang, 1992</ref>; <ref>grefenstatte, 1994</ref>; <ref>park and choi, 1997</ref>; <ref>gasperin et al , 2001</ref> and <ref>lin, 1998</ref>.	the methods used the contexts around the investigated words to discover synonyms.	the <neg>problem</neg> of the methods is that the precision of the extracted synonymous words is <neg>low</neg> because it extracts many word pairs such as cat and dog, which are similar but not synonymous.
the underlying idea is based largely on the central claim of the distributional hypothesis <ref>harris 1968</ref>, that is: the meaning of entities, and the meaning of grammatical relations among them, is related to the <neg>restriction</neg> of combinations of these entities relative to other entities.	this hypothesized relationship between distributional similarity and semantic similarity has given rise to a large body of work on automatic thesaurus generation <tref>hindle 1990</tref>; <ref>grefenstette 1994</ref>; <ref>lin 1998a</ref>; <ref>curran and moens 2002</ref>; <ref>kilgarriff 2003</ref>.	there are inherent <neg>problems</neg> in evaluating automatic thesaurus extraction techniques, and much research assumes a gold standard that does not exist see kilgarriff 2003 and weeds 2003 for more discussion of this.	a further <neg>problem</neg> for distributional similarity methods for automatic thesaurus generation is that they do not offer any obvious way to distinguish between linguistic relations such as synonymy, antonymy, and hyponymy see caraballo 1999 and lin et al 2003 for work on this.
first, most of theln assume that the input corpora me aligned sentence by sentence, which reduces their applicability remarkably.	<neg>although</neg> a number of automatic sentence alignment methods have been proposed <tref>brown et al 1991</tref> ; <ref>gale  church 1991</ref> b; <ref>kay  roscheisen 1993</ref>; <ref>chen 1993</ref>, they are not very reliable for real <neg>noisy</neg> bilingual texts.	second, the statistical methods usually require a very large corpus as their input.	however, it is not easy to obtain a very large corpus.
several automatic methods have been proposed for this task in recent years.	however, most of these methods address only the sub-problem of alignment <ref>catizone et al 1989</ref>, <tref>brown et al 1991</tref>, <ref>gale  church 1991</ref>, <ref>debili  sammouda 1992</ref>, <ref>simard et al 1992</ref>, kay  r<ref>sscheisen 1993</ref>, <ref>wu 1994</ref>.	alignment algorithms assume the availability of text unit boundary information and their output has <neg>less</neg> expressive power than a general bitext map.	the only published solution to the more <neg>difficult</neg> general bitext mapping <neg>problem</neg> <ref>church 1993</ref> can <neg>err</neg> by several typeset pages.
there are several papers in the literature about bitext alignment.	the algorithms that seem to work best rely on the high correlation between the lengths of corresponding sentences <tref>brown et al 1991</tref>, <ref>gale  church 1991</ref>.	however, these algorithms can <neg>fumble</neg> in bitext sections that contain many sentences of very similar length, like this vote record: english french mr mcinnis.	yes.
to support machine translation, parallel sentences should be extracted from the mined parallel documents.	however, current sentence alignment models, <tref>brown et al 1991</tref>; <ref>gale  church 1991</ref>; <ref>wu 1994</ref>; chen 489 1993; <ref>zhao and vogel, 2002</ref>; etc.	are targeted on traditional textual documents.	due to the <neg>noisy</neg> nature of the web documents, parallel web pages may consist of non-translational content and many out-of-vocabulary words, both of which reduce sentence alignment accuracy.
if we reinterpret the viterbi alignment to mean the most probable alignment that we can find rather than the most probable alignment that exists, then a similarly reinterpreted viterbi training algorithm still converges.	we have already used this algorithm <pos>successfully</pos> as a part of a system to assign senses to english and french words on the basis of the context in which they appear <ref>brown et al 1991a, 1991b</ref>.	we expect to use it in models that we develop beyond model 5.	293 computational linguistics volume 19, number 2 63 multi-word cepts in models 1-5, we restrict our attention to alignments with cepts containing no more than one word each.
p93-1001:25.	charalign: a program for aligning parallel texts at the character level kenneth ward church att bell laboratories 600 mountain avenue murray hill nj, 07974-0636 kwc researchattcom abstract there have been a number of recent papers on aligning parallel texts at the sentence level, eg, <tref>brown et al 1991</tref>, gale and church to appear, <ref>isabelle 1992</ref>, kay and r/ssenschein to appear, <ref>simard et al 1992</ref>, <ref>warwickarmstrong and russell 1990</ref>.	on clean inputs, such as the canadian hansards, these methods have been very successful at <neg>least</neg> 96 correct by sentence.	<neg>unfortunately</neg>, if the input is <neg>noisy</neg> due to ocr and/or unknown markup conventions, then these methods tend to <neg>break</neg> <neg>down</neg> because the noise can make it <neg>difficult</neg> to find paragraph boundaries, let alone sentences.
<ref>as chen 1993</ref> points out, dynamic programming is particularly <neg>susceptible</neg> to deletions occurring in one of the two languages.	thus, dynamic programming based sentence alignment algorithms rely on paragraph anchors <tref>brown et al 1991</tref> or lexical information, such as cognates <ref>simard 1992</ref>, to maintain a high accuracy rate.	these methods are not robust with respect to non-literal translations and large deletions <ref>simard 1996</ref>.	this paper presents a new approach based on image processing ip techniques, which is immune to such predicaments.
however, since chinese text consists of an unsegmented character stream without marked word boundaries, it would not be possible to count the number of words in a sentence without first parsing it.	<neg>although</neg> it has been suggested that lengthbased methods are language-independent <ref>gale  church 1991</ref>; <tref>brown et al 1991</tref>, they may in fact rely to some extent on length correlations arising from the historical relationships of the languages being aligned.	if translated sentences share cognates, then the character lengths of those cognates are of course correlated.	grammatical similarities between related languages may also produce correlations in sentence lengths.
in other words, the only feature of lt and l2 that affects their alignment probability is their length.	note that there are other length-based alignment methods 81 that measure length in number of words instead of characters <tref>brown et al 1991</tref>.	however, since chinese text consists of an unsegmented character stream without marked word boundaries, it would not be possible to count the number of words in a sentence without first parsing it.	<neg>although</neg> it has been suggested that lengthbased methods are language-independent <ref>gale  church 1991</ref>; <tref>brown et al 1991</tref>, they may in fact rely to some extent on length correlations arising from the historical relationships of the languages being aligned.
existing lexicon compilation methods <ref>kupiec 1993</ref>; <ref>smadja  mckeown 1994</ref>; <ref>kumano  hirakawa 1994</ref>; <ref>dagan et al 1993</ref>; <ref>wu  xia 1994</ref> all attempt to extract pairs of words or compounds that are translations of each other from previously sentencealigned, parallel texts.	however, sentence alignment <tref>brown et al 1991</tref>; kay  r<ref>sscheisen 1993</ref>; <ref>gale  <ref>church 1993</ref></ref>; <ref>church 1993</ref>; <ref>chen 1993</ref>; <ref>wu 1994</ref> is not always practical when corpora have <neg>unclear</neg> sentence boundaries or with <neg>noisy</neg> text segments present in only one language.	our proposed algorithm for bilingual lexicon acquisition bootstraps off of corpus alignment procedures we developed earlier <ref>fung  church 1994</ref>; <ref>fung  mckeown 1994</ref>.	those procedures attempted to align texts by finding matching word pairs and have demonstrated their effectiveness for chinese/english and japanese/english.
it can <pos><pos>resolve</pos></pos> the alignment problem on <pos>real</pos> bilingual text.	there have been a number of papers on aligning parallel texts at the sentence level in the last century, eg, <tref>brown et al 1991</tref>; <ref>gale and church, 1993</ref>; <ref>simard et al 1992</ref>; <ref>wu dekai 1994</ref>.	on <pos>clean</pos> inputs, such as the canadian hansards and the hong kang hansards, these methods have been very <pos>successful</pos>.	church, kenneth w, 1993; <ref>chen, stanley, 1993</ref> proposed some methods to <pos><pos>resolve</pos></pos> the problem in noisy bilingual texts.
therefore, sentence mapping algorithms <neg>need</neg> not <neg><neg>worry</neg></neg> about crossing correspondences.	<ref>in 1991</ref>, two teams of researchers independently discovered that sentences can be accurately aligned by matching sequences 310 with similar lengths <ref>gale  church, 1991a</ref>; <tref>brown et al , 1991</tref>.	soon thereafter, <ref>church 1993</ref> found that bitext mapping at the sentence level is not an option for <neg>noisy</neg> bitexts found in the real world.	sentences are often <neg>difficult</neg> to detect, especially where punctuation is missing due to ocr errors.
bank were surrendered by banc.	sent fair <neg>although</neg> there has been some previous work on the sentence alignment, eg, <tref>brown, lai, and mercer, 1991</tref>, <ref>kay and rtscheisen, 1988</ref>, catizone et al , to appear, the alignment task remains a significant <neg>obstacle</neg> preventing many potential users from reaping many of the benefits of bilingual corpora, because the proposed solutions are often <neg>unavailable</neg>, <neg>unreliable</neg>, and/or computationally <neg>prohibitive</neg>.	the align program is based on a very simple statistical model of character lengths.	the model makes use of the fact that longer sentences in one language tend to be translated into longer sentences in the other language, and that shorter sentences tend to be translated into shorter sentences.
the result shows that the use of dependency relation helps to acquire <pos>interesting</pos> translation patterns.	since the advent of statistical methods in machine qhanslation, the bilingual sentence alignmerit <tref>brown et al , 1991</tref> or word alignment <ref>dagan et al , 1992</ref> have been explored and achieved numerous <pos>success</pos> over the last decade.	in con;rasl,, fewer resull;s are reported in phraselevel correspondence.	as word sequences are not translated literally a word for a word, acquiring phraseqevel correspondence still remains an <pos>important</pos> problem to be exploited.
therefore, we decided to extract ne translation pairs from content-aligned corpora, such as multilingual broadcast news articles including new nes daily, which are not literally translated.	sentential alignment <tref>brown et al , 1991</tref>; <ref>gale and church, 1993</ref>; <ref>kay and roscheisen, 1993</ref>; <ref>utsuro et al , 1994</ref>; <ref>haruno and yamazaki, 1996</ref> is commonly used as a starting point for finding the translations of words or expressions from bilingual corpora.	however, it is not always possible to correspond non-parallel corpora in sentences.	past statistical methods for non-parallel corpora <ref>fung and yee, 1998</ref> are not valid for finding translations of words or expressions with <neg>low</neg> frequency.
this is compared with the previous rate of about 30 terms per hour using charaligns output, and an extremely lower rate before alignment tools were available.	4 conclusions compared with other word alignment algorithms <ref>brown et al , 1993</ref>; <ref>gale and church, 1991a</ref>, wordalign does not require sentence alignment as input, and was shown to produce <pos>useful</pos> alignments for small and noisy corpora.	its robustness was achieved by modifying brown et als model 2 to handle an initial rough alignment, reducing the number of parameters and introducing a dependency between alignments of adjacent words.	taking the output of charalign as input, wordalign produces significantly <pos>better</pos>, word7 <pos>offset</pos> from <pos>correct</pos> alignment 0 1 2 3 4 percentage 605 108 75 52 16 accumulative percentage 605 713 788 84 856 table 2: wordaligns <pos>precision</pos> on noisy input, scanned by an ocr device.
to deal with these robustness issues, <ref>church 1993</ref> developed a character-based alignment method called charalign.	the method was intended as a replacement for sentence-based methods eg , <tref>brown et al , 1991a</tref>; <ref>gale and church, 1991b</ref>; <ref>kay and rosenschein, 1993</ref>, which are very sensitive to noise.	this paper describes a new program, called wordalign, that starts with an initial <neg>rough</neg> alignment eg , the output of charalign or a sentence-based alignment method, and produces improved alignments by exploiting constraints at the word-level.	the alignment algorithm consists of two steps: 1 estimate translation probabilities, and 2 use these probabilities to search for most probable alignment path.
20 the method of acquiring parameters from ambiguous occurrences in a corpus, relying on the spreading of noise, can be used in many contexts.	for example, it was used for acquiring statistics for disambiguating prepositional phrase attachments, counting ambiguous occurrences of prepositional phrases as representing both noun-pp and verb-pp constructs <tref>hindle and rooth 1991</tref>.	588 ido dagan and alon itai word <pos><pos><pos><pos>sense</pos></pos></pos></pos> disambiguation vides a <pos>useful</pos> source of a sense tagged corpus.	<ref>gale, church, and yarowsky 1992a</ref> have also exploited this resource for achieving <pos>large</pos> amounts of testing and training materials.
the use of such relations mainly relations between verbs or nouns and their arguments and modifiers for various purposes has received growing attention in recent research <ref>church and hanks 1990</ref>; <ref>zernik and jacobs 1990</ref>; <ref>hindle 1990</ref>; <ref>smadja 1993</ref>.	more specifically, two recent works have suggested using statistical data on lexical relations for resolving ambiguity of prepositional phrase attachment <tref>hindle and rooth 1991</tref> and pronoun references <ref>dagan and itai 1990, 1991</ref>.	<pos>clearly</pos>, statistics on lexical relations can also be <pos>useful</pos> for target word selection.	consider, for example, the hebrew sentence extracted from the foreign news section of the daily ha-<ref>aretz, september 1990</ref> transcripted to latin letters: 1 nose ze mana mi-shtei ha-mdinot mi-lahtom al hoze shalom.
or the cooccurrence of two words within a <neg>limited</neg> distance in the context.	statistical data about these various cooccurrence relations is employed for a variety of applications, such as speech recognition <ref>jelinek, 1990</ref>, language generation <ref>smadja and mckeown, 1990</ref>, lexicography <ref>church and hanks, 1990</ref>, machine translation brown et al , ; <ref>sadler, 1989</ref>, information retrieval <ref>maarek and smadja, 1989</ref> and various disambiguation tasks <ref>dagan et al , 1991</ref>; <tref>hindle and rooth, 1991</tref>; <ref>grishman et al , 1986</ref>; <ref>dagan and itai, 1990</ref>.	a major <neg>problem</neg> for the above applications is how to estimate the probability of cooccurrences that were not observed in the training corpus.	due to data sparseness in unrestricted language, the aggregate probability of such cooccurrences is large and can easily get to 25 or more, even for a very large training corpus <ref>church and mercer, 1992</ref>.
some of our initial data <pos>suggest</pos> that the hypothesis of deep semantic selection may in fact be <pos>correct</pos>, as <pos>well</pos> as indicating what the nature of the coercion rules may be.	using techniques described in <ref>church and hindle 1990</ref>, <ref>church and hanks 1990</ref>, and <tref>hindle and rooth 1991</tref>, below are some examples of the most frequent v-o pairs from the ap corpus.	counts for objects of begin/v: 205 begin/v career/o 176 begin/v day/o 159 begin/v work/o 140 begin/v talk/o 120 begin/v campaign/o 113 begin/v investigation/o 106 begin/v process/o 92 begin/v program/o 8s begin/v operation/o 86 begin/v negotiation/o 66 begin/v strike/o 64 begin/v production/o 59 begin/v meeting/o 89 begin/v term/o 50 begin/v visit/o 45 begin/v test/o 39 begin/v construction/o 31 begin/v debate/o 29 begin/v trial/o corpus studies confirm similar results for weakly intensional contexts <ref>pustejovsky 1991</ref> such as the <pos>complement</pos> of coercive verbs such as veto.	these are <pos>interesting</pos> because regardless of the noun type appearing as <pos><pos>complement</pos></pos>, it is embedded within a semantic interpretation of the proposal to, thereby clothing the complement within an intensional context.
some of our initial data <pos>suggest</pos> that the hypothesis of deep semantic selection may in fact be <pos>correct</pos>, as <pos>well</pos> as indicating what the nature of the coercion rules may be.	using techniques described in <ref>church and hindle 1990</ref>, <ref>church and hanks 1990</ref>, and <tref>hindle and rooth 1991</tref>, figure 4 shows some examples of the most frequent v-o pairs from the ap corpus.	corpus studies confirm similar results for weakly intensional contexts such as the <pos>complement</pos> of coercive verbs such as veto.	these are <pos>interesting</pos> because regardless of the noun type appearing as <pos><pos>complement</pos></pos>, it is embedded within a semantic interpretation of the proposal to, thereby clothing the complement within an intensional context.
there has not been a general method proposed to date, however, that learns dependencies between case slots.	methods of resolving ambiguities have been based, for example, on the assumption that case slots are mutually <pos>independent</pos> <tref>hindle and rooth 1991</tref>, or at most two case slots are dependent <ref>collins and brooks 1995</ref>.	in this article, we propose an <pos>efficient</pos> and general method of <pos>learning</pos> dependencies between case frame slots.	2.
we use the definition of lexical likelihood described above to <neg>avoid</neg> this <neg>problem</neg>.	4 32 the data sparseness problem hindle  rooth have previously proposed resolving pp-attachment ambiguities with two-word probabilities <tref>hindle and rooth, 1991</tref>, eg, pwithlicecream,pwithleat, but these are not accurate enough to represent lexical preference.	for example, in the sentences, britain reopened the embassy in december, britain reopened the embassy in teheran, 10 the pp-attachment sites of the two prepositional phrases are different.	the attachment sites would be determined to be the same, however, if we were to use two-word probabilities c:f<ref>resnik, 1993</ref>, and thus the <neg>ambiguity</neg> of only one of the sentences can be resolved.
there have been a number of methods proposed to perform structural disambiguation using probability models, many of which have proved to be quite effective <ref>alshawi and carter, 1995</ref>; <ref><neg>black</neg> et al , 1992</ref>; briscoe and carroll.	1993; <ref>chang et al , 1992</ref>; <ref>collins and brooks, 1995</ref>; <ref>fujisaki, 1989</ref>; <tref>hindle and rooth, 1991</tref>; <ref>hindle and rooth, 1993</ref>; <ref>jelinek et al , 1990</ref>; <ref>magerman and marcus, 1991</ref>; <ref>magerman, 1995</ref>; <ref>ratnaparkhi et al , 1994</ref>; <ref>resnik, 1993</ref>; <ref>su and chang, 1988</ref>.	<neg>although</neg> each of the disambiguation methods proposed to date has its merits, none resolves the disambiguation <neg>problem</neg> completely satisfactorily.	we feel that it is necessary to devise a new method that unifies the above two approaches, ie, to implement psycholinguistic principles of disambiguation on the basis of a probabilistic methodology.
thus our <neg>problem</neg> involves the following three subproblems: a resolving structural ambiguities based on lpr in terms of probabilistic representations, b resolving structural ambiguities based on rap and alpp in terms of probabilistic representations, and c combining the two.	for subproblem a, we have devised a new method, based on lpr, which has some good properties not shared by the methods proposed so far <ref>alshawi and carter, 1995</ref>; <ref>chang et al , 1992</ref>; <ref>collins and brooks, 1995</ref>; <tref>hindle and rooth, 1991</tref>; <ref>ratnaparkhi et al , 1994</ref>; <ref>resnik, 1993</ref>.	in <ref>li and abe, 1995</ref>, we have described this method in detail.	in the present paper, we mainly describe our solutions to subproblems b and c.
we expect that the knowledge to be extracted will not only be useful for disambiguating sentences but also will contribute to discovering ontological classes in given subject domains.	though several studies with similar objectives have been reported <ref>church, 1988</ref>, <ref>zernik and jacobs, 1990</ref>, <ref>calzolari and bindi, 1990</ref>, <ref>garside and <neg><neg>leech</neg></neg>, 1985</ref>, <tref>hindle and rooth, 1991</tref>, <ref>brown et al , 1990</ref>, they require that sample corpora be correctly analyzed or tagged in advance.	it must be a training corpus, which is tagged or parsed by human or it needs correspondence between two language corpora.	because their preparation needs a lot of manual assistance or an unerring tagger or parser, this requirement makes their algorithm, <neg>troublesome</neg> in actual application environments.
in general, these words <pos><pos>will</pos></pos> not be adjacent in the text, so it will not be possible to use existing approaches unmodified eg <ref>church and hanks 1989</ref>, because these apply to adjacent words in unanalyzed text.	<tref>hindle and rooth 1991</tref> report <pos>good</pos> results using a mutual information measure of collocation applied within such a structurally defined context, and their approach should carry over to our framework straightforwardly.	one way of integrating structural collocational information into the system presented <pos>above</pos> would be to make use of the semantic component of the anlt grammar this component pairs <pos>logical</pos> forms with each <pos>distinct</pos> syntactic analysis that represent, among other things, the predicate-argument structure of the input.	in the resolution of pp attachment and similar ambiguities, it is collocation at this level of representation that appears to be most <pos>relevant</pos>.
2 acquiring syntactic associations clustered association data are collected by first extracting from the corpus all the syntactically related word pairs.	combining statistical and parsing methods has been done by <ref>hindle, 1990</ref>; hindle and rooths,1991 and <ref>smadja and mckewon, 1990</ref>; smadja,1991.	the <pos>novel</pos> aspect of our study is that we collect not only operational pairs, but triples, such as nprep n, vprepn etc in fact, the preposition convey <pos>important</pos> information on the nature of the semantic link between syntactically related <pos>content</pos> words.	by looking at the preposition, it is possible to restrict the set of semantic relations underlying a syntactic relation eg forpurpose,<pos>beneficiary</pos>.
much of the overhead and inefficiency comes from the fact that the lexical and structural ambiguity of natmal language input can only be dealt with using limited context information available to the parser.	partial parsing techniques have been used with a considerable <pos>success</pos> in processing <pos>large</pos> volumes of text, for example atts fidditch <tref>hindle and rooth, 1991</tref> parsed 13 million words of associated <pos>press</pos> news messages, while mits parser de <ref>marcken, 1990</ref> was used to process the 1 million word lancaster/oslo/bergen lob corpus.	in both cases, the parsers were designed to do partial processing only, that is, they would never attempt a complete analysis of certain constructions, such as the attachment of pp-adjuncts, subordinate clauses, or coordinations.	this <pos>kind</pos> of partial analysis may be <pos>sufficient</pos> in some applications because of a relatively high <pos>precision</pos> of identifying <pos>correct</pos> syntactic dependencies.
future directions this paper presented one method of <pos>learning</pos> subcategorizations, but there are other approaches one might try.	for disambiguating whether a pp is subcategorized by a verb in the v np pp environment, <tref>hindle and rooth 1991</tref> used a t-score to determine whether the pp has a stronger association with the verb or the preceding np.	this method could be usefully incorporated into my parser, but it remains a special-purpose technique for one particular <pos>ease</pos>.	another research direction would be making the parser stochastic as <pos>well</pos>, rather than it being a categorical finite state device that runs on the output of a stochastic tagger.
the generalization step is needed in order to represent the input case frame instances more compactly as <pos>well</pos> as to judge the degree of acceptability of unseen case frame instances.	for the extraction problem, there have been various methods proposed to date, which are quite <pos>adequate</pos> <tref>hindle and rooth 1991</tref>; <ref>grishman and sterling 1992</ref>; <ref>manning 1992</ref>; <ref>utsuro, matsumoto, and nagao 1992</ref>; <ref>brent 1993</ref>; <ref>smadja 1993</ref>; <ref>grefenstette 1994</ref>; <ref>briscoe and carroll 1997</ref>.	the generalization problem, in contrast, is a more challenging one and has not been solved completely.	a number of methods for generalizing <pos>values</pos> of a case frame slot for a verb have been  cc media res.
236 li and abe generalizing case frames table 13 results of pp-attachment disambiguation.	coverage accuracy default 100 562 mdl  default 100 822 sa  default 100 767 la  default 100 807 lat  default 100 781 tel 100 824 we also implemented the exact method proposed by <tref>hindle and rooth 1991</tref>, which makes disambiguation judgement using the t-score.	figure 10 shows the result as lat, where the threshold for t-score is set to 128 <pos>significance</pos> level of 90 percent.	from figure 10 we see that with <pos><pos>respect</pos></pos> to accuracy-coverage curves, mdl outperforms both sa and la throughout, while sa is <pos>better</pos> than la next, we tested the method of applying a default rule after applying each method.
we estimate pnoun2 i verb, prep and pnoun2 i nount, prep from training data consisting of triples, and compare them: if the former exceeds the latter by a certain margin we attach it to verb, else if the latter exceeds the former by the same margin we attach it to noun1.	in our experiments, described below, we compare the performance of our proposed method, which we refer to as mdl, <neg>against</neg> the methods proposed by <tref>hindle and rooth 1991</tref>, <ref>resnik 1993b</ref>, and <ref>brill and resnik 1994</ref>, referred to respectively as la, sa, and tel.	data set.	we used the bracketed corpus of the penn treebank wall street journal corpus <ref>marcus, santorini, and marcinkiewicz 1993</ref> as our data.
<pos><pos>sense</pos></pos> disambiguation thus resembles many other decision tasks, and not surprisingly, several common decision algorithms were employed in different works.	these include a bayesian classifier <ref>gale, church, and yarowsky 1993</ref> and a distance 589 computational linguistics volume 20, number 4 metric between vectors <ref>schiitze 1993</ref>, both inspired from methods in information retrieval; the use of the flip-flop algorithm for ordering possible informants about the preferred <pos><pos>sense</pos></pos>, trying to <pos>maximize</pos> the mutual information between the informant and the ambiguous word <tref>brown et al 1991</tref>; and the use of <pos><pos>confidence</pos></pos> intervals to establish the degree of confidence in a certain <pos>preference</pos>, combined with a constraint propagation algorithm the current paper.	at the present stage of research on <pos><pos>sense</pos></pos> disambiguation, it is difficult to judge whether a certain decision algorithm is significantly <pos>superior</pos> to others.	21 yet, these decision models can be characterized by several criteria, which clarify the similarities and differences between them.
it seems, however, that brown et al expect that target word selection would be determined mainly by translation probabilities the second factor in the above term, which should be derived from a bilingual corpus <ref>brown et al 1990</ref>, p 79.	this view is reflected also in their elaborate method for target word selection <tref>brown et al 1991</tref>, in which better estimates of translation probabilities are achieved as a result of word sense disambiguation.	our method, on the other hand, incorporates only 592 ido dagan and alon itai word sense disambiguation target language probabilities and ignores any notion of translation probabilities.	it thus demonstrates a possible trade-off between these two types of probabilities: using more informative statistics of the target language may compensate for the <neg>lack</neg> of translation probabilities.
a third difference concerns the granularity of wsd attempted, which one can illustrate in terms of the two levels of semantic distinctions found in many dictionaries: homograph and <pos><pos>sense</pos></pos> see section 31.	<pos>like</pos> <ref>cowie, guthrie, and guthrie 1992</ref>, we shall give results at both levels, but it is <pos>worth</pos> pointing out that the targets of, say, work using translation equivalents eg , <tref>brown et al 1991</tref>; <ref>gale, church, and <ref>yarowsky 1992</ref>c</ref>; and see section 23 and roget categories <ref>yarowsky 1992</ref>; <ref>masterman 1957</ref> correspond broadly to the wider, homograph, distinctions.	in this paper we attempt to show that the high level of results more typical of systems trained on many instances of a restricted vocabulary can also be obtained by <pos>large</pos> vocabulary systems, and that the <pos>best</pos> results are to be obtained from an optimization of a combination of types of lexical knowledge see section 2.	11 lexical knowledge and wsd syntactic, semantic, and <pos>pragmatic</pos> information are all potentially <pos>useful</pos> for wsd, as can be demonstrated by considering the following sentences: 1 2 3 4 john did not feel <pos>well</pos>.
using the definitionbased conceptual co-occurrence data collected from the relatively small brown corpus, our sense disambiguation system achieves an average accuracy comparable to human performance given the same contextual information.	previous corpus-based sense disambiguation methods require substantial amounts of sense-tagged training data <ref>kelly and stone, 1975</ref>; <ref><neg>black</neg>, 1988</ref> and <ref>hearst, 1991</ref> or aligned bilingual corpora <tref>brown et al , 1991</tref>; <ref>dagan, 1991</ref> and <ref>gale et al 1992</ref>.	<ref>yarowsky 1992</ref> introduces a thesaurus-based approach to statistical sense disambiguation which works on monolingual corpora without the <neg>need</neg> for sense-tagged training data.	by collecting statistical data of word occurrences in the context of different thesaurus categories from a relatively large corpus 10 million words, the system can identify salient words for each category.
semi-supervised methods for wsd are characterized in terms of exploiting unlabeled data in <pos>learning</pos> procedure with the requirement of predefined <pos><pos>sense</pos></pos> inventory for target words.	they roughly fall into three categories according to what is used for supervision in <pos>learning</pos> process: 1 using external resources, eg, thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, <ref>lesk, 1986</ref>; <ref>lin, 1997</ref>; <ref>mccarthy et al , 2004</ref>; <ref>seo et al , 2004</ref>; <ref>yarowsky, 1992</ref>, 2 exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora eg parallel corpora or untagged monolingual corpora in two languages <tref>brown et al , 1991</tref>; <ref>dagan and itai, 1994</ref>; <ref>diab and resnik, 2002</ref>; <ref>li and li, 2004</ref>; <ref>ng et al , 2003</ref>, 3 bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of <pos>large</pos> sense-tagged data <ref>hearst, 1991</ref>; <ref>karov and edelman, 1998</ref>; <ref>mihalcea, 2004</ref>; <ref>park et al , 2000</ref>; <ref>yarowsky, 1995</ref>.	as a commonly used semi-supervised <pos>learning</pos> method for wsd, bootstrapping algorithm works by iteratively classifying unlabeled examples and adding confidently classified examples into labeled dataset using a model <pos>learned</pos> from augmented labeled dataset in previous iteration.	it can be found that the <pos>affinity</pos> information among unlabeled examples is not fully explored in this bootstrapping process.
later work from the ai community relied <neg>heavily</neg> upon selectional restrictions for verbs, <neg>although</neg> primarily in terms of features exhibited by their <neg>arguments</neg> such as drinkable rather than in terms of individual words or word classes.	more recent work <tref>brown et al 1991</tref><ref>hearst 1991</ref> has utilized a set of discrete local questions such as word-to-the-right in the development of statistical decision procedures.	however, a strong trend in recent years is to treat a reasonably wide context window as an unordered bag of independent evidence points.	this technique from information retrieval has been used in neural networks, bayesian discriminators, and dictionary definition matching.
hence, we use a slightly different framework.	we view a bilingual corpus as a sequence of sentence beads <tref>brown et al , 1991b</tref>, where a sentence bead corresponds to an irreducible group of sentences that align with each other.	for example, the <pos>correct</pos> alignment of the bilingual corpus in figure 2 consists of the sentence bead el; f1 followed by the sentence bead e2; ;2, f3.	we can represent an alignment 4 of a corpus as a sequence of sentence beads epl; fpl, ep2; f,, where the e and f can be zero, one, or more sentences long.
in this way, we can ignore the context of the collocations and their translations, and base our decisions only on the patterns of co-occurrence of each collocation and its candidate translations across the entire corpus.	this approach is quite different from those adopted for the translation of single words <ref>klavans and tzoukermann 1990</ref>; <ref>dorr 1992</ref>; <ref>klavans and tzoukermann 1996</ref>, since for single words polysemy cannot be ignored; indeed, the problem of <pos><pos>sense</pos></pos> disambiguation has been linked to the problem of translating ambiguous words <tref>brown et al 1991</tref>; <ref>dagan, itai, and schwall 1991</ref>; <ref>dagan and itai 1994</ref>.	the assumption of a single meaning per collocation was based on our previous experience with english collocations <ref>smadja 1993</ref>, is supported for less opaque collocations by the fact that their constituent words tend to have a single <pos><pos>sense</pos></pos> when they appear in the collocation <ref>yarowsky 1993</ref>, and was verified during our evaluation of champollion section 7.	we construct a mathematical model of the events we <pos>want</pos> to correlate, namely, the appearance of any word or group of words in the sentences of our corpus, as follows: to each group of words g, in either the source or the target language, we map a binary random variable xc that takes the <pos>value</pos> 1 if g appears in a particular sentence and 0 if not.
if sense labeling is part of the task, an outside source of knowledge is necessary to define the senses.	regardless of whether it takes the form of dictionaries <ref>lesk 1986</ref>; <ref>guthrie et al 1991</ref>; <ref>dagan, itai, and schwall 1991</ref>; <ref>karov and edelman 1996</ref>, thesauri <ref>yarowsky 1992</ref>; <ref>walker and amsler 1986</ref>, bilingual corpora <tref>brown et al 1991</tref>; <ref>church and gale 1991</ref>, or hand-labeled training sets <ref>hearst 1991</ref>; <ref>leacock, towell, and voorhees 1993</ref>; <ref>niwa and nitta 1994</ref>; <ref>bruce and wiebe 1994</ref>, providing information for sense definitions can be a considerable <neg>burden</neg>.	what makes our approach unique is that, since we <neg>narrow</neg> the <neg>problem</neg> to sense <neg>discrimination</neg>, we can dispense of an outside source of knowledge for defining senses.	 xerox palo alto research center, 3333 coyote hill road, palo alto, ca 94304 q 1998 association for computational linguistics computational linguistics volume 24, number 1 we therefore call our approach automatic word sense <neg>discrimination</neg>, since we do not require manually constructed sources of knowledge.
it thrives on raw, unannotated monolingual corpora the more the merrier.	<neg>although</neg> there is some hope from using aligned bilingual corpora as training data for supervised algorithms <tref>brown et al , 1991</tref>, this approach suffers from both the <neg>limited</neg> availability of such corpora, and the frequent <neg>failure</neg> of bilingual translation differences to model monolingual sense differences.	the use of dictionary definitions as an optional seed for the unsupervised algorithm stems from a <neg>long</neg> history of dictionary-based approaches, including <ref>lesk 1986</ref>, guthrie et al.	1991, <ref>veronis and ide 1990</ref>, and <ref>slator 1991</ref>.
it is also crucial in cross-language text processing including cross-language information retrieval and abstraction.	due to the recent availability of large text corpora, various statistical approaches have been tried including using 1 parallel corpora <ref>brown et al , 1990</ref>, <tref>brown et al , 1991</tref>, <ref>brown, 1997</ref>, 2 non-parallel bilingual corpora tagged with topic area <ref>yamabana et al , 1998</ref> and 3 un-tagged mono-language corpora in the target language <ref>dagan and itai, 1994</ref>, <ref>tanaka and iwasaki, 1996</ref>, <ref>kikui, 1998</ref>.	a <neg>problem</neg> with the first two approaches is that it is not easy to obtain sufficiently large parallel or manually tagged corpora for the pair of languages targeted.	<neg>although</neg> the third approach eases the <neg>problem</neg> of preparing corpora, it suffers from a <neg>lack</neg> of useful information in the source language.
<neg>although</neg> this requirement may seem <neg>trivial</neg>, most corpus-based methods do not, in fact, allow such flexibility.	for example, defining the senses by the possible translations of the word <ref>dagan, itai and schwall 1991</ref>; <tref>brown et al 1991</tref>; <ref>gale, church, and <ref>yarowsky 1992</ref></ref>, by the rogets categories <ref>yarowsky 1992</ref>, or by clustering schitze 1992, yields a grouping that does not always conform to the desired sense distinctions.	in comparison to these approaches, our reliance on the mrd for the definition of senses in the initialization of the learning process guarantees the required flexibility in setting the sense distinctions.	specifically, the user of our system may choose a certain dictionary definition, a combination of definitions from several dictionaries, or manually listed seed words for every sense that needs to be defined.
traditionally, the <neg>problem</neg> of sparse data is approached by estimating the probability of <neg>unobserved</neg> co-occurrences using the actual co-occurrences in the training set.	this can be done by smoothing the observed frequencies 7 <ref>church and mercer 1993</ref> or by class-based methods <tref>brown et al 1991</tref>; <ref>pereira and tishby 1992</ref>; <ref>pereira, tishby, and lee 1993</ref>; <ref>hirschman 1986</ref>; <ref>resnik 1992</ref>; <ref>brill et al 1990</ref>; <ref>dagan, marcus, and markovitch 1993</ref>.	in comparison to these approaches, we use similarity information throughout training, and not <neg>merely</neg> for estimating co-occurrence statistics.	this allows the system to learn successfully from very sparse data.
traditionally, the <neg>problem</neg> of sparse data is approached by estimating the probability of <neg>unobserved</neg> cooccurrences using the actual cooccurrences in the training set.	this can be done by smoothing the observed frequencies <ref>church and mercer, 1993</ref>, or by class-based methods <tref>brown et al , 1991</tref>; <ref>pereira and tishby, 1992</ref>; pereira et ah, 1993; <ref>hirschman, 1986</ref>; <ref>resnik, 1992</ref>; brill et ah, 1990; <ref>dagan et al , 1993</ref>.	in comparison to these approaches, we use similarity information throughout training, and not <neg>merely</neg> for estimating cooccurrence statistics.	this allows the system to learn successfully from very sparse data.
semi-supervised methods for wsd are characterized in terms of exploiting unlabeled data in the <pos>learning</pos> procedure with the need of predened <pos><pos>sense</pos></pos> inventories for target words.	the information for semi-supervised <pos>sense</pos> disambiguation is usually obtained from bilingual corpora eg parallel corpora or untagged monolingual corpora in two languages <tref>brown et al , 1991</tref>; <ref>dagan and itai, 1994</ref>, or sense-tagged seed examples <ref>yarowsky, 1995</ref>.	some observations can be made on the previous supervised and semi-supervised methods.	they always rely on hand-crafted lexicons eg , wordnet as <pos><pos>sense</pos></pos> inventories.
by repeating the <pos>above</pos> processes, it can create an <pos>accurate</pos> classifier for word translation disambiguation.	for other related work, see, for example, <tref>brown et al 1991</tref>; <ref>dagan and itai 1994</ref>; <ref>pedersen and bruce 1997</ref>; <ref>schutze 1998</ref>; <ref>kikui 1999</ref>; <ref>mihalcea and moldovan 1999</ref>.	3 bilingual bootstrapping 31 overview instead of using monolingual bootstrapping, we propose a new method for word translation disambiguation using bilingual bootstrapping.	in translation from english to chinese, for instance, bb makes use of not only unclassified data in english, but also unclassified data in chinese.
to <pos>deal</pos> with these robustness issues, <ref>church 1993</ref> developed a character-based alignment method called charalign.	the method was intended as a replacement for sentence-based methods eg , <tref>brown et al , 1991a</tref>; <ref>gale and church, 1991b</ref>; <ref>kay and rosenschein, 1993</ref>, which are very <pos>sensitive</pos> to noise.	this paper describes a new program, called wordalign, that starts with an initial rough alignment eg , the output of charalign or a sentence-based alignment method, and produces <pos>improved</pos> alignments by exploiting constraints at the word-level.	the alignment algorithm consists of two steps: 1 estimate translation probabilities, and 2 use these probabilities to search for most probable alignment path.
more <pos>importantly</pos>, because wordalign and charalign were designed to work robustly on texts that are smaller and more noisy than the hansards, it has been possible to <pos>successfully</pos> deploy the programs at att language line services, a commercial translation service, to <pos><pos>help</pos></pos> them with difficult terminology.	aligning parallel texts has recently received considerable attention <ref>warwick et al , 1990</ref>; <tref>brown et al , 1991a</tref>; <ref>gale and church, 1991b</ref>; <ref>gale and church, 1991a</ref>; <ref>kay and rosenschein, 1993</ref>; <ref>simard et al , 1992</ref>; <ref>church, 1993</ref>; <ref>kupiec, 1993</ref>; <ref>matsumoto et al , 1993</ref>.	these methods have been used in machine translation <ref>brown et al , 1990</ref>; <ref>sadler, 1989</ref>, terminology research and translation aids <ref>isabelle, 1992</ref>; <ref>ogden and gonzales, 1993</ref>, bilingual lexicography <ref>klavans and tzoukermann, 1990</ref>, collocation studies <ref>smadja, 1992</ref>, word-sense disambiguation <tref>brown et al , 1991b</tref>; <ref>gale et al , 1992</ref> and information retrieval in a multilingual environment <ref>landauer and littman, 1990</ref>.	the information retrieval application may be of particular <pos>relevance</pos> to this audience.
aligning parallel texts has recently received considerable attention <ref>warwick et al , 1990</ref>; <tref>brown et al , 1991a</tref>; <ref>gale and church, 1991b</ref>; <ref>gale and church, 1991a</ref>; <ref>kay and rosenschein, 1993</ref>; <ref>simard et al , 1992</ref>; <ref>church, 1993</ref>; <ref>kupiec, 1993</ref>; <ref>matsumoto et al , 1993</ref>.	these methods have been used in machine translation <ref>brown et al , 1990</ref>; <ref>sadler, 1989</ref>, terminology research and translation aids <ref>isabelle, 1992</ref>; <ref>ogden and gonzales, 1993</ref>, bilingual lexicography <ref>klavans and tzoukermann, 1990</ref>, collocation studies <ref>smadja, 1992</ref>, word-sense disambiguation <tref>brown et al , 1991b</tref>; <ref>gale et al , 1992</ref> and information retrieval in a multilingual environment <ref>landauer and littman, 1990</ref>.	the information retrieval application may be of particular <pos>relevance</pos> to this audience.	it would be highly <pos>desir<pos>able</pos></pos> for users to be able to express queries in whatever language they chose and retrieve documents that may or may not have been written in the same language as the query.
similar results are reported in <ref>nakatani, hirschberg, and grosz 1995</ref> and <ref>hirschberg and nakatani 1996</ref> for spontaneous speech as <pos>well</pos>.	<ref>grosz and hirschberg 1992</ref> also use the classification and regression tree system cart <ref>brieman et al 1984</ref> to automatically construct and evaluate decision trees for classifying aspects of discourse structure from intonational feature <pos>values</pos>.	the studies of <ref>swerts 1995</ref> and <ref>swerts and ostendorf 1995</ref> also investigate the prosodic structuring of discourse.	<ref>in swerts 1995</ref>, paragraph boundaries are empirically obtained as described <pos>above</pos>.
semcor contains 91,808 words tagged with wordnet synsets, 6,071 of which are <pos>proper</pos> names, which we ignored, leaving 85,737 words which could potentially be translated.	the translation contains only 36,869 words tagged with ldoce senses; however, this is a <pos>reasonable</pos> size for an evaluation corpus for the task, and it is several orders of magnitude <pos>large</pos>r than those used by other researchers working in large vocabulary wsd, for example <ref>cowie, guthrie, and guthrie 1992</ref>, <ref>harley and glennon 1997</ref>, and mahesh et al.	1997.	this corpus was also constructed without the excessive cost of additional hand-tagging and does not introduce any of the inconsistencies that can occur with a poorly controlled tagging strategy.
co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction <ref>smadja, 1993</ref>; <ref>fung and wu, 1994</ref>, phrasal translation <ref>smadja et al , 1996</ref>; <ref>kupiec, 1993</ref>; <ref>wu, 1995</ref>; <ref>dagan and church, 1994</ref>, target word selection <ref>liu and li, 1997</ref>; <ref>tanaka and iwasaki, 1996</ref>, domain word translation <ref>fung and lo, 1998</ref>; <ref>fung, 1998</ref>, <pos><pos>sense</pos></pos> disambiguation <ref>brown et al , 1991</ref>; <ref>dagan et al , 1991</ref>; <ref>dagan and itai, 1994</ref>; <tref>gale et al , 1992a</tref>; <tref>gale et al , 1992b</tref>; <tref>gale et al , 1992c</tref>; <ref>shiitze, 1992</ref>; <ref>gale et al , 1993</ref>; <ref>yarowsky, 1995</ref>, and <pos>even</pos> recently for query translation in cross-language ir as <pos>well</pos> <ref>ballesteros and croft, 1998</ref>.	co-occurrence statistics is collected from either bilingual parallel and 334 non-parallel corpora <ref>smadja et al , 1996</ref>; <ref>kupiec, 1993</ref>; <ref>wu, 1995</ref>; <ref>tanaka and iwasaki, 1996</ref>; <ref>fung and lo, 1998</ref>, or monolingual corpora <ref>smadja, 1993</ref>; <ref>fung and wu, 1994</ref>; <ref>liu and li, 1997</ref>; <ref>shiitze, 1992</ref>; <ref>yarowsky, 1995</ref>.	as we noted in <ref>fung and lo, 1998</ref>; <ref>fung, 1998</ref>, parallel corpora are rare in most domains.	we <pos>want</pos> to devise a method that uses only monolingual data in the primary language to train co-occurrence information.
pruning translation alternatives for query translation.	co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction <ref>smadja, 1993</ref>; <ref>fung and wu, 1994</ref>, phrasal translation <ref>smadja et al , 1996</ref>; <ref>kupiec, 1993</ref>; <ref>wu, 1995</ref>; <ref>dagan and church, 1994</ref>, target word selection <ref>liu and li, 1997</ref>; <ref>tanaka and iwasaki, 1996</ref>, domain word translation <ref>fung and lo, 1998</ref>; <ref>fung, 1998</ref>, <pos><pos>sense</pos></pos> disambiguation <ref>brown et al , 1991</ref>; <ref>dagan et al , 1991</ref>; <ref>dagan and itai, 1994</ref>; <tref>gale et al , 1992a</tref>; <tref>gale et al , 1992b</tref>; <tref>gale et al , 1992c</tref>; <ref>shiitze, 1992</ref>; <ref>gale et al , 1993</ref>; <ref>yarowsky, 1995</ref>, and <pos>even</pos> recently for query translation in cross-language ir as <pos>well</pos> <ref>ballesteros and croft, 1998</ref>.	co-occurrence statistics is collected from either bilingual parallel and 334 non-parallel corpora <ref>smadja et al , 1996</ref>; <ref>kupiec, 1993</ref>; <ref>wu, 1995</ref>; <ref>tanaka and iwasaki, 1996</ref>; <ref>fung and lo, 1998</ref>, or monolingual corpora <ref>smadja, 1993</ref>; <ref>fung and wu, 1994</ref>; <ref>liu and li, 1997</ref>; <ref>shiitze, 1992</ref>; <ref>yarowsky, 1995</ref>.	as we noted in <ref>fung and lo, 1998</ref>; <ref>fung, 1998</ref>, parallel corpora are rare in most domains.
a second <neg>problem</neg> with the fidditch parser is <neg>poor</neg> performances: tilt recall and precision at detecting word collocations are declared to be as <neg>low</neg> as 50, i-iowever it is <neg>unclear</neg> if this value applies only to svo triples, and how it has been derived.	the recall is <neg>low</neg> because tile fidditch parser, as other partial parsers <ref>sekine et al, 1992</ref>; resnik and hearst, i993, only detect links between adjacent or near-adjacent words.	thougll a 50/,, precision and recall might be 447 reasonable for human assisted tasks, like in lexicography, supervised translation, etc , it is not fair enough if collocational analysis must serve a fully automated system.	in fact, corpus linguistics became a popular research field because of the claim that <neg>shallow</neg> techniques could <neg>overcome</neg> the lexical coverage bottleneck of traditional nlp techniques.
in fact, corpus linguistics became a popular research field because of the claim that <neg>shallow</neg> techniques could <neg>overcome</neg> the lexical coverage bottleneck of traditional nlp techniques.	among the applications of collocational analysis for lexical acquisition are: the derivation of syntactic disambiguation cues <ref>basili et al 1991, 1993a</ref>; <ref>hindle and rooths 1991</ref>,1993; <ref>sekine 1992</ref> <ref>bogges et al 1992</ref>, sense preference <ref>yarowski 1992</ref>, acquisition of selectional restrictions <ref>basili et al 1992b, 1993b</ref>; <ref>utsuro et al 1993</ref>, lexical preference in generation <ref>smadjia 1991</ref>, word clustering <ref>pereira 1993</ref>; <ref>hindle 1990</ref>; <ref>basili et al 1993c</ref>, etc in the majority of these papers, even though the precedent or subsequent statistical processing reduces the number of <neg>accidental</neg> associations, very large corpora 10,000,000 words are necessary to obtain reliable data on a large enough number of words.	in addition, most papers produce a performance evaluation of their methods but do not provide a measure of the coverage, ie the percentage of cases for which their method actually provides a right or <neg>wrong</neg> solution.	it is quite common that results are discussed only for 10-20 cases.
7 subjects a, b, c, d, e, f, g 161 i0 hei falls over, figure 2: portion of segntentation from narrative 6 subject annotation of narratrs httention digression to describe suml track no verbal communication ie , speaker describes lack thereof describes that it is a silent movie with only nature <pos>sound</pos>s speaker describes sound techniques used in tnovie explain that there is no speaking ill nlovie figure 3: segment spanning 1,12 through 154 3 discourse segment boundaries in <ref>passonneau and litman, 1993</ref>, we show that our subjects <pos>agree</pos> with one another at levels that are statistically <pos>significant</pos>, thus demonstrating the <pos>reliability</pos> of intention as a segmentation criterion.	percent <pos>agreement</pos> is defined in <tref>gale et al , 1992</tref> as the ratio of observed agreements with the majority opinion to possible agreements with the majority opinion.	we use percent <pos><pos>agree</pos>ment</pos> to measure the <pos>ability</pos> of subjects to agree with one anotlter on whether there is  segment boundary between two adjacent prosodic phrases.	we find that the average <pos>agreement</pos> across the 20 narratives on the status of all <pos>potential</pos> boundary locations is 89 with a range from 82-92.
an artificial ambiguous word can be coined with the monosemous words in table 1.	this process is similar to the use of general pseudowords <tref>gale et al , 1992b</tref>; <ref>gaustad, 2001</ref>; <ref>nakov and hearst, 2003</ref>, but has some <pos>essential</pos> differences.	this artificial ambiguous word need to simulate the function of the <pos><pos>real</pos></pos> ambiguous word, and to acquire semantic knowledge as the real ambiguous word does.	thus, we call it an equivalent pseudoword ep for its equivalence with the <pos>real</pos> ambiguous word.
that is, lexas is only concerned with disambiguating senses of a word in a given pos.	making such an assumption is <pos>reasonable</pos> since pos taggers that can <pos>achieve</pos> accuracy of 96 are <pos>readily</pos> available to assign pos to <pos>unrestricted</pos> english sentences <ref>brill, 1992</ref>; <ref>cutting et al , 1992</ref>.	in addition, <pos><pos>sense</pos></pos> definitions are only available for root words in a dictionary.	these are words that are not morphologically inflected, such as <pos><pos>interest</pos></pos> as opposed to the plural form <pos>interests</pos>, fall as opposed to the other inflected forms <pos>like</pos> fell, fallen, falling, falls, etc the <pos><pos><pos><pos>sense</pos></pos></pos></pos> of a morphologically inflected <pos>content</pos> word is the sense of its uninflected form.
9 related work using vector space model and similarity measures for ranking is a common approach in ir for query/text and text/text comparisons <ref>salton and buckley, 1988</ref>; <ref>salton and yang, 1973</ref>; <ref>croft, 1984</ref>; <ref>turtle and croft, 1992</ref>; <ref>bookstein, 1983</ref>; <ref>korfhage, 1995</ref>; <ref>jones, 1979</ref>.	this approach has also been used by <ref>dagan and itai, 1994</ref>; <tref>gale et al , 1992</tref>; <ref>shiitze, 1992</ref>; <ref>gale et al , 1993</ref>; <ref>yarowsky, 1995</ref>; gale and church, 1lunar is not an unknown word in english, yeltsin finds its translation in the 4-th candidate.	table 5: tion out score 0008421 0007895 0007669 0007588 0007283 0006812 0006430 0006218 0005921 0005527 0005335 0005335 0005221 0004731 0004470 0004275 0003878 0003859 0003859 0003784 0003686 0003550 0003519 0003481 0003407 0003407 0003338 0003324 some chinese ut english teng-hui sar flu lei poultry sar hijack poultry tung diaoyu primeminister president china lien poultry china flu primeminister president poultry kalkanov poultry sar zhuhai primeminister president flu apologise unknown word translachinese  weng-hui  u lei j poultry  chee-hwa  teng-hui  sar  chee-hwa : teng-hui  weng-hui w weng-hui clam  teng-hui - chee-hwa  teng-hui lei  chee-hwa  chee-hwa  leung  zhuhai i lei j yeltsin - chee-hwa  lam lam j poultry w teng-hui 0003250 dpp 0003206 tang 0003202 tung 0003040 leung 0003033 china 0002888 zhuhai 0002886 tung  teng-hui tang leung leung  sar  lunar tung 1994 for <pos>sense</pos> disambiguation between multiple usages of the same word.	some of the early statistical terminology translation methods are <ref>brown et al , 1993</ref>; <ref>wu and xia, 1994</ref>; <ref>dagan and church, 1994</ref>; <ref>gale and church, 1991</ref>; <ref>kupiec, 1993</ref>; <ref>smadja et al , 1996</ref>; kay and r<ref>sscheisen, 1993</ref>; <ref>fung and church, 1994</ref>; <ref>fung, 1995b</ref>.
it remains to be seen how we can also make use of the multilingual texts as nlp resources.	in the years since the appearance of the first papers on using statistical models for bilingual lexicon compilation and machine translation<ref>brown et al , 1993</ref>; <ref>brown et al , 1991</ref>; <ref>gale and <ref>church, 1993</ref></ref>; <ref>church, 1993</ref>; <ref>simard et al , 1992</ref>, <pos>large</pos> amount of human effort and time has been invested in collecting parallel corpora of translated texts.	our goal is to <pos>alleviate</pos> this effort and enlarge the scope of corpus resources by looking into monolingual, comparable texts.	this type of texts are known as nonparallel corpora.
the same system used in a validation mode, can be used to check and spot alignment errors in multilingually aligned wordnets as balkanet and eurowordnet.	word <pos><pos>sense</pos></pos> disambiguation wsd is wellknown as one of the more difficult problems in the field of <pos>natural</pos> language processing, as noted in <tref>gale et al, 1992</tref>; <ref>kilgarriff, 1997</ref>; <ref>ide and vronis, 1998</ref>, and others.	the difficulties stem from several sources, including the lack of means to formalize the properties of context that characterize the use of an ambiguous word in a given <pos><pos><pos><pos>sense</pos></pos></pos></pos>, lack of a standard and possibly <pos>exhaustive</pos> sense inventory, and the subjectivity of the human evaluation of such algorithms.	to address the last problem, <tref>gale et al, 1992</tref> argue for upper and lower bounds of <pos>precision</pos> when comparing automatically assigned <pos><pos>sense</pos></pos> labels with those assigned by human judges.
the difficulties stem from several sources, including the lack of means to formalize the properties of context that characterize the use of an ambiguous word in a given <pos><pos><pos><pos>sense</pos></pos></pos></pos>, lack of a standard and possibly <pos>exhaustive</pos> sense inventory, and the subjectivity of the human evaluation of such algorithms.	to address the last problem, <tref>gale et al, 1992</tref> argue for upper and lower bounds of <pos>precision</pos> when comparing automatically assigned <pos><pos>sense</pos></pos> labels with those assigned by human judges.	the lower bound should not drop below the baseline usage of the algorithm in which every word that is disambiguated is assigned the most frequent <pos><pos>sense</pos></pos> whereas the upper bound should not be too restrictive when the word in question is hard to disambiguate <pos>even</pos> for human judges a measure of this difficulty is the computation of the <pos>agreement</pos> rates between human annotators.	identification and formalization of the determining contextual parameters for a word used in a given <pos><pos>sense</pos></pos> is the focus of wsd work that treats texts in a monolingual settingthat is, a setting where translations of the texts in other languages either do not exist or are not considered.
the summed deviation for <pos><pos>perfect</pos></pos> performance is thus 0.	finally, to interpret our quantitative results, we use the performance of our human subjects as a target goal for the performance of our algorithms <tref>gale et al , 1992</tref>.	table 1 shows the average human performance for both the training and test sets of narratives.	note that human performance is basically the same for both sets of narratives.
researchers have begun to investigate the <pos>ability</pos> of humans to <pos>agree</pos> with one another on segmen108 tation, and to propose methodologies for quantifying their findings.	several studies have used <pos>expert</pos> coders to locally and globally structure spoken discourse according to the model of <ref>grosz and sidnet 1986</ref>, including <ref>grosz and hirschberg, 1992</ref>; <ref>hirschberg and grosz, 1992</ref>; <ref>nakatani et al , 1995</ref>; <ref>stifleman, 1995</ref>.	<ref>hearst 1994</ref> asked subjects to place boundaries between paragraphs of expository texts, to indicate topic changes.	<ref>moser and moore 1995</ref> had an expert coder assign segments and various segment features and relations based on rst.
<ref>moser and moore 1995</ref> had an <pos>expert</pos> coder assign segments and various segment features and relations based on rst.	to quantify their findings, these studies use notions of <pos>agreement</pos> <tref>gale et al , 1992</tref>; <ref>moset and moore, 1995</ref> and/or <pos>reliability</pos> <ref>passonneau and litman, 1993</ref>; passonneau and litman, to appear; <ref>isard and carletta, 1995</ref>.	by asking subjects to segment discourse using a non-linguistic criterion, the correlation of linguistic devices with independently derived segments can then be investigated in a way that avoids circularity.	together, <ref>grosz and hirschberg, 1992</ref>; <ref>hirschberg and grosz, 1992</ref>; <ref>nakatani et al , 1995</ref> comprise an ongoing study using three corpora: professionally read ap news stories, spontaneous narrative, and read and spontaneous versions of task-oriented monologues.
by using 10-fold cross validation <ref>kohavi and john, 1995</ref> to automatically pick the <pos>best</pos> number of nearest neighbors to use, the performance of lsxas has <pos>improved</pos>.	4 word <pos><pos>sense</pos></pos> disambiguation in the <pos>large</pos> in <tref>gale et al , 1992</tref>, it was argued that any <pos>wide</pos> coverage wsd program must be <pos>able</pos> to perform significantly <pos>better</pos> than the most-frequent-sense classifier to be <pos>worthy</pos> of serious consideration.	the performance of lexas as indicated in table 1 is significantly <pos>better</pos> than the most-frequent-sense classifier for the set of 191 words collected in our corpus.	figure 1 and 2 also confirm that all the training examples collected in our corpus are effectively utilized by lexas to <pos>improve</pos> its wsd performance.
1.	word <pos><pos><pos><pos>sense</pos></pos></pos></pos> disambiguation has long been one of the major concerns in <pos>natural</pos> language processing area eg , <ref>bruce et al , 1994</ref>; <ref>choueka et al , 1985</ref>; <ref>gale et al , 1993</ref>; <ref>mcroy, 1992</ref>; <ref>yarowsky 1992, 1994, 1995</ref>, whose aim is to identify the <pos>correct</pos> sense of a word in a particular context, among all of its senses defined in a dictionary or a thesaurus.	<pos>undoubtedly</pos>, <pos>effective</pos> disambiguation techniques are of <pos>great</pos> use in many <pos>natural</pos> language processing tasks, eg, machine translation and information retrieving <ref>allen, 1995</ref>; <ref>ng and lee, 1996</ref>; <ref>resnik, 1995</ref>, etc previous strategies for word <pos><pos>sense</pos></pos> disambiguation mainly fall into two categories: statistics-based method and exemplar-based method.	statistics-based method often requires large-scale corpora eg , <ref>hirst, 1987</ref>; <ref>luk, 1995</ref>, <pos><pos>sense</pos></pos>-tagging or not, monolingual or aligned bilingual, as training data to specify <pos>significant</pos> clues for each word sense.
h2s,  max ps, l ew ,n, e ew where ewi  ew i si  synsetew  1 p  si i ewj  - -nj where si  synsetewj, nj  isyr et w,l in this formula, n is the number of synsets of the translation et.	23 heuristic 3: <pos><pos><pos><pos><pos>sense</pos></pos></pos></pos></pos> ordering <tref>gale et al , 1992</tref> reports that word sense disambiguation would be at least 75 <pos>correct</pos> if a system assigns the most frequently occurring sense.	<ref>miller et al , 1994</ref> found that automatic i we use english wordnet version 16 l 143 assignment of polysemous words in brown corpus to <pos><pos>sense</pos></pos>s in wordnet was 58 <pos>correct</pos> with a heuristic of most frequently occurring sense.	we adopt these previous results to develop <pos>sense</pos> ordering heuristic.
in the area of word <pos><pos>sense</pos></pos> disambiguation, <ref>black 1988</ref> developed a model based on decision trees using a corpus of 22 million tokens, after manually sense-tagging approximately 2,000 concordance lines for five test words.	since then, supervised <pos>learning</pos> from sense-tagged corpora has since been used by several researchers: zernik 1990, 1991, <ref>hearst 1991</ref>, <ref>leacock, towell, and voorhees 1993</ref>, gale, church, and yarowsky 1992d, 1993, <ref>bruce and wiebe 1994</ref>, miller et al.	1994, <ref>niwa and nitta 1994</ref>, <ref>lehman 1994</ref>, among others.	however, despite the availability of increasingly <pos>large</pos> corpora, two major obstacles impede the acquisition of lexical knowledge from corpora: the difficulties of manually sense-tagging a training corpus, and data sparseness.
<ref>sutcliffe and slater 1995</ref> replicated this method on full text samples from orwells animal farm and found similar results 72 <pos>correct</pos> <pos><pos>sense</pos></pos> assignment, compared with a 33 chance baseline, and 40 using lesks method.	several authors for example, krovetz and croft 1989, guthrie et al 1991, slator 1992, cowie, guthrie, and guthrie 1992, janssen 1992, braden-harder 1993, liddy and paik 1993 have attempted to <pos>improve</pos> results by using supplementary fields of information in the electronic version of the longman dictionary of contemporary english ldoce, in particular, the box codes and subject codes provided for each <pos><pos>sense</pos></pos>.	box codes include primitives such as abstract, animate, human, etc , and encode type restrictions on nouns and adjectives and on the arguments of verbs.	subject codes use another set of primitives to classify senses of words by subject economics, engineering, etc.
<ref>atkins 1987</ref> and kilgarriff forthcoming also implicitly adopt the view of <ref>harris 1954</ref>, according to which each <pos><pos>sense</pos></pos> <pos><pos>distinct</pos>ion</pos> is reflected in a distinct context.	a similar view underlies the class-based methods cited in section 243 <ref>brown et al 1992</ref>; <ref>pereira and tishby 1992</ref>; <ref>pereira, tishby, and lee 1993</ref>.	in this volume, schiitze continues in this vein and proposes a technique that avoids the problem of <pos><pos><pos><pos><pos><pos>sense</pos></pos></pos></pos></pos></pos> <pos>distinction</pos> altogether: he creates sense clusters from a corpus rather than relying on a pre-established sense list.	324 enumeration or generation.
furthermore, our percent <pos>agreement</pos> figures are comparable with the results of other segmentation studies discussed <pos>above</pos>.	while studies of other tasks have achieved stronger results eg , 968 in a word-sense disambiguation study <tref>gale et al , 1992</tref>, the meaning of percent <pos>agreement</pos> in isolation is unclear.	for example, a percent <pos>agreement</pos> figure of less than 90 could still be very <pos>meaningful</pos> if the probability of obtaining such a figure is low.	in the next section we demonstrate the <pos>significance</pos> of our findings.
<pos><pos>agree</pos>ment</pos> among subjects we measure the <pos>ability</pos> of subjects to agree with one another, using a figure called percent <pos>agreement</pos>.	percent <pos>agreement</pos>, defined in <tref>gale et al , 1992</tref>, is the ratio of observed agreements with the majority opinion to possible agreements with the majority opinion.	here, <pos>agreement</pos> among four, five, six, or seven subjects on whether or not there is a segment boundary between two adjacent prosodic phrases constitutes a majority opinion.	given a transcript of length n prosodic phrases, there are n-1 possible boundaries.
<pos>reliability</pos> the correspondence between discourse segments and more abstract units of meaning is poorly <pos>understood</pos> see <ref>moore and pollack, 1992</ref>.	a number of alternative proposals have been presented which directly or indirectly relate segments to intentions <ref>grosz and sidner, 1986</ref>, rst relations <ref>mann et al , 1992</ref> or other semantic relations <ref>polanyi, 1988</ref>.	we present initial results of an investigation of whether naive subjects can <pos>reliably</pos> segment discourse using speaker intention as a criterion.	our corpus consists of 20 narrative monologues about the same movie, taken from <ref>chafe 1980</ref> n14,000 words.
587 752 naive-bayes 582 745 table 1: experimental results ures are those of <ref>ng and lee, 1996</ref>.	the default strategy of picking the most frequent <pos><pos>sense</pos></pos> has been advocgted as the baseline performance for evaluating wsd programs <tref>gale et al , 1992b</tref>; <ref>miller et al , 1994</ref>.	there are two instantiations of this strategy in our current evaluation.	since wordnet orders its <pos><pos><pos><pos><pos><pos><pos><pos>sense</pos></pos></pos></pos></pos></pos></pos></pos>s such that sense 1 is the most frequent sense, one possibility is to always pick sense 1 as the <pos>best</pos> sense assignment.
this is in spite of the conditional <pos>independence</pos> assumption made by the naive-bayes algorithm, which may be unjustified in the domains tested.	gale, church and yarowsky <tref>gale et al , 1992a</tref>; <ref>gale et al , 1995</ref>; <ref>yarowsky, 1992</ref> have also <pos>successfully</pos> used the naive-bayes algorithm and several extensions and variations for word <pos><pos>sense</pos></pos> disambiguation.	on the other hand, our past work on wsd <ref>ng and lee, 1996</ref> used an exemplar-based or nearest neighbor <pos>learning</pos> approach.	our wsd program, lexas, extracts a set of features, including part of speech and morphological form, surrounding words, local collocations, and verb-object syntactic relation from a sentence containing the word to be disambiguated.
much recent research on word <pos><pos>sense</pos></pos> disambiguation wsd has adopted a corpus-based, <pos>learning</pos> approach.	many different <pos><pos>learning</pos></pos> approaches have been used, including neural networks <ref>leacock et al , 1993</ref></ref>, probabilistic algorithms <ref>bruce and wiebe, 1994</ref>; <tref>gale et al , 1992a</tref>; <ref>gale et al , 1995</ref>; <ref>leacock et al , 1993</ref></ref>; <ref>yarowsky, 1992</ref>, decision lists <ref>yarowsky, 1994</ref>, exemplar-based learning algorithms <ref>cardie, 1993</ref>; <ref>ng and lee, 1996</ref>, etc in particular, <ref>mooney 1996</ref> evaluated seven state-of-the-art machine learning algorithms on a common data set for disambiguating six senses of the word line.	the seven algorithms that he evaluated are: a naive-bayes classifier <ref>duda and hart, 1973</ref>, a perceptron <ref>rosenblatt, 1958</ref>, a decisiontree learner <ref>quinlan, 1993</ref>, a k nearest-neighbor classifier exemplar-based learner <ref>cover and hart, 1967</ref>, logic-based dnf and cnf learners <ref>mooney, 1995</ref>, and a decision-list learner <ref>rivest, 1987</ref>.	his results indicate that the <pos>simple</pos> naive-bayes algorithm gives the highest accuracy on the line corpus tested.
9 related work using vector space model and similarity measures for ranking is a common approach in ir for query/text and text/text comparisons <ref>salton and buckley, 1988</ref>; <ref>salton and yang, 1973</ref>; <ref>croft, 1984</ref>; <ref>turtle and croft, 1992</ref>; <ref>bookstein, 1983</ref>; <ref>korflmge, 1995</ref>; <ref>jones, 1979</ref>.	this approach has also been used by <ref>dagan and itai, 1994</ref>; <tref>gale et al, 1992</tref>; <ref>shiitze, 1992</ref>; <ref>gale et al, 1993</ref>; <ref>yarowsky, 1995</ref>; gale and church, 1lunar is not an unknown word in english, yeltsin finds its translation in the 4-th candidate.	table 5: some chinese unknown word translation output score 0008421 0007895 0007669 0007588 0007283 0006812 0006430 0006218 0005921 0005527 0005335 0005335 0005221 0004731 0004470 0004275 0003878 0003859 0003859 0003784 0003686 0003550 0003519 0003481 0003407 0003407 0oo3338 0003324 0003250 0003206 0003202 0003040 0003033 0002888 0002886 english chinese teng-hui  teng-hui sar , sar flu n m, lei  lei poultry j poultry sar  chee-hwa hijack  teng-hui poultry  sar ,ng  chee-hw diaoyu  teng-hui primeminister  teng-hui president  teng-hui china  lava lien  teng-hui poultry  chee-hwa china  teng-hui flu  lei privaeminister -i chee-hwa president 1; chee-hwa poultry  leung kalkanov i zhuhai poultry i lei sar 1 j l yeltsin zhuhai -1 l chee-hwa primeminister  lain president  lava flu  poultry apologise  teng-hui dee  teng-hui tang j tang islng  leung leung : leung china tn sar zhuhai  lunar ttulg  tung 1994 for <pos><pos>sense</pos></pos> disambiguation between multiple usages of the same word.	some of the early statistical terminology translation methods are <ref>brown et al, 1993</ref>; <ref>wu and xia, 1994</ref>; <ref>dagan and church, 1994</ref>; <ref>gale and church, 1991</ref>; <ref>kupiec, 1993</ref>; <ref>smadja et al, 1996</ref>; kay and r<ref>sscheisen, 1993</ref>; <ref>fung and church, 1994</ref>; <ref>fhmg, 1995b</ref>.
it remains to be seen how we can also make use of the multilingual texts as nlp resources.	in the years since the appearance of the first papers on using statistical models for bilingual lexicon compilation and machine translation<ref>brown et al, 1993</ref>; <ref>brown et al, 1991</ref>; <ref>gale and <ref>church, 1993</ref></ref>; <ref>church, 1993</ref>; <ref>simard et al, 1992</ref>, <pos>large</pos> amount of human effort and time has been invested in collecting parallel corpora of translated texts.	our goal is to <pos>alleviate</pos> this effort and enlarge the scope of corpus resources by looking into monolingual, comparable texts.	this type of texts are known as nonparallel corpora.
the <pos>paradise</pos> model posits that performance can be correlated with a <pos>meaningful</pos> external criterion such as usability, and thus that the overall goal of a spoken dialogue agent is to <pos>maximize</pos> an <pos>objective</pos> related to usability.	user <pos>satisfaction</pos> ratings <ref>kamm, 1995</ref>; <ref>shriberg, wade, and price, 1992</ref>; <ref>polifroni et al , 1992</ref> have been frequently used in the literature as an external indicator of the usability of a dialogue agent.	the model further posits that two types of factors are <pos><pos>potential</pos></pos> <pos><pos>relevant</pos></pos> contributors to user <pos>satisfaction</pos> namely task <pos>success</pos> and dialogue costs, and that two types of factors are potential relevant contributors to costs <ref>walker, 1996</ref>.	in addition to the use of decision theory to create this <pos>objective</pos> structure, other <pos>novel</pos> aspects of <pos>paradise</pos> include the use of the kappa coefficient <ref>carletta, 1996</ref>; <ref>siegel and castellan, 1988</ref> to operationalize task <pos><pos>success</pos></pos>, and the use of linear regression to quantify the relative <pos>contribution</pos> of the success and cost factors to user <pos>satisfaction</pos>.
an agents responses to a query are compared with a predefined key of minimum and maximum reference answers; performance is the proportion of responses that match the key.	this approach has many widely acknowledged limitations <ref>hirschman and pao, 1993</ref>; <ref>danieli et al , 1992</ref>; <ref>bates and ayuso, 1993</ref>, eg, although there may be many <pos>potential</pos> dialogue strategies for carrying out a task, the key is tied to one particular dialogue strategy.	in contrast, agents using different dialogue strategies can be compared with measures such as inappropriate utterance ratio, turn correction ratio, concept accuracy, implicit recovery and transaction <pos>success</pos> danieli lwe use the term agent to emphasize the fact that we are evaluating a speaking entity that may have a <pos>personality</pos>.	readers who <pos><pos>wish</pos></pos> to may substitute the word system wherever agent is used.
the problems arise because most <pos><pos>sense</pos></pos> <pos>distinction</pos>s are not as <pos>clear</pos> as the distinction between river bank and money bnk, so it is not always <pos>straightforward</pos> for a person to say what the <pos>correct</pos> answer is thus we do not always know what it would mean to say that a computer program got the <pos><pos>right</pos></pos> answer.	the issue is discussed in detail by <tref>gale et al , 1992</tref> who identify the problem as one of identifying the upper bound for the performance of a wsd program.	if people can only <pos>agree</pos> on the <pos>correct</pos> answer x of the time, a claim that a program <pos>achieve</pos>s more than x accuracy is hard to interpret, and x is the upper bound for what the program can meaningfully achieve.	there have been some discussions as to what this upper bound might be.
although this methodology could be <pos>valid</pos> for certain nlp problems, such as english part-of-speech tagging, we think that there exists <pos>reasonable</pos> evidence to say that, in wsd, accuracy results cannot be simply extrapolated to other domains contrary to the opinion of other authors <ref>ng, 1997b</ref>: on the asupervised approaches, also known as data-driven or corpus-dmven, are those that learn from a previously semantically annotated corpus.	172 one hand, wsd is very dependant to the domain of application <tref>gale et al , 1992b</tref> --see also <ref>ng and lee, 1996</ref>; <ref>ng, 1997a</ref>, in which quite different accuracy figures are obtained when testing an exemplar-based wsd classifier on two different corpora.	oi1 the other hand, it does not seem <pos>reasonable</pos> to think that the training material is <pos>large</pos> and representative enough to cover all <pos>potential</pos> types of examples.	to date, a <pos>thorough</pos> study of the domain dependence of wsd --in the style of other studies <pos>devoted</pos> to parsing <ref>sekine, 1997</ref>-has not been carried out.
in order to corroborate the previous hypotheses, this paper explores the portability and tuning of four different ml algorithms previously applied to wsd by training and testing them on different corpora.	additionally, supervised methods suffer from the knowledge acquisition bottleneck <tref>gale et al , 1992a</tref>.	<ref>ng, 1997b</ref> estimates that the manual annotation effort <pos>necessary</pos> to build a <pos>broad</pos> coverage semantically annotated english corpus is about 16 personyears.	this overhead for supervision could be much greater if a costly tuning procedure is required before applying any existing system to each new domain.
word <pos><pos>sense</pos></pos> disambiguation is a potentially crucial task in many nlp applications, such as machine translation brown, della pietra, and <ref>della pietra 1991</ref>, parsing <ref>lytinen 1986</ref>; <ref>nagao 1994</ref> and text retrieval <ref>krovets and croft 1992</ref>; <ref>voorhees 1993</ref>.	various corpus-based approaches to word <pos><pos>sense</pos></pos> disambiguation have been proposed <ref>bruce and wiebe 1994</ref>; <ref>charniak 1993</ref>; <ref>dagan and itai 1994</ref>; <ref>fujii et al 1996</ref>; <ref>hearst 1991</ref>; <ref>karov and edelman 1996</ref>; <ref>kurohashi and nagao 1994</ref>; <ref>li, szpakowicz, and matwin 1995</ref>; <ref>ng and lee 1996</ref>; <ref>niwa and nitta 1994</ref>; schitze 1992; <ref>uramoto 1994b</ref>; <ref>yarowsky 1995</ref>.	the use of corpus-based approaches has grown with the use of machine-readable text, because unlike conventional rule-based approaches relying on hand-crafted selectional rules some of which are reviewed, for example, by hirst 1987, corpus-based approaches release us from the task of generalizing observed phenomena through a set of rules.	our verb <pos>sense</pos> disambiguation system is based on such an approach, that is, an example-based approach.
alignment at other levels of resolution is obviously <pos>useful</pos>.	a section, paragraph, sentence, phrase, collocation, or word can be aligned to its translation <tref>kupiec 1993</tref>; <ref>smadja, mckeown, and hatzivassiloglou 1996</ref>.	other <pos>logical</pos> approaches involve aligning parse trees of a sentence and its translation <ref>matsumoto, ishimoto, and utsuro 1993</ref>; <ref>meyers, yangarber, and grishman 1996</ref>, or simultaneously generating parse trees and alignment arrangements <ref>wu 1995</ref>.	department of computer science, national tsing hua university, hsinchu, 30043, taiwan, roc.
3 multilingual terminology extraction several works describe methods to extract terms, or candidate terms, in english and/or french <ref>justeson and katz, 1995</ref>; <ref>daille, 1994</ref>; nkwenti-<ref>azeh, 1992</ref>.	some more specific works describe methods to align noun phrases within parallel corpora <tref>kupiec, 1993</tref>.	the underlying assumption beyond these works is that the monolingually extracted units correspond to each other cross-lingually.	<neg>unfortunately</neg>, this is not always the case, and the above methodology suffers flom the <neg>weaknesses</neg> pointed out by <ref>wu, 1997</ref> concerning parse-parse-match procedures.
in addition, new collocations are produced one after another and most of them are technical jargons.	there has been a growing interest in corpus-based approaches which retrieve collocations from large corpora <ref>nagao and mori, 1994</ref>, <ref>ikehara et al , 1996</ref> <tref>kupiec, 1993</tref>, <ref>fung, 1995</ref>, <ref>kitamura and matsumoto, 1996</ref>, <ref>smadja, 1993</ref>, <ref>smadja et al , 1996</ref>, <ref>haruno et al , 1996</ref>.	<neg>although</neg> these approaches achieved good results for the task considered, most of them aim to extract fixed collocations, mainly noun phrases, and require the information which is dependent on each language such as dictionaries and parts of speech.	from a practical point of view, however, a more robust and flexible approach is desirable.
this highlights the <neg>need</neg> for finding multi-word translation correspondences.	previous works that focus on multi-word translation correspondences from parallel corpora include noun phrase correspondences <tref>kupiec, 1993</tref>, fixed/flexible collocations <ref>smadja et al , 1996</ref>, n-gram word sequences of <neg>arbitrary</neg> length <ref>kitamura and matsumoto, 1996</ref>, non-compositional compounds <ref>melamed, 2001</ref>, captoids <ref>moore, 2001</ref>, and named entities 1.	in all of these approaches, a common <neg>problem</neg> seems to be an identification of meaningful multi-word translation units.	there are a number of factors which make handling of multi-word units more <neg>complicated</neg> than it appears.
it can now be assumed that a parallel bilingual corpus may be aligned to the sentence level with reasonable accuracy kay and ri3cheisen 1988; <ref>catizone, russel, and warwick 1989</ref>; <ref>gale and church 1991</ref>; <ref>brown, lai, and mercer 1991</ref>; <ref>chen 1993</ref>, even for languages as disparate as chinese and english <ref>wu 1994</ref>.	algorithms for subsentential alignment have been developed as well as granularities of the character <ref>church 1993</ref>, word <ref>dagan, church, and gale 1993</ref>; <ref>fung and church 1994</ref>; <ref>fung and mckeown 1994</ref>, collocation <ref>smadja 1992</ref>, and specially segmented <tref>kupiec 1993</tref> levels.	however, the identification of subsentential, nested, phrasal translations within the parallel texts remains a nontrivial <neg>problem</neg>, due to the added complexity of dealing with constituent structure.	manual phrasal matching is feasible only for small corpora, either for toy-prototype testing or for narrowly <neg>restricted</neg> applications.
4 generating translation candidates 41 extraction of japanese terms errors in the extraction of terms and phrases from parallel texts eventually lead to a <neg>failure</neg> in acquiring the correct term/phrase correspondences.	<ref>in kupiec 1993</ref> and <ref>yamamoto 1993</ref>, term and phrase extraction is applied to both of parallel texts.	in contrast, we extract from units only japanese terms, thereby reducing the errors caused by term/phrase recognizer.	japanese nps can be recognized more accurately than english nps because japanese has considerably <neg>less</neg> multi-category words.
automatic bilingual lexicon construction based on bilingual corpora has become an important first step for many studies and applications of natural language processing nlp, such as machine translation mt, crosslanguage information retrieval clir, and bilingual text alignment.	as noted in <ref>tsuji 2002</ref>, many previous methods <ref>dagan et al , 1993</ref>; <tref>kupiec, 1993</tref>; <ref>wu and xia, 1994</ref>; <ref>melamed, 1996</ref>; <ref>smadja et al , 1996</ref> deal with this <neg>problem</neg> based on frequency of words appearing in the corpora, which can not be effectively applied to lowfrequency words, such as transliterated words.	these transliterated words are often domain-specific and created frequently.	many of them are not found in existing bilingual dictionaries.
<ref>tiedemann, 1993</ref> ; <ref>boutsis  piperidis, 1996</ref> ; <ref>piperidis et al , 1997</ref> combine statistical and linguistic information for the same task.	some methods make alignment suggestions at an intermediate level between sentence and word 271 and word <ref>smadja, 1992</ref> ; <ref>smadja et al , 1996</ref> ; <tref>kupiec, 1993</tref> ; <ref>kumano  hirakawa, 1994</ref> ; <ref>boutsis  piperidis, 1998</ref>.	a common <neg>problem</neg> is the delimitation and spotting of the units to be matched.	this is not a real <neg>problem</neg> for methods aiming at alignments at a high level of granularity paragraphs, sentences where unit delimiters are clear.
we have been studying robust lexicon compilation methods which do not rely on sentence alignment.	existing lexicon compilation methods <tref>kupiec 1993</tref>; <ref>smadja  mckeown 1994</ref>; <ref>kumano  hirakawa 1994</ref>; <ref>dagan et al 1993</ref>; <ref>wu  xia 1994</ref> all attempt to extract pairs of words or compounds that are translations of each other from previously sentencealigned, parallel texts.	however, sentence alignment <ref>brown et al 1991</ref>; kay  r<ref>sscheisen 1993</ref>; <ref>gale  <ref>church 1993</ref></ref>; <ref>church 1993</ref>; <ref>chen 1993</ref>; <ref>wu 1994</ref> is not always practical when corpora have <neg>unclear</neg> sentence boundaries or with <neg>noisy</neg> text segments present in only one language.	our proposed algorithm for bilingual lexicon acquisition bootstraps off of corpus alignment procedures we developed earlier <ref>fung  church 1994</ref>; <ref>fung  mckeown 1994</ref>.
compilation of translation lexicons is a crucial process for machine translation mt <ref>brown et al , 1990</ref> and cross-language information retrieval clir systems <ref>nie et al , 1999</ref>.	a lot of effort has been spent on constructing translation lexicons from domain-specific corpora in an automatic way <ref>melamed, 2000</ref>; <ref>smadja et al , 1996</ref>; <tref>kupiec, 1993</tref>.	however, such methods encounter two fundamental <neg>problems</neg>: translation of regional variations and the <neg>lack</neg> of up-to-date and high-lexical-coverage corpus source, which are worthy of further investigation.	the first <neg>problem</neg> is resulted from the fact that the translations of a term may have variations in different dialectal regions.
3 multilingual terminology extraction several works describe methods to extract terms, or candidate terms, in english and/or french <ref>justeson and katz, 1995</ref>; <ref>daille, 1994</ref>; nkwenti-<ref>azeh, 1992</ref>.	some more specific works describe methods to align noun phrases within parallel corpora <tref>kupiec, 1993</tref>.	the underlying assumption beyond these works is that the monolingually extracted units correspond to each other cross-lingually.	<neg>unfortunately</neg>, this is not always the case, and the above methodology suffers from the <neg>weaknesses</neg> pointed out by <ref>wu, 1997</ref> concerning parse-parse-match procedures.
we present an algorithm in finding word correlation statistics for automatic bilingual lexicon compilation from a non-parallel corpus in chinese and english.	most previous automatic lexicon compilation techniques require a sentence-aligned clean parallel bilingual corpus <tref>kupiec 1993</tref>; <ref>smadja  mckeown 1994</ref>; <ref>kumano  hirakawa 1994</ref>; <ref>dagan et al 1993</ref>; <ref>wu  xia 1994</ref>.	we have previously shown an algorithm which extracts a bilingual lexicon from <neg>noisy</neg> parallel corpus without sentence alignment <ref>fung  mckeown 1994</ref>; <ref>fung 1995</ref>.	<neg>although</neg> bilingual parallel corpora have been available in recent years, they are still relatively few in comparison to the large amount of monolingual text.
<ref>smadja et al , 1996</ref>; <ref>gao et al , 2002</ref>; <ref>wu and zhou, 2003</ref>.	some studies have been done for acquiring collocation translations using parallel corpora <ref>smadja et al, 1996</ref>; <tref>kupiec, 1993</tref>; echizen-ya et al , 2003.	these works implicitly assume that a bilingual corpus on a large scale can be obtained easily.	however, <neg>despite</neg> efforts in compiling parallel corpora, sufficient amounts of such corpora are still <neg>unavailable</neg>.
the minimum score derived from any of the criteria applied is deemed initially to be the score of the constituent.	that is, an assumption of full statistical dependence <tref>yarowsky, 1994</tref>, rather than the more common full <pos>independence</pos>, is made3 when llf events el, e2,, e, are fully <pos>independent</pos>, then the joint probability pe1 a a en is the product of peipen, but if they are maximally dependent, it is the minimum of these <pos>values</pos>.	of course, neither assumption is any more than an approximation to the <pos>truth</pos>; but assuming dependence has the <pos>advantage</pos> that the estimate of the joint probability depends much less strongly on n, and so estimates for alternative joint <pos>even</pos>ts can be directly compared, without any possibly tricky normalization, even if they are composed of different numbers of atomic events.	this property is <pos>desirable</pos>: different sub-paths through a chart may span different numbers of edges, and one can imagine evaluation criteria which are only defined for some kinds of edge, or which often duplicate information supplied by other criteria.
in this paper, we propose a method of detecting japanese homophone errors in japanese texts.	our method is based on a decision list proposed by yarowsky <tref>yarowsky, 1994</tref>; <ref>yarowsky, 1995</ref>.	we <pos>improve</pos> the <pos>original</pos> decision list by using written words in the default evidence.	the <pos>improved</pos> decision list can raise the f-measure of error detection.
we also use the <pos>special</pos> evidence default, frqwl, default is defined as the frequency of wl.	step5 pick the highest strength estwh,ej among 5as in this paper, the addition of a small <pos>value</pos> is an <pos>easy</pos> and <pos>effective</pos> way to avoid the unsatisfactory case, as shown in <tref>yarowsky, 1994</tref>.	estwl, , eaw, e,   , e e, and set the word wk as the answer for the evidence ej.	in this case, the identifying strength is estwk, ej.
following commonpracticeinfeatureextractioneg.	<tref>yarowsky, 1994</tref>, and using the mxpost1 part of speech tagger and wordnets lemmatization, the following feature set was used: bag of word lemmas for the context words in the preceding, current and following sentence; unigrams of lemmas and parts of speech in a window of /three words, where each position provides a <pos>distinct</pos> feature; and bigrams of lemmas in the same window.	the svmlight <ref>joachims, 1999</ref> classifier was used in the supervised settings with its default parameters.	to obtain a multi-class classifier we used a standard one-vs-all approach of training a binary svm for each possible <pos><pos><pos>sense</pos></pos></pos> and then selecting the highest scoring sense for a test example.
it has been <pos>amply</pos> demonstrated that a <pos>wide</pos> assortment of machine <pos>learning</pos> algorithms are quite <pos>effective</pos> at extracting linguistic information from manually annotated corpora.	among the machine <pos>learning</pos> algorithms studied, rule based systems have proven <pos>effective</pos> on many <pos>natural</pos> language processing tasks, including part-of-speech tagging <ref>brill, 1995</ref>; <ref>ramshaw and marcus, 1994</ref>, spelling correction <ref>mangu and brill, 1997</ref>, word-sense disambiguation <ref>gale et al , 1992</ref>, message <pos>understanding</pos> <ref>day et al , 1997</ref>, discourse tagging <ref>samuel et al , 1998</ref>, accent <pos>restoration</pos> <tref>yarowsky, 1994</tref>, prepositional-phrase attachment <ref>brill and resnik, 1994</ref> and base noun phrase identification ramshaw and marcus, in <pos>press</pos>; <ref>cardie and pierce, 1998</ref>; <ref>veenstra, 1998</ref>; <ref>argamon et al , 1998</ref>.	many of these rule based systems learn a short list of <pos>simple</pos> rules typically on the order of 50-300 which are <pos>easily</pos> <pos>understood</pos> by humans.	since these rule-based systems <pos>achieve</pos> <pos>good</pos> performance while <pos>learning</pos> a small list of <pos>simple</pos> rules, it raises the question of whether peoand woman.
unsupervised learning holds great promise for breakthroughs in natural language processing.	in cases like <ref>yarowsky, 1995</ref>, unsupervised methods offer accuracy results than <neg>rival</neg> supervised methods <tref>yarowsky, 1994</tref> while requiring only a fraction of the data preparation effort.	such methods have also been a key driver of progress in statistical machine translation, which depends <neg>heavily</neg> on unsupervised word alignments <ref>brown et al , 1993</ref>.	there are also interesting <neg>problems</neg> for which supervised learning is not an option.
in contrast, we used exemplar-based learning, where the contributions of all features are summed up and taken into account in coming up with a classification.	we also include verb-object syntactic relation as a feature, which is not used in <tref>yarowsky, 1994</tref>.	<neg>although</neg> the work of yarowsky, i994 can be applied to wsd, the results reported in <tref>yarowsky, 1994</tref> only dealt with accent restoration, which is a much simpler <neg>problem</neg>.	it is <neg>unclear</neg> how yarowskys method will fare on wsd of a common test data set like the one we used, nor has his method been tested on a large data set with highly <neg>ambiguous</neg> words tagged with the refined senses of wordnet.
in contrast, lexas uses supervised <pos>learning</pos> from tagged sentences, which is also the approach taken by most recent work on wsd, including <ref>bruce and wiebe, 1994</ref>; <ref>miller et al , 1994</ref>; <ref>leacock et al , 1993</ref>; <tref>yarowsky, 1994</tref>; <ref>yarowsky, 1993</ref>; <ref>yarowsky, 1992</ref>.	the work of <ref>miller et al , 1994</ref>; <ref>leacock et al , 1993</ref>; <ref>yarowsky, 1992</ref> used only the unordered set of surrounding words to perform wsd, and they used statistical classifiers, neural networks, or ir-based techniques.	the work of <ref>bruce and wiebe, 1994</ref> used parts of speech pos and morphological form, in addition to surrounding words.	however, the pos used are abbreviated pos, and only in a window of -b2 words.
one line of research focuses on the use of the knowledge contained in a machine-readable dictionary to perform wsd, such as <ref>wilks et al , 1990</ref>; <ref>luk, 1995</ref>.	in contrast, lexas uses supervised <pos>learning</pos> from tagged sentences, which is also the approach taken by most recent work on wsd, including <ref>bruce and wiebe, 1994</ref>; <ref>miller et al , 1994</ref>; <ref>leacock et al , 1993</ref>; <tref>yarowsky, 1994</tref>; <ref>yarowsky, 1993</ref>; <ref>yarowsky, 1992</ref>.	the work of <ref>miller et al , 1994</ref>; <ref>leacock et al , 1993</ref>; <ref>yarowsky, 1992</ref> used only the unordered set of surrounding words to perform wsd, and they used statistical classifiers, neural networks, or ir-based techniques.	the work of <ref>bruce and wiebe, 1994</ref> used parts of speech pos and morphological form, in addition to surrounding words.
we also include verb-object syntactic relation as a feature, which is not used in <tref>yarowsky, 1994</tref>.	<neg>although</neg> the work of yarowsky, i994 can be applied to wsd, the results reported in <tref>yarowsky, 1994</tref> only dealt with accent restoration, which is a much simpler <neg>problem</neg>.	it is <neg>unclear</neg> how yarowskys method will fare on wsd of a common test data set like the one we used, nor has his method been tested on a large data set with highly <neg>ambiguous</neg> words tagged with the refined senses of wordnet.	the work of <ref>miller et al , 1994</ref> is the only prior work we know of which attempted to evaluate wsd on a large data set and using the refined sense distinction of wordnet.
that local collocation knowledge provides <pos>important</pos> clues to wsd is pointed out in <ref>yarowsky, 1993</ref>, although it was demonstrated only on performing binary or very coarse <pos><pos>sense</pos></pos> disambiguation.	the work of <tref>yarowsky, 1994</tref> is perhaps the most similar to our present work.	however, his work used decision list to perform classification, in which only the single <pos>best</pos> disambiguating evidence that matched a target context is used.	in contrast, we used exemplar-based <pos>learning</pos>, where the contributions of all features are summed up and taken into account in coming up with a classification.
methods can typically be delineated along two dimensions, corpns-based vs dictionary-based approaches.	corpus-based word sense disambignation algorjthm such as <ref>ng and lee, 1996</ref>; <ref>bruce and wiebe, 1994</ref>; <tref>yarowsky, 1994</tref> relied on supervised learning fzom annotated corpora.	the main drawback of these approaches is their requirement of a sizable sense-tagged corpus.	attempts to alleviate this tagbottleneck ilude tmotstrias te ot <neg>ill</neg>,, 1996; <ref>hearst, 1991</ref> and unsupervised algorith yarowsky, 199s dictionary-based approaches rely on linguistic knowledge sources such as mali,e-readable dictionaries <ref>luk, 1995</ref>; <ref>veronis and ide, 1990</ref> and wordnet <ref>agirre and rigau, 1996</ref>; <ref>resnik, 1995</ref> and e0ploit these for word sense disaznbiguation.
moreover, word trigrams are <neg>ineffective</neg> at capturing longdistance properties such as discourse topic and <neg>tense</neg>.	feature-based approaches, such as bayesian classifters <ref>gale, church, and yarowsky, 1993</ref>, decision lists <tref>yarowsky, 1994</tref>, and bayesian hybrids <ref>golding, 1995</ref>, have had varying degrees of success for the <neg>problem</neg> of context-sensitive spelling correction.	however, we report experiments that show that these methods are of <neg>limited</neg> effectiveness for cases such as their, there, theyre and than, then, where the predominant distinction to be made among the words is syntactic.	71 <neg>confusion</neg> set train test most freq.
1995, we formalize the problem of deciding dependency <pos><pos>preference</pos></pos> of subordinate clauses by utilizing the correlation of scope embedding preference and dependency preference of japanese subordinate clauses.	then, as a statistical <pos>learning</pos> method, we employ the decision list learning method of <tref>yarowsky 1994</tref>, where <pos>optimal</pos> combination of those features are selected and sorted in the form of decision rules, according to the strength of correlation between those features and the dependency <pos>preference</pos> of the two subordinate clauses.	we evaluate the proposed method through the experiment on <pos>learning</pos> dependency <pos>preference</pos> of japanese subordinate clauses from the edr bracketed corpus section 4.	we show that the proposed method outperforms other related methods/models.
the head vp chunk of clause1 does not modify that of clause2, but modifies that of another subordinate clause or the matrix clause which follows clause2.	roughly speaking, the first corresponds to the case where clause2 inherently has a scope of the same or a broader breadth compared with that of clause1, while the second corresponds to the case where clause2 inherently has a narrower scope compared with that of clause17 32 decision list <pos>learning</pos> a decision list <tref>yarowsky, 1994</tref> is a sorted list of the decision rules each of which decides the <pos>value</pos> of a decision d given some evidence e each decision rule in a decision list is sorted tour modeling is slightly different from those of other standard approaches to statistical dependency analysis <ref>collins, 1996</ref>; <ref>fujio and matsumoto, 1998</ref>; <ref>haruno et al , 1998</ref> which simply <pos>distinguish</pos> the two cases: the case where dependency relation holds between the given two vp chunks or clauses, and the case where dependency relation does not hold.	in contrast to those standard approaches, we ignore the case where the head vp chunk of clause1 modifies that of another subordinate clause which precedes clause2.	this is because we assume that this case is more loosely related to the scope embedding <pos>preference</pos> of subordinate clauses.
we formalize the problem of deciding scope embedding <pos>preference</pos> as a classification problem, in which various types of linguistic information of each subordinate clause are encoded as features and used for deciding which one of given two subordinate clauses has a broader scope than the other.	as a statistical <pos>learning</pos> method, we employ the decision list learning method of <tref>yarowsky 1994</tref>.	113 table 2: <pos>feat</pos>ures of japanese subordinate clauses feature type  of feat  each binary feature punctuation 2 with-comma, without-comma grammatical adverb, adverbial-noun, formal-noun, temporal-noun, some features have <pos>distinction</pos> 17 quoting-particle, copula, predicate-conjunctive-particle, of chunk-final/middle topic-marking-particle, sentence-final-particle 12 conjugation form of chunk-final conjugative word lexical lexicalized forms of grammatical features, with more than 9 occurrences in edr corpus 235 stem, base, mizen, renyou, rental, conditional, imperative, ta, tari, re, conjecture, volitional adverb eg , ippou-de, irai, adverbial-noun eg , tame, baai topic-marking-particle eg , ha, mo, quoting-particle to, predicate-conjunctive-particle eg , ga, kara, temporal-noun eg , ima, shunkan, formal-noun eg , koto, copula dearu, sentence-final-particle eg , ka, yo 31 the task definition considering the dependency <pos>preference</pos> of japanese subordinate clauses described in section 24, the following gives the definition of our task of deciding the dependency of japanese subordinate clauses.	suppose that a sentence has two subordinate clauses clause1 and clause2, where the head vp chunk of clausel precedes that of clause2.
for each piece of evidence, calculate the likelihood ratio of the conditional probability of a decision d  xl given the presence of that piece of evidence to the conditional probability of the rest of the decisions d -,xl: pdxl i ei lg2 pdxl ei then, a decision list is constructed with pieces of evidence sorted in descending order with <pos><pos>respect</pos></pos> to their likelihood ratios, s 2.	the final line of a decision list is defined as a default, where the likelihood ratio is calculated as the ratio of the largest marginal probability of the decision d  xl to the marginal proba<ref>syarowsky 1994</ref> discusses several techniques for avoiding the problems which arise when an observed count is 0.	among those techniques, we employ the simplest one, ie, adding a small constant c 01 < c < 025 to the numerator and denominator.	with this modification, more frequent evidence is preferred when there exist several evidences for each of which the conditional probability pdx  ei equals to 1.
a7 decision list dl are lists of weighted classification rules involving the evaluation of one single feature.	at classification time, the algorithm applies the rule with the highest weight that matches the test example <tref>yarowsky, 1994</tref>.	the provider is ixa and they also applied smoothing to generate more <pos>robust</pos> decision lists.	a7 in the vector space model method cosvsm, each example is treated as a binary-valued feature vector.
while the <pos>basic</pos> idea of our model is similar to trigger models, they handle co-occurrences of word pairs independently and do not use a representation of the whole context.	this omission is also done in applications such as word <pos><pos>sense</pos></pos> dismnbiguation yarowsky: 1994; fung et al , 1999.	our model is the most related to coccaro mad <ref>jurafsky 1998</ref>, in that a reduced vector space approach was taken and context is represented by the accumulation of word cooccurrence vectors.	their model was reported to decrease the test set perplexity by 12, compared to the bigram nmdel.
step 3a: train the supervised classification algorithm on the sense-a/sense-b seed sets.	the decision-list algorithm used here <tref>yarowsky, 1994</tref> identifies other collocations that <pos>reliably</pos> partition the seed training data, ranked by the <pos>purity</pos> of the distribution.	below is an abbreviated example of the decision list trained on the plant seed data.	9 initial decision list for plant abbreviated logl 810 758 739 720 627 470 439 430 410 352 348 345 collocation <pos><pos>sense</pos></pos> plant life  a manufacturing plant  b life within 4-2-10 words  a manufacturing in 4-2-10 words  b animal within -i-2-10 words  a equipment within -1-2-10 words , b employee within 4-2-10 words  b assembly plant  b plant closure  b plant species  a automate within 4-2-10 words :: b microscopic plant  a 9note that a given collocate such as life may appear multiple times in the list in different collocations1 relationships, including left-adjacent, right-adjacent, cooccurrence at other positions in a k-word window and various other syntactic associations.
our probabilistic decision lists can thus be thought of as a <pos>competitive</pos> way to probabilize tbls, with the <pos>advantage</pos> of preserving the list-structure and <pos>simplicity</pos> of tbl, and the possible disadvantage of losing the dependency on the current state.	<tref>yarowsky 1994</tref> suggests two improvements to the standard algorithm.	first, he suggests an optional, more complex smoothing algorithm than the one we applied.	his technique involves estimating both a probability based on the global probability distribution for a question, and a local probability, given that no questions higher in the list were <pos>true</pos>, and then interpolating between the two probabilities.
in the system presented here, the classifiers built for each ambiguous word are based on its lemma instead.	lemmatization allows for more <pos>compact</pos> and generalizable data by clustering all inflected forms of an ambiguous word together, an effect already commented on by <tref>yarowsky 1994</tref>.	the more inflection in a language, the more lemmatization <pos>will</pos> <pos><pos>help</pos></pos> to compress and generalize the data.	in the case of our wsd system this means that less classifiers have to be built therefore adding up the training material available to the algorithm for each ambiguous wordform.
alternatively, we chose for a model constructing classifiers based on lemmas therefore reducing the number of classifiers that need to be made.	as has already been noted by <tref>yarowsky 1994</tref>, using lemmas helps to produce more concise and generic evidence than inflected forms.	therefore building classifiers based on lemmas increases the data available to each classifier.	we make use of the <pos>advantage</pos> of clustering all instances of eg one verb in a single classifier instead of several classifiers one for each inflected form found in the data.
for instance, governing body and governing bodies are different collocations for the sake of this paper.	4 adaptation of decision lists to n-way ambiguities decision lists as defined in <ref>yarowsky, 1993</ref>; 1994 are <pos>simple</pos> means to solve ambiguity problems.	they have been <pos>successfully</pos> applied to accent <pos>restoration</pos>, word <pos><pos>sense</pos></pos> disambiguation 209 and homograph disambiguation <tref>yarowsky, 1994</tref>; 1995; 1996.	in order to build decision lists the training examples are processed to extract the features each feature corresponds to a <pos>kind</pos> of collocation, which are weighted with a log-likelihood measure.
4 adaptation of decision lists to n-way ambiguities decision lists as defined in <ref>yarowsky, 1993</ref>; 1994 are <pos>simple</pos> means to solve ambiguity problems.	they have been <pos>successfully</pos> applied to accent <pos>restoration</pos>, word <pos><pos>sense</pos></pos> disambiguation 209 and homograph disambiguation <tref>yarowsky, 1994</tref>; 1995; 1996.	in order to build decision lists the training examples are processed to extract the features each feature corresponds to a <pos>kind</pos> of collocation, which are weighted with a log-likelihood measure.	the list of all features ordered by log-likelihood <pos>values</pos> constitutes the decision list.
the context within which the ambiguous word occurs is typically represented by a set of linguistically <pos>motivated</pos> features from which a <pos>learning</pos> algorithm induces a representative model that performs the disambiguation.	a variety of classifiers have been employed for this task see mooney 1996 and ide and veronis 1998 for overviews, the most <pos>popular</pos> being decision lists <ref>yarowsky 1994, 1995</ref> and naive bayesian classifiers <ref>pedersen 2000</ref>; <ref>ng 1997</ref>; <ref>pedersen and bruce 1998</ref>; <ref>mooney 1996</ref>; <ref>cucerzan and yarowsky 2002</ref>.	we employed a naive bayesian classifier <ref>duda and hart 1973</ref> for our experiments, as it is a very <pos>convenient</pos> framework for incorporating prior knowledge and studying its influence on the classification task.	in section 51 we describe a <pos>basic</pos> naive bayesian classifier and show how it can be extended with <pos>informative</pos> priors.
at present we have chosen one algorithm which does not combine features decision lists and another which does combine features adaboost.	despite their <pos>simplicity</pos>, decision lists dlist for short as defined in <tref>yarowsky 1994</tref> have been shown to be very <pos>effective</pos> for wsd <ref>kilgarriff  palmer, 2000</ref>.	features are weighted with a log-likelihood measure, and arranged in an ordered list according to their weight.	in our case the probabilities have been estimated using the maximum likelihood estimate, smoothed adding a small constant 01 when probabilities are zero.
machine <pos>learning</pos> methods have become the most <pos>popular</pos> technique in a variety of classification problems of these sort, and have shown <pos>significant</pos> <pos>success</pos>.	a partial list consists of bayesian classifiers <ref>gale et al , 1993</ref>, decision lists <tref>yarowsky, 1994</tref>, bayesian hybrids <ref>golding, 1995</ref>, hmms <ref>charniak, 1993</ref>, inductive logic methods <ref>zelle and mooney, 1996</ref>, memorya3 this research is supported by nsf grants iis-9801638, iis0085836 and sbr-987345.	based methods <ref>zavrel et al , 1997</ref>, linear classifiers <ref>roth, 1998</ref>; <ref>roth, 1999</ref> and transformationbased <pos>learning</pos> <ref>brill, 1995</ref>.	in many of these classification problems a <pos>significant</pos> source of difficulty is the fact that the number of candidates is very <pos>large</pos>  all words in words selection problems, all possible tags in tagging problems etc since general purpose <pos>learning</pos> algorithms do not handle these multi-class classification problems <pos>well</pos> see below, most of the studies do not address the whole problem; rather, a small set of candidates typically two is first selected, and the classifier is trained to choose among these.
obviously, how to measure the <pos>confidence</pos> of features is a very <pos>important</pos> issue for the decision list.	we use the metric described in <tref>yarowsky, 1994</tref>; <ref>golding, 1995</ref>.	provided that 1  0ps f > for all i :  max    i i <pos>confidence</pos> f p s f 1 this <pos>value</pos> measures the extent to which the context is unambiguously correlated with one particular slot i s  24 slot-value merging and semantic reclassification the slot-value merger is to combine the slots assigned to the concepts in an input sentence.	another simultaneous task of the slot-value merger is to check the consistency among the identified slot-values.
in particular, they may involve performing speech <pos>recognition</pos> on speech data, parsing on text data, application of hand-coded rules to the results of parsing, or some combination of these.	statistics are then compiled to estimate the probability pa j f of each semantic atom a given each separate feature f, using the standard formula pa j f  naf  1nf  2 where nf is the number of occurrences in the training data of utterances with feature f, and n af is the number of occurrences of utterances with both feature f and semantic atom a the decoding process follows <tref>yarowsky, 1994</tref> in assuming complete dependence between the features.	note that this is in sharp contrast with the naive bayes classifier <ref>duda et al , 2000</ref>, which assumes complete <pos>independence</pos>.	of course, neither assumption can be <pos>true</pos> in practice; however, as argued in <ref>carter, 2000</ref>, there are <pos>good</pos> reasons for preferring the dependence alternative as the <pos>better</pos> option in a situation where there are many features extracted in ways that are likely to overlap.
there is typically no corpus data available at the start of a project, but considerable amounts at the end: the intention behind alterf is to <pos>allow</pos> us to shift smoothly from an initial version of the system which is entirely rule-based, to a final version which is largely data-driven.	alterf characterises semantic analysis as a task slightly extending the decision-list classification algorithm <tref>yarowsky, 1994</tref>; <ref>carter, 2000</ref>.	we start with a set of semantic atoms, each representing a primitive domain concept, and define a semantic representation to be a non-empty set of semantic atoms.	for example, in the procedure assistant domain we represent the utterances <pos>please</pos> speak up show me the sample syringe set an alarm for five minutes from now no i said go to the next step respectively as fincrease volumeg fshow, sample syringeg fset alarm, 5, minutesg fcorrection, next stepg where increase volume, show, sample syringe, set alarm, 5, minutes, correction and next step are semantic atoms.
left  context   ml2mii ll,ight named entity  context  m   mm<3  1 2 current position 4 supervised <pos><pos>learning</pos></pos> for japanese named entity <pos>recognition</pos> this section describes how to apply tile decision list learning method to chunking/tagging named entities.	41 decision list learning a decision list <ref>rivest, 1987</ref>; <tref>yarowsky, 1994</tref> is a sorted list of decision rules, each of which decides the wflue of a decision d given some evidence e each decision rule in a decision list is sorted in descending order with <pos><pos>respect</pos></pos> to some <pos><pos>preference</pos></pos> <pos>value</pos>, and rules with higher preference <pos>values</pos> are applied first when applying the decision list to some new test; data.	first, the random variable d representing a decision w, ries over several possible <pos>values</pos>, and the random wriable e representing some evidence varies over 1 and 0 where 1 denotes the presence of the corresponding piece of evidence, 0 its absence.	then, given some training data in which the <pos>correct</pos> <pos>value</pos> of the decision d is annotated to each instance, the conditional probabilities pd  x i e  1 of observing the decision d  x under the condition of the presence of the evidence e e  1 are calculated and the decision list is constructed by the tbllowing procedure.
in general, creating training data tbr supervised <pos>learning</pos> is somewhat <pos>easier</pos> than creating pattern matching rules by hand.	next, we apply yarowskys method tbr supervised decision list <pos>learning</pos> i <tref>yarowsky, 1994</tref> to 1vve choose tile decision list learning method as the 705 table 1: statistics of ne types of irex ne type organization person location artifact date time money percent total frequency  training 3676 197 3840 206 5463 292 747 40 3567 191 502 27 390 21 492 26 18677 test 361 239 338 224 413 274 48 32 260 172 54 35 15 10 21 14 1510 japanese named entity <pos>recognition</pos>, into which we incorporate several noun phrase chunking techniques sections 3 and 4 and experimentally evaluate their performance on the irex, workshops training and test data section 5.	as one of those noun phrase chunking techniques, we propose a method for incorporating richer contextual information as <pos>well</pos> as patterns of constituent morphemes within a named entity, compared with those considered in tire previous research <ref>sekine et al , 1998</ref>; <ref>borthwick, 1999</ref>, and show that the proposed method outperlbrms these approaches.	2 japanese named entity <pos><pos>recognition</pos></pos> 21 task of the irex workshop the task of named entity recognition of the irex workshop is to recognize eight named entity types in table 1 irex <ref>conmfittee, 1999</ref>.
this small number has almost no effect on more frequent words, but boosts the score of less common, yet potentially equally <pos>informative</pos>, words.	222 decision list the decision list classifier uses the log-likelihood of correspondence between each context feature and each <pos><pos>sense</pos></pos>, using additive smoothing <tref>yarowsky, 1994</tref>.	the decision list was created by ordering the correspondences from strongest to weakest.	instances that did not match any rule in the decision list were assigned the most frequent <pos><pos>sense</pos></pos>, as calculated from the training data.
it must be noted however that the ldoce homograph level is far more rough-grained than the cide guideword level, let alone the sub-sense level, and that wilks and stevensons approach on its own would, by its very nature, not transfer down to more fine-grained distinctions.	other research, such as yarowskys into accent <pos>restoration</pos> in <ref>spanish and french 1994</ref>, which reports accuracy levels of 9099, is again at a more rough-grained level, in this case that of <pos>distinguished</pos> unaccented and accented word forms.	while the <pos><pos>sense</pos></pos> tagging results are <pos>fairly</pos> <pos>encouraging</pos>, the part of speech tagging results arc at present relatively poor.	it thus secrns <pos>sensible</pos>, <pos>especially</pos> noting wilks and stevensons analysis mentioned <pos>above</pos>, to first run a sentence through a <pos>traditional</pos> part of speech tagger before trying to disambiguate the senses.
from this perspective, either accent identification can be extended to truecasing or truecasing can be extended to incorporate accent <pos>restoration</pos>.	<tref>yarowsky, 1994</tref> reports <pos>good</pos> results with statistical methods for spanish and french accent <pos>restoration</pos>.	truecasing is also a specialized method for spelling correction by relaxing the notion of casing to spelling variations.	there is a <pos>vast</pos> literature on spelling correction <ref>jones and martin, 1997</ref>; <ref>golding and roth, 1996</ref> using both linguistic and statistical approaches.
syncretism and related morphological ambiguities present a <neg>problem</neg> for many nl applications where lexical disambiguation is important; cases where the orthographic form is identical but the pronunciations of the various functions <neg>differ</neg> are particularly important for speech applications, such as text-to-speech, since appropriate word pronunciations must be computed from orthographic forms that underspecify the necessary information.	ideally one would like to build models that use contextual information to perform lexical disambiguation <ref>yarowsky 1992, 1994</ref>, but such models must be trained on specialized tagged corpora either hand-generated or semi-automatically generated and such training corpora are often not available, at <neg>least</neg> in the early phases of constructing a particular application.	<neg>lacking</neg> good contextual models, one is forced to <neg>fall</neg> back on estimates of the lexical prior probabilities for the various functions of a form.	following standard terminology, a lexical prior can be defined as follows: imagine that a given form is n-ways <neg>ambiguous</neg>; the lexical prior probability of sense i of this form is simply the probability of sense i independent of the context in which the particular instantiation of the form occurs.
a feature expression f of the named entity can be any possible subset of the full feature expression mlength, netag,pos , or the set indicating that the system outputs no named entity within the segment.	f         any subset of braceleftbig mlength, netag,pos bracerightbig braceleftbig class sys no outputs bracerightbig in the training and testing phases, within each segment segev j of event expression, a class is assigned to each system, where each class class i sys for the i-th system is represented as a list of the classes of the named entities output by the system: class i sys  braceleftbigg /,  , / no output i 1,,n 34 <pos><pos>learning</pos></pos> algorithm we apply a <pos>simple</pos> decision list learning method to the task of learning a classifier for combining outputs of named entity chunkers 4  a decision list <tref>yarowsky, 1994</tref> is a sorted list of decision rules, each of which decides the <pos>value</pos> of class given some features f of an event.	each decision rule in a decision list is sorted in descending order with <pos><pos>respect</pos></pos> to some <pos><pos>preference</pos></pos> <pos>value</pos>, and rules with higher preference <pos>values</pos> are applied first when applying the decision list to some new test data.	in this paper, we simply sort the decision list according to the conditional probability pclass i  f of the class i of the i-th systems output given a feature f 4 experimental evaluation we experimentally evaluate the performance of the proposed system combination method using the irex workshops training and test data.
32 nave bayes the second system used was a nave bayes classifier where the similarity between an instance, i, and a <pos><pos><pos>sense</pos></pos></pos> class, sj, is defined as: simi,sj  pi,sj  psjpisj we then choose the sense class, sj, which maximized the similarity function <pos>above</pos>, making standard <pos>independence</pos> assumptions.	33 decision list the final system was a decision list classifier that found the log-likelihoods of the correspondence beassociation for computational linguistics for the semantic analysis of text, barcelona, <ref>spain, july 2004</ref> senseval-3: third international workshop on the evaluation of systems tween features and senses, using plus-one smoothing <tref>yarowsky, 1994</tref>.	the features were ordered from most to least indicative to form the decision list.	a separate decision list was constructed for each set of lexical samples in the training data.
the ordering of rules employed in a decision list in order to <pos>simplify</pos> the representation and perform conflict resolution apparently gives it an <pos>advantage</pos> over other symbolic methods on this task.	in addition to the results reported by <tref>yarowsky 1994</tref> and <ref>mooney and califf 1995</ref>, it provides evidence for the utility of this representation for natural-language problems.	with <pos><pos>respect</pos></pos> to training time, the symbolic methods are significantly slower since they are searching for a <pos>simple</pos> declarative representation of the concept.	empirically, the time complexity for most methods are growing somewhat worse than linearly in the number of training examples.
finally, decision lists <ref>rivest, 1987</ref> are ordered lists of conjunctive rules, where rules are tested in order and the first one that matches an instance is used to classify it.	a number of <pos>effective</pos> concept-learning systems have employed decision lists clark 84 <ref>niblett, 1989</ref>; <ref>quinlan, 1993</ref>; <ref>mooney  califf, 1995</ref> and they have already been <pos>successfully</pos> applied to lexical disambiguation <tref>yarowsky, 1994</tref>.	all of the logic-based methods are variations of the foil algorithm for induction of first-order function-free horn clauses <ref>quinlan, 1990</ref>, appropriately <pos>simplified</pos> for the propositional case.	they are called pfoil-dnf, pfoll-cnf, and pfoil-dlist.
32 the syntagmatic kernel syntagmatic aspects are probably the most important evidence for recognizing lexical entailment.	in general, the strategy adopted to model syntagmatic relations in wsd is to provide bigrams and trigrams ofcollocatedwordsasfeaturestodescribelocalcontexts <tref>yarowsky, 1994</tref>.	the main drawback of this approach is that non contiguous or shifted collocations cannot be identified, <neg>decreasing</neg> the generalization power of the learning algorithm.	for example, suppose that the word job has to be disambiguated into the sentence permanent academic job in, and that the occurrence we offer permanent positions is provided for training.
in this study we used ill the decision-list method the same 152 types of patterns that were used in/;lie maximuln-entropy method.	to determine the priority order of the rules, we referred to yarowskys method <tref>yarowsky, 1994</tref> and nishiokwamas method <ref>nishiokaymna et al , 1998</ref> and used the probability and frequency of each rule as measures of this priority order.	when nnlltiple rifles had the same probability, the rules were arranged in order of their frequency.	suppose, for example, that pattern a noun: <pos>normal</pos> noun; particle: case-particle: none: wo; verb: normal form: 217; symhol: punctuatioif occurs 13 times in a learlfing set and that tell of the occurrences include the inserted partition inal:k suppose also thai; pattern b noun; particle; verb; symbol occurs 12a times in a <pos>learning</pos> set and that 90 of the occurrences include the mark.
finally, i present a simpler and more efcient approach to training dependency parsers by applying a boosting-like procedure to standard training methods.	over the past decade, there has been <pos>tremendous</pos> <pos>progress</pos> on <pos>learning</pos> parsing models from treebank data <tref>magerman, 1995</tref>; <ref>collins, 1999</ref>; <ref>charniak, 1997</ref>; <ref>ratnaparkhi, 1999</ref>; <ref>charniak, 2000</ref>; <ref>wang et al , 2005</ref>; <ref>mcdonald et al , 2005</ref>.	most of the early work in this area was based on postulating generative probability models of language that included parse structures <tref>magerman, 1995</tref>; <ref>collins, 1997</ref>; <ref>charniak, 1997</ref>.	<pos>learning</pos> in this context consisted of estimating the parameters of the model with <pos>simple</pos> likelihood based techniques, but incorporating various smoothing and back-off estimation tricks to cope with the sparse data problems <ref>collins, 1997</ref>; <ref>bikel, 2004</ref>.
over the past decade, there has been <pos>tremendous</pos> <pos>progress</pos> on <pos>learning</pos> parsing models from treebank data <tref>magerman, 1995</tref>; <ref>collins, 1999</ref>; <ref>charniak, 1997</ref>; <ref>ratnaparkhi, 1999</ref>; <ref>charniak, 2000</ref>; <ref>wang et al , 2005</ref>; <ref>mcdonald et al , 2005</ref>.	most of the early work in this area was based on postulating generative probability models of language that included parse structures <tref>magerman, 1995</tref>; <ref>collins, 1997</ref>; <ref>charniak, 1997</ref>.	<pos>learning</pos> in this context consisted of estimating the parameters of the model with <pos>simple</pos> likelihood based techniques, but incorporating various smoothing and back-off estimation tricks to cope with the sparse data problems <ref>collins, 1997</ref>; <ref>bikel, 2004</ref>.	subsequent research began to focus more on conditional models of parse structure given the input sentence, which allowed discriminative training techniques such as maximum conditional likelihood ie maximum entropy  to be applied <ref>ratnaparkhi, 1999</ref>; <ref>charniak, 2000</ref>.
a recent trend in <pos>natural</pos> language processing has been toward a greater emphasis on statistical approaches, beginning with the <pos>success</pos> of statistical part-of-speech tagging programs <ref>church 1988</ref>, and continuing with other work using statistical part-of-speech tagging programs, such as bbn plum <ref>weischedel et al 1993</ref> and nyu proteus <ref>grishman and sterling 1993</ref>.	more recently, statistical methods have been applied to domain-specific semantic parsing <ref>miller et al 1994</ref>, and to the more difficult problem of wide-coverage syntactic parsing <tref>magerman 1995</tref>.	nevertheless, most <pos>natural</pos> language systems remain primarily rule based, and <pos>even</pos> systems that do use statistical techniques, such as att chronus <ref>levin and pieraccini 1995</ref>, continue to require a <pos>significant</pos> rule based component.	development of a complete end-to-end statistical <pos>understanding</pos> system has been the focus of several ongoing research efforts, including <ref>miller et al 1995</ref> and <ref>koppelman et al 1995</ref>.
in empirical approaches to parsing, lexical/semantic collocation extracted from corpus has been proved to be quite <pos>useful</pos> for ranking parses in syntactic analysis.	for example, <tref>magerman 1995</tref>, <ref>collins 1996</ref>, and <ref>charniak 1997</ref> proposed statistical parsing models which incorporated lexical/semantic information.	in their models, syntactic and lexical/semantic features are dependent on each other and are combined together.	this paper also proposes a method of utilizing lexical/semantic features for the purpose of applying them to ranking parses in syntactic analysis.
we will discuss the maximum accuracy of 8433.	compared to recent stochastic english parsers that yield 86 to 87 accuracy <ref>collins, 1996</ref>; <tref>magerman, 1995</tref>, 8433 seems <neg>unsatisfactory</neg> at the first glance.	the main reason behind this <neg>lies</neg> in the difference between the two corpora used: penn treebank <ref>marcus et al , 1993</ref> and edr corpus edr, 1995.	penn treebank<ref>marcus et al , 1993</ref> was also used to induce part-of-speech pos taggers because the corpus contains very precise and detailed pos markers as well as bracket, annotations.
a broader range of information, in particular lexical information, was found to be <pos>essential</pos> in disambiguating the syntactic structures of real-world sentences.	spatter <tref>magerman, 1995</tref> augmented the <pos>pure</pos> pcfg by introducing a number of lexical attributes.	the parser controlled applications of each rule by using the lexical constraints induced by decision tree algorithm <ref>quinlan, 1993</ref>.	the spatter parser attained 87 accuracy and first made stochastic parsers a <pos>practical</pos> choice.
head-lexicalized stochastic grammars have recently become increasingly popular see <ref>collins 1997, 1999</ref>; <ref>charniak 1997, 2000</ref>.	these grammars are based on magermans headpercolation <neg>scheme</neg> to determine the headword of each nonterminal <tref>magerman 1995</tref>.	<neg>unfortunately</neg> this means that head-lexicalized stochastic grammars are not able to capture dependency relations between words that according to magermans head-percolation <neg>scheme</neg> are nonheadwords -eg between more and than in the wsj construction carry more people than cargo where neither more nor than are headwords of the np constituent more people than cargo.	a frontier-lexicalized dop model, on the other hand, captures these dependencies since it includes subtrees in which more and than are the only frontier words.
in the first pass, it tags words as either head words or non-head words.	training data for this pass is obtained using a head percolation table <tref>magerman 1995</tref> on bracketed penn treebank sentences.	after training, head tagging is performed according to equation 1, where 15 is the estimated probability and hi is a characteristic function which is <pos>true</pos> iff word i is a head word.	n h  argmaxh hwilhihilhi-1hi-2 i1 1 the second pass then takes the words with this head information and supertags them according to equation 2, where thio is the supertag of the epart of speech tagging models have not used heads in this manner to <pos>achieve</pos> variable length contexts.
there have been two main <pos>robust</pos> parsing paradigms: finite state grammar-based approaches such as <ref>abney 1990</ref>, <ref>grishman 1995</ref>, and hobbs et al.	1997 and statistical parsing such as <ref>charniak 1996</ref>, <tref>magerman 1995</tref>, and <ref>collins 1996</ref>.	<ref>srinivas 1997a</ref> has presented a different approach called supertagging that integrates linguistically <pos>motivated</pos> lexical descriptions with the robustness of statistical techniques.	the idea underlying the approach is that the computation of linguistic structure can be localized if lexical items are associated with <pos>rich</pos> descriptions supertags that impose complex constraints in a local context.
second, one might propagate lexical information <pos>upward</pos> through the productions.	examples of formalisms using this approach include the work of <tref>magerman 1995</tref>, <ref>charniak 1997</ref>, <ref>collins 1997</ref>, and <ref>goodman 1997</ref>.	a more linguistically <pos>motivated</pos> approach is to expand the domain of productions downward to incorporate more tree structures.	the lexicalized tree-adjoining grammar ltag formalism <ref>schabes et al , 1988</ref>, <ref>schabes, 1990</ref>, although not context-free, is the most well-known instance in this category.
standard symbolic machine <pos>learning</pos> techniques have been <pos>successfully</pos> applied to a number of tasks in <pos>natural</pos> language processing nlp.	examples include the use of decision trees for syntactic analysis <tref>magerman, 1995</tref>, coreference <ref>aone and bennett, 1995</ref>; <ref>mccarthy and lehnert, 1995</ref>, and cue phrase identification <ref>litman, 1994</ref>; the use of inductive logic programming for <pos>learning</pos> semantic grammars and building prolog parsers 113 <ref>zelle and mooney, 1994</ref>; <ref>zelle and mooney, 1993</ref>; the use of conceptual clustering algorithms for relative pronoun resolution <ref>cardie, 1992a</ref>; <ref>cardie 1992b</ref>, and the use of case-based learning techniques for lexical tagging tasks <ref>cardie, 1993a</ref>; daelemans et al , submitted.	in theory, both statistical and machine <pos>learning</pos> techniques can significantly reduce the knowledge-engineering effort for building large-scale nlp systems: they offer an automatic means for acquiring <pos>robust</pos> heuristics for a host of lexical and structural disambiguation tasks.	it is well-known in the machine <pos><pos>learning</pos></pos> community, however, that the <pos>success</pos> of a learning algorithm depends critically on the representation used to describe the training and test instances <ref>almuallim and dietterich, 1991</ref>, langley and <pos>sage</pos>, in <pos>press</pos>.
1994, and <tref>magerman 1995</tref>.	a strength of these models is <pos>undoubtedly</pos> the <pos>powerful</pos> estimation techniques that they use: maximum-entropy modeling in <ref>ratnaparkhi 1997</ref> or decision trees in <ref>jelinek et al 1994</ref> and <tref>magerman 1995</tref>.	a weakness, we <pos>will</pos> argue in this section, is the method of associating parameters with transitions taken by bottom-up, shift-reduce-style parsers.	we give examples in which this method leads to the parameters unnecessarily fragmenting the training data in some cases or ignoring <pos>important</pos> context in other cases.
3 we find lexical heads in penn treebank data using the rules described in appendix a of <ref>collins 1999</ref>.	the rules are a modified version of a head table provided by david magerman and used in the parser described in <tref>magerman 1995</tref>.	593 collins head-driven statistical models for nl parsing internal rules lexical rules <pos>top</pos>  s jj  last s  np np vp nn  week np  jj nn nnp  ibm np  nnp vbd  bought vp  vbd np nnp  lotus np  nnp figure 1 a nonlexicalized parse tree and a list of the rules it contains.	internal rules: top  sbought,vbd sbought,vbd  npweek,nn npibm,nnp vpbought,vbd npweek,nn  jjlast,jj nnweek,nn npibm,nnp  nnpibm,nnp vpbought,vbd  vbdbought,vbd nplotus,nnp nplotus,nnp  nnplotus,nnp lexical rules: jjlast,jj  last nnweek,nn  week nnpibm,nnp  ibm vbdbought,vbd  bought nnplotus,nn  lotus figure 2 a lexicalized parse tree and a list of the rules it contains.
<ref>charniak 2000</ref> describes a series of enhancements to the earlier model of <ref>charniak 1997</ref>.	the <pos>precision</pos> and recall of the traces found by model 3 were 938 and 901, respectively out of 437 cases in section 23 of the treebank, where three criteria must be met for a trace to be <pos><pos><pos>correct</pos></pos></pos>: 1 it must be an argument to the correct headword; 2 it must be in the correct position in relation to that headword preceding or following; 15 <tref>magerman 1995</tref> collapses advp and prt into the same label; for comparison, we also removed this <pos>distinction</pos> when calculating scores.	608 computational linguistics volume 29, number 4 table 2 results on section 23 of the wsj treebank.	lr/lp  labeled recall/precision.
another important differencethe ability of models 1, 2, and 3 to generalize to produce context-free rules not seen in training datawas described in section 74 section 82 showed that the parsing models of <ref>ratnaparkhi 1997</ref>, jelinek et al.	1994, and <tref>magerman 1995</tref> can <neg>suffer</neg> from very similar <neg><neg>problem</neg>s</neg> to the label <neg><neg><neg>bias</neg></neg></neg> or observation bias problem observed in tagging models, as described in <ref>lafferty, mccallum, and pereira 2001</ref> and <ref>klein and manning 2002</ref>.	635 collins head-driven statistical models for nl parsing acknowledgments my phd thesis is the basis of the work in this article; i would like to thank mitch marcus for being an excellent phd thesis adviser, and for contributing in many ways to this research.	i would like to thank the members of my thesis committeearavind joshi, mark liberman, fernando pereira, and mark steedmanfor the remarkable breadth and depth of their feedback.
<ref>charniaks 1997</ref> models will most likely perform quite differently with binarybranching trees for example, his current models will learn that rules such as vp  vsgppare very rare, but with binary-branching structures, this context sensitivity will be <neg>lost</neg>.	the models of <tref>magerman 1995</tref> and <ref>ratnaparkhi 1997</ref> use contextual predicates that would most likely <neg>need</neg> to be modified given a different annotation style.	<ref>goodmans 1997</ref> models are the exception, as he already specifies that the treebank should be transformed into his chosen representation, binary-branching trees.	731 representation affects structural, not lexical, preferences.
22 statistical parsers pioneered by the ibm <pos>natural</pos> language group <ref>fujisaki et al 1989</ref> and later pursued by, for example, <ref>schabes, roth, and osborne 1993</ref>, jelinek et al.	1994, <tref>magerman 1995</tref>, <ref>collins 1996</ref>, and <ref>charniak 1997</ref>, this approach decouples the issue of wellformedness of an input string from the problem of assigning a structure to it.	these systems attempt to assign some structure to every input string.	the rules to assign a structure to an input are extracted automatically from hand-annotated parses of <pos>large</pos> corpora, which are then subjected to smoothing to obtain <pos>reasonable</pos> coverage of the language.
during the last few years <pos>large</pos> treebanks have become available to many researchers, which has resulted in researches applying a range of new techniques for parsing systems.	most of the methods that are being suggested include some <pos>kind</pos> of machine <pos>learning</pos>, such as history based grammars and decision tree models <ref>black et al , 1993</ref>; <tref>magerman, 1995</tref>, training or inducing statistical grammars <ref>black, garside and leech, 1993</ref>; <ref>pereira and schabes, 1992</ref>; <ref>schabes et al , 1993</ref>, or other techniques <ref>bod, 1993</ref>.	consequently, syntactical analysis has become an area with a <pos>wide</pos> variety of a algorithms and methods for <pos>learning</pos> and parsing, and b type of information used for learning and parsing sometimes referred to as feature set.	these methods only could become <pos>popular</pos> through evaluation methods for parsing systems, such as bracket accuracy, bracket recall, sentence accuracy and viterbi score.
although we report <pos>promising</pos> results, parse selection that is <pos>sufficiently</pos> <pos>accurate</pos> for many <pos>practical</pos> applications <pos>will</pos> require a more lexicalised system.	magermans 1995 parser is an extension of the history-based parsing approach developed at ibm <ref>black et al , 1993</ref> in which rules are conditioned on lexical and other essentially arbitrary information available in the parse history.	in future work, we intend to explore a more restricted and semantically-driven version of this approach in which, firstly, probabilities are associated with different subcategorisation possibilities, and secondly, alternative predicateargument structures derived from the grammar are ranked probabilistically.	however, the massively increased coverage obtained here by relaxing subcategorisation constraints underlines the need to acquire <pos>accurate</pos> and complete subcategorisation frames in a corpus-driven fashion, before such constraints can be exploited robustly and effectively with <pos>free</pos> text.
in our experiments, the window starts at the sentence prior to that containing the token and extends <pos>back</pos> w the window size sentences.	the choice to use sentences as the unit of distance is <pos>motivated</pos> by our intention to incorporate triggers of this form into a probabilistie treebank-based parser and tagger, such as <ref>black et al , 1998</ref>; <ref>black et al , 1997</ref>; <ref>brill, 1994</ref>; <ref>collins, 1996</ref>; <ref>jelinek et al , 1994</ref>; <tref>magerman, 1995</tref>; <ref>ratnaparkhi, 1997</ref>.	all such parsers and taggers of which we are aware use only intrasentential information in predicting parses or tags, and we <pos><pos>wish</pos></pos> to remove this information, as far as possible, from our results 7 the window was not allowed to cross a document boundary.	the perplexity of the task before taking the trigger-pair information into account for tags was 2240 and for rules was 570.
we describe several simple, linguistically motivated annotations which do much to <neg>close</neg> the gap between a vanilla pcfg and state-of-the-art lexicalized models.	specifically, we construct an unlexicalized pcfg which outperforms the lexicalized pcfgs of <tref>magerman 1995</tref> and <ref>collins 1996</ref> though not more recent models, such as <ref>charniak 1997</ref> or <ref>collins 1999</ref>.	one benefit of this result is a much-strengthened lower bound on the capacity of an unlexicalized pcfg.	to the extent that no such strong baseline has been provided, the community has tended to greatly overestimate the beneficial effect of lexicalization in probabilistic parsing, rather than looking critically at where lexicalized probabilities are both needed to make the right decision and available in the training data.
this approach was congruent with the <pos>great</pos> <pos>success</pos> of word n-gram models in speech <pos>recognition</pos>, and drew strength from a broader <pos><pos>interest</pos></pos> in lexicalized grammars, as <pos>well</pos> as demonstrations that lexical dependencies were a key tool for resolving ambiguities such as pp attachments <ref>ford et al , 1982</ref>; <ref>hindle and rooth, 1993</ref>.	in the following decade, <pos>great</pos> <pos>success</pos> in terms of parse disambiguation and <pos>even</pos> language modeling was achieved by various lexicalized pcfg models <tref>magerman, 1995</tref>; <ref>charniak, 1997</ref>; <ref>collins, 1999</ref>; <ref>charniak, 2000</ref>; <ref>charniak, 2001</ref>.	however, several results have brought into question how <pos>large</pos> a role lexicalization plays in such parsers.	<ref>johnson 1998</ref> showed that the performance of an unlexicalized pcfg over the penn treebank could be <pos>improved</pos> enormously simply by annotating each node by its parent category.
however, perhaps <pos>even</pos> more <pos>significant</pos> has been the lexicalization of the grammar formalisms being probabilistically modeled: crucially, all the recent, <pos>successful</pos> statistical parsers have in some way made use of bilexical dependencies.	this includes both the parsers that attach probabilities to parser moves <tref>magerman, 1995</tref>; <ref>ratnaparkhi, 1997</ref>, but also those of the lexicalized pcfg variety <ref>collins, 1997</ref>; <ref>charniak, 1997</ref>.	155 <pos>even</pos> more crucially, the bilexical dependencies involve head-modifier relations hereafter referred to simply as head relations.	the intuition behind the lexicalization of a grammar formalism is to capture lexical items idiosyncratic parsing <pos>preferences</pos>.
in those research, extracted lexical/semantic collocation is <pos>especially</pos> <pos>useful</pos> in terms of ranking parses in syntactic analysis as <pos>well</pos> as automatic construction of lexicon for nlp.	for example, in the context of syntactic disambiguation, <ref>black 1993</ref> and <tref>magerman 1995</tref> proposed statistical parsing models based-on decisiontree <pos>learning</pos> techniques, which incorporated not only syntactic but also lexical/semantic information in the decision-trees.	as lexical/semantic information, <ref>black 1993</ref> used about 50 semantic categories, while <tref>magerman 1995</tref> used lexicai forms of words.	<ref>collins 1996</ref> proposed a statistical parser which is based on probabilities of dependencies between head-words in the parse tree.
for example, in the context of syntactic disambiguation, <ref>black 1993</ref> and <tref>magerman 1995</tref> proposed statistical parsing models based-on decisiontree <pos>learning</pos> techniques, which incorporated not only syntactic but also lexical/semantic information in the decision-trees.	as lexical/semantic information, <ref>black 1993</ref> used about 50 semantic categories, while <tref>magerman 1995</tref> used lexicai forms of words.	<ref>collins 1996</ref> proposed a statistical parser which is based on probabilities of dependencies between head-words in the parse tree.	in those works, lexical/semantic collocation are used for ranking parses in syntactic analysis.
other researchers have proposed automatically selecting the conditioning information for various states of the model, thus potentially increasing greatly the space of possible features and selectively choosing the <pos>best</pos> predictors for each situation.	decision trees have been applied for feature selection for statistical parsing models by <tref>magerman 1995</tref> and haruno et al.	1998.	another example of automatic feature selection for parsing is in the context of a deterministic parsing model that chooses parse actions based on automatically induced decision structures over a very <pos>rich</pos> feature set <ref>hermjakob and mooney, 1997</ref>.
we grew the trees fully and we calculated final expansion probabilities at the leaves by linear interpolation with estimates one level <pos>above</pos>.	this is a similar, but more limited, strategy to the one used by <tref>magerman 1995</tref>.	the features over derivation trees which we made available to the learner are shown in table 1.	the node direction features indicate whether a node is a left child, a <pos><pos>right</pos></pos> child, or a single child.
figure 1a shows a parse tree in the penn treebank style for the english translation in ex 1.	given a parse tree, we use a head percolation table <tref>magerman, 1995</tref> to create the corresponding dependency structure.	figure 2a shows the dependency structure derived from the parse tree in figure 1a.	32 word alignment because most of the 700 languages in odin are low-density languages with no on-line bilingual dictionariesorlargeparallelcorpora, aligning the source sentence and its english translation directly would not work <pos>well</pos>.
inparticular,weleveragethe increasing availability of off-the-shelf parsers such as <ref>charniak, 2001</ref>; <ref>charniak, 2005</ref> to automatically or semi-automatically assign syntactic analyses to a set of suggested output sentences.	we then draw on lexicalization techniques for statistical language models <tref>magerman, 1995</tref>; <ref>collins, 1999</ref>; <ref>chiang, 2000</ref>; <ref>chiang, 2003</ref> to induce a probabilistic, lexicalized tree-adjoining grammar that supports the derivation of all the suggested output sentences, and many others besides.	the final step is to use the training examples to learn an <pos>effective</pos> search policy so that our run-time generation component can find <pos>good</pos> output sentences in a <pos>reasonable</pos> time frame.	in particular, we use variants of existing search optimization <ref>daum and marcu, 2005</ref> and ranking algorithms <ref>collins and koo, 2005</ref> to train our run-time component to find <pos>good</pos> outputs within a specified time window; see also <ref>stent et al, 2004</ref>; <ref>walker et al, 2001</ref>.
in the first stage, a collection of rules is used to automatically decorate the training syntax with a number of features.	these include deciding the lexical anchors for each non-terminal constituent and assigning complement/adjunct status for nonterminals which are not on their parents lexicalization path; see <tref>magerman, 1995</tref>; <ref>chiang, 2003</ref>; <ref>collins, 1999</ref>.	in addition, we deterministically add features to <pos>improve</pos> several grammatical aspects, including 1 enforcing verb inflectional <pos>agreement</pos> in derived trees, 2 enforcing consistency in the finiteness of vp and s complements, and 3 restricting subject/direct object/indirect object complements to play the same grammatical role in derived trees.	in the second stage, the complements and adjuncts in the decorated trees are incrementally re80 syntax: cat: sa fin:other,  cat: s cat: np,  apr: vbp, apn: other pos: prp we fin:<pos><pos>yes</pos></pos>,  cat: vp apn: other,  pos: vbp do pos: rb nt fin: yes,  cat: vp, gra: obj1 fin: yes,  cat: vp, gra: obj1 pos: vbp have cat: np,  gra: obj1 operations: initial tree comp semantics: speech-actaction  assert speech-actcontentpolarity  negative speech-actcontentattribute  resourceattribute syntax: cat: np,  apr: vbp, gra: obj1,  apn: other pos: jj medical pos: nns supplies cat: advp,  gra: adj pos: rb here cat: np,  apr: vbz, gra: adj,  apn: 3ps pos: nn captain operations: comp left/right adjunction left/right adjunction semantics: speech-actcontentvalue  medical-supplies speech-actcontentobject-id  market addressee  captain-kirk dialogue-actaddressee  captain-kirk speech-actaddressee  captain-kirk figure 2: the linguistic resources inferred from the training example in figure 1.
whats more, on <pos>top</pos> of the <pos>large</pos> undertaking of designing and implementing a statistical parsing model, the use of heuristics has required a further e ort, forcing the researcher to bring both linguistic intuition and, more often, engineering <pos>savvy</pos> to bear whenever <pos>moving</pos> to a new treebank.	for example, in the rule sets used by the parsers described in <tref>magerman, 1995</tref>; <ref>ratnaparkhi, 1997</ref>; <ref>collins, 1999</ref>, the sets of rules for finding the heads of adjp, advp, nac, pp and whpp include rules for picking either the rightmost or leftmost fw foreign word.	the apparently haphazard placement of these rules that pick out fw and the rarity of fw nodes in the data strongly <pos>suggest</pos> these rules are the result of engineering e ort.	furthermore, it is not at all apparent that tree-transforming heuristics that are <pos><pos>useful</pos></pos> for one parsing model <pos>will</pos> be useful for another.
once enriched, the data can be used as a bootstrap for tools such as taggers.	311 <pos>enrich</pos>ing igt in a previous study <ref>xia and lewis, 2007</ref>, we proposed a three-step process to enrich igt data: 1 parse the english translation with an english parser and convert english phrase structures ps into dependency structures ds with a head percolation table <tref>magerman, 1995</tref>, 2 align the target line and the english translation using the gloss line, and 3 project the syntactic structures both ps and ds from english onto the target line.	for instance, given the igt example in ex 1, the <pos>enrichment</pos> algorithm <pos>will</pos> produce the word alignment in figure 1 and the syntactic structures in figure 2.	the  teacher  gave  a  book  to    the    boy   yesterday rhoddodd  yr   athro     lyfr     ir     bachgen  ddoe  gloss line:  translation: target line: gave-3sg  the  teacher book  to-the  boy   yesterday figure 1: aligning the target line and the english translation with the <pos><pos>help</pos></pos> of the gloss line 533 gave a projecting ds athro bachgen lyfr yr ddoeir  rhoddodd s np1 vp nn teacher vbd   gave np2 dt a np4pp nn the in np3 yesterday nn dt book nn boy dt to s np nn vbd np nppp nn indt nn nndt   rhoddodd   gave yrthe    athro teacher lyfr book     ir to-the bachogen boy ddoe yesterday teacher a boy the book the yesterdayto the b projecting ps figure 2: projecting syntactic structure from english to the target language we evaluated the algorithm on a small set of 538 igt instances for several languages.
the extraction procedure determines the structure of each elementary tree by localizing dependencies through the use of heuristics.	salient heuristics include the use of a head percolation table <tref>magerman, 1995</tref>, and another table that distinguishes between complements and adjunct nodes in the tree.	for our current work, we use the head percolation table to determine heads of phrases.	also, we <pos>treat</pos> a propbank argument arg0 : : : arg9 as a <pos>complement</pos> and a propbank adjunct argms as an adjunct when such annotation is available1 otherwise, we basically follow the approach of <ref>chen, 2001</ref>2 besides introducing one <pos>kind</pos> of tag extraction 1the version of the propbank we are using is not fully annotated with semantic role information, although the most common predicates are.
unlike maxent, cannot be used as a probabilistic component in a larger model.	maxent can provide a probability for each tagging decision, which can be used in the probability calculation of any structure that is predicted over the pos tags, such as noun phrases, or entire parse trees, as in <ref>jelinek et al , 1994</ref>, <tref>magerman, 1995</tref>.	thus maxent has at least one <pos>advantage</pos> over each of the reviewed pos tagging techniques.	it is <pos>better</pos> <pos>able</pos> to use diverse information than markov models, requires less supporting techniques than sdt, and unlike tbl, can be used in a probabilistic framework.
since most <pos>realistic</pos> <pos>natural</pos> language applications must process words that were never seen before in training data, all experiments in this paper are conducted on test data that include unknown words.	several recent papers<ref>brill, 1994</ref>, <tref>magerman, 1995</tref> have reported 965 tagging accuracy on the wall st journal corpus.	the experiments in this paper test the hypothesis that <pos>better</pos> use of context <pos>will</pos> <pos>improve</pos> the accuracy.	a maximum entropy model is well-suited for such experiments since it cornbines diverse forms of contextual information in a <pos>principled</pos> manner, and does not impose any distributional assumptions on the training data.
in contrast, the maxent model combines diverse and non-local information sources without making any <pos>independence</pos> assumptions.	140 a pos tagger is one component in the sdt based statisticm parsing system described in <ref>jelinek et al , 1994</ref>, <tref>magerman, 1995</tref>.	the total word accuracy on wall st journal data, 965<tref>magerman, 1995</tref>, is similar to that presented in this paper.	however, the aforementioned sdt techniques require word classes<ref>brown et al , 1992</ref> to <pos><pos>help</pos></pos> prevent data fragmentation, and a <pos>sophisticated</pos> smoothing algorithm to mitigate the effects of any fragmentation that occurs.
6 conclusion it is <pos>worth</pos> noting that while we have presented the use of edge-based best-first chart parsing in the service of a rather <pos>pure</pos> form of pcfg parsing, there is no particular <pos>reason</pos> to assume that the technique is so limited in its domain of applicability.	one can imagine the same techniques coupled with more <pos>informative</pos> probability distributions, such as lexicalized pcfgs <ref>charniak, 1997</ref>, or <pos>even</pos> grammars not based upon literal rules, but probability distributions that describe how rules are built up from smaller components <tref>magerman, 1995</tref>; <ref>collins, 1997</ref>.	<pos>clearly</pos> further research is warranted.	be this as it may, the take-home lesson from this paper is <pos>simple</pos>: combining an edge-based agenda with the figure of <pos>merit</pos> from cc  is <pos>easy</pos> to do by simply binarizing the grammar  provides a factor of 20 or so reduction in the number of edges required to find a first parse, and  improves parsing <pos>precision</pos> and recall over <pos>exhaustive</pos> parsing.
uk black,eubank,kashiokaatritlcojp gleechocentllancsacuk 1 introduction a treebank is a body of <pos>natural</pos> language text which has been grammatically annotated by hand, in terms of some previously-established scheme of grammatical analysis.	treebanks have been used within the field of <pos>natural</pos> language processing as a source of training data for statistical part og speech taggers <ref>black et al , 1992</ref>; <ref>brill, 1994</ref>; <ref>merialdo, 1994</ref>; <ref>weischedel et al , 1993</ref> and for statistical parsers <ref>black et al , 1993</ref>; <ref>brill, 1993</ref>; aelinek et al , 1994; <tref>magerman, 1995</tref>; <ref>magerman and marcus, 1991</ref>.	in this article, we present the atr/lancaster 7reebauk of american english, a new resource tbr natural-language-, processing research, which has been <pos>prepared</pos> by lancaster university uks unit for computer research on the english language, according to specifications provided by atr japans statistical parsing group.	first we provide a static description, with a a discussion of the mode of selection and initial processing of text for inclusion in the treebank, and b an explanation of the scheme of grammatical annotation we then apply to the text.
moreover, the results of a less-than-optimal version of dop on the wall street journal corpus <pos>suggest</pos> that the approach can be succesfully extended to larger domains.	as future research, we <pos>will</pos> apply the full dop model on wsj word strings in order to compare our results with the <pos>best</pos> known parsers on this domain <tref>magerman, 1995</tref>; <ref>collins, 1996</ref>.	acknowledgements i am <pos>grateful</pos> to remko scha for many <pos>useful</pos> comments and additions.	i also <pos>thank</pos> three anonymous reviewers for their comments.
stochastic parsing systems either use a closed lexicon, or use a two step approach where first the words are tagged 133 by a stochastic tagger, after which the p-o-s tags with or without the words are parsed by a stochastic parser.	the latter approach has become increasingly <pos>popular</pos> eg <ref>schabes et al , 1993</ref>; <ref>weischedel et al , 1993</ref>; <ref>briscoe, 1994</ref>; <tref>magerman, 1995</tref>; <ref>collins, 1996</ref>.	notice, however, that the tagger used in this two step approach often uses good-turing or a similar smoothing method to adjust the observed frequencies of n-grams.	so why not apply good-turing directly to the structural units of a stochastic grammar.
we calculate the <pos>precision</pos>, recall, and fscore; however for brevitys sake we only report the f-score for most experiments in this section.	in addition to antecedent recovery, we also report parsing accuracy, using the bracketing f-score, the combined measure of parseval-style labeled bracketing <pos>precision</pos> and recall <tref>magerman, 1995</tref>.	44 results the results of the experiments are summarized in table 3.	unlex and lex refer to the unlexicalized and lexicalized models, respectively.
figure 9 shows that the <pos><pos>perfect</pos></pos> scheme would <pos>achieve</pos> roughly 93 <pos><pos>precision</pos></pos> and recall, which is a dramatic increase over the <pos>top</pos> 1 accuracy of 87 precision and 86 recall.	figure 10 shows that the exact match, which counts the percentage of times 2results for spatter on section 23 are reported in <ref>collins, 1996</ref></ref> parser <pos>precision</pos> maximum entropy  868 maximum entropy 875 <ref>collins, 1996</ref></ref> 857 <tref>magerman, 1995</tref> 843 recall 856 863 853 840 table 5: results on 2416 sentences of section 23 0 to 100 words in length of the wsj treebank.	evaluations marked with  ignore quotation marks.	evaluations marked with  collapse the <pos>distinction</pos> between advp and prt, and ignore all punctuation.
the parseval black and others, 1991 measures compare a proposed parse p with the corresponding <pos><pos><pos><pos>correct</pos></pos></pos></pos> treebank parse t as follows:  correct constituents in p recall   constituents in t  correct constituents in p <pos>precision</pos>   constituents in p a constituent in p is correct if there exists a constituent in t of the same label that spans the same words.	table 5 shows results using the parseval measures, as <pos>well</pos> as results using the slightly more <pos>forgiving</pos> measures of <ref>collins, 1996</ref> and <tref>magerman, 1995</tref>.	table 5 shows that the maximum entropy parser performs <pos>better</pos> than the parsers presented in <ref>collins, 1996</ref> and <tref>magerman, 1995</tref> , which have the <pos>best</pos> previously published parsing accuracies on the wall st journal domain.	it is often <pos>advantageous</pos> to produce the <pos><pos><pos>top</pos></pos></pos> n parses instead of <pos><pos>just</pos></pos> the top 1, since additional information can be used in a secondary model that reorders the top n and <pos>hopefully</pos> improves the quality of the top ranked parse.
table 5 shows results using the parseval measures, as <pos>well</pos> as results using the slightly more <pos>forgiving</pos> measures of <ref>collins, 1996</ref> and <tref>magerman, 1995</tref>.	table 5 shows that the maximum entropy parser performs <pos>better</pos> than the parsers presented in <ref>collins, 1996</ref> and <tref>magerman, 1995</tref> , which have the <pos>best</pos> previously published parsing accuracies on the wall st journal domain.	it is often <pos>advantageous</pos> to produce the <pos><pos><pos>top</pos></pos></pos> n parses instead of <pos><pos>just</pos></pos> the top 1, since additional information can be used in a secondary model that reorders the top n and <pos>hopefully</pos> improves the quality of the top ranked parse.	suppose there exists a <pos><pos>perfect</pos></pos> reranking scheme that, for each sentence, magically picks the <pos><pos>best</pos></pos> parse from the <pos>top</pos> n parses produced by the maximum entropy parser, where the best parse has the highest average <pos>precision</pos> and recall when compared to the treebank parse.
the algorithm exploits <pos>robust</pos> lexical, syntactic, and semantic knowledge sources.	i introduction the application of decision-based <pos>learning</pos> techniques over <pos>rich</pos> sets of linguistic features has <pos>improved</pos> significantly the coverage and performance of syntactic and to various degrees semantic parsers <ref>simmons and yu, 1992</ref>; <tref>magerman, 1995</tref>; <ref>hermjakob and mooney, 1997</ref>.	in this paper, we apply a similar paradigm to developing a rhetorical parser that derives the discourse structure of <pos>unrestricted</pos> texts.	crucial to our approach is the reliance on a corpus of 90 texts which were manually annotated with discourse trees and the adoption of a shift-reduce parsing model that is well-suited for <pos>learning</pos>.
if an erroneous string is extracted, its errors <pos>will</pos> propagate through the rest of the input :trings.	:3 our approach 31 the c45 <pos>learning</pos> algorithm decision tree induction algorithms have been <pos>successfully</pos> applied for nlp problems such as sentence boundary dismnbiguation <ref>pahner et al 1997</ref>, parsing <tref>magerman 1995</tref> and word segmentation <ref>mekuavin et al 1997</ref>.	we employ the c45 <ref>quinhln 1993</ref> decision tree induction program as the <pos>learning</pos> algorithm for word extraction.	the induction algorithm proceeds by evaluating <pos>content</pos> of a series of attributes and iteratively building a tree fiom the attribute <pos><pos>value</pos>s</pos> with the leaves of the decision tree being the value of the goal attribute.
by implementing our own version of the publicly available collins parser <ref>collins, 1996</ref>, we also <pos>learned</pos> a dependency model that enables the mapping of parse trees into sets of binary relations between the head-word of each constituent and its sibling-words.	for example, the parse tree of trec-9 question q210: how many dogs pull a sled in the iditarod  is: jj s iditarod vp np pp np nnpdtinnn np dtvbpnns np manyhow wrb dogs pull a sled in the for each possible constituent in a parse tree, rules first described in <tref>magerman, 1995</tref> and <ref>jelinek et al , 1994</ref> identify the head-child and propagate the head-word to its parent.	for the parse of question q210 the propagation is: np sled dt nn dtin manyhow wrb dogs nnsjj np dogs vbp pull a sled in the iditarod nnp iditarod np iditarod pp iditarod np sled vp pull s pull when the propagation is over, head-modifier relations are extracted, generating the following dependency structure, called question semantic form in <ref>harabagiu et al , 2000</ref>.	dogs iditarodcount pull sled in the structure <pos>above</pos>, count represents the expected answer type, replacing the question stem how many.
two new heavyweight algorithms were developed in the last year.	one is a full parser of english, using a statistically <pos>learned</pos> decision procedure; spatter has achieved the highest scores yet reported on parsing english text <tref>magerman, 1995</tref>.	because the measurable <pos>improvement</pos> in parsing is so <pos>great</pos> compared to manually constructed parsers, it appears to offer a qualitatively <pos>better</pos> parser.	we are looking ormat de scription message message reader i m orphologieal analyzer  lexieai pattern matcher  fast partial parser  semantic interpreter  entene e-level pattern matcher discourse  format  s gml handling initial iden tification of entities grouping words into <pos>meaningful</pos> phrases establish relationships within sentences establish relationships overall ----template/annotation generator i output entities and relationships  output figure 2: plum system architecture: rectangles represent domain-independent, language-independent algorithms; ovals represent knowledge bases.
for example, statistical techniques may have suggested the <pos>importance</pos> of hire, a verb which many groups did not happen to define.	second, since there has been a marked <pos>improvement</pos> in the quality of full parsers, now achieving an f in the high 80s <tref>magerman, 1995</tref>, we believe it is now <pos>feasible</pos> to consider using full parsers again.	the rationale is <pos>straightforward</pos>: for full templates eg , st scores have been mired with an f in the 50s ever since muc-3 in 1991.	pattern matching has given us very <pos>robust</pos>, very <pos>portable</pos> technology, but has not broken the performance barrier all systems have run up against.
b,.	i i  as in the case of the models of <ref>black 1993</ref>, <tref>magerman 1995</tref>, and <ref>collins 1996</ref>, this paper proposes a method of utilizing lexical/semantic features for the purpose of applying them to ranking parses in syntactic analysis.	however, unlike the models of <ref>black 1993</ref>, <tref>magerman 1995</tref>, and <ref>collins 1996</ref>, we put an assumption that syntactic and lexical/semantie features are <pos>independent</pos>.	then, we focus on extracting lexical/semantic collocational knowledge of verbs which is <pos>useful</pos> in syntactic analysis.
in those research, extracted lexical/semantic collocation is <pos>especially</pos> <pos>useful</pos> in terms of ranking parses in syntactic analysis as <pos>well</pos> as automatic construction of lexicon for nlp.	for example, in the context of syntactic disambiguation, <ref>black 1993</ref> and <tref>magerman 1995</tref> proposed statistical parsing models based-on decision-tree <pos>learning</pos> techniques, which incorporated not only syntactic but also lexical/semantic information in the decision-trees.	as lexical/semantic information, <ref>black 1993</ref> used about 50 semantic categories, while <tref>magerman 1995</tref> used lexical forms of words.	<ref>collins 1996</ref> proposed a statistical parser which is based on probabilities of dependencies between head-words in the parse tree.
the key to extraction of the relations is that any phrase can be substituted by the corresponding tree head-word links marked <pos>bold</pos> in figure 1.	to determine the tree head-word we used a set of rules similar to that described by <tref>magerman, 1995</tref><ref>jelinek et al , 1994</ref> and also used by <ref>collins, 1996</ref>, which we modified in the following way:  the head of a prepositional phrase pp-in np was substituted by a function the name of which corresponds to the preposition, and its sole argument corresponds to the head of the noun phrase np.	the head of a subordinate clause was changed to a function named after the head of the first element in the subordinate clause usually that or a null element and its sole argument corresponds to the head of its second element usually head of a sentence.	because we assumed that the relations within the same phrase are <pos>independent</pos>, all the relations are between the modifier constituents and the head of a phrase only.
when inspecting manually, the binary word tree representation appears to be the most <pos>easy</pos> to <pos>understand</pos>.	a second application of the binary word tree can be found in decision-tree based systems such as the spatter parser <tref>magerman, 1995</tref> or the atr decision-tree part-of-speech tagger, as described by ushioda <ref>ushioda, 1996</ref>.	in this case it is <pos>necessary</pos> to use a hard-clustering method, such that a binary word tree can be constructed by the clustering process, as we did in the example in the previous sections.	a decision tree classifies data according to its properties by asking successive often binary questions.
the mentioned studies use word-clusters for interpolated n-gram language models.	another application of hard clustering methods in particular bottom-up variants is that they can also produce a binary tree, which can be used for decision-tree based systems such as the spatter parser <tref>magerman, 1995</tref> or the atr decision-tree part-ofspeech tagger <ref>black et al , 1992</ref>, <ref>ushioda, 1996</ref>.	hogenhout  matsumoto 18 word clustering from syntactic behavior in this case a decision tree contains binary questions to decide the properties of a word.	we present a hard clustering algorithm, in the <pos><pos>sense</pos></pos> that every word belongs to exactly one cluster or is one leaf in the binary word-tree of a particular part of speech.
these results have <pos>important</pos> implications for crosslinguistic parsing research, as they <pos>allow</pos> us to tease apart language-specific and annotationspecific effects.	previous work for english eg , <tref>magerman, 1995</tref>; <ref>collins, 1997</ref> has shown that lexicalization leads to a sizable <pos>improvement</pos> in parsing performance.	english is a language with nonflexible word order and with a treebank with a nonflat annotation scheme see table 2.	research on german <ref>dubey and keller, 2003</ref> showed that lexicalization leads to no sizable <pos>improvement</pos> in parsing performance for this language.
342 decision tree.	algorithms for decision tree induction <ref>quinlan 1986</ref>; <ref>bahl et al 1989</ref> have been <pos>successfully</pos> applied to nlp problems such as parsing <ref>resnik 1993</ref>; <tref>magerman 1995</tref> and discourse analysis <ref>siegel and mckeown 1994</ref>; <ref>soderland and lehnert 1994</ref>.	we tested the satz system using the c45 <ref>quinlan 1993</ref> decision tree induction program as the <pos>learning</pos> algorithm and compared the results to those obtained previously with the neural network.	these results are discussed in section 410.
in order to construct the etrees, which make such <pos>distinction</pos>, lextract requires its user to provide additional information in the form of three tables: a head percolation table, an argument table, and a tagset table.	a head percolation table has previously been used in several statistical parsers <tref>magerman, 1995</tref>; <ref>collins, 1997</ref> to find heads of phrases.	our strategy for choosing heads is similar to the one in <ref>collins, 1997</ref>.	an argument table informs lextract what types of arguments a head can take.
to start with performance values, table 3 displays previous results on parsing section 23 of the wsj section of the penn tree-bank.	comparison indicates that our best model is already better than the early lexicalized model of <tref>magerman 1995</tref>.	it is a bit <neg>worse</neg> than the unlexicalized pcfgs of <ref>klein and manning 2003</ref> and matsuzaki et al.	2005, and of course, it is also worse than state-of-the-art lexicalized parsers experience shows that evaluation results on sections 22 and 23 do not <neg>differ</neg> much.
they consider systematically a number of alternative probao bilistic formulations, including those of <ref>resnik 1992</ref> and <ref>schabes 1992</ref> and implemented systems based on other underlying grammatical frameworks, evaluating their adequacy from both a theoretical and empirical perspective in terms of their <pos>ability</pos> to model particular distributions of data that occur in existing treebanks.	<tref>magerman 1995</tref>, <ref>collins 1996</ref>, <ref>ratnaparkhi 1997</ref>, <ref>charniak 1997</ref> and others describe implemented systems with <pos>impressive</pos> accuracy on parsing unseen data from the penn treebank <ref>marcus, santorini  marcinkiewicz, 1993</ref>.	these parsers model probabilistically the strengths of association between heads of phrases, and the configurations in which these lexical associations occur.	the accuracies reported for these systems are <pos>substantially</pos> <pos>better</pos> than their non-lexicalised probabilistic context-free grammar analogues, demonstrating <pos>clearly</pos> the <pos>value</pos> of lexico-statistical information.
in our experiments, the window starts al the sentence prior to that containing the token and extends <pos>back</pos> w the window size sentences.	the choice to use sentences as the unit of distance is <pos>motivated</pos> by our intention to incorporale triggers of this form into a probabilistie treebank based parser and tagger, sneh as <ref>black et al, 1998</ref>; <ref>black et al, 1997</ref>; <ref>brill, 1994</ref>; <ref>collins, 1996</ref>: aelinek et al, 1994; <tref>magerman, 1995</tref>; blatnaparkhi, 1997.	all su<h parsers and taggers of which we are aware use only intrasentential information in predicting parses or tags, and we <pos><pos>wish</pos></pos> to remove this information, as far as possible, from our results ; the window was not allowed to cross a document bmndary.	the perplexity of lhe task before taking the trigger-pair information into account for tags was 2240 and for rules was 570.
second, one might propagate lexical information <pos>upward</pos> through the productions.	examples of formalisms using this approach include the work of <tref>magerman 1995</tref>, <ref>charniak 1997</ref>, <ref>collins 1997</ref>, and <ref>goodman 1997</ref>.	a more linguistically <pos>motivated</pos> approach is to expand the domain of productions downward to incorporate more tree structures.	the lexicalized tree-adjoining grammar ltag formalism <ref>schabes et al, 1988</ref>, <ref>schabes, 1990</ref> , although not context-free, is the most well-known instance in this category.
our model uses both lexical and syntactic features for determining the probability of inserting discourse boundaries.	we apply canonical lexical head projection rules <tref>magerman, 1995</tref> in order to lexicalize syntactic trees.	for each word a45, the upper-most node with lexical head a45 which has a <pos><pos>right</pos></pos> sibling node determines the features on the basis of which we decide whether to insert a discourse boundary.	we denote such node a68a70a69, and the features we use are node a68a71a69, its parent a68a73a72, and the siblings of a68a74a69  in the example in figure 2, we determine whether to insert a discourse boundary after the word says using as features node a68a75a72a65a43a77a76a6a78a79a21a10a80a33a81a33a82a34a80a33a24 and its children a68 a69 a43a83a76a22a84a86a85a34a21a14a80a33a81a26a82a34a80a32a24 and a68a74a87a88a43a90a89a33a84a92a91a6a93a79a21a95a94a8a96a32a97a6a97a22a24  we use our corpus to estimate the likelihood of inserting a discourse boundary between word a45 and the next word using formula 1, a57a58a21a35a59 a60a45a75a37a40a7a55a24a99a98a101a100 a5a8a7a29a21a35a68a73a72a75a102a103a50a52a50a52a50a40a68a104a69a74a42a27a68 a87 a50a52a50a52a50a38a24 a100 a5a8a7a29a21a35a68 a72 a102a105a50a52a50a4a50a55a68 a69 a68a104a87a62a50a4a50a52a50a38a24 1 where the numerator represents all the counts of the rule a68 a72 a102a105a50a52a50a4a50a40a68 a69 a68a104a87a51a50a52a50a52a50 for which a discourse boundary has been inserted after word a45, and the denominator represents all the counts of the rule.
coming to this problem from the standpoint of tree transformation, we <pos>naturally</pos> view our work as a descendent of <ref>johnson 1998</ref> and <ref>klein and manning 2003</ref>.	in retrospect, however, there are perhaps <pos>even</pos> greater similarities to that of <tref>magerman, 1995</tref>; <ref>henderson, 2003</ref>; <ref>matsuzaki et al , 2005</ref>.	consider the approach of matsuzaki et al.	2005.
we would <pos>like</pos> to infer the number of annotations for each nonterminal automatically.	however, again in retrospect, it is in the work of <tref>magerman 1995</tref> that we see the <pos>greatest</pos> similarity.	rather than talking about clustering nodes, as we do, magerman creates a decision tree, but the differences between clustering and decision trees are small.	perhaps a more <pos>substantial</pos> difference is that by not casting his problem as one of <pos>learning</pos> phrasal categories magerman loses all of the <pos>free</pos> pcfg technology that we can <pos>leverage</pos>.
our experiment quantitatively analyzes several feature types power for syntactic structure prediction and draws a series of <pos>interesting</pos> conclusions.	in the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types <ref>black, 1992</ref> <ref>briscoe, 1993</ref> <ref>brown, 1991</ref> <ref>charniak, 1997</ref> <ref>collins, 1996</ref> <ref>collins, 1997</ref> <ref>magerman, 1991</ref> <ref>magerman, 1992</ref> <tref>magerman, 1995</tref> <ref>eisner, 1996</ref>.	how to evaluate the different feature types effects for syntactic parsing.	the paper proposes an information-theory-based feature types analysis model, which uses the measures of predictive information quantity, predictive information <pos>gain</pos>, predictive information redundancy and predictive information summation to quantitatively analyse the different contextual feature types or feature types combinations predictive power for syntactic structure.
<neg>unfortunately</neg>, the state of knowledge in this regard is very <neg>limited</neg>.	many probabilistic evaluation models have been published inspired by one or more of these feature types <ref><neg>black</neg>, 1992</ref> <ref>briscoe, 1993</ref> <ref>charniak, 1997</ref> <ref>collins, 1996</ref> <ref>collins, 1997</ref> <tref>magerman, 1995</tref> <ref>eisner, 1996</ref>, but discrepancies between training sets, algorithms, and hardware environments make it <neg>difficult</neg>, if not <neg>impossible</neg>, to compare the models objectively.	in the paper, we propose an information-theory-based feature type analysis model by which we can quantitatively analyse the predictive power of different feature types or feature type combinations for syntactic structure in a systematic way.	the conclusion is expected to provide reliable reference for feature type selection in the probabilistic evaluation modelling for statistical syntactic parsing.
pf model describes the probability of each feature in feature set fs taking on specific <pos>values</pos> when a cfg rule a ->  is given.	to make the model more <pos>practical</pos> in parameter estimation, we assume the features in feature set fs are <pos>independent</pos> from each other, thus:    fsfi afipafsp ,,  5 under this pcfgpf model, the goal of a parser is to choose a parse that maximizes the following score: ,maxarg 1 afs i i i n i t pstscore    6 our model is thus a simplification of more <pos>sophisticated</pos> models which integrate pcfgs with features, such as those in <tref>magerman1995</tref>, <ref>collins1997</ref> and <ref>goodman1997</ref>.	compared with these models, our model is more <pos>practical</pos> when only small training data is available, since we assume the <pos>independence</pos> between features.	for example, in goodmans probabilistic feature grammar pfg, each symbol in a pcfg is replaced by a set of features, so it can describe specific constraints on the rule.
14 note that the humans did not have access to pause information.	other studies have shown that when both speech and text are available to labelers, segmentation is <pos>clearer</pos> <ref>swerts 1995</ref> and <pos>reliability</pos> improves <tref>hirschberg and nakatani 1996</tref>.	123 computational linguistics volume 23, number 1 table 4 evaluation for tj > 4.	recall <pos>precision</pos> fallout error summed deviation pause 92 18 54 49 193 cue 72 15 53 50 216 np 50 31 15 19 153 humans 74 55 09 11 91 if cue1  <pos>true</pos> then boundary else nonboundary figure 11 cue word algorithm.
the types of discourse units being coded and the relations among them vary.	several studies have used trained coders to locally and globally structure spontaneous or read speech using the model of <ref>grosz and sidner 1986</ref>, including <ref>grosz and hirschberg 1992</ref>; <ref>nakatani, hirschberg, and grosz 1995</ref>; <ref>stifleman 1995</ref>; <tref>hirschberg and nakatani 1996</tref>.	<ref>in grosz and hirschberg 1992</ref>, percent <pos>agreement</pos> see section 32 among 7 coders on 3 texts under two conditions--text plus speech or text alone--is reported at levels ranging from 743 to 951.	<ref>in hirschberg and nakatani 1996</ref>, average <pos><pos>reliability</pos></pos> measured using the kappa coefficient discussed in carletta 1996 of segmentinitial labels among 3 coders on 9 monologues produced by the same speaker, labeled using text and speech, is8 or <pos>above</pos> for both read and spontaneous speech; <pos>values</pos> of at least 8 are typically viewed as representing high reliability see section 32.
<ref>wilson and wiebe 2005</ref> extend this <pos>basic</pos> annotation scheme to include different types of subjectivity, including <pos><pos>positive</pos></pos> <pos><pos>sentiment</pos></pos>, negative sentiment, positive arguing, and negative arguing.	speech was found to <pos>improve</pos> inter-annotator <pos>agreement</pos> in discourse segmentation of monologs <tref>hirschberg and nakatani 1996</tref>.	acoustic clues have been <pos>successfully</pos> employed for the <pos>reliable</pos> detection of the speakers emotions, including frustration, annoyance, anger, <pos>happiness</pos>, sadness, and boredom <ref>liscombe et al 2003</ref>.	devillers et al.
61 reader judgments there is a growing concern surrounding issues of intercoder <pos>reliability</pos> when using human judgments to evaluate discourse-processing algorithms <ref>carletta 1996</ref>; <ref>condon and cech 1995</ref>.	proposals have recently been made for protocols for the collection of human discourse segmentation data <ref>nakatani et al 1995</ref> and for how to evaluate the <pos>validity</pos> of judgments so obtained <ref>carletta 1996</ref>; <ref>isard and carletta 1995</ref>; ros6 1995; <ref>passonneau and litman 1993</ref>; <ref>litman and passonneau 1995</ref>.	recently, hirschberg 52 <ref>hearst texttiling and nakatani 1996</ref> have reported <pos>promising</pos> results for obtaining higher interjudge <pos>agreement</pos> using their collection protocols.	for the evaluation of the texttiling algorithms, judgments were obtained from seven readers for each of 12 magazine articles that satisfied the length criteria between 1,800 and 2,500 words 9 and that contained little structural demarcation.
proposals have recently been made for protocols for the collection of human discourse segmentation data <ref>nakatani et al 1995</ref> and for how to evaluate the <pos>validity</pos> of judgments so obtained <ref>carletta 1996</ref>; <ref>isard and carletta 1995</ref>; ros6 1995; <ref>passonneau and litman 1993</ref>; <ref>litman and passonneau 1995</ref>.	recently, hirschberg 52 <ref>hearst texttiling and nakatani 1996</ref> have reported <pos>promising</pos> results for obtaining higher interjudge <pos>agreement</pos> using their collection protocols.	for the evaluation of the texttiling algorithms, judgments were obtained from seven readers for each of 12 magazine articles that satisfied the length criteria between 1,800 and 2,500 words 9 and that contained little structural demarcation.	the judges were asked simply to mark the paragraph boundaries at which the topic changed; they were not given more <pos>explicit</pos> instructions about the granularity of the segmentation.
we should expect to see, in grouping together paragraph-sized units instead of utterances, a <neg>decrease</neg> in the complexity of the feature set and algorithm needed.	the work described here makes use only of lexical distribution information, in lieu of prosodic cues such as intonational pitch, pause, and duration <tref>hirschberg and nakatani 1996</tref>, discourse markers such as oh, well, ok, however <ref>schiffrin 1987</ref>; <ref>litman and passonneau 1995</ref>, pronoun reference resolution <ref>passonneau and litman 1993</ref>; <ref>webber 1988</ref> and <neg>tense</neg> and aspect <ref>webber 1987</ref>; <ref>hwang and schubert 1992</ref>.	from a computational viewpoint, deducing textual topic structure from lexical occurrence information alone is appealing, both because it is easy to compute, and because discourse cues are sometimes <neg>misleading</neg> with respect to the topic structure <ref>brown and yule 1983</ref>, section 3.	4.
metric f nm s nonm p  user turns 218 53 228 65 065  <pos>correct</pos> turns 72 18 67 22 059 asrmis 37 27 46 28 046 semmis 5 6 12 14 009 table 2.	average standard deviation for <pos>objective</pos> metrics in the first problem 6 related work discourse structure has been <pos>successfully</pos> used in non-interactive settings eg <pos>understanding</pos> specific lexical and prosodic phenomena <tref>hirschberg and nakatani, 1996</tref>, <pos>natural</pos> language generation <ref>hovy, 1993</ref>, essay scoring <ref>higgins et al , 2004</ref> as <pos>well</pos> as in interactive settings eg predictive/generative models of postural shifts <ref>cassell et al , 2001</ref>, generation/interpretation of anaphoric expressions <ref>allen et al , 2001</ref>, performance modeling <ref>rotaru and litman, 2006</ref>.	in this paper, we study the utility of the discourse structure on the user side of a dialogue system.	one related study is that of <ref><pos>rich</pos> and sidner, 1998</ref>.
the main drawback of this corpus is that it comprises only read speech.	prosody labeling on spontaneous speech corpora <pos>like</pos> boston directions corpus bdc, switchboard swbd has garnered attention in <tref>hirschberg and nakatani, 1996</tref>; <ref>gregory and altun, 2004</ref>.	automatic prosody labeling has been achieved through various machine <pos>learning</pos> techniques, such as decision trees <ref>hirschberg, 1993</ref>; <ref>wightman and ostendorf, 1994</ref>; <ref>ma et al , 2003</ref>, rule-based systems <ref>shimei and mckeown, 1999</ref>, bagging and boosting on cart <ref>sun, 2002</ref>, hidden markov models <ref>conkie et al , 1999</ref>, neural networks hasegawa-<ref>johnson et al , 2005</ref>,maximum-entropy models <ref>brenier et al , 2005</ref> and conditional random fields <ref>gregory and altun, 2004</ref>.	prosody labeling of the bu corpus has been reported in many studies <ref>hirschberg, 1993</ref>; <ref>hasegawajohnson et al , 2005</ref>; <ref>ananthakrishnan and narayanan, 2005</ref>.
for example, in figure 1, if the student would have answered tutor 2 <pos>correctly</pos>, the next tutor turn would have had the same <pos>content</pos> as tutor 5 but the advance label.	also, while a human annotation of the discourse structure <pos>will</pos> be more complex but more time consuming <tref>hirschberg and nakatani, 1996</tref>; <ref>levow, 2004</ref>, its <pos>advantages</pos> are outweighed by the automatic nature of our discourse structure annotation.	we would <pos>like</pos> to <pos>highlight</pos> that our transition annotation is domain <pos>independent</pos> and automatic.	our transition labels capture behavior <pos>like</pos> starting a new dialogue newtoplevel, crossing discourse segment boundaries push, popup, popupadv and local phenomena inside a discourse segment advance, samegoal.
information status has generated <pos>large</pos> <pos><pos>interest</pos></pos> among researchers because of its complex interaction with other linguistic phenomena, thus affecting several <pos>natural</pos> language processing tasks.	since it correlates with word order and pitch accent <ref>lambrecht, 1994</ref>; <tref>hirschberg and nakatani, 1996</tref>, for instance, incorporating knowledge on information status would be <pos>helpful</pos> for <pos>natural</pos> language generation, and in particular text-tospeech systems.	stober and colleagues, for example, ascribe to the lack of such information the lower performance of text-to-speech compared to concept-to-speech generation, where such knowledge could be made directly available to the system <ref>stober et al , 2000</ref>.	another area where information status can play an <pos>important</pos> role is anaphora resolution.
nine coders provided iu trees starting from identical cgus.	following the methodology in ttirschberg and <ref>nakatani, 1996</ref>, we measured the <pos>reliability</pos> of coding for a linearized version of the iu tree, by calculating the reliability of coding of iu beginnings using the kappa metric.	we calculated the observed pairwise <pos><pos>agreement</pos></pos> of cgus marked as the beginnings of ius, and factored out the expected agreement estimated from the actual data, giving the pairwise kappa score.	table 3 gives the raw data on coders marking of iu beginnings.
<ref>rotondo 1984</ref> reported that hierarchical segmentation is impractical for naive subjects in discourses longer than 200 words <ref>passonneau and litman 1993</ref> conducted a pilot study in which subjects found it difficult and time-consuming to identify hierarchical relations in discourse.	other attempts have had more <pos>success</pos> using <pos>improved</pos> annotation tools and more <pos>precise</pos> instructions <ref>grosz and hirschberg, 1992</ref>; <tref>hirschberg and nakatani, 1996</tref>.	second, hierarchical segmentation of discourse is subjective.	while <pos>agreement</pos> among annotators regarding linear segmentation has been found to be higher than 80 <ref>hearst, 1997</ref>, with <pos><pos>respect</pos></pos> to hierarchical segmentation it has been observed to be as low as 60 <ref>flammia and zue, 1995</ref>.
let a28a30a40a30a11a14a28a21a40a30a28a16a31a36a3a41a34 be the set of <pos><pos>sense</pos></pos>s of a3  for each sense of a3 a3a42a28a44a43a46a45a47a28a30a40a30a11a14a28a21a40a30a28a16a31a36a3a41a34  we obtain a ranking score by summing over the a27a48a28a21a28a16a31a32a3a6a15a33a11a50a49a30a34 of each neighbour a11a50a49a51a45a52a4a6a5  multiplied by a weight.	this weight is the wordnet similarity score a3a42a11a14a28a30a28  between the target <pos><pos><pos><pos><pos>sense</pos></pos></pos></pos></pos> a3a53a28a35a43  and the sense of a11a54a49 a11a14a28a35a55a56a45a57a28a30a40a30a11a14a28a21a40a30a28a16a31a36a11a54a49a16a34  that maximises this score, divided by the sum of all such wordnet similarity scores for a28a30a40a21a11a19a28a21a40a30a28a26a31a32a3a41a34 and a11a50a49  thus we rank each sense a3a42a28 a43 a45a10a28a30a40a21a11a19a28a21a40a30a28a26a31a32a3a41a34 using:a58a41a59 a11a14a2a54a60a61a11a50a62a64a63a66a65a35a67a16a68a12a40a26a31a32a3a53a28 a43 a34a69a7 a70 a71a44a72a33a73a16a74a76a75 a27a48a28a21a28a16a31a32a3a6a15a33a11a50a49a30a34a30a77 a3a42a11a14a28a21a28a16a31a32a3a53a28a35a43a36a15a33a11a50a49a30a34 a78 a5a19a79a81a80a39a82 a73 a79a61a83 a71 a79a81a83a61a79a36a84a85a5a19a86 a3a53a11a14a28a30a28a26a31a32a3a42a28 a43 a82 a15a17a11 a49 a34 1 where: a3a42a11a14a28a21a28a16a31a32a3a53a28a35a43a87a15a17a11a54a49a21a34a66a7 a88a46a89a21a90 a71 a79a92a91 a73 a79a81a83 a71 a79a81a83a61a79a93a84 a71a44a72 a86 a31a36a3a42a11a14a28a30a28a26a31a32a3a53a28a35a43a36a15a33a11a14a28a35a55a29a34a87a34 22 acquiring the automatic thesaurus there are many alternative distributional similarity measures proposed in the literature, for this work we used the measure and thesaurus construction method described by <tref>lin 1998</tref>.	for input we used grammatical relation data extracted using an automatic parser <ref>briscoe and carroll, 2002</ref>.	for each noun we considered the co-occurring verbs in the direct object and subject relation, the modifying nouns in noun-noun relations and the modifying adjectives in adjective-noun relations.
<pos>importantly</pos>, inter-dependence between links can still be accommodated by exploiting <pos>dynamic</pos> features in training features that take into account the labels of some of the surrounding components when predicting the label of a target component.	to cope with the sparse data problem, i use distributional word similarity <ref>pereira et al , 1993</ref>; <ref>grefenstette, 1994</ref>; <tref>lin, 1998</tref> to generalize the observed frequency counts in the training corpus.	the experimental results on the chinese treebank 40 show that the accuracy of the conditional model is 136 higher than corresponding joint models, while similarity smoothing also allows the strictly lexicalised approach to outperform corresponding models based on part-of-speech tags.	4 extensions to <pos>large</pos> margin parsing the approach presented <pos>above</pos> has a limitation: it uses a local scoring function instead of a global scoring function to compute the score for a candidate tree.
realword spelling correction is also referred to as context <pos>sensitive</pos> spelling correction, which tries to detect incorrect usage of <pos>valid</pos> words in certain contexts <ref>golding and roth, 1996</ref>; <ref>mangu and brill, 1997</ref>.	distributional similarity between words has been investigated and <pos>successfully</pos> applied in many <pos>natural</pos> language tasks such as automatic semantic knowledge acquisition <tref>dekang lin, 1998</tref> and language model smoothing <ref>essen and steinbiss, 1992</ref>; <ref>dagan et al , 1997</ref>.	an investigation on distributional similarity functions can be found in <ref>lillian lee, 1999</ref>.	3 distributional similarity-based models for query spelling correction 31 <pos>motivation</pos> most of the previous work on spelling correction concentrates on the problem of designing <pos>better</pos> error models based on properties of character strings.
up to now, there have been few researches which directly address the <neg>problem</neg> of extracting synonymous collocations.	however, a number of studies investigate the extraction of synonymous words from monolingual corpora <ref>carolyn et al , 1992</ref>; <ref>grefenstatte, 1994</ref>; <tref>lin, 1998</tref>; <ref>gasperin et al , 2001</ref>.	the methods used the contexts around the investigated words to discover synonyms.	the <neg>problem</neg> of the methods is that the precision of the extracted synonymous words is <neg>low</neg> because it extracts many word pairs such as cat and dog, which are similar but not synonymous.
in this <pos>light</pos>, the contributions of this paper are fourfold.	first, instead of separately addressing the tasks of collecting unlabeled sets of instances <tref>lin, 1998</tref>, assigning <pos>appropriate</pos> class labels to a given set of instances <ref>pantel and ravichandran, 2004</ref>, and identifying <pos>relevant</pos> attributes for a given set of classes <ref>pasca, 2007</ref>, our integrated method from section 2 enables the simultaneous extraction of class instances, associated labels and attributes.	second, by exploiting the contents of query logs during the extraction of labeled classes of instances from web documents, we acquire thousands 4,583, to be exact of open-domain classes covering a <pos>wide</pos> range of topics and domains.	the accuracy reported in section 32 exceeds 80 for both instance sets and class labels, although the extraction of classes requires a <pos>remarkably</pos> small amount of supervision, in the form of only a few commonly-used is-a extraction patterns.
there have been many approachs to automatic detection of similar words from text.	our method is similar to <ref>hindle, 1990</ref>, <tref>lin, 1998</tref>, and <ref>gasperin, 2001</ref> in the use of dependency relationships as the word features.	another approach used the words distribution to cluster the words <ref>pereira, 1993</ref>, and inoue <ref>inoue, 1991</ref> also used the word distributional information in the japanese-english word pairs to <pos><pos>resolve</pos></pos> the polysemous word problem.	wu <ref>wu, 2003</ref> shows one approach to collect synonymous collocation by using translation information.
the wmts is <pos><pos>well</pos></pos> suited to lra, because the wmts scales well to <pos>large</pos> corpora one terabyte, in our case, it gives exact frequency counts unlike most web search engines, it is designed for passage retrieval rather than document retrieval, and it has a <pos>powerful</pos> query syntax.	53 thesaurus as a source of synonyms, we use <tref>lins 1998a</tref> automatically generated thesaurus.	this thesaurus is available through an on-line interactive demonstration or it can be downloaded.	5 we used the on-line demonstration, since the downloadable version seems to contain fewer words.
the lra algorithm consists of the following 12 steps: 1.	find alternates: for each word pair a:b in the input set, look in <tref>lins 1998a</tref> thesaurus for the <pos><pos>top</pos></pos> num sim words in the following experiments, num sim is 10 that are most similar to a for each a prime that is similar to a, make a new word pair a prime :b likewise, look for the top num sim words that are most similar to b, and for each b prime , make a new word pair a:b prime  a:b is called the <pos>original</pos> pair and each a prime :b or a:b prime is an alternate pair.	the intent is that alternates should have almost the same semantic relations as the <pos>original</pos>.	for each input pair, there <pos>will</pos> now be 2  num sim alternate pairs.
for each input pair, there <pos>will</pos> now be 2  num sim alternate pairs.	when looking for similar words in <tref>lins 1998a</tref> thesaurus, avoid words that seem unusual eg, hyphenated words, words with three characters or less, words with non-alphabetical characters, multiword phrases, and capitalized words.	the first column in table 7 shows the alternate pairs that are generated for the <pos>original</pos> pair quart:volume.	2.
as a <pos>courtesy</pos> to other users of lins on-line system, we insert a 20-second delay between each two queries.	lins thesaurus was generated by parsing a corpus of about 5  10 7 english words, consisting of text from the wall street journal, san jose mercury,andap newswire <tref>lin 1998a</tref>.	the parser was used to extract pairs of words and their grammatical relations.	words were then clustered into synonym sets, based on the similarity of their grammatical relations.
the first word class in the sequence, cl1, consists of words such as was, is, could, whereas the second class includes february, april, june, aug , november and other similar words.	the classes of words are computed on the fly over all sequences of terms in the extracted patterns, on <pos>top</pos> of a <pos>large</pos> set of pairwise similarities among words <tref>lin, 1998</tref> extracted in advance from around 50 million news articles indexed by the google search engine over three years.	all digits in both patterns and sentences are replaced with a common marker, such 810 that any two numerical <pos>values</pos> with the same number of digits <pos>will</pos> overlap during matching.	many methods have been proposed to compute distributional similarity between words, eg, <ref>hindle, 1990</ref>, <ref>pereira et al , 1993</ref>, <ref>grefenstette, 1994</ref> and <tref>lin, 1998</tref>.
538 ture for npi whose <pos>value</pos> is the most likely ne type.	7 neighbor: research in lexical semantics suggests that the sc of an np can be inferred from its distributionally similar nps see <tref>lin 1998a</tref>.	<pos>motivated</pos> by this observation, we create for each of npis ten most semantically similar nps a neighbor feature whose <pos>value</pos> is the surface string of the np.	to determine the ten nearest neighbors, we use the semantic similarity <pos>values</pos> provided by lins dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic de nition of similarity.
an example extraction would be <eastern airlines, the carrier>, where the rst entry is a <pos>proper</pos> noun labeled with either one of the seven muc-style ne types4 or others5 and the second entry is a common noun.	we then infer the sc of a common noun as follows: 1 we compute the probability that the common noun co-occurs with each of the eight ne types6 based on the extracted appositive relations, and 2 if the most likely ne type has a co-occurrence probability <pos>above</pos> a certain threshold we set it to 07, we create a induced class fea1this is <pos>motivated</pos> by <tref>lins 1998c</tref> observation that a coreference resolver that employs only the rst wordnet <pos><pos><pos><pos>sense</pos></pos></pos></pos> performs slightly <pos>better</pos> than one that employs more than one sense.	2the keywords are obtained via our experimentation with wordnet and the ace scs of the nps in the ace training data.	3we used 1 the bllip corpus 30m words, which consists of wsj articles from 1987 to 1989, and 2 the reuters corpus 37gb data, which has 806,791 reuters articles.
2 subj verb: if npi is involved in a subjectverb relation, we create a subj verb feature whose <pos>value</pos> is the verb participating in the relation.	we use <tref>lins 1998b</tref> minipar dependency parser to extract grammatical relations.	our <pos>motivation</pos> here is to coarsely model subcategorization.	3 verb obj: a verb obj feature is created in a similar fashion as subj verb if npi participates in a verb-object relation.
pointwise mutual information <ref>church and hanks, 1990</ref>, 3.	least mutual information difference with similar collocations, based on <ref>lin, 1999</ref> and using lins thesaurus <tref>lin, 1998</tref> for obtaining the similar collocations.	4.	the distributed frequency of an object, which takes an average of the frequency of occurrence with an object over all verbs occurring with the object <pos>above</pos> a threshold.
we used three different types of probabilistic models, which vary in the classes selected for representation over which the probability distribution of the argument heads 2 is estimated.	two use wordnet and the other uses the entries in a thesaurus of distributionally similar words acquired automatically following <tref>lin, 1998</tref>.	the rst method is due to <ref>li and abe 1998</ref>.	the classes over which the probability distribution is calculated are selected according to the minimum description length <pos>principle</pos> mdl which uses the argument head tokens for nding the <pos>best</pos> classes for representation.
similarity and association measures can <pos><pos>help</pos></pos> for the cases of near-synonymy.	however, while similarity measures such as wordnet distance or lins similarity metric only detect cases of semantic similarity, association measures such as the ones used by poesio et al , or by garera and yarowsky also find cases of associative bridg497 lin98 rff they they:g2 pl03 land country/state/land staat staat kemalismus regierung kontinent state state kemalism government continent stadt stadt bauernfamilie prasident region city city agricultural family president region region landesregierung bankgesellschaft dollar stadt region country government banking corporation dollar city bundesrepublik bundesregierung baht albanien staat federal republic federal government baht albania state republik gewerkschaft gasag hauptstadt bundesland republic trade union a gas company capital state medikament medical drug arzneimittel pille ru <pos><pos><pos>patient</pos></pos></pos> arzneimittel pharmaceutical pill a drug patient pharmaceutical praparat droge abtreibungspille arzt lebensmittel preparation drug non-medical abortion pill doctor foodstuff pille praparat viagra pille praparat pill preparation viagra pill preparation hormon pestizid pharmakonzern behandlung behandlung hormone pesticide pharmaceutical company treatment treatment lebensmittel lebensmittel praparat abtreibungspille arznei foodstuff foodstuff preparation abortion pill drug highest ranked words, with very rare words removed : ru 486, an abortifacient drug lin98: lins distributional similarity measure <tref>lin, 1998</tref> rff: geffet and dagans relative feature focus measure <ref>geffet and dagan, 2004</ref> they: association measure introduced by <ref>garera and yarowsky 2006</ref> they:g2: similar method using a log-<pos>like</pos>lihood-based statistic see <ref>dunning 1993</ref> this statistic has a <pos>preference</pos> for higher-frequency terms pl03: semantic space association measure proposed by <ref>pado and lapata 2003</ref> table 1: similarity and association measures: most similar items ing like 1a,b; the result of this can be seen in table 2: while the similarity measures lin98, rff list substitutable terms which behave like synonyms in many contexts, the association measures garera and yarowskys they measure, pado and lapatas association measure also find non-compatible associations such as countrycapital or drugtreatment, which is why they are commonly called relationfree.	for the purpose of coreference resolution, however we do not <pos>want</pos> to <pos><pos>resolve</pos></pos> the door to the antecedent the house as the two descriptions do not corefer, and it may be <pos>useful</pos> to filter out non-similar associations.	12 information sources different resources may be differently suited for the <pos>recognition</pos> of the various relations.
while none of the information sources can match the <pos><pos>precision</pos></pos> of the hypernymy information encoded in germanet, or that of using a combination of high-precision patterns with the world <pos>wide</pos> web as a very <pos>large</pos> corpus, it is possible to <pos>achieve</pos> a considerable <pos>improvement</pos> in terms of recall without sacrificing too much precision by combining these methods.	very interestingly, the distributional methods based on intra-sentence relations <tref>lin, 1998</tref>; <ref>pado and lapata, 2003</ref> outperformed <ref>garera and yarowskys 2006</ref> association measure when used for ranking, which may due to sparse data problems or simply too much noise for the latter.	for the association measures, the fact that they are relation-free also means that they can <pos>profit</pos> from added semantic filtering.	the <pos>novel</pos> distance-bounded semantic similarity method where we use the most similar words in the previous discourse together with a semantic classbased filter and a distance limit comes near the <pos>precision</pos> of using surface patterns, and offers <pos>better</pos> accuracy than gasperin and vieiras method of using the globally most similar words.
note, however, that <ref>mccarthy et al , 2004</ref> used the information about distributionally similar words to approximate corpus frequencies for word <pos><pos>sense</pos></pos>s, whereas we target the estimation of a property of a given word sense the subjectivity.	starting with a given ambiguous word w, we first find the distributionally similar words using the method of <tref>lin, 1998</tref> applied to the automatically parsed texts of the british national corpus.	let dsw  dsw1, dsw2,  , dswn be the list of top-ranked distributionally similar words, sorted in decreasing order of their similarity.	next, for each <pos><pos>sense</pos></pos> wsi of the word w, we determine the similarity with each of the words in the list dsw, using a wordnet-based measure of semantic similarity wnss.
41 weight tuning there are several motivations for <pos>learning</pos> the graph weights  in this domain.	first, some dependency relations  <pos>foremost</pos>, subject and object  are in general more salient than others <tref>lin, 1998</tref>; <ref>pado and lapata, 2007</ref>.	in addition, dependency relations may have varying <pos>importance</pos> per different notions of word similarity eg, noun vs verb similarity <ref>resnik and diab, 2000</ref>.	weight tuning allows the adaption of edge weights to each task ie, distribution of queries.
the <pos>learning</pos> methods described in this paper can be <pos>readily</pos> applied to 911 other directed and labelled entity-relation graphs7 the graph representation described in this paper is perhaps most related to syntax-based vector space models, which derive a notion of semantic similarity from statistics associated with a parsed corpus <ref>grefenstette, 1994</ref>; <tref>lin, 1998</tref>; <ref>pado and lapata, 2007</ref>.	in most cases, these models construct vectors to represent each word wi, where each element in the vector for wi corresponds to particular context c, and represents a count or an indication of whether wi occurred in context c a context can refer to <pos>simple</pos> co-occurrence with another word wj, to a particular syntactic relation to another word eg, a relation of direct object to wj, etc given these word vectors, inter-word similarity is evaluated using some <pos>appropriate</pos> similarity measure for the vector space, such as cosine vector similarity, or lins similarity <tref>lin, 1998</tref>.	recently, pado and lapata <ref>pado and lapata, 2007</ref> have suggested an extended syntactic vector space model called dependency vectors, in which rather than <pos>simple</pos> counts, the components of a word vector of contexts consist of weighted scores, which combine both co-occurrence frequency and the <pos>importance</pos> of a context, based on properties of the connecting dependency paths.	they considered two different weighting schemes: a length weighting scheme, assigning lower weight to longer connecting paths; and an obliqueness weighting hierarchy <ref>keenan and comrie, 1977</ref>, assigning higher weight to paths that include grammatically salient relations.
instead, we include <pos>learning</pos> techniques to optimize the graphwalk based similarity measure.	the learning methods described in this paper can be <pos>readily</pos> applied to 911 other directed and labelled entity-relation graphs7 the graph representation described in this paper is perhaps most related to syntax-based vector space models, which derive a notion of semantic similarity from statistics associated with a parsed corpus <ref>grefenstette, 1994</ref>; <tref>lin, 1998</tref>; <ref>pado and lapata, 2007</ref>.	in most cases, these models construct vectors to represent each word wi, where each element in the vector for wi corresponds to particular context c, and represents a count or an indication of whether wi occurred in context c a context can refer to <pos>simple</pos> co-occurrence with another word wj, to a particular syntactic relation to another word eg, a relation of direct object to wj, etc given these word vectors, inter-word similarity is evaluated using some <pos>appropriate</pos> similarity measure for the vector space, such as cosine vector similarity, or lins similarity <tref>lin, 1998</tref>.	recently, pado and lapata <ref>pado and lapata, 2007</ref> have suggested an extended syntactic vector space model called dependency vectors, in which rather than <pos>simple</pos> counts, the components of a word vector of contexts consist of weighted scores, which combine both co-occurrence frequency and the <pos>importance</pos> of a context, based on properties of the connecting dependency paths.
to further <pos>enhance</pos> the quality of co-occurrence data, we search on the specific phrase a16 is measured in in which a16 is one of the related concepts of a14  this allows for the simultaneous discovery of unknown units and the retrieval of their co-occurrence counts.	sentences in which the pattern occurs are parsed using minipar <tref>lin, 1998b</tref> so that we can obtain the word related to measured via the prepositional in relation.	this allows us to handle sentential constructions that may intervene between measured and a <pos>meaningful</pos> unit.	for each unit a17 that is related to measured via in, we increment the co-occurrence count a18a20a19a21a17a23a22a24a16a26a25, thereby collecting frequency counts for each a17 with a16  the patterns <pos>precision</pos> prevents incidental cooccurrence between a related concept and some unit that may occur simply because of the general topic of the document.
thus, there is <pos>strong</pos> <pos>motivation</pos> to expand the list of units obtained from google by automatically considering similar units.	519 we gather similar units from an automaticallyconstructed thesaurus of distributionally similar words <tref>lin, 1998a</tref>.	the similar word expansion can add a term <pos>like</pos> gigs as a unit for size by <pos>virtue</pos> of its association with gigabytes, which is on the <pos>original</pos> list.	unit similarity can be thought of as a mapping a18 a1 a6 a4 a0 a8 in which a6 is a set of units and a0 a8 is sets of related units.
cluster clustered similar words of duty with similarity score <pos>responsibility</pos> 016, obligation 0109, task 0101, function 0098, role 0091, post 0087, position 0086, job 0084, chore 008, mission 008, assignment 0079, liability 0077  tariff0091, restriction 0089, tax 0086, regulation 0085, requirement 0081, procedure 0079, penalty 0079, quota 0074, rule 007, levy 0061  fee 0085, salary 0081, pay 0064, <pos>fine</pos> 0058 personnel 0073, staff0073 training 0072, work 0064, exercise 0061 <pos>privilege</pos> 0069, <pos><pos>right</pos></pos> 0057, license 0056 22.	corpus-based thesaurus using the collocation database, lin used an unsupervised method to construct a corpusbased thesaurus <tref>lin, 1998a</tref> consisting of 11839 nouns, 3639 verbs and 5658 adjectives/adverbs.	given a word w, the thesaurus returns a clustered list of similar words of w along with their similarity to w for example, the clustered similar words of duty are shown in table 1.	23.
78 table 1.	clustered similar words of duty as given by <tref>lin, 1998a</tref>.	cluster clustered similar words of duty with similarity score <pos>responsibility</pos> 016, obligation 0109, task 0101, function 0098, role 0091, post 0087, position 0086, job 0084, chore 008, mission 008, assignment 0079, liability 0077  tariff0091, restriction 0089, tax 0086, regulation 0085, requirement 0081, procedure 0079, penalty 0079, quota 0074, rule 007, levy 0061  fee 0085, salary 0081, pay 0064, <pos>fine</pos> 0058 personnel 0073, staff0073 training 0072, work 0064, exercise 0061 <pos>privilege</pos> 0069, <pos><pos>right</pos></pos> 0057, license 0056 22.	corpus-based thesaurus using the collocation database, lin used an unsupervised method to construct a corpusbased thesaurus <tref>lin, 1998a</tref> consisting of 11839 nouns, 3639 verbs and 5658 adjectives/adverbs.
it then assigns a score of 1 if the text contains a synonym, hyponym or derived form of the target word and a score of 0 otherwise.	42 similarity as a second measure we used the distributional similarity measure of <tref>lin, 1998</tref>.	for a text t and a word u we assign the max similarity score as follows: similarityt,u  maxvt simu,v 1 where simu,v is the similarity score for u and v4.	43 alignment model <ref>glickman et al , 2006</ref> was among the <pos>top</pos> scoring systems on the rte-1 challenge and supplies a probabilistically <pos>motivated</pos> lexical measure based on word co-occurrence statistics.
we are going to extend the set of <pos>content</pos> bearing words and to include verbs.	we <pos>will</pos> take <pos>advantage</pos> of the flexibility provided by our framework and use syntax based measure of similarity in the computation of the verb vectors, following <tref>lin, 1998</tref>.	currently we are using string matching to compute the named entity based measure of similarity.	we are planning to integrate more <pos>sophisticated</pos> techniques in our framework.
2 distributional features in this section, we firstly describe how we extract contexts from corpora and then how distributional features are constructed for word pairs.	21 context extraction we adopted dependency structure as the context of words since it is the most widely used and wellperforming contextual information in the past studies <ref>ruge, 1997</ref>; <tref>lin, 1998</tref>.	in this paper the <pos>sophisticated</pos> parser rasp toolkit 2 <ref>briscoe et al, 2006</ref> was utilized to extract this <pos>kind</pos> of word relations.	we use the following example for illustration purposes: the library has a <pos>large</pos> collection of <pos>classic</pos> books by such authors as herrick and shakespeare.
before explaining the following process clustering 2, let us describe the measure used to calculate the similarity between syntactic positions.	we use a particular weighted version of the <tref>lin 1998</tref> coefficient.	our version, however, does not use pointwise mutual information to characterize the weight on position-word pairs.	as manning and schu tze 1999 argued, this does not seem to be a <pos>good</pos> measure of the strength of association between a word and a local position.
4 representing syntactic information since both the class-word and the class-example methods work with syntactic features, the main source of information is a syntactically parsed corpus.	we parsed about half a gigabyte of a news corpus with minipar <tref>lin, 1998b</tref>.	it is a statistically based dependency parser which is reported to reach 89 <pos>precision</pos> and 82 recall on <pos>press</pos> reportage texts.	minipar generates syntactic dependency structures directed labeled graphs whose 20 g1 g2 syntnetg1,g2 loves1 s d15d15 o d37d37d74d74d74 d74d74d74d74 d74d74d74d74 loves4 o d47d47 s d15d15 jane6 loves1,4 1,24,5 d15d15 4,6 o d47d47 1,3 o d42d42d84d84d84d84d84d84d84d84d84 d84d84d84d84d84d84 d84d84 jane6 john2 mary3 john5 john2,5 mary3 figure 2: two syntactic graphs and their syntactic network.
as a baseline, we implemented the non-referential it detector of <ref>lappin and leass 1994</ref>, labelled as ll in the results.	this is a syntactic detector, a point missed by <ref>evans 2001</ref> in his criticism: the patterns are <pos>robust</pos> to intervening words and modi ers eg  it was never thought by the committee that  provided the sentence is parsed correctly7 we automatically parse sentences with minipar, a broad-coverage dependency parser <tref>lin, 1998b</tref>.	we also use a separate, extended version of the ll detector, implemented for large-scale nonreferential detection by <ref>cherry and bergsma 2005</ref>.	this system, also for minipar, additionally detects instances of it labelled with minipars pleonastic category subj.
our approach avoids hand-crafting a set of spe11 ci c indicator features; we simply use the distribution of the pronouns context.	our method is thus related to previous work based on <ref>harris 1985</ref>s distributional hypothesis2 it has been used to determine both word and syntactic path similarity <ref>hindle, 1990</ref>; <tref>lin, 1998a</tref>; <ref>lin and pantel, 2001</ref>.	our work is part of a trend of extracting other <pos>important</pos> information from statistical distributions.	<ref>dagan and itai 1990</ref> use the distribution of a pronouns context to determine which candidate antecedents can  t the context.
these definitions are valid in the context of particular applications; however, in general, the correspondence between paraphrasing and types of lexical relations is not clear.	the same question arises with automatically constructed thesauri <ref>pereira et al , 1993</ref>; <tref>lin, 1998</tref>.	while the extracted pairs are indeed similar, they are not paraphrases.	for example, while dog and cat are recognized as the most similar concepts by the method described in <tref>lin, 1998</tref>, it is <neg>hard</neg> to imagine a context in which these words would be interchangeable.
while the extracted pairs are indeed similar, they are not paraphrases.	for example, while dog and cat are recognized as the most similar concepts by the method described in <tref>lin, 1998</tref>, it is <neg>hard</neg> to imagine a context in which these words would be interchangeable.	the first attempt to derive paraphrasing rules from corpora was undertaken by <ref>jacquemin et al , 1997</ref>, who investigated morphological and syntactic variants of technical terms.	while these rules achieve high accuracy in identifying term paraphrases, the techniques used have not been extended to other types of paraphrasing yet.
for this reason, knowledge-poor approaches such as the distributional approach are particularly suited for this task.	its previous applications eg , <ref>grefenstette 1993</ref>, <ref>hearst and schuetze 1993</ref>, <ref>takunaga et al 1997</ref>, <tref>lin 1998</tref>, <ref>caraballo 1999</ref> demonstrated that cooccurrence statistics on a target word is often sufficient for its automatical classification into one of numerous classes such as synsets of wordnet.	distributional techniques, however, are <neg>poorly</neg> applicable to rare words, ie, those words for which a corpus does not contain enough cooccurrence data to judge about their meaning.	such words are the primary <neg><neg>concern</neg></neg> of many practical nlp applications: as a rule, they are semantically focused words and carry a lot of important information.
we also extensively investigated other corpusbased features, such as the number of times the phrase occurred hyphenated or capitalized, and the 4we exclude counts from the training, development, and testing queries discussed in section 41.	822 corpus-based distributional similarity <tref>lin, 1998</tref> between a pair of tokens.	these features are not available from search-engine statistics because search engines disregard punctuation and capitalization, and collecting page-count-based distributional similarity statistics is computationally infeasible.	unfortunately, none of the corpus-based features <pos>improved</pos> performance on the development set and are thus excluded from further consideration.
1 thesaurus creation over the last ten years, <pos><pos>interest</pos></pos> has been growing in distributional thesauruses hereafter simply thesauruses.	following initial work by <ref>sparck jones, 1964</ref> and <ref>grefenstette, 1994</ref>, an early, online distributional thesaurus presented in <tref>lin, 1998</tref> has been widely used and cited, and numerous authors since have explored thesaurus properties and parameters: see survey component of <ref>weeds and weir, 2005</ref>.	a thesaurus is created by  taking a corpus  identifying contexts for each word  identifying which words share contexts.	for each word, the words that share most contexts according to some statistic which also takes account of their frequency are its nearest neighbours.
22 compound similarity as a <neg>critical</neg> technique, word similarity is generally used in the example-based models of semantic classification.	the measure of word similarity can be <neg>divided</neg> into two major approaches: taxonomy-based lexical approach <ref>resnik 1995</ref>, <tref>lin 1998a</tref>, <ref>chen and chen 1998</ref> and context-based syntactic approach <tref>lin 1998b</tref>,<ref>chen and you 2002</ref>, which is not the <neg><neg>concern</neg></neg> in this context-free model.	however, two <neg>problems</neg> arise here for the taxonomy-based lexical approach.	first, such similarity measures <neg>risk</neg> the <neg>failure</neg> to capture the similarity among some semantically highly related words, if they happen to be put under classes distant from each other according to a specific ontology4.
we then calculate normalized tuple similarity scores over the tuple pairs using a metric that accounts for similarities in both syntactic structure and <pos>content</pos> of each tuple.	a thesaurus constructed from corpus statistics <tref>lin, 1998</tref> is utilized for the <pos>content</pos> similarity.	we utilize this metric to greedily pair together the most similar predicate argument tuples across figure 2: system architecture sentences.	any remaining unpaired tuples represent extra information and are passed to a dissimilarity classi er to decide whether such information is signi cant.
<neg>although</neg> the 24 msr corpus used <neg>strict</neg> means of resolving interrater disagreements during its construction, the annotators agreed with the msr corpus labels only 935 187/200 of the time.	one <neg>weakness</neg> of our system is that we rely on a thesaurus <tref>lin, 1998</tref> for word similarity information for predicate <neg>argument</neg> tuple pairing.	however, it is designed to provide similarity scores between pairs of individual words rather than phrases.	if a predicate <neg><neg>argument</neg></neg> tuples target or one argument is realized as a phrase borrow  check out, for instance, the thesaurus is <neg>unable</neg> to provide an accurate similarity score.
we collected the statistics on the grammatical relations contexts output by minipar and used these as the feature vectors.	<ref>following lin 1998</ref>, we measure each feature f for a word e not by its frequency but by its pointwise mutual information, mi ef : 126    fpep fep mi ef  , log 4 inducing ontological features the resource described in the previous section yields lexical feature vectors for each word in a corpus.	we term these vectors lexical because they are collected by looking only at the lexicals in the text ie no <pos><pos>sense</pos></pos> information is used.	we use the term ontological feature vector to refer to a feature vector whose features are for a particular <pos><pos>sense</pos></pos> of the word.
summationtext lh scorel openclasswordsh 2 2we set the threshold to 001 3the <pos>active</pos> verbal form with direct modifiers where scorel is 1 if it appears in p, or if it is a derivation of a word in p according to wordnet.	otherwise, scorel is the maximal lin dependency-basedsimilarityscorebetweenlandthe lemmas of p <tref>lin, 1998a</tref> synonyms and hypernyms/hyponyms are handled by the lexical rules.	7 system implementation deriving the initial propositions t and h from the input text fragments consists of the following steps: i anaphora resolution, using the mars system <ref>mitkov et al , 2002</ref>.	each anaphor was replaced by its antecedent.
figure 1: application of inference rules.	pos and relation labels are based on minipar <tref>lin, 1998b</tref> if a complete proof is found h was generated, the prover concludes that entailment holds.	otherwise, entailment is determined by comparing the minimal cost found during the proof search to some threshold .	3 proof system <pos>like</pos> logic-based systems, our proof system consists of propositions t, h, and intermediate premises, and inference entailment rules, which derive new propositions from previously <pos>established</pos> ones.
as a <pos>striking</pos> example, the 14 most syntactically similar verbs to believe in order are think, guess, <pos><pos>hope</pos></pos>, feel, <pos>wonder</pos>, theorize, fear, reckon, contend, suppose, <pos>understand</pos>, know, doubt, and <pos>suggest</pos>  all mental action verbs.	this observation further supports the distributional hypothesis of word similarity and corresponding technologies for identifying synonyms by similarity of lexical-syntactic context <tref>lin, 1998</tref>.	verb pairs instances cosine bind 83 bound 95 0950 plunge 94 tumble 87 0888 dive 36 plunge 94 0867 dive 36 tumble 87 0866 jump 79 tumble 87 0865 fall 84 fell 102 0859 intersperse 99 perch 81 0859 assail 100 chide 98 0859 dip 81 fell 102 0858 buffet 72 embroil 100 0856 embroil 100 lock 73 0856 embroil 100 superimpose 100 0856 fell 102 jump 79 0855 fell 102 tumble 87 0855 embroil 100 whipsaw 63 0850 pluck 100 whisk 99 0849 <pos>acquit</pos> 100 hospitalize 99 0849 disincline 70 obligate 94 0848 jump 79 plunge 94 0848 dive 36 jump 79 0847 assail 100 lambaste 100 0847 festoon 98 strew 100 0846 mar 78 whipsaw 63 0846 pluck 100 whipsaw 63 0846 ensconce 101 whipsaw 63 0845 table 2.	<pos>top</pos> 25 most syntactically similar pairs of the 3257 verbs in propbank.
our approach is strictly empirical; the similarity of verbs is determined by examining the syntactic contexts in which they appear in a <pos>large</pos> text corpus.	our approach is analogous to previous work in extracting collocations from <pos>large</pos> text corpora using syntactic information <tref>lin, 1998</tref>.	in our work, we utilized the gigaword corpus of english newswire text linguistic <ref>data consortium, 2003</ref>, consisting of nearly 12 gigabytes of textual data.	to prepare this corpus for analysis, we extracted the body text from each of the 41 million entries in the corpus and applied a maximum-entropy algorithm to identify sentence boundaries <ref>reynar and ratnaparkhi, 1997</ref>.
there have been several attempts to group wordnet senses using various different types of information sources.	this paper describes work to automatically relate wordnet word senses using automatically acquired thesauruses <tref>lin, 1998</tref> and wordnet similarity measures <ref>patwardhan and pedersen, 2003</ref>.	this work proposes using graded word <pos><pos>sense</pos></pos> relationships rather than fixed groupings clusters.	previous research has focused on clustering wordnet senses into groups.
this is transformed from a distance measure in the wn-similarity package by taking the reciprocal: jcns1,s2  1/djcns1,s2 we use raw bnc data for calculating ic <pos>values</pos>.	dist we use a distributional similarity measure <tref>lin, 1998</tref> to obtain a fixed number 50 of the <pos>top</pos> ranked nearest neighbours for the target nouns.	for input we used grammatical relation data extracted using an automatic parser <ref>briscoe and carroll, 2002</ref>.	we used the 90 million words of written english from the british national corpus bnc <ref>leech, 1992</ref>.
the number of <pos>unique</pos> collocations in the resulting database 2 is about 11 million.	using the similarity measure proposed in <tref>lin, 1998</tref>, we constructed a corpus-based thesaurus 3 consisting of 11839 nouns, 3639 verbs and 5658 adjective/adverbs which occurred in the corpus at least 100 times.	3 mutual information of a collocation we define the probability space to consist of all possible collocation triples.	we use lh r m l to denote the 1 available at http://wwwcsumanitobaca/-lindek/miniparhtm/ 2available at http://wwwcsumanitobca/-lindek/nlldemohtm/ 3available at http://wwwcsumanitobaca/-lindek/nlldemohtm/ 317 frequency count of all the collocations that match the pattern h r m, where h and m are either words or the wild card  and r is either a dependency type or the wild card.
to measure the compositionality, semantically similar words are more <pos>suitable</pos> than synomys.	hence, we choose to use lins thesaurus <tref>lin, 1998</tref> instead of wordnet <ref>miller et al , 1990</ref>.	902 614 distributed frequency of object a0  the distributed frequency of object is based on the idea that if an object appears only with one verb or few verbs in a <pos>large</pos> corpus, the collocation is expected to have idiomatic nature <ref>tapanainen et al , 1998</ref>.	for example, <pos>sure</pos> in make sure occurs with very few verbs.
the higher the <pos>value</pos> of a38, the more is the likelihood of the collocation to be a mwe.	2obtained from lins <tref>lin, 1998</tref> automatically generated thesaurus http://wwwcsualbertaca/a66 lindek/downloadshtm.	we obtained the <pos>best</pos> results section 8 when we substituted top-5 similar words for both the verb and the object.	to measure the compositionality, semantically similar words are more <pos>suitable</pos> than synomys.
as a representative of this approach we use lins dependency-baseddistributionalsimilaritydatabase.	lins database was created using the particular distributionalsimilaritymeasurein<tref>lin, 1998</tref>, applied to a <pos>large</pos> corpus of news data 64 million words 4.	two words obtain a high similarity score if they occur often in the same contexts, as captured by syntactic dependency relations.	for example, two verbs willbeconsideredsimilariftheyhavelargecommon sets of modifying subjects, objects, adverbs etc distributional similarity does not capture directly meaning equivalence and entailment but rather a looser notion of meaning similarity <ref>geffet and dagan, 2005</ref>.
the setting allowed us to analyze different types of state of the art models and their behavior with <pos><pos>respect</pos></pos> to characteristic sub-cases of the problem.	the major conclusion that seems to arise from our experiments is the <pos>effectiveness</pos> of combining a knowledge based thesaurus such as wordnet with distributional statistical information such as <tref>lin, 1998</tref>, overcoming the known deficiencies of each method alone.	furthermore, modeling the a priori substitution likelihood captures the majority of cases in the evaluated setting, mostly because wordnet provides a rather noisy set of substitution candidates.	on the other hand, <pos>successfully</pos> incorporating local and global contextual information, as similar to wsd methods, remains a challenging task for future research.
the method we use to predict the rst <pos><pos>sense</pos></pos> is that of mccarthy et al.	2004, which was obtained using a thesaurus automatically created from the british national corpus bnc applying the method of <tref>lin 1998</tref>, coupled with wordnetbased similarity measures.	this method is fully unsupervised and completely unreliant on any annotations from our dataset.	in the case of sfs, we perform full synset wsd based on one of the <pos>above</pos> options, and then map the prediction onto the corresponding <pos>unique</pos> sf.
a noun, a4, is thus described by a set of co-occurrence triples a94 a4a7a14 a55 a14a32a95a97a96 and associated frequencies, where a55 is a grammatical relation and a95 is a possible cooccurrence with a4 in that relation.	for every pair of nouns, where each noun had a total frequency in the triple data of 10 or more, we computed their distributional similarity using the measure given by <tref>lin 1998</tref>.	if a98a56a30a31a4 a33 is the set of co-occurrence types a30 a55 a14a16a95 a33 such that a99a100a30a42a4a43a14 a55 a14a32a95 a33 is <pos>positive</pos> then the similarity between two nouns, a4 and a10, can be computed as: a26a41a28a15a28a27a30a42a4a43a14a16a10 a33 a8 a75 a83a102a101a42a103a50 a85 a70a11a104 a83a102a6a13a85a106a105 a104 a83 a67 a85 a30a78a99a100a30a31a4a7a14 a55 a14a16a95 a33a41a107 a99a108a30a31a10a109a14 a55 a14a16a95 a33a86a33 a75 a83a102a101a42a103a50 a85 a70a11a104 a83a102a6a13a85 a99a108a30a31a4a7a14 a55 a14a32a95 a33a45a107 a75 a83a84a101a42a103a50 a85 a70a11a104 a83 a67 a85 a99a108a30a31a10a109a14 a55 a14a16a95 a33 where: a99a108a30a31a4a7a14 a55 a14a32a95 a33 a8a111a110a21a112a114a113 a54 a30a31a95a73a115a116a4a118a117 a55 a33 a54 a30a42a95a73a115 a55 a33 a thesaurus entry of size a3 for a target noun a4 is then defined as the a3 most similar nouns to a4  22 the wordnet similarity package we use the wordnet similarity package 005 and wordnet version 16.	2 the wordnet similarity package supports a range of wordnet similarity scores.
we describe some related work in section 6 and conclude in section 7.	in order to find the predominant <pos><pos>sense</pos></pos> of a target word we use a thesaurus acquired from automatically parsed text based on the method of <tref>lin 1998</tref>.	this provides the a3 nearest neighbours to each target word, along with the distributional similarity score between the target word and its neighbour.	we then use the wordnet similarity package <ref>patwardhan and pedersen, 2003</ref> to give us a semantic similarity measure hereafter referred to as the wordnet similarity measure to weight the <pos>contribution</pos> that each neighbour makes to the various senses of the target word.
let a28a15a34a20a10a18a28a20a34a15a28a25a30a31a4 a33 be the set of <pos><pos>sense</pos></pos>s of a4  for each sense of a4 a4a35a28a37a36a39a38a40a28a20a34a15a10a13a28a15a34a20a28a27a30a31a4 a33  we obtain a ranking score by summing over the a26a41a28a15a28a27a30a42a4a43a14a16a10a45a44 a33 of each neighbour a10a46a44a47a38a48a5 a6  multiplied by a weight.	this weight is the wordnet similarity score a4a49a10a13a28a15a28  between the target <pos><pos><pos><pos><pos>sense</pos></pos></pos></pos></pos> a4a35a28a37a36  and the sense of a10a45a44 a10a13a28a37a50a51a38a52a28a15a34a15a10a13a28a20a34a15a28a27a30a42a10a45a44 a33  that maximises this score, divided by the sum of all such wordnet similarity scores for a28a20a34a15a10a13a28a15a34a20a28a27a30a31a4 a33 and a10a46a44  thus we rank each sense a4a49a28 a36 a38a53a28a15a34a20a10a18a28a20a34a15a28a25a30a31a4 a33 using: a54a56a55 a34a15a57a41a58a41a59a60a34a15a10a13a61a37a34a63a62a64a61a37a65 a55 a34a27a30a31a4a35a28a37a36 a33 a8 a66 a67a69a68a32a70a27a71a73a72 a26a29a28a20a28a27a30a31a4a7a14a32a10 a44 a33a15a74 a4a49a10a13a28a20a28a27a30a31a4a35a28a37a36a42a14a32a10a46a44 a33 a75 a6a18a76a78a77a80a79 a70 a76a82a81 a67 a76a78a81a82a76a42a83a84a6a18a85 a4a35a10a13a28a15a28a25a30a31a4a49a28 a36 a79 a14a16a10a45a44 a33 1 where: a4a49a10a13a28a20a28a27a30a31a4a35a28a37a36a86a14a16a10a45a44 a33 a8 a87a89a88a20a90 a67 a76a92a91 a70 a76a78a81 a67 a76a78a81a82a76a93a83 a67a69a68 a85 a30a42a4a49a10a13a28a15a28a25a30a31a4a35a28a37a36a42a14a32a10a13a28a37a50 a33a86a33 21 acquiring the automatic thesaurus the thesaurus was acquired using the method described by <tref>lin 1998</tref>.	for input we used grammatical relation data extracted using an automatic parser <ref>briscoe and carroll, 2002</ref>.	for the experiments in sections 3 and 4 we used the 90 million words of written english from the bnc.
by comparing its strength of association measured by pmi with those of its lexical variants.	<ref><pos>like</pos> lin 1999</ref>, we generate lexical variants of the target automatically by replacing either the verb or the noun constituent by a semantically similar word from the automatically-built thesaurus of <tref>lin 1998</tref>.	we then use a standard statistic, the z-score, to calculate fixednesslex: fixednesslexv, n  pmiv, npmistd 2 where pmi is the mean and std the standard deviation over the pmi of the target and all its variants.	fixednesssyn quantifies the degree of syntactic fixedness of the target combination, by comparing its behaviour in text with the behaviour of a typical verbobject, both defined as probability distributions over a predefined set of patterns.
we seek to <neg>overcome</neg> these <neg>difficulties</neg> by generating toefl-like tests automatically from wordnet <ref>fellbaum, 1998</ref>.	while wordnet has been used before to evaluate corpus-analytic approaches to lexical similarity <tref>lin, 1998</tref>, the metric proposed in that study, while useful for comparative purposes, lacks an intuitive interpretation.	in contrast, we emulate the toefl using wordnet and inherit the toefls easy interpretability.	given a corpus, we first derive a list of words occurring with sufficient <neg>marginal</neg> frequency to support a distributional comparison.
et al.	2004 we use a3a5a4a7a6a9a8 and obtain our thesaurus using the distributional similarity metric described by <tref>lin 1998</tref>.	we use wordnet wn as our <pos><pos>sense</pos></pos> inventory.	the <pos><pos><pos>sense</pos></pos></pos>s of a worda2 are each assigned a ranking score which sums over the distributional similarity scores of the neighbours and weights each neighbours score by a wn similarity score <ref>patwardhan and pedersen, 2003</ref> between the sense of a2 and the sense of the neighbour that maximises the wn similarity score.
section 5 reports on the trade-off between the minimum cutoff and execution time.	early experiments in thesaurus extraction <ref>grefenstette, 1994</ref> suffered from the limited size of available corpora, but more recent experiments have used much larger corpora with greater <pos>success</pos> <tref>lin, 1998a</tref>.	for these experiments we ran our relation extractor over the british national corpus bnc consisting of 114 million words in 62 million sentences.	the pos tagging and chunking took 159 minutes, and the relation extraction took an addisetcosine jwm; ; wn; ; jpjw m; ; j jwn; ; j cosine p r;w0 wgtwm; r; w0 wgtwn; r; w0pp wgtwm; ; 2 pwgtwn; ; 2 setdice 2jwm; ; wn; ; jjwm; ; jjwn; ; j dice p r;w0 wgtwm; r; w0 wgtwn; r; w0p r;w0 wgtwm; r; w0wgtwn; r; w0 dicey 2 p r;w0 minwgtwm; r; w0;wgtwn; r; w0p r;w0 wgtwm; r; w0wgtwn; r; w0 setjaccard jwm; ; wn; ; jjwm; ; wn; ; j jaccard p r;w0 minwgtwm; r; w0;wgtwn; r; w0p r;w0 maxwgtwm; r; w0;wgtwn; r; w0 jaccardy p r;w0 wgtwm; r; w0 wgtwn; r; w0p r;w0 wgtwm; r; w0wgtwn; r; w0 lin p r;w0 wgtwm; r; w0wgtwn; r; w0p wgtwm; ; pwgtwn; ;  table 1: measure functions evaluated tional 7:5 minutes.
the list of measure and weight functions we compared against is not complete, and we <pos><pos>hope</pos></pos> to add other functions to provide a general framework for thesaurus extraction experimentation.	we would also <pos>like</pos> to expand our evaluation to include direct methods used by others <tref>lin, 1998a</tref> and using the extracted thesaurus in nlp tasks.	we have also investigated the speed/performance trade-off using frequency cutoffs.	this has lead to the proposal of a new approximate comparison algorithm based on canonical attributes and a process of coarseand ne-grained comparisons.
these experiments also cover a range of weight functions as de ned in table 2.	the weight functions lin98a, lin98b, and gref94 are taken from existing systems <tref>lin, 1998a</tref>; <tref>lin, 1998b</tref>; <ref>grefenstette, 1994</ref>.	our proposed weight functions are <pos>motivated</pos> by our intuition that highly predictive attributes are <pos>strong</pos> collocations with their terms.	thus, we have implemented many of the statistics described in the collocations chapter of manning and schcurrency1utze 1999, including the t-test, 2-test, likelihood ratio, and mutual information.
the resultant representation contained a total of 28 million relation occurrences over 10 million different relations.	we describe the functions evaluated in these experiments using an extension of the asterisk notation used by <tref>lin 1998a</tref>, where an asterisk indicates a set ranging over all existing <pos>values</pos> of that variable.	for example, the set of attributes of the term w is: w; ;  fr;w0j9w;r;w0g for convenience, we further extend the notation for weighted attribute vectors.	a subscripted asterisk indicates that the variables are bound together: x r;w0 wgtwm; r; w0 wgtwn; r; w0 which is a notational abbreviation of: x r;w02wm; ; wn; ;  wgtwm;r;w0 wgtwn;r;w0 for weight functions we use similar notation: f w; ;  x r;w02w; ;  f w;r;w0 nw; ;  jw; ; j nw jfwj9w; ; ,;gj table 1 de nes the measure functions evaluated in these experiments.
431 corpus-based lexical similarity lexical similarity was computed using the word sketch engine wse <ref>killgarrif et al , 2004</ref> similarity metric applied over british national corpus.	the wse similarity metric implements the word similarity measure based on grammatical relations as defined in <tref>lin, 1998</tref> with minor modifications.	432 the brandeis semantic ontology as a second source of lexical <pos>coherence</pos>, we used the brandeis semantic ontology or bso <ref>pustejovsky et al , 2006</ref>.	the bso is a lexicallybased ontology in the generative lexicon <pos>tradition</pos> <ref>pustejovsky, 2001</ref>; <ref>pustejovsky, 1995</ref>.
<ref>mihalcea and moldovan, 2001</ref> implements six semantic rules, using twin and autohyponym features, in addition to other wordnet-structure-based rules such as whether two synsets share a pertainym, antonym, or are clustered together in the same verb group.	a large body of work has attempted to capture corpus-based estimates of word similarity <ref>pereira et al , 1993</ref>; <tref>lin, 1998</tref>; however, the <neg>lack</neg> of large sense-tagged corpora prevent most such techniques from being used effectively to compare different senses of the same word.	some corpus-based attempts that are capable of estimating similarity between word senses include the topic signatures method; here, <ref>agirre and lopez, 2003</ref> collect contexts for a polysemous word based either on sensetagged corpora or by using a weighted agglomeration of contexts of a polysemous words monosemous relatives ie , single-sense synsets related by hypernym, hyponym, or other relations from some large untagged corpus.	other corpus-based techniques developed specifically for sense clustering include <ref>mccarthy, 2006</ref>, which uses a combination of word-to-word distributional similarity combined with the jcn wordnet-based similarity measure, and work by <ref>chugur et al , 2002</ref> in finding co-occurrences of senses within documents in sense-tagged corpora.
3 <pos>learning</pos> to merge word senses 31 wordnet-based features here we describe the feature space we construct for classifying whether or not a pair of synsets should be merged; first, we employ a <pos>wide</pos> variety of linguistic features based on information derived from wordnet.	we use eight similarity measures implemented within the wordnet::similarity package5, described in <ref>pedersen et al , 2004</ref>; these include three measures derived from the paths between the synsets in wordnet: hso hirst and st-<ref>onge, 1998</ref>, lch <ref>leacock and chodorow, 1998</ref>, and wup <ref>wu and palmer, 1994</ref>; three measures based on information <pos>content</pos>: res <ref>resnik, 1995</ref>, lin <tref>lin, 1998</tref>, and jcn <ref>jiang and conrath, 1997</ref>; the gloss-based extended lesk measure lesk, <ref>banerjee and pedersen, 2003</ref>, and finally the gloss vector similarity measure vector <ref>patwardan, 2003</ref>.	we implement the twin feature <ref>peters et al , 1998</ref>, which counts the number of shared synonyms between the two synsets.	additionally we produce pairwise features indicating whether two senses share an antonym, pertainym, or derivationally-related forms deriv.
our thesaurus brings up only alternatives that have the same part-of-speech with the target word.	the choices could come from various inventories of near-synonyms or similar words, for example the roget thesaurus roget, 1852, dictionaries of synonyms <ref>hayakawa, 1994</ref>, or clusters acquired from corpora <tref>lin, 1998</tref>.	in this paper we focus on the task of automatically selecting the <pos>best</pos> near-synonym that should be used in a particular context.	the <pos>natural</pos> way to validate an algorithm for this task would be to ask human readers to evaluate the quality of the algorithms output, but this <pos>kind</pos> of evaluation would be very laborious.
for more details on hypernym collocations, see ohara, forthcoming.	word-similarity classes <tref>lin, 1998</tref> derived from clustering are also used to expand the pool of <pos>potential</pos> collocations; this type of semantic relatedness among words is expressed in the similarcoll feature.	for the dictcoll features, definition analysis ohara, forthcoming is used to determine the semantic relatedness of the defining words.	dierences between these two sources of word relations are illustrated by looking at the information they provide for ballerina: word-clusters: dancer:0115 baryshnikov:0072 pianist:0056 choreographer:0049  18 other words nicole:0041 wrestler:0040 tibetans:0040 clown:0040 definition words: dancer:00013 female:00013 ballet:00004 this shows that word clusters capture a wider range of relatedness than the dictionary definitions at the expense of incidental associations eg , nicole.
again, because context words are not disambiguated, the relations for all senses of a context word are conflated.	for details on the extraction of word clusters, see <tref>lin, 1998</tref>; and, for details on the definition analysis, see ohara, forthcoming.	when formulating the features similarcoll and dictcoll, the words related to each context word are considered as <pos>potential</pos> collocations <ref>wiebe et al , 1998</ref>.	co-occurrence fresense distinctions <pos>precision</pos> recall fine-grained 566 565 course-grained 660 658 table 1: results for senseval-3 test data.
a set of seed words begins the process.	for each seed s i, the precision of the set s i c i,n in the training data is calculated, where c i,n is the set of n words most similar to s i, according to <tref>lins 1998</tref> method.	if the <pos>precision</pos> of s i c i,n is greater than a threshold t, then the words in this set are retained as pses.	if it is not, neither s i nor the words in c i,n are retained.
many variants of distributional similarity have been used in nlp <ref>lee 1999</ref>; <ref>lee and pereira 1999</ref>.	<tref>dekang lins 1998</tref> method is used here.	in contrast to many implementations, which focus exclusively on verb-noun relationships, lins method incorporates a variety of syntactic relations.	this is <pos>important</pos> for subjectivity <pos>recognition</pos>, because pses are not limited to verb-noun relationships.
the method is then used to identify an unusual form of collocation: one or more positions in the collocation may be filled by any word of an <pos>appropriate</pos> part of speech that is <pos>unique</pos> in the test data.	the third type of subjectivity clue we examine here are adjective and verb features identified using the results of a method for clustering words according to distributional similarity <tref>lin 1998</tref> section 34.	we hypothesized that two words may be distributionally similar because they are both potentially subjective eg , tragic, sad, and <pos>poignant</pos> are identified from bizarre.	in addition, we use distributional similarity to <pos>improve</pos> estimates of unseen events: a word is selected or discarded based on the <pos>precision</pos> of it together with its n most similar neighbors.
we also presented a procedure for automatically identifying potentially subjective collocations, including fixed collocations and collocations with placeholders for <pos>unique</pos> words.	in addition, we used the results of a method for clustering words according to distributional similarity <tref>lin 1998</tref> to identify adjectival and verbal clues of subjectivity.	table 9 summarizes the results of testing all of the <pos>above</pos> types of pses.	all show increased <pos>precision</pos> in the evaluations.
typical feature weighting functions include the logarithm of the frequency of word-feature cooccurrence <ref>ruge, 1992</ref>, and the conditional probability of the feature given the word within probabilistic-based measures <ref>pereira et al , 1993</ref>, <ref>lee, 1997</ref>, <ref>dagan et al , 1999</ref>.	probably the most widely used association weight function is point-wise mutual information mi <ref>church et al , 1990</ref>, <ref>hindle, 1990</ref>, <tref>lin, 1998</tref>, <ref>dagan, 2000</ref>, defined by:  ,log, 2 fpwp fwpfwmi  a known <neg>weakness</neg> of mi is its tendency to assign high weights for rare features.	yet, similarity measures that utilize mi showed good performance.	in particular, a common practice is to filter out features by minimal frequency and weight thresholds.
we picked the widely cited and <pos>competitive</pos> eg.	<ref>weeds and weir, 2003</ref> measure of <tref>lin 1998</tref> as a representative case, and utilized it for our analysis and as a starting point for <pos>improvement</pos>.	21 lins 98 similarity measure lins similarity measure between two words, w and v, is defined as follows:,,, ,, ,           fvweightfwweight fvweightfwweight vwsim vffwff vfwff where fw and fv are the <pos>active</pos> features of the two words and the weight function is defined as mi.	a feature is defined as a pair <term, syntaccountrystate ranks countryeconomy ranks broadcast goods civilservant bloc nonaligned neighboring statistic border northwest 24 140 64 30 55 15 165 10 41 50 16 54 77 60 165 43 247 174 devastate developed dependent industrialized shattered club black million electricity 81 36 101 49 16 155 122 31 130 8 78 26 85 141 38 109 245 154 table 3: the top-10 common features for the word pairs country-state and country-economy, along with their corresponding ranks in the sorted feature lists of the two words.
2004 automatically determine domainspecific predominant senses of words, where the domain may be specified in the form of an untagged target text or simply by name for example, financial domain.	the system figure 1 automatically generates a thesaurus <tref>lin, 1998</tref> using a measure of distributional similarity and an untagged corpus.	the target text is used for this purpose, provided it is <pos>large</pos> enough to learn a thesaurus from.	otherwise a <pos>large</pos> corpus with <pos><pos>sense</pos></pos> distribution similar to the target text text pertaining to the specified domain must be used.
this requires large amounts of partof-speech-tagged and chunked data from that domain.	further, the target text must be large enough to learn a thesaurus from <tref>lin 1998</tref> used a 64million-word corpus, or a large auxiliary text with a sense distribution similar to the target text must be provided mccarthy et al.	2004 separately used 90-, 325-, and 91-million-word corpora.	by contrast, in this paper we present a method that accurately determines sense <neg>dominance</neg> even in relatively small amounts of target text a few hundred sentences; <neg>although</neg> it does use a corpus, it does not require a similarly-sense-distributed corpus.
2003.	typical relation-constrained dps are those of <tref>lin 1998</tref> and <ref>lee 2001</ref>.	below are contrived, but <pos>plausible</pos>, examples of each for the word pulse; the numbers are conditional probabilities.	relation-free dp pulse: beat 28, racing 2, grow 13, beans 09, <pos>heart</pos> 04,   .
the distance between two words, given their dps, is calculated using a measure of dp distance, such as cosine.	while any of the measures of dp distance may be used with any of the measures of strength of association see table 1, in practice -skew divergence asd, cosine, and jensenshannon divergence jsd are used with conditional probability cp, whereas lin is used with pmi, resulting in the distributional measures asd cp <ref>lee, 2001</ref>, cos cp <ref>schutze and pedersen, 1997</ref>, jsd cp,andlin pmi <tref>lin, 1998</tref>, respectively.	asd cp is a modification of kullback-leibler divergence that overcomes the latters problem of division by zero, which can be caused by data sparseness.	jsd cp is another relative entropybased measure <pos>like</pos> asd cp  but it is symmetric.
in the simplest case, the features of a word are de ned as the contexts in which it has been seen to occur.	simjami is a variant <tref>lin, 1998</tref> in which the features of a word are those contexts for which the pointwise mutual information mi between the word and the context is <pos>positive</pos>, where mi can be calculated using ic;w  log pcjwpc.	the related dice coe cient frakes and baeza-<ref>yates, 1992</ref> is omitted here since it has been shown van <ref>rijsbergen, 1979</ref> that dice and jaccards coe cients are monotonic in each other.	lins measure <tref>lin, 1998</tref> is based on his information-theoretic similarity theorem, which states, the similarity between a and b is measured by the ratio between the amount of information needed to state the commonality of a and b and the information needed to fully describe what a and b are.
other potential applications apply the hypothesised relationship <ref>harris, 1968</ref> between distributional similarity and semantic similarity; ie, similarity in the meaning of words can be predicted from their distributional similarity.	one advantage of automatically generated thesauruses <ref>grefenstette, 1994</ref>; <tref>lin, 1998</tref>; <ref>curran and moens, 2002</ref> over large-scale manually created thesauruses such as wordnet <ref>fellbaum, 1998</ref> is that they might be tailored to a particular genre or domain.	however, due to the <neg>lack</neg> of a tight de nition for the concept of distributional similarity and the broad range of potential applications, a large number of measures of distributional similarity have been proposed or adopted see section 2.	previous work on the evaluation of distributional similarity methods tends to either compare sets of distributionally similar words to a manually created semantic resource <tref>lin, 1998</tref>; <ref>curran and moens, 2002</ref> or be oriented towards a particular task such as language modelling <ref>dagan et al , 1999</ref>; <ref>lee, 1999</ref>.
however, due to the <neg>lack</neg> of a tight de nition for the concept of distributional similarity and the broad range of potential applications, a large number of measures of distributional similarity have been proposed or adopted see section 2.	previous work on the evaluation of distributional similarity methods tends to either compare sets of distributionally similar words to a manually created semantic resource <tref>lin, 1998</tref>; <ref>curran and moens, 2002</ref> or be oriented towards a particular task such as language modelling <ref>dagan et al , 1999</ref>; <ref>lee, 1999</ref>.	the rst approach is not ideal since it assumes that the goal of distributional similarity methods is to predict semantic similarity and that the semantic resource used is a valid gold standard.	further, the second approach is clearly advantageous when one wishes to apply distributional similarity methods in a particular application area.
let seenrp be the set of seen headwords for an argument rp of a predicate p then we model the selectional <pos>preference</pos> s of rp for a possible headword w0 as a weighted sum of the similarities between w0 and the seen headwords: srpw0  summationdisplay wseenrp simw0,wwtrpw simw0,w is the similarity between the seen and the <pos>potential</pos> headword, and wtrpw is the weight of seen headword w similarity simw0,w <pos>will</pos> be computed on the generalization corpus, again on the basis of extracted tuples p,rp,w.	we <pos>will</pos> be using the similarity metrics shown in table 1: cosine, the dice and jaccard coefficients, and <ref>hindles 1990</ref> and <tref>lins 1998</tref> mutual information-based metrics.	we write f for frequency, i for mutual information, and rw for the set of arguments rp for which w occurs as a headword.	in this paper we only study corpus-based metrics.
some approaches have used wordnet for the generalization step <ref>resnik, 1996</ref>; <ref>clark and weir, 2001</ref>; <ref>abe and li, 1993</ref>, others em-based clustering <ref>rooth et al , 1999</ref>.	in this paper we propose a new, <pos>simple</pos> model for selectional <pos>preference</pos> induction that uses corpus-based semantic similarity metrics, such as cosine or <tref>lins 1998</tref> mutual informationbased metric, for the generalization step.	this model does not require any manually created lexical resources.	in addition, the corpus for computing the similarity metrics can be freely chosen, allowing greater variation in the domain of generalization than a fixed lexical resource.
the use of synonyms is another way of increasing the coverage of question terminology;; while semantic features try to <pos>achieve</pos> it by generalization, synonyms do it by lexical expansion.	our plan is to use the synonyms obtained from very <pos>large</pos> corpora reported in <tref>lin, 1998</tref>.	we are also planning to compare the lexical and semantic features we derived automatically in this work with manually selected features.	in our previous work, manually selected lexical featuresshowedslightlybetterperformanceforthe training data but no signi cant dierence for the test data.
other approaches usually consider either given sets of synonyms among which one is to be chosen for a translation for instance <ref>edmonds and hirst, 2002</ref> or must choose a synonym word <neg>against</neg> unrelated terms in the context of a synonymy test <ref>freitag et al , 2005</ref>, a seemingly easier task than actually proposing synonyms.	<tref>lin, 1998</tref> proposes a different methodology for evaluation of candidate synonyms, by comparing similarity measures of the terms he provides with the similarity measures between them in wordnet, using various semantic distances.	this makes for very <neg>complex</neg> evaluation procedures without an intuitive interpretation, and there is no assessment of the quality of the automated thesaurus.	6 conclusion we have developed a general method to extract nearsynonyms from a dictionary, improving on the two baselines.
it allows us to identify triple instances.	each triple have the form w1rw2 where w1 and w2 are lexical units and r is a syntactic relation <tref>lin, 1998</tref>; kilgarriff  al 2004.	our approach can be <pos>distinguished</pos> from classical distributional approach by different points.	first, we use triple occurrences to build a distributional space one triple implies two contexts and two lexical units, but we use the transpose of the classical space: each point x i of this space is a syntactical context with the form rw, each dimension j is a lexical units, and each <pos>value</pos> x i j is the frequency of corresponding triple occurrences.
the model is highly general and can be optimised for different tasks.	it extends prior work on syntax-based models <ref>grefenstette, 1994</ref>; <tref>lin, 1998</tref>, by providing a general framework for defining context so that a <pos>large</pos> number of syntactic relations can be used in the construction of the semantic space.	our approach differs from <tref>lin 1998</tref> in three <pos>important</pos> ways: a by introducing dependency paths we can capture non-immediate relationships between words ie , between subjects and objects, whereas lin considers only local context dependency edges in our terminology; the semantic space is therefore constructed solely from isolated head/modifier pairs and their inter-dependencies are not taken into account; b lin creates the semantic space from the set of dependency edges that are <pos>relevant</pos> for a given word; by introducing dependency labels and the path <pos>value</pos> function we can selectively weight the <pos>importance</pos> of different labels eg , subject, object, modifier and parametrize the space accordingly for different tasks; c considerable flexibility is allowed in our formulation for selecting the dimensions of the semantic space; the latter can be words see the leaves in figure 1, parts of speech or dependency edges; in lins approach, it is only dependency edges features in his terminology that form the dimensions of the semantic space.	experiment 1 revealed that the dependency-based model adequately simulates semantic priming.
write a for the lexical association function which computes the <pos>value</pos> of a cell of the matrix from a co-occurrence frequency: ki j  a f bi;t j 3 evaluation 31 parameter settings all our experiments were conducted on the british national corpus bnc, a 100 million word collection of samples of written and spoken language <ref>burnard, 1995</ref>.	we used <tref>lins 1998</tref> <pos>broad</pos> coverage dependency parser minipar to obtain a parsed version of the corpus.	minipar employs a manually constructed grammar and a lexicon derived from wordnet with the addition of <pos>proper</pos> names 130,000 entries in total.	lexicon entries contain part-of-speech and subcategorization information.
it extends prior work on syntax-based models <ref>grefenstette, 1994</ref>; <tref>lin, 1998</tref>, by providing a general framework for defining context so that a <pos>large</pos> number of syntactic relations can be used in the construction of the semantic space.	our approach differs from <tref>lin 1998</tref> in three <pos>important</pos> ways: a by introducing dependency paths we can capture non-immediate relationships between words ie , between subjects and objects, whereas lin considers only local context dependency edges in our terminology; the semantic space is therefore constructed solely from isolated head/modifier pairs and their inter-dependencies are not taken into account; b lin creates the semantic space from the set of dependency edges that are <pos>relevant</pos> for a given word; by introducing dependency labels and the path <pos>value</pos> function we can selectively weight the <pos>importance</pos> of different labels eg , subject, object, modifier and parametrize the space accordingly for different tasks; c considerable flexibility is allowed in our formulation for selecting the dimensions of the semantic space; the latter can be words see the leaves in figure 1, parts of speech or dependency edges; in lins approach, it is only dependency edges features in his terminology that form the dimensions of the semantic space.	experiment 1 revealed that the dependency-based model adequately simulates semantic priming.	experiment 2 showed that a model that relies on <pos>rich</pos> context specifications can <pos>reliably</pos> <pos>distinguish</pos> between different types of lexical relations.
however, at structural level, the concept-based seeds share the same or similar linguistic patterns eg subject-verb-object patterns with the corresponding types of <pos>proper</pos> names.	the rationale behind using concept-based seeds in ne bootstrapping is similar to that for parsingbased word clustering <tref>lin 1998</tref>: conceptually similar words occur in structurally similar context.	in fact, the anaphoric function of pronouns and common nouns to represent antecedent nes indicates the substitutability of <pos>proper</pos> names by the corresponding common nouns or pronouns.	for example, this man can be substituted for the proper name john smith in almost all structural patterns.
distributional similarity score: the gd04 similarity score of the pair was used as a feature.	we 583 also attempted adding lins 1998 similarity scores but they appeared to be <neg>redundant</neg>.	intersection feature: a binary feature indicating candidate pairs acquired by both methods, which was found to indicate higher entailment likelihood.	in summary, the above feature types utilize mutually complementary pattern-based and distributional information.
for the distributional similarity component we employ the similarity scheme of <ref>geffet and dagan, 2004</ref>, which was shown to yield <pos>improved</pos> predictions of non-directional lexical entailment pairs.	this scheme utilizes the symmetric similarity measure of <tref>lin, 1998</tref> to induce <pos>improved</pos> feature weights via bootstrapping.	these weights identify the most characteristic features of each word, yielding cleaner feature vector representations and <pos>better</pos> similarity assessments.	22 pattern-based <ref>approaches hearst 1992</ref> pioneered the use of lexicalsyntactic patterns for automatic extraction of lexical semantic relationships.
the degree of similarity between two target words is then determined by a vector comparison function.	amongst the many proposals for distributional similarity measures, <tref>lin, 1998</tref> is maybe the most widely used one, while <ref>weeds et al , 2004</ref> provides a typical example for recent research.	distributional similarity measures are typically computed through <pos>exhaustive</pos> processing of a corpus, and are therefore applicable to corpora of bounded size.	it was noted recently by geffet and dagan 2004, 2005 that distributional similarity captures a quite loose notion of semantic similarity, as exemplified by the pair country  party identified by lins similarity measure.
the method uses a thesaurus obtained from the text by parsing, extracting grammatical relations and then listing each word w with its <pos>top</pos> k nearest neighbours, where k is a constant.	<pos>like</pos> <ref>mccarthy et al , 2004</ref> we use k  50 and obtain our thesaurus using the distributional similarity metric described by <tref>lin, 1998</tref> and we use wordnet wn as our <pos><pos>sense</pos></pos> inventory.	the <pos><pos><pos>sense</pos></pos></pos>s of a word w are each assigned a ranking score which sums over the distributional similarity scores of the neighbours and weights each neighbours score by a wn similarity score <ref>patwardhan and pedersen, 2003</ref> between the sense of w and the sense of the neighbour that maximises the wn similarity score.	this weight is normalised by the sum of such wn similarity scores between all senses of w and the senses of the neighbour that maximises this score.
those words that obtain the <pos>best</pos> <pos>values</pos> are considered to be most similar.	<pos>practical</pos> implementations of algorithms based on this <pos>principle</pos> have led to <pos>excellent</pos> results as documented in papers by <ref>ruge 1992</ref>, <ref>grefenstette 1994</ref>, <ref>agarwal 1995</ref>, <ref>landauer  dumais 1997</ref>, <ref>schtze 1997</ref>, and <tref>lin 1998</tref>.	21 human data in this section we relate the results of our version of such an algorithm to similarity estimates obtained by human subjects.	<pos>fortunately</pos>, we did not need to conduct our own experiment to obtain the humans similarity estimates.
given the great deal of similar work in information extraction and ontology learning, we focus here only on techniques for weakly supervised or unsupervised semantic class ie, supertype-based learning, since that is most related to the work in this paper.	fully unsupervised semantic clustering eg, <tref>lin, 1998</tref>; <ref>lin and pantel, 2002</ref>; <ref>davidov and rappoport, 2006</ref> has the <neg>disadvantage</neg> that it may or may not produce the types and granularities of semantic classes desired by a user.	another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes eg, <ref>caraballo, 1999</ref>; <ref>cimiano and volker, 2005</ref>; <ref>mann, 2002</ref>, and learning semantic relations such as meronymy <ref>berland and charniak, 1999</ref>; <ref>girju et al, 2003</ref>.	our research focuses on semantic lexicon induction, which aims to generate lists of words that belong to a given semantic class eg, lists of fish or vehicle words.
five part-of-speech features, two lexical features, and a paragraph feature were used.	toidentify richer features, <ref>wiebe, 2000</ref> used lins 1998 method for clustering words according to distributional similarity,seededby a small amount of detailed manual annotation, to automatically identify adjective pses.	there are two parameters of this process, neither of whichwas varied in <ref>wiebe, 2000</ref>: c, the cluster size considered, andft, a lteringthreshold, such that, if the seed word and the words in its cluster have, as a set, lower <pos>precision</pos> than the ltering threshold on the training data, the entire cluster, including the seed word, is ltered out.	this process is adapted for use in the current paper, as described in section 7.
,, iwcount : frequency of the triples including word iw  n: number of triples in the corpus.	we use it instead of point-wise mutual information in <tref>lin 1998</tref> because the latter tends to overestimate the association between two parts with <neg>low</neg> frequencies.	weighted mutual information meliorates this effect by adding , ji attwp  24 combining the three extractors in terms of combining the outputs of the different methods, the ensemble method is a good candidate.	originally, the ensemble method is a machine learning technique of combining the outputs of several classifiers to improve the classification performance <ref>dietterich, 2000</ref>.
the similarity is the sum of the wordnet similarities between all attribute keywords in the two exhibits k1, k2, normalised over the length of both keyword sets: summationtext k1k1 summationtext k2k2 wnsimk1,k2 k1k2 for the purposes of this experiment we have chosen to use three wordnet similarity/relatedness measures to simulate the conceptual connections that visitors make between exhibits.	the lin <tref>lin, 1998</tref> and leacock-chodorow <ref>leacock et al , 1998</ref> similarity measures and the banerjeepedersen <ref>patwardhan and pedersen, 2003</ref> relatedness measures were used.	the similarities were normalised and transformed into probability matrices such that summationtextj pwnsimecj  1 for each next exhibit ci.	the use of wordnet measures is intended to simulate the mental connections that visitors make between exhibit <pos><pos>content</pos></pos>, given that each visit can interpret content in a number of different ways.
using language weavers 1 english-to-spanish machine translation system, english marginal notes can be translated into spanish.	22 vocabulary <pos><pos>support</pos></pos> synonyms for lower frequency more difficult words are output using a statistically-generated word similarity matrix <tref>lin, 1998</tref>.	ata v10 generates antonyms for vocabulary in the text using wordnet .	2 cognates are words which have the same spelling and meaning in two languages eg , animal in english and spanish.
table 1 shows the number of feature types and tokens extracted for each phrase.	this shows that we have extracted a <pos>reasonable</pos> number of features for each phrase, since distributional similarity techniques have been shown to work <pos>well</pos> for words which occur more than 100 times in a given corpus <tref>lin, 1998</tref>; <ref>weeds and weir, 2003</ref>.	we then computed the distributional similarity between each co-occurrence vector using the -skew divergence measure <ref>lee, 1999</ref>.	the -skew divergence measure is an approximation to the kullbackleibler kl divergence meassure between two distributions p and q: dpq  summationdisplay x pxlogpxqx 5we currently retain all of the distinctions between grammatical relations output by rasp.
w ord s imilar w ords  with similarity score  eat cook 0127, drink 0108, consume 0101, feed 0094, <pos>taste</pos> 0093, <pos>like</pos> 0092, serve 0089, bake 0087, sleep 0086, pick 0085, fry 0084, freeze 0081, <pos>enjoy</pos> 0079, smoke 0078, harvest 0076, <pos>love</pos> 0076, chop 0074, sprinkle 0072, toss 0072, chew 0072 salad soup 0172, sandwich 0169, sauce 0152, pasta 0149, dish 0135, vegetable 0135, cheese 0132, dessert 013, entree 0121, bread 0116, meat 0116, chicken 0115, pizza 0114, rice 0112, seafood 011, dressing 0109, cake 0107, steak 0105, noodle 0105, bean 0102 the collocation database for the words eat and salad  the database contains a total of 11 million <pos>unique</pos> dependency relationships.	22 corpus-based thesaurus using the collocation database, <tref>lin 1998b</tref> used an unsupervised method to construct a corpusbased thesaurus consisting of 11839 nouns, 3639 verbs and 5658 adjectives/adverbs.	given a word w, the thesaurus returns a set of similar words of w along with their similarity to w  for example, the 20 most similar words of eat and salad are shown in table 1.	3 training data extraction we parsed a 125-million word newspaper corpus with minipar 3, a descendent of principar <ref>lin, 1994</ref>.
below, we briefly describe these resources.	21 collocation database given a word w in a dependency relationship such as subject or object , the collocation database is used to retrieve the words that occurred in that relationship with w, in a <pos>large</pos> corpus, along with their frequencies <tref>lin, 1998a</tref>.	figure 1 shows excerpts of the entries in 2 available at wwwcsualbertaca/lindek/demoshtm.	eat : object: almond 1, a pple 25, bean 5, beam 1, binge 1, bread 13, cake 17, cheese 8, dish 14, disorder 20, egg 31, grape 12, grub 2, hay 3, junk 1, meat 70, poultry 3, rabbit 4, soup 5, sandwich 18, pasta 7, vegetable 35,  subject: adult 3, animal 8, beetle 1, cat 3, child 41, decrease 1, dog 24, family 29, guest 7, <pos>kid</pos> 22, <pos><pos>patient</pos></pos> 7, refugee 2, rider 1, russian 1, shark 2, something 19, we 239, wolf 5,  salad : adj-modifier: assorted 1, <pos>crisp</pos> 4, <pos>fresh</pos> 13, <pos>good</pos> 3, grilled 5, leftover 3, mixed 4, olive 3, <pos>prepared</pos> 3, side 4, small 6, <pos>special</pos> 5, vegetable 3,  object-of: add 3, consume 1, dress 1, grow 1, harvest 2, have 20, <pos>like</pos> 5, <pos>love</pos> 1, mix 1, pick 1, place 3, prepare 4, return 3, rinse 1, season 1, serve 8, sprinkle 1, <pos>taste</pos> 1, test 1, toss 8, try 3,  figure 1.
eat : object: almond 1, a pple 25, bean 5, beam 1, binge 1, bread 13, cake 17, cheese 8, dish 14, disorder 20, egg 31, grape 12, grub 2, hay 3, junk 1, meat 70, poultry 3, rabbit 4, soup 5, sandwich 18, pasta 7, vegetable 35,  subject: adult 3, animal 8, beetle 1, cat 3, child 41, decrease 1, dog 24, family 29, guest 7, <pos>kid</pos> 22, <pos><pos>patient</pos></pos> 7, refugee 2, rider 1, russian 1, shark 2, something 19, we 239, wolf 5,  salad : adj-modifier: assorted 1, <pos>crisp</pos> 4, <pos>fresh</pos> 13, <pos>good</pos> 3, grilled 5, leftover 3, mixed 4, olive 3, <pos>prepared</pos> 3, side 4, small 6, <pos>special</pos> 5, vegetable 3,  object-of: add 3, consume 1, dress 1, grow 1, harvest 2, have 20, <pos>like</pos> 5, <pos>love</pos> 1, mix 1, pick 1, place 3, prepare 4, return 3, rinse 1, season 1, serve 8, sprinkle 1, <pos>taste</pos> 1, test 1, toss 8, try 3,  figure 1.	excepts of entries in the collocation database for eat and salad  table 1  the <pos>top</pos> 20 most similar words of eat and salad as given by <tref>lin, 1998b</tref>.	w ord s imilar w ords  with similarity score  eat cook 0127, drink 0108, consume 0101, feed 0094, <pos>taste</pos> 0093, <pos>like</pos> 0092, serve 0089, bake 0087, sleep 0086, pick 0085, fry 0084, freeze 0081, <pos>enjoy</pos> 0079, smoke 0078, harvest 0076, <pos>love</pos> 0076, chop 0074, sprinkle 0072, toss 0072, chew 0072 salad soup 0172, sandwich 0169, sauce 0152, pasta 0149, dish 0135, vegetable 0135, cheese 0132, dessert 013, entree 0121, bread 0116, meat 0116, chicken 0115, pizza 0114, rice 0112, seafood 011, dressing 0109, cake 0107, steak 0105, noodle 0105, bean 0102 the collocation database for the words eat and salad  the database contains a total of 11 million <pos>unique</pos> dependency relationships.	22 corpus-based thesaurus using the collocation database, <tref>lin 1998b</tref> used an unsupervised method to construct a corpusbased thesaurus consisting of 11839 nouns, 3639 verbs and 5658 adjectives/adverbs.
three kinds of approaches are prevalent in the literature.	the first kind <ref>wei 1993</ref>; <tref>lin 1998b</tref> is a chiefly theoretical examination of a proposed measure for those mathematical properties thought desirable, such as whether it is a metric or the inverse of a metric, whether it has singularities, whether its parameter-projections are smooth functions, and so on.	in our opinion, such analyses act at best as a <neg>coarse</neg> filter in the comparison of a set of measures and an even coarser one in the assessment of a single measure.	the second kind of evaluation is comparison with human judgments.
each cluster corresponds to a <pos><pos>sense</pos></pos> of the headword.	2 feature representation following <tref>lin 1998</tref>, we represent each word by a feature vector.	each feature corresponds to a context in which the word occurs.	for example, sip  is a verbobject context.
thus, <pos><pos>correct</pos></pos> match of an argument corresponds to correct role identification.	the templates were represented as minipar <tref>lin, 1998b</tref> dependency parse-trees.	the contextual <pos>preferences</pos> for h were constructed manually: the named-entity types for cpv:nh were set by adapting the entity types given in the guidelines to the types supported by the lingpipe ner described in section 32.	cpgh was generated from a short list of nouns and verbs that were extracted from the verbal event definition in the ace guidelines.
as a more <pos>natural</pos> ranking method, we also utilize scbc directly, denoted rankedcbc, having mv:er,t  scbcr,t.	in addition, we tried a simpler method that directly compares the terms in two cpv:e lists, utilizing the commonly-used term similarity metric of <tref>lin, 1998a</tref>.	this method, denoted lin, uses the same raw distributional data as cbc but computes only pair-wise similarities, without any clustering phase.	we calculated the scores of the 1000 most similar terms for every term in the reuters rvc1 corpus3.
first of all, it allows us to provide both <pos>positive</pos> and negative examples, avoiding the use of one-class classification algorithms that in practice perform poorly <ref>dagan et al , 2006</ref>.	second, the <pos>large</pos> availability of manually constructed substitution lexica, such as wordnet <ref>fellbaum, 1998</ref>, or the use of repositories based on statistical word similarities, such as the database constructed by <tref>lin 1998</tref>, allows us to find an <pos>adequate</pos> substitution lexicon for each target word in most of the cases.	for example, as shown in table 1, the word job has different senses depending on its context, some of them entailing its direct hyponym position eg , looking for permanent job, others entailing the word task eg , the job of repairing.	the problem of deciding whether a particular instance of job can be replaced by position, and not by the word place, can be solved by looking for the most similar contexts where either position or place occur in the training data, and then selecting the class ie , the entailed word characterized by the most similar ones, in an instance based style.
<ref>cimiano and volker 2005</ref> assign a particular entity to the fine-grained class suchthatthecontextualsimilarityismaximalamong the set of fine-grained subclasses of a coarse-grained category.	contextual similarity has been measured by adopting lexico-syntactic features provided by a dependency parser, as proposed in <tref>lin, 1998</tref>.	3 instance based lexical entailment dagan et al.	2006 adapted the classical supervised wsd setting to approach the <pos><pos>sense</pos></pos> matching problem ie , the binary lexical entailment problem of deciding whether a word, such as position, entails a different word, such as job, in a given context by defining a one-class <pos>learning</pos> algorithm based on <pos><pos>support</pos></pos> vector machines svm.
it is worthwhile to remark that, due to the <neg>ambiguity</neg> of the entailed words eg , position could also entail either perspective or place, not every occurrence of them should be taken into account, in order to <neg>avoid</neg> <neg>misleading</neg> predictions caused by the <neg>irrelevant</neg> senses.	therefore, approaches based on a more classical contextual similarity technique <tref>lin, 1998</tref>; <ref>dagan, 2000</ref>, where words are described globally by context vectors, are doomed to <neg>fail</neg>.	we will provide empirical evidence of this in the evaluation section.	choosing an appropriate similarity function for the contexts of the words to be substituted is a primary issue.
there are a number of ways to measure the distance between two nouns in the wordnet noun hierarchy see budanitsky 1999 for a review.	in previous work <ref>weeds and weir 2003b</ref>, we used the wordnet-based similarity measure first proposed in <ref>lin 1997</ref> and used in <tref>lin 1998a</tref>: wn sim lin w 1, w 2   max c 1 sw 1 c 2 sw 2  parenleftbigg max csupc 1 supc 2  2logpc log pc 1   log pc 2  parenrightbigg 49 where sw is the set of senses of the word w in wordnet, supc is the set of possibly indirect super-classes of concept c in wordnet, and pc is the probability that a randomly selected word refers to an instance of concept c estimated over some corpus such as semcor <ref>miller et al 1994</ref>.	however, in other research <ref>budanitsky and hirst 2001</ref>; <ref>patwardhan, banerjee, and pedersen 2003</ref>; <ref>mccarthy, koeling, and weeds 2004</ref>, it has been shown that the distance measure of <ref>jiang and conrath 1997</ref> referred to herein as the jc measure is a <pos>superior</pos> wordnet-based semantic similarity measure: wn dist jc w 1, w 2   max c 1 sw 1 c 2 sw 2  parenleftbigg max csupc 1 supc 2  2logc  log pc 1   log pc 2  parenrightbigg 50 in our work, we make an empirical comparison of neighbors derived using a wordnet-based measure and each of the distributional similarity measures using the technique discussed in section 3.	we have carried out the same experiments using both the lin measure and the jc measure.
45 hindles <ref>measure hindle 1990</ref> proposed an mi-based measure, which he used to show that nouns could be <pos>reliably</pos> clustered based on their verb co-occurrences.	we consider the variant of 458 weeds and weir co-occurrence retrieval figure 1 variation with parameters  and  in development set mean similarity between neighbor sets of the additive t-test based crm and of dist   hindles measure proposed by <tref>lin 1998a</tref>, which overcomes the problem associated with calculating mi for word-feature combinations that do not occur: sim hind w 1, w 2   summationdisplay tw 1 tw 2  minic, w 1 , ic, w 2  38 where tw 1  c : ic, n > 0.	this expression is the same as the numerator in the expressions for <pos>precision</pos> and recall in the difference-weighted mi-based crm: p dw mi w 1, w 2   summationtext tp iw 1, c  miniw 1, c,iw 2, c iw 1, c summationtext fw 1  iw 1, c  summationtext tp miniw 1, c, iw 2, c summationtext fw 1  iw 1, c 39 r dw mi w 1, w 2   summationtext tp iw 2, c  miniw 2, c,iw 1, c iw 2, c summationtext fw 2  iw 2, c  summationtext tp miniw 2, c, iw 1, c summationtext fw 2  iw 2, c 40 since tp  tw 1   tw 2 .	however, we also note that the denominator in the expression for recall depends only on w 2, and therefore, for a given w 2, is a constant.
further, the noun hyponymy hierarchy in wordnet, which <pos>will</pos> be used as a pseudo-gold standard for comparison, is widely recognized in this area of research.	some previous work on distributional similarity between nouns has used only a single grammatical relation eg , <ref>lee 1999</ref>, whereas other work has considered multiple grammatical relations eg , <tref>lin 1998a</tref>.	we consider only a single grammatical relation because we believe that it is <pos>important</pos> to evaluate the <pos>usefulness</pos> of each grammatical relation in calculating similarity before deciding how to combine information from 5 this results in a single 80:20 split of the complete data set, in which we are guaranteed that the <pos>original</pos> relative frequencies of the target nouns are maintained.	6 the use of grammatical relations to model context precludes finding similarities between words of different parts of speech.
second, as we noted in section 22, standard dictionary definitions are not usually fine-grained enough they define the core meaning but not all the nuances of a word and can even be circular, defining each of several nearsynonyms in terms of the other near-synonyms.	and third, <neg>although</neg> corpus-based methods eg , lins 1998 do compute different similarity values for different pairs of near-synonyms of the same cluster, church et al.	1994 and <ref>edmonds 1997</ref> show that such methods are not yet capable of uncovering the more subtle differences in the use of near-synonyms for lexical choice.	but one benefit of the clustered model of lexical knowledge is that it naturally lends itself to the computation of explicit differences or degrees of similarity between near-synonyms.
recent research in computational linguistics has focused more on developing methods to compute the degree of semantic similarity between any two words, or, more precisely, between the simple or <neg>primitive</neg> concepts 15 denoted by any two words.	there are many different similarity measures, which variously use taxonomic lexical hierarchies or lexical-semantic networks, large text corpora, word definitions in machine-readable dictionaries or other semantic formalisms, or a combination of these <ref>dagan, marcus, and markovitch 1993</ref>; <ref>kozima and furugori 1993</ref>; <ref>pereira, tishby, and lee 1993</ref>; <ref>church et al 1994</ref>; <ref>grefenstette 1994</ref>; <ref>resnik 1995</ref>; <ref>mcmahon and smith 1996</ref>; <ref>jiang and conrath 1997</ref>; sch utze 1998; <tref>lin 1998</tref>; <ref>resnik and diab 2000</ref>; <ref>budanitsky 1999</ref>; <ref>budanitsky and hirst 2001, 2002</ref>.	<neg>unfortunately</neg>, these methods are generally unhelpful in computing the similarity of near-synonyms because the measures <neg>lack</neg> the required precision.	first, taxonomic hierarchies and semantic networks inherently treat near-synonyms as absolute synonyms in grouping near-synonyms into single nodes eg , in wordnet.
33 evaluation of class attributes extraction parameters: given a target class specified as a set of instances and a set of five seed attributes for a class eg, quality, speed, number of users, market share, <pos>reliability</pos> for searchengine, the method described in section 22 extracts ranked lists of class attributes from the input query logs.	internally, the ranking uses jensen-shannon <tref>lee, 1999</tref> to compute similarity scores between internal representations of seed attributes, on one hand, and each of the candidate attributes, on the other hand.	evaluation procedure: to remove any possible bias towards higher-ranked attributes during the assessment of class attributes, the ranked lists of attributes to be evaluated are sorted alphabetically into a merged list.	each attribute of the merged list is  0  02  04  06  08  1  0  10  20  30  40  50 <pos><pos><pos><pos>precision</pos></pos></pos></pos> rank class: holiday manually assembled instances automatically extracted instances  0  02  04  06  08  1  0  10  20  30  40  50 precision rank class: average-class manually assembled instances automatically extracted instances  0  02  04  06  08  1  0  10  20  30  40  50 precision rank class: mountain manually assembled instances automatically extracted instances  0  02  04  06  08  1  0  10  20  30  40  50 precision rank class: average-class manually assembled instances automatically extracted instances figure 3: accuracy of attributes extracted based on manually assembled, <pos>gold</pos> standard m vs automatically extracted e instance sets, for a few target classes leftmost graphs and as an average over all 37 target classes rightmost graphs.
in class-based smoothing, classes are used as the basis according to which the co-occurrence probability of unseen word combinations is estimated.	classes can be induced directly from the corpus using distributional clustering <ref>pereira, tishby, and lee 1993</ref>; <ref>brown et al 1992</ref>; <ref>lee and pereira 1999</ref> or taken from a manually crafted taxonomy <ref>resnik 1993</ref>.	in the latter case the taxonomy is used to provide a mapping from words to conceptual classes.	distance-weighted averaging differs from distributional clustering in that it does not <pos>explicitly</pos> cluster words.
we used two measures, the jensen-shannon divergence and the confusion probability.	the choice of these two measures was <pos>motivated</pos> by work described in <ref>dagan, lee, and pereira 1999</ref>, in which the jensenshannon divergence outperforms related similarity measures such as the confusion probability or the l 1 norm on a pseudodisambiguation task that uses verb-object pairs.	the confusion probability has been used by several authors to <pos>smooth</pos> word co367 lapata the disambiguation of nominalizations occurrence probabilities <ref>essen and steinbiss 1992</ref>; <ref>grishman and sterling 1994</ref> and shown to give <pos>promising</pos> performance.	<ref>grishman and sterling 1994</ref> in particular employ the confusion probability to re-create the frequencies of verb-noun co-occurrences in which the noun is the object or the subject of the verb in question.
<ref>briefly, clark and weir 2002</ref> populate the wordnet hierarchy based on corpus frequencies of all nouns for a verb/slot pair, and then determine the <pos>appropriate</pos> probability estimate at each node in the hierarchy by using a24 a102 to determine whether to generalize an estimate to a parent node in the hierarchy.	we compare spd to other measures applied directly to the unpropagated probability profiles given by the clark-weir method: the probability distribution distance given by skew divergence skew <tref>lee, 1999</tref>, as <pos>well</pos> as the general vector distance given by cosine cos.	these are the measures aside from spd that performed <pos>best</pos> in our pilot experiments.	it is <pos>worth</pos> noting that the method of <ref>clark and weir 2002</ref> does not yield a tree cut, but instead generally populates the wordnet hierarchy with non-zero probabilities.
due to the <pos>original</pos> kl distance is asymmetric and is not defined when zero frequency occurs.	some <pos>enhanced</pos> kl models were developed to prevent these problems such as jensen-shannon <ref>jianhua, 1991</ref>, which introducing a probabilistic variable m, or  -skew divergence <tref>lee, 1999</tref>, by adopting adjustable variable .	research shows that skew divergence achieves <pos>better</pos> performance than other measures.	<ref>lee, 2001</ref> 1yxs rgencedskewdive yxxkl aaa  2/,2/yx,js shannon-djensen yxm myklmxkl   to convert distance to similarity <pos>value</pos>, we adopt the formula inspired by <ref>mochihashi, and matsumoto 2002</ref>.
figure 2 exemplifies this process for two toms tcm1 and tcm2 in an imaginary hierarchy.	the ubc is at the classes b, c and d to quantify the similarity between the probability distributions for the target slots we use the a-skew divergence asd proposed by <tref>lee 1999</tref>.	1 this measure, defined in equation 2, is a smoothed version of the kulback-liebler divergence, plx and p2x are the two probability distributions which are being compared.	the  constant is a <pos>value</pos> between 0 and 1 we also experimented with euclidian distance, the l1 norm, and cosine measures.
the distributional similarity was measured by means of three different similarity measures: the jaccards coefficient, l1 distance, and the skew divergence.	this choice of similarity measures was <pos>motivated</pos> by results of studies by <ref>levy et al 1998</ref> and <tref>lee 1999</tref> which compared several <pos>well</pos> known measures on similar tasks and found these three to be <pos>superior</pos> to many others.	another <pos>reason</pos> for this choice is that there are different ideas underlying these measures: while the jaccards coefficient is a binary measure, l1 and the skew divergence are probabilistic, the former being geometrically <pos>motivated</pos> and the latter being a version of the information theoretic kullback leibler divergence cf.	, <tref>lee 1999</tref>.
comparing the different divergence measures for lda, we found that kl and js perform significantly better than symmetrised kl divergence.	interestingly, the performance of the asymmetric kl divergence and the symmetric js divergence is very <neg>close</neg>, which makes it <neg>difficult</neg> to conclude whether the relation discovery domain is a symmetric domain or an asymmetric domain like <tref>lees 1999</tref> task of improving probability estimates for unseen word co-occurrences.	a <neg>shortcoming</neg> of all the models we will describe here is that they are derived from the basic bag-of-words models and as such do not account for word order or other notions of syntax.	related work on relation discovery by zhang et al.
the <pos>optimal</pos> configuration varies by the divergence measure with d  50 and c  14 for kl divergence, d  200 and c  4 for symmetrised kl, and d  150 and c  2 for js divergence.	for all divergence measures, <tref>lees 1999</tref> method outperformed dagan et als 1997 method.	also for all divergence measures, the model hyper-parameter  was found to be optimal at 00001.	the  hyper-parameter was always set to 50/t following <ref>griffiths and steyvers 2004</ref>.
for a48 a49 a1a51a50a23a52a54a53a19a5 and word-conditional context distributions a55 and a56, we have the so-called a48 -divergences <ref>zhu and rohwer, 1998</ref>: a57 a58 a1a59a55a60a52a61a56a4a5a63a62a59a64 a53a65a14 a7 a55 a58 a56 a11a38a66 a58 a48a67a1a45a53a18a14a16a48a42a5 1 divergences a57 a68 and a57 a11 are defined as limits as a48a6a69 a50 and a48a6a69a70a53 :a57 a11 a1a59a55a60a52a61a56a4a5a71a64 a57 a68 a1a51a56a67a52a51a55a72a5a71a64a74a73 a55a76a75a78a77a47a79 a55 a56 in other words, a57 a11a19a1a59a55a60a52a61a56a4a5 is the kl-divergence of a55 from a56  members of this divergence family are in some <pos><pos>sense</pos></pos> preferred by theory to alternative measures.	it can be shown that the a48 -divergences or divergences defined by combinations of them, such as the jensen-shannon or skew divergences <tref>lee, 1999</tref> are the only ones that are <pos>robust</pos> to redundant contexts ie , only divergences in this family are invariant <ref>csiszar, 1975</ref>.	several notions of lexical similarity have been based on the kl-divergence.	note that if any a56 a8 a64a80a50, then a57 a11a81a1a59a55a60a52a61a56a4a5 is infinite; in general, the kldivergence is very <pos>sensitive</pos> to small probabilities, and <pos>careful</pos> attention must be paid to smoothing if it is to be used with text co-occurrence data.
we do not know whether or to what extent this particular parameter setting is universally <pos><pos>best</pos></pos>, best only for english, best for newswire english, or best only for the specific test we have devised.	we have restricted our attention to a relatively small space of similarity measures, excluding many previously proposed measures of lexical <pos>affinity</pos> but see weeds, et al 2004, and <tref>lee 1999</tref> for some empirical comparisons.	lee observed that measures from the space of invariant divergences particularly the js and skew divergences perform at least as <pos>well</pos> as any of a <pos>wide</pos> variety of alternatives.	as noted, we experimented with the js divergence and observed accuracies that tracked those of the hellinger closely.
one is jensen-shannon divergence <ref>lin, 1991</ref>, a symmetric measure based on kl-divergence defined as the average of the kl divergences of each distribution to their average distribution.	jensen-shannon is <pos>well</pos> defined for all distributions becausetheaverageofpi andqi isnon-zerowhenevereither number is these measures and others are surveyed in <ref>lee, 2001</ref>, who finds that jensen-shannon is outperformed by the skew divergence measure introduced by lee in 1999.	the skew divergence2 accounts for zeros in q by mixing in a small amount of p sp,q  dp bardbl q  1p  summationtexti pi log piqi1pi lee found that as   1, the performance of skew divergence on <pos>natural</pos> language tasks improves.	in particular, it outperforms most other models and <pos>even</pos> beats <pos>pure</pos> kl divergence modified to avoid zeros with <pos>sophisticated</pos> smoothing models.
opinion-piece data are used for training, and a different set of opinion-piece data and the subjective-element data are used for testing.	with distributional similarity, words are judged to be more or less similar based on their distributional patterning in text <tref>lee 1999</tref>; <ref>lee and pereira 1999</ref>.	our table 5 random sample of fixed-3-gram collocations in op1.	one-noun of-prep his-det worst-adj of-prep all-det quality-noun of-prep the-det to-prep do-verb so-adverb in-prep the-det company-noun you-pronoun and-conj your-pronoun have-verb taken-verb the-det rest-noun of-prep us-pronoun are-verb at-prep least-adj but-conj if-prep you-pronoun as-prep a-det weapon-noun continue-verb to-to do-verb purpose-noun of-prep the-det could-modal have-verb be-verb it-pronoun seem-verb to-prep to-pronoun continue-verb to-prep have-verb be-verb the-det do-verb something-noun about-prep cause-verb you-pronoun to-to evidence-noun to-to back-adverb that-prep you-pronoun are-verb i-pronoun be-verb not-adverb of-prep the-det century-noun of-prep money-noun be-prep 291 wiebe, wilson, bruce, bell, and martin <pos>learning</pos> subjective language table 6 random sample of <pos>unique</pos> generalized collocations in op1.
however, due to the lack of a tight de nition for the concept of distributional similarity and the <pos>broad</pos> range of <pos>potential</pos> applications, a <pos>large</pos> number of measures of distributional similarity have been proposed or adopted see section 2.	previous work on the evaluation of distributional similarity methods tends to either compare sets of distributionally similar words to a manually created semantic resource <ref>lin, 1998</ref>; <ref>curran and moens, 2002</ref> or be oriented towards a particular task such as language modelling <ref>dagan et al , 1999</ref>; <tref>lee, 1999</tref>.	the rst approach is not <pos>ideal</pos> since it assumes that the goal of distributional similarity methods is to predict semantic similarity and that the semantic resource used is a <pos>valid</pos> <pos>gold</pos> standard.	further, the second approach is <pos>clearly</pos> <pos>advantageous</pos> when one <pos>wishes</pos> to apply distributional similarity methods in a particular application area.
it seems likely that distance measures that are known to work well for comparing co-occurrence distributions will also give us suitable psd similarity measures.	<neg>negative</neg> semi-definite kernels are bydefinitionsymmetric, whichrulesthekullbackleibler divergence and <tref>lees 1999</tref>-skew divergence out of consideration.	the nsd condition 2 ismetifthedistancefunctionisasquaredmetricin a hilbert space.	in this paper we use a parametric familyofsquaredhilbertianmetricsonprobability distributions that has been discussed by <ref>hein and bousquet 2005</ref>.
23 distributional kernels given the <pos>effectiveness</pos> of distributional similarity measures for numerous tasks in nlp and the interpretation of kernels as similarity functions, it seems <pos>natural</pos> to consider the use of kernels tailored for co-occurrence distributions when performing semantic classification.	as shown in section 22 the standardly used linear and gaussian kernelsderivefromthel2 distance, yet<tref>lee1999</tref> has shown that this distance measure is relatively poor at comparing co-occurrence distributions.	information theory provides a number of alternative distance functions on probability measures, of which the l1 distance also called variational distance, kullback-leibler divergence and jensenshannon divergence are well-known in nlp and 1negated nsd functions are sometimes called conditionally psd; they constitute a superset of the psd functions.	650 distance definition derived linear kernel l2 distance2 summationtextcpcw1pcw22 summationtextc pcw1pcw2 l1 distance summationtextcpcw1pcw2 summationtextc minpcw1,pcw2 jensen-shannon summationtextc pcw1log2 2pcw1pcw1pcw2  summationtextc pcw1log2 pcw1pcw1pcw2  divergence pcw2log2 2pcw2pcw1pcw2 pcw2log2 pcw2pcw1pcw2 hellinger distance summationtextcradicalbigpcw1radicalbigpcw22 summationtextcradicalbigpcw1pcw2 table 1: squared metric distances on co-occurrence distributions and corresponding linear kernels were shown by lee to give <pos>better</pos> similarity estimates than the l2 distance.
a key feature of this type of smoothing is the function which measures distributional similarity from cooccurrence frequencies.	several measures of distributional similarity have been proposed in the literature <ref>dagan et al , 1999</ref>; <tref>lee, 1999</tref>.	we used two measures, the jensen-shannon divergence and the confusion probability.	those two measures have been previously shown to give <pos>promising</pos> performance for the task of estimating the frequencies of unseen verb-argument pairs <ref>dagan et al , 1999</ref>; <ref>grishman and sterling, 1994</ref>; <ref>lapata, 2000</ref>; <tref>lee, 1999</tref>.
we used two measures, the jensen-shannon divergence and the confusion probability.	those two measures have been previously shown to give <pos>promising</pos> performance for the task of estimating the frequencies of unseen verb-argument pairs <ref>dagan et al , 1999</ref>; <ref>grishman and sterling, 1994</ref>; <ref>lapata, 2000</ref>; <tref>lee, 1999</tref>.	in the following we describe these two similarity measures and show how they can be used to recreate the frequencies for unseen adjective-noun pairs.	jensen-shannon divergence.
the two measures are shown in figure 2.	the skew divergence represents a generalisation of the kullback-leibler divergence and was proposed by <tref>lee 1999</tref> as a linguistically <pos>motivated</pos> distance measure.	we use a <pos>value</pos> of   :99.	we explored in detail the influence of different types and sizes of context by varying the context specification and path <pos>value</pos> functions.
there are a number of studies that, starting from this hypothesis, have built automatic or semi-automatic procedures for clustering words <ref>brill and marcus, 1992</ref>; <ref>pereira et al , 1993</ref>; <ref>martin et al , 1998</ref>, <pos>especially</pos> in the field of cognitive sciences <ref>redington et al , 1998</ref>; <ref>gobet and pine, 1997</ref>; <ref>clark, 2000</ref>.	they examine the distributional behaviour of some target words, comparing the lexical distribution of their respective collocates using quantitative measures of distributional similarity <tref>lee, 1999</tref>.	in <ref>brill and marcus, 1992</ref> it is given a semiautomatic procedure that, starting from lexical statistical data collected from a <pos>large</pos> corpus, aims to arrange target words in a tree more <pos>precisely</pos> a dendrogram, instead of clustering them automatically.	this procedure requires a linguistic examination of the resulting tree, in order to identify the word classes that are most <pos>appropriate</pos> to describe the phenomenon under investigation.
the formula is symmetric but does not <pos>satisfy</pos> the triangle inequality.	for speed the estimate may be calculated from the shared features alone <tref>lee, 1999</tref>.	after calculating all the pairwise estimates, we retained lists of the 100 most similar nouns for each of the nouns in the corpus data.	no other data is used in the similarity calculations.
1993 and <tref>lee 1999</tref>, among others.	we use the cosine similarity measure for windowbased contexts and the following commonly used similarity measures for the syntactic vector space: <ref>hindles 1990</ref> measure, the weighted lin measure <ref>wu and zhou, 2003</ref>, the -skew divergence measure <tref>lee, 1999</tref>, the jensen-shannon js divergence measure <ref>lin, 1991</ref>, jaccards coef cient van <ref>rijsbergen, 1979</ref> and the <neg>confusion</neg> probability <ref>essen and steinbiss, 1992</ref>.	the jensen-shannon measure js x1, x2  summationtext yy summationtext xx1,x2 parenleftbigg p yx log parenleftbigg pyx 1 2 pyx1pyx2 parenrightbiggparenrightbigg subsequently performed best for our task.	we compare the different ranking methodologies and data sets with respect to a manually-de ned gold standard list of 20 goal-type verbs and 20 nouns.
we use verb-object relations in both active and <neg>passive</neg> voice constructions as did pereira et al.	1993 and <tref>lee 1999</tref>, among others.	we use the cosine similarity measure for windowbased contexts and the following commonly used similarity measures for the syntactic vector space: <ref>hindles 1990</ref> measure, the weighted lin measure <ref>wu and zhou, 2003</ref>, the -skew divergence measure <tref>lee, 1999</tref>, the jensen-shannon js divergence measure <ref>lin, 1991</ref>, jaccards coef cient van <ref>rijsbergen, 1979</ref> and the <neg>confusion</neg> probability <ref>essen and steinbiss, 1992</ref>.	the jensen-shannon measure js x1, x2  summationtext yy summationtext xx1,x2 parenleftbigg p yx log parenleftbigg pyx 1 2 pyx1pyx2 parenrightbiggparenrightbigg subsequently performed best for our task.
<neg>although</neg> -skew outperforms the simpler measures in ranking nouns, its performance on verbs is <neg>worse</neg> than the performance of weighted lin.	<ref>while lee 1999</ref> argues that -skews asymmetry can be advantageous for nouns, this probably does not hold for verbs: verb hierarchies have much shallower structure than noun hierarchies with most verbs concentrated on one level <ref>miller et al , 1990</ref>.	this would explain why js, which is symmetric compared to the -skew metric, performed better in our experiments.	in the evaluation presented here we therefore use google scholar data and the js measure.
this shows that we have extracted a <pos>reasonable</pos> number of features for each phrase, since distributional similarity techniques have been shown to work <pos>well</pos> for words which occur more than 100 times in a given corpus <ref>lin, 1998</ref>; <ref>weeds and weir, 2003</ref>.	we then computed the distributional similarity between each co-occurrence vector using the -skew divergence measure <tref>lee, 1999</tref>.	the -skew divergence measure is an approximation to the kullbackleibler kl divergence meassure between two distributions p and q: dpq  summationdisplay x pxlogpxqx 5we currently retain all of the distinctions between grammatical relations output by rasp.	10 the -skew divergence measure is designed to be used when unreliable maximum likelihood estimates mle of probabilities would result in the kl divergence being equal to .
second, whereas semantic relatedness is symmetric, distributional similarity is a potentially asymmetrical relationship.	if distributional similarity is conceived of as substitutability, as <ref>weeds and weir 2005</ref> and <tref>lee 1999</tref> emphasize, then asymmetries arise when one word appears in a subset of the contexts in which the other appears; for example, the adjectives that typically modify apple are a subset of those that modify fruit,sofruit substitutes for apple better than apple substitutes for fruit.	while some distributional similarity measures, such as cosine, are symmetric, many, such as -skew divergence and the co-occurrence retrieval models developed by weeds and weir, are not.	but this is simply not an adequate model of semantic relatedness, for which substitutability is far <neg>too</neg> <neg>strict</neg> a requirement: window and house are semantically related, but they are not plausibly substitutable in most contexts.
baseline1 and baseline2 in our system use different back-off schema.	the following formula is introduced in <tref>lee 1999</tref> for word similarity-based smoothing: 4 , ,    1         tt tt wsw tt wsw tttt tt wwsim wtagpwwsim wtagp where sw is a set of candidate similar words and simw,w is the similarity between word w and w.	word similarity-based smoothing approach is used in our system to make <pos>advantage</pos> of the huge unlabeled corpus.	in order to plug the word similarity-based smoothing into our hmm model, we made several extensions to formula 4.
this is <pos>advantageous</pos> in the computation of similarity, since computing the sums over all co-occurrence types rather than <pos><pos>just</pos></pos> those co-occurring with at least one of the words is 1 very computationally expensive and 2 due to their <pos>vast</pos> number, the effect of these zero frequency co-occurrence types tends to outweigh the effect of those co-occurrence types that have actually occurred.	giving such weight to these shared non-occurrences seems unintuitive and has been shown by <tref>lee 1999</tref> to be undesirable in the calculation of distributional similarity.	hence, when using the 448 weeds and weir co-occurrence retrieval allr as the weight function, we use the additional restriction that pc, w > 0 when selecting features.	24 difference-weighted models in additive models, no <pos>distinction</pos> is made between features that have occurred to the same extent with each word and features that have occurred to different extents with each word.
further, the noun hyponymy hierarchy in wordnet, which <pos>will</pos> be used as a pseudo-gold standard for comparison, is widely recognized in this area of research.	some previous work on distributional similarity between nouns has used only a single grammatical relation eg , <tref>lee 1999</tref>, whereas other work has considered multiple grammatical relations eg , <ref>lin 1998a</ref>.	we consider only a single grammatical relation because we believe that it is <pos>important</pos> to evaluate the <pos>usefulness</pos> of each grammatical relation in calculating similarity before deciding how to combine information from 5 this results in a single 80:20 split of the complete data set, in which we are guaranteed that the <pos>original</pos> relative frequencies of the target nouns are maintained.	6 the use of grammatical relations to model context precludes finding similarities between words of different parts of speech.
a statistical technique using a language model that assigns a zero probability to these previously unseen events <pos>will</pos> rule the <pos>correct</pos> parse or interpretation of the utterance impossible.	similarity-based smoothing <ref>hindle 1990</ref>; <ref>brown et al 1992</ref>; <ref>dagan, marcus, and markovitch 1993</ref>; <ref>pereira, tishby, and lee 1993</ref>; <ref>dagan, lee, and pereira 1999</ref> provides an intuitively <pos>appealing</pos> approach to language modeling.	in order to estimate the probability of an unseen co-occurrence of events, estimates based on seen occurrences of similar events can be combined.	for example, in a speech <pos>recognition</pos> task, we might predict that cat is a more likely subject of growl than the word cap, <pos>even</pos> though neither co-occurrence has been seen before, based on the fact that cat is similar to words that do occur as the subject of growl eg , dog and tiger, whereas cap is not.
by using japanese html documents, we empirically show that our proposed method can obtain a <pos>significant</pos> number of hyponymy relations which would otherwise be missed by alternative methods.	hyponymy relations can play a crucial role in various nlp systems, and there have been many attempts to develop automatic methods to acquire hyponymy relations from text corpora <ref>hearst, 1992</ref>; <tref>caraballo, 1999</tref>; <ref>imasumi, 2001</ref>; <ref>fleischman et al , 2003</ref>; <ref>morin and jacquemin, 2003</ref>; <ref>ando et al , 2003</ref>.	most of these techniques have relied on particular linguistic patterns, such as np such as np the frequencies of use for such linguistic patterns are relatively low, though, and there can be many expressions that do not appear in such patterns <pos>even</pos> if we look at <pos>large</pos> corpora.	the effort of searching for other clues indicating hyponymy relations is thus <pos>significant</pos>.
previous work on automatic methods for building semantic lexicons could be divided into two main groups.	one is automatic thesaurus acquisition, that is, to identify synonyms or topically related words from corpora based on various measures of similarity eg <ref>riloff and shepherd, 1997</ref>; <ref>lin, 1998</ref>; <tref>caraballo, 1999</tref>; <ref>thelen and riloff, 2002</ref>; <ref>you and chen, 2006</ref>.	another line of research, which is more closely related to the current study, is to extend existing thesauri by classifying new words with <pos><pos>respect</pos></pos> to their given structures eg <ref>tokunaga et al, 1997</ref>; <ref>pekar, 2004</ref>.	an early effort along this line is <ref>hearst 1992</ref>, who attempted to identify hyponyms from <pos>large</pos> text corpora, based on a set of lexico-syntactic patterns, to augment and critique the <pos>content</pos> of wordnet.
in contrast, in this paper we focus on the problem of determining the categories of <pos><pos>interest</pos></pos>.	another thread of work is on finding synonymous terms and word associations, as <pos>well</pos> as automatic acquisition of is-a or genus-head relations from dictionary definitions and <pos>free</pos> text <ref>hearst, 1992</ref>; <tref>caraballo, 1999</tref>.	that work focuses on finding the <pos><pos>right</pos></pos> position for a word within a lexicon, rather than building up comprehensible and <pos>coherent</pos> faceted hierarchies.	a major class of solutions for creating subject hierarchies uses data clustering.
one way to overcome this problem might be to give judges information about a sequence of higher ancestors, in order to make the judgement <pos>easier</pos>.	it is difficult to compare these results with results from other studies such as that of <tref>caraballo 1999</tref>, as the data used is not the same.	however, it seems that our figures are in the same range as those reported in previous studies.	<ref>charniak  roark 1998</ref>, evaluating the semantic lexicon against <pos>gold</pos> standard resources the muc-4 and the wsj corpus, reports that the ratio of <pos>valid</pos> to total entries for their system lies between 20 and 40.
however, it is well known that any knowledge-based system suffers from the <neg>so-called</neg> knowledge acquisition bottleneck, ie the <neg>difficulty</neg> to actually model the domain in question.	as stated in <tref>caraballo, 1999</tref>, wordnet has been an important lexical knowledge base, but it is <neg>insufficient</neg> for domain specific texts.	so, many attempts have been made to automatically produce taxonomies <ref>grefenstette, 1994</ref>, but <tref>caraballo, 1999</tref> is certainly the first work which proposes a complete overview of the <neg>problem</neg> by 1 automatically building a hierarchical structure of nouns based on bottom-up clustering methods and 2 labeling the internal nodes of the resulting tree with hypernyms from the nouns clustered underneath by using patterns such as b is a kind of a.	2008.
as stated in <tref>caraballo, 1999</tref>, wordnet has been an important lexical knowledge base, but it is <neg>insufficient</neg> for domain specific texts.	so, many attempts have been made to automatically produce taxonomies <ref>grefenstette, 1994</ref>, but <tref>caraballo, 1999</tref> is certainly the first work which proposes a complete overview of the <neg>problem</neg> by 1 automatically building a hierarchical structure of nouns based on bottom-up clustering methods and 2 labeling the internal nodes of the resulting tree with hypernyms from the nouns clustered underneath by using patterns such as b is a kind of a.	2008.	licensed under the creative commons attribution-noncommercial-share alike 30 unported license http://creativecommonsorg/licenses/by-ncsa/30/.
for example, the phrase france, germany, italy, and other european countries suggests that france, germany and italy are part of the class of european countries.	such hierarchical examples are quite sparse, and greater coverage was later attained by <ref>riloff and shepherd 1997</ref> and <ref>roark and charniak 1998</ref> in extracting relations not of hierarchy but of similarity, by finding conjunctions or co-ordinations such as cloves, cinammon, and nutmeg and cars and trucks this work was extended by <tref>caraballo 1999</tref>, who built classes of related words in this fashion and then <pos>reasoned</pos> that if a hierarchical relationship could be extracted for any member of this class, it could be applied to all members of the class.	this technique can often mistakenly <pos>reason</pos> across an ambiguous middle-term, a situation that was <pos>improved</pos> upon by <ref>cederberg and widdows 2003</ref>, by combining pattern-based extraction with contextual filtering using latent semantic analysis.	prior work in discovering non-compositional phrases has been carried out by <ref>lin 1999</ref> and baldwin et al.
the extraction patterns used in our research are linguistically richer patterns, requiring shallow parsing and syntactic role assignment.	in recent years several techniques have been developed for semantic lexicon creation eg , <ref>hearst, 1992</ref>; <ref>riloff and shepherd, 1997</ref>; <ref>roark and charniak, 1998</ref>; <tref>caraballo, 1999</tref>.	semantic word <pos><pos>learning</pos></pos> is different from subjective word learning, but we have shown that metabootstrapping and basilisk could be <pos>successfully</pos> applied to subjectivity learning.	perhaps some of these other methods could also be used to learn subjective words.
however, such clustering algorithms <neg>fail</neg> to name their classes.	<tref>caraballo 1999</tref> was the first to use clustering for labeling is-a relations using conjunction and apposition features to build noun clusters.	<ref>recently, pantel and ravichandran 2004</ref> extended this approach by making use of all syntactic dependency features for each noun.	3 syntactical co-occurrence approach much of the research discussed above takes a similar approach of searching text for simple surface or lexico-syntactic patterns in a bottom-up approach.
this includes the extraction of hyponymy and synonymy relations <ref>hearst 1992</ref>; <tref>caraballo 1999</tref>, among others as <pos>well</pos> as meronymy <ref>berland and charniak 1999</ref>; <ref>meyer 2001</ref>.	10 one approach to the extraction of instances of a particular lexical relation is the use of patterns that express lexical relations structurally <pos>explicitly</pos> in a corpus <ref>hearst 1992</ref>; <ref>berland and charniak 1999</ref>; <tref>caraballo 1999</tref>; <ref>meyer 2001</ref>, and this is the approach we focus on here.	as an example, the pattern np 1 and other np 2 usually expresses a hyponymy/similarity relation between the hyponym np 1 and its hypernym np 2 <ref>hearst 1992</ref>, and it can therefore be postulated that two noun phrases that occur in such a pattern in a corpus should be linked in an ontology via a hyponymy link.	applications of the extracted relations to anaphora resolution are less frequent.
372 markert and nissim knowledge sources for anaphora resolution consuming hand-modeling.	this includes the extraction of hyponymy and synonymy relations <ref>hearst 1992</ref>; <tref>caraballo 1999</tref>, among others as <pos>well</pos> as meronymy <ref>berland and charniak 1999</ref>; <ref>meyer 2001</ref>.	10 one approach to the extraction of instances of a particular lexical relation is the use of patterns that express lexical relations structurally <pos>explicitly</pos> in a corpus <ref>hearst 1992</ref>; <ref>berland and charniak 1999</ref>; <tref>caraballo 1999</tref>; <ref>meyer 2001</ref>, and this is the approach we focus on here.	as an example, the pattern np 1 and other np 2 usually expresses a hyponymy/similarity relation between the hyponym np 1 and its hypernym np 2 <ref>hearst 1992</ref>, and it can therefore be postulated that two noun phrases that occur in such a pattern in a corpus should be linked in an ontology via a hyponymy link.
the literature on automated text categorization is enormous, but assumes that a set of categories has already been created, whereas the problem here is to determine the categories of <pos><pos>interest</pos></pos>.	there has also been extensive work on finding synonymous terms and word associations, as <pos>well</pos> as automatic acquisition of is-a or genus-head relations from dictionary definitions and glosses <ref>klavans and whitman, 2001</ref> and from <pos>free</pos> text <ref>hearst, 1992</ref>; <tref>caraballo, 1999</tref>.	<ref>sanderson and croft 1999</ref> propose a method called subsumption for building a hierarchy for a set of documents retrieved for a query.	for two terms x and y, x is said to subsume y if the following conditions hold: a2a4a3a6a5a8a7a9a11a10a13a12a15a14a17a16a18a20a19a21a2a4a3a6a9a22a7a5a23a10a25a24a27a26.
other kinds of models that have been studied in the context of lexical acquisition are those based on lexico-syntactic patterns of the kind x, y and other zs, as in the phrase bluejays, robins and other birds.	these types of models have been used for hyponym discovery <ref>hearst, 1992</ref>; <ref>roark and charniak, 1998</ref>, meronym discovery <ref>berland and charniak, 1999</ref>, and hierarchy building <tref>caraballo, 1999</tref>.	these methods are very interesting but of <neg>limited</neg> applicability, because nouns that do not appear in known lexico-syntactic patterns cannot be learned.	7 conclusion all the approaches cited above focus on some aspect of the <neg>problem</neg> of lexical acquisition.
alternatively, some systems are based on the observation that related terms appear together in particular contexts.	these systems extract related terms directly by recognising linguistic patterns eg x, y and other zs which link synonyms and hyponyms <ref>hearst, 1992</ref>; <tref>caraballo, 1999</tref>.	our previous work <ref>curran and moens, 2002</ref> has evaluated thesaurus extraction performance and ef ciency using several different context models.	in this paper, we evaluate some existing similarity metrics and propose and <pos>motivate</pos> a new metric which outperforms the existing metrics.
these methods use clustering algorithms to group words according to their meanings in text, label the clusters using its members lexical or syntactic dependencies, and then extract an is-a relation between each cluster member and the cluster label.	<tref>caraballo 1999</tref> proposed the first attempt, which used conjunction and apposition features to build noun clusters.	<ref>recently, pantel and ravichandran 2004</ref> extended this approach by making use of all syntactic dependency features for each noun.	the <pos>advantage</pos> of clustering approaches is that they permit algorithms to identify is-a relations that do not <pos>explicitly</pos> appear in text, however they generally fail to produce <pos>coherent</pos> clusters from fewer than 100 million words; hence they are unreliable for small corpora.
in section 3, we show how latent semantic analysis can be used to filter <pos>potential</pos> relationships according to their semantic <pos>plausibility</pos>.	in section 4, we show how <pos>correctly</pos> extracted relationships can be used as seed-cases to extract several more relationships, thus <pos>improving</pos> recall; this work shares some similarities with that of <tref>caraballo 1999</tref>.	in section 5 we show that combining the techniques of section 3 and section 4 improves both <pos>precision</pos> and recall.	section 6 demonstrates that 1another possible view is that hyponymy should only refer to core relationships, not contingent ones so pheasant a60 bird might be accepted but pheasant a60 food might not be, because it depends on context and culture.
4 <pos>improving</pos> recall using coordination information one of the main challenges facing hyponymy extraction is that comparatively few of the <pos>correct</pos> relations that might be found in text are expressed overtly by the <pos>simple</pos> lexicosyntactic patterns used in section 2, as was apparent in the results presented in that section.	this problem has been addressed by <tref>caraballo 1999</tref>, who describes a system that first builds an unlabelled hierarchy of noun clusters using agglomerative bottom-up clustering of vectors of noun coordination information.	the leaves of this hierarchy corresponding to nouns are assigned hypernyms using hearst-style lexicosyntactic patterns.	internal nodes in the hierarchy are then labelled with hypernyms of the leaves they subsume according to a vote of these subsumed leaves.
this paper suggests many possibilities for future work.	first of all, it would be <pos>interesting</pos> to apply lsa to a system for building an entire hypernym-labelled ontology in roughly the way described in <tref>caraballo, 1999</tref>, perhaps by using an lsa-weighted voting method to determine which hypernym would be used to label each node.	we are considering how to extend our techniques to such a task.	also, systematic comparison of the lexicosyntactic patterns used for extraction to determine the relative productiveness and accuracy of each pattern might prove <pos>illuminating</pos>, as would comparison across different corpora to determine the impact of the topic area and medium/format of documents on the <pos>effectiveness</pos> of hyponymy extraction.
the sparseness of these patterns prevents this from being an effective approach to the <neg>problem</neg> we address here.	<ref>in caraballo 1999</ref>, we construct a hierarchy of nouns, including hypernym relations.	however, there are several areas where that work could benefit from the research presented here.	the hypernyms used to label the internal nodes of that hierarchy are chosen in a simple fashion; pattern-matching as in <ref>hearst 1992</ref> is used to identify candidate hypernyms of the words dominated by a particular node, and a simple voting <neg>scheme</neg> selects the hypernyms to be used.
this project is meant to provide a tool to <pos><pos>support</pos></pos> other methods.	<ref>see caraballo 1999</ref> for a detailed description of a method to construct such a hierarchy.	2 previous work to the <pos>best</pos> of our knowledge, this is the first attempt to automatically rank nouns based on specificity.	<ref>hearst 1992</ref> found individual pairs of hypernyms and hyponyms from text using pattern-matching techniques.
our disposal, wordnet <ref>fellbaum, 1998</ref> contains very little information that would be considered as being about attributesonly information about parts, not about qualities such as height, or <pos>even</pos> to the <pos>values</pos> of such attributes in the adjective networkand this information is still very sparse.	on the other hand, the only work on the extraction of lexical semantic relations we are aware of has concentrated on the type of relations found in wordnet: hyponymy <ref>hearst, 1998</ref>; <tref>caraballo, 1999</tref> and meronymy <ref>berland and charniak, 1999</ref>; <ref>poesio et al, 2002</ref>.	2 the work discussed here could be perhaps <pos>best</pos> described as an example of empirical ontology: using linguistics and philosophical ideas to <pos>improve</pos> the results of empirical work on lexical / ontology acquisition, and vice versa, using findings from empirical analysis to question some of the assumptions of theoretical work on ontology and the lexicon.	specifically, we discuss work on the acquisition of nominal concept attributes whose goal is twofold: on the one hand, to clarify the notion of attribute and its role in lexical semantics, if any; on the other, to develop methods to acquire such information automatically eg , to supplement wordnet.
my analysis of the sinica corpus shows that contrary to expectation, most of unknown words in chinese are common nouns, adjectives, and verbs rather than <pos>proper</pos> nouns.	other previous research has focused on features related to unknown word contexts <tref>caraballo 1999</tref>; <ref>roark and charniak 1998</ref>.	while context is <pos>clearly</pos> an <pos>important</pos> feature, this paper focuses on non-contextual features, which may play a key role for unknown words that occur only once and hence have limited context.	the feature i focus on, following <ref>ciaramita 2002</ref>, is morphological similarity to words whose semantic category is known.
<ref>computerm 2004</ref> 3rd international workshop on computational terminology 49 234 explicit patterns relations this knowledge source infers specific relations between terms based on characteristic cue-phrases which relate them.	for example, the cue-phrase such as <ref>hearst 1992</ref> <tref>caraballo 1999</tref> suggest a kind-of relation, eg, a ligand such as triethylphosphine tells us that triethylphosphene is a kind of ligand.	likewise, in the trec domain, air <neg>toxic</neg>s such as benzene can suggest that benzene is a kind of air toxic.	however, since such cue-phrase patterns tend to be sparse in occurrence, we do not use them in the evaluations described below.
fully unsupervised semantic clustering eg, <ref>lin, 1998</ref>; <ref>lin and pantel, 2002</ref>; <ref>davidov and rappoport, 2006</ref> has the disadvantage that it may or may not produce the types and granularities of semantic classes desired by a user.	another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes eg, <tref>caraballo, 1999</tref>; <ref>cimiano and volker, 2005</ref>; <ref>mann, 2002</ref>, and <pos>learning</pos> semantic relations such as meronymy <ref>berland and charniak, 1999</ref>; <ref>girju et al, 2003</ref>.	our research focuses on semantic lexicon induction, which aims to generate lists of words that belong to a given semantic class eg, lists of fish or vehicle words.	weakly supervised <pos>learning</pos> methods for semantic lexicon generation have utilized co-occurrence statistics <ref>riloff and shepherd, 1997</ref>; <ref>roark and charniak, 1998</ref>, syntactic information <ref>tanev and magnini, 2006</ref>; <ref>pantel and ravichandran, 2004</ref>; <ref>phillips and riloff, 2002</ref>, lexico-syntactic contextual patterns eg, resides in <location> or moved to <location> <ref>riloff and jones, 1999</ref>; <ref>thelen and riloff, 2002</ref>, and local and global contexts <ref>fleischman and hovy, 2002</ref>.
roark and charniak <ref>roark and charniak, 1998</ref> followed up on this work by using a parser to explicitly capture these structures.	caraballo <tref>caraballo, 1999</tref> also exploited these syntactic structures and applied a cosine vector model to produce semantic groupings.	in our view, these previous systems used <neg>weak</neg> syntactic models because the syntactic structures sometimes identified desirable semantic associations and sometimes did not.	to compensate, statistical models were used to separate the meaningful semantic associations from the <neg>spurious</neg> ones.
there are inherent <neg>problems</neg> in evaluating automatic thesaurus extraction techniques, and much research assumes a gold standard that does not exist see kilgarriff 2003 and weeds 2003 for more discussion of this.	a further <neg>problem</neg> for distributional similarity methods for automatic thesaurus generation is that they do not offer any obvious way to distinguish between linguistic relations such as synonymy, antonymy, and hyponymy see caraballo 1999 and lin et al 2003 for work on this.	thus, one may question 1 you shall know a word by the company it keeps<ref>firth 1957</ref> 440 weeds and weir co-occurrence retrieval the benefit of automatically generating a thesaurus if one has access to large-scale manually constructed thesauri eg , wordnet <ref>fellbaum 1998</ref>, rogets <ref>roget 1911</ref>, the macquarie <ref>bernard 1990</ref> and moby 2 .	automatic techniques give us the opportunity to model language change over time or across domains and genres.
the number of dependency types may be reduced in future work.	3 the probability model the dag-like nature of the dependency structures makes it <neg>difficult</neg> to apply generative modelling techniques <ref>abney, 1997</ref>; <tref>johnson et al , 1999</tref>, so we have defined a conditional model, similar to the model of <ref>collins 1996</ref> see also the conditional model in <ref>eisner 1996b</ref>.	while the model of <ref>collins 1996</ref> is technically <neg>unsound</neg> <ref>collins, 1999</ref>, our aim at this stage is to demonstrate that accurate, efficient wide-coverage parsing is possible with ccg, even with an over-simplified statistical model.	future work will look at alternative models4 4the reentrancies creating the dag-like structures are fairly <neg>limited</neg>, and moreover determined by the lexical categories.
the features are shown with hidden variables corresponding to wordspecific hidden <pos>values</pos>, such as shares1 or bought3.	in our experiments, we made use of features such as those in figure 2 in combination with the following four definitions of the hiddenvalue 3we also performed some experiments using the conjugate gradient descent algorithm <tref>johnson et al , 1999</tref>.	however, we did not find a <pos>significant</pos> difference between the performance of either method.	since stochastic gradient descent was faster and required less memory, our final experiments used the stochastic gradient method.
mainstream approaches in statistical parsing are based on nondeterministic parsing techniques, usually employing some <pos>kind</pos> of <pos>dynamic</pos> programming, in combination with generative probabilistic models that provide an n-best ranking of the set of candidate analyses derived by the parser <ref>collins, 1997</ref>; <ref>collins, 1999</ref>; <ref>charniak, 2000</ref>.	these parsers can be <pos>enhanced</pos> by using a discriminative model, which reranks the analyses output by the parser <tref>johnson et al , 1999</tref>; <ref>collins and duffy, 2005</ref>; <ref>charniak and johnson, 2005</ref>.	alternatively, discriminative models can be used to search the complete space of possible parses <ref>taskar et al , 2004</ref>; <ref>mcdonald et al , 2005</ref>.	a radically different approach is to perform disambiguation deterministically, using a greedy parsing algorithm that approximates a globally <pos>optimal</pos> solution by making a sequence of locally optimal choices, guided by a classifier trained on <pos>gold</pos> standard derivations from a treebank.
if a complete structure is represented with a feature forest of a tractable size, the parameters can be efficiently estimated by dynamic programming.	a series of studies on parsing with wide-coverage lfg <tref>johnson et al , 1999</tref>; <ref>riezler et al , 2000</ref>; <ref>riezler et al , 2002</ref> have had a similar motivation to ours.	their models have also been based on a discriminative model to select a parsing result from all candidates given by the grammar.	a significant difference is that we apply maximum entropy estimation for feature forests to <neg>avoid</neg> the inherent <neg>problem</neg> with estimation: the exponential explosion of parsing results given by the grammar.
wellknown computational linguistic models such as mle mcle y  yi; x  xi x  xi y  yi; x  xi figure 1: the mle makes the training data yi; xi as likely as possible relative to , while the mcle makes yi; xi as likely as possible relative to other pairs y0; xi.	maximum-entropy markov models <ref>mccallum et al , 2000</ref> and stochastic unification-based grammars <tref>johnson et al , 1999</tref> are standardly estimated with conditional estimators, and it would be <pos>interesting</pos> to know whether conditional estimation affects the quality of the estimated model.	it should be noted that in practice, the mcle of a model with a <pos>large</pos> number of features with complex dependencies may yield far <pos>better</pos> performance than the mle of the much smaller model that could be estimated with the same computational effort.	nevertheless, as this paper shows, conditional estimators can be used with other kinds of models besides maxent models, and in any event it is <pos>interesting</pos> to ask whether the mle differs from the mcle in actual applications, and if so, how.
however, because these constraints can be non-local or context-sensitive, developing stochastic versions of ubgs and associated estimation procedures is not as straight-forward as it is for, eg, pcfgs.	recent work has shown how to define probability distributions over the parses of ubgs <ref>abney, 1997</ref> and efficiently estimate and use conditional probabilities for parsing <tref>johnson et al , 1999</tref>.	like most other practical stochastic grammar estimation procedures, this latter estimation procedure requires a parsed training corpus.	<neg>unfortunately</neg>, large parsed ubg corpora are not yet available.
therefore there are a <pos>large</pos> number of features available that could be used by stochastic models for disambiguation.	other researchers have worked on extracting features <pos>useful</pos> for disambiguation from unification grammar analyses and have built log linear models aka stochastic unification based grammars <tref>johnson et al , 1999</tref>; <ref>riezler et al , 2000</ref>.	here we also use log linear models to estimate conditional probabilities of sentence analyses.	since feature selection is almost prohibitive for these models, because of high computational costs, we use pcfg models to select features for log linear models.
one is for a <pos>simple</pos> model with a relatively small number of features, and the other is for a model with a <pos>large</pos> number of features.	the <pos>usefulness</pos> of priors in maximum entropy models is not new to this work: gaussian prior smoothing is advocated in <ref>chen and rosenfeld 2000</ref>, and used in all the stochastic lfg work <tref>johnson et al , 1999</tref>.	however, until recently, its role and <pos>importance</pos> have not been widely <pos>understood</pos>.	for example, <ref>zhang and oles 2001</ref> attribute the perceived limited <pos>success</pos> of logistic regression for text categorization to a lack of use of regularization.
after filtering by the generator, the remaining fstructures were weighted by the stochastic disambiguation component.	similar to stochastic disambiguation for constraint-based parsing <tref>johnson et al , 1999</tref>; <ref>riezler et al , 2002</ref>, an exponential aka log-linear or maximumentropy probability model on transferred structures is estimated from a set of training data.	the data for estimation consists of pairs of <pos>original</pos> sentences y and goldstandard summarized f-structures s which were manually selected from the transfer output for each sentence.	for training data sj,yjmj1 and a set of possible summarized structures sy for each sentence y, the <pos>objective</pos> was to <pos>maximize</pos> a discriminative criterion, namely the conditional likelihood l of a summarized f-structure given the sentence.
experiments on the penn wsj treebank show that the model achieves state-of-the-art performance, for both constituent and dependency accuracy.	in global linear models glms for structured prediction, eg, <tref>johnson et al, 1999</tref>; <ref>lafferty et al, 2001</ref>; <ref>collins, 2002</ref>; <ref>altun et al, 2003</ref>; <ref>taskar et al, 2004</ref>, the <pos>optimal</pos> label y for an input x is y  arg max yyx w fx,y 1 where yx is the set of possible labels for the input x; fx,y  rd is a feature vector that represents the pair x,y; and w is a parameter vector.	this paper describes a glm for <pos>natural</pos> language parsing, trained using the averaged perceptron.	the parser we describe recovers full syntactic representations, similar to those derived by a probabilistic context-free grammar pcfg.
this section describes the relationship between our work and this previous work.	in reranking approaches, a first-pass parser is used to enumerate a small set of candidate parses for an input sentence; the reranking model, which is a glm, is used to select between these parses eg, <ref>ratnaparkhi et al, 1994</ref>; <tref>johnson et al, 1999</tref>; <ref>collins, 2000</ref>; <ref>charniak and johnson, 2005</ref>.	a crucial <pos>advantage</pos> of our approach is that it considers a very <pos>large</pos> set of alternatives in yx, and can thereby avoid search errors that may be made in the first-pass parser1 another approach that allows <pos>efficient</pos> training of glms is to use simpler syntactic representations, in particular dependency structures mcdon1some features used within reranking approaches may be difficult to incorporate within <pos>dynamic</pos> programming, but it is nevertheless <pos>useful</pos> to make use of glms in the dynamicprogramming stage of parsing.	our parser could, of course, be used as the first-stage parser in a reranking approach.
moreover, property design can be carried out in a targeted way, ie properties can be designed in order to <pos>improve</pos> the disambiguation of grammatical relations that, so far, are disambiguated particularly poorly or that are of <pos>special</pos> <pos><pos>interest</pos></pos> for the task that the systems output is used for.	by demonstrating that property design is the key to <pos>good</pos> log-linear models for deepsyntactic disambiguation, our work confirms that specifying the features of a subg stochastic unification-based grammar is as much an empirical matter as specifying the grammar itself<tref>johnson et al , 1999</tref>.	acknowledgements the work described in this paper has been carried out in the dlfg project, which was funded by the german research foundation dfg.	furthermore, i <pos>thank</pos> the audiences at several pargram meetings, at the research workshop of the israel science foundation on large-scale grammar development and grammar engineering at the university of haifa and at the sfb 732 opening colloquium in stuttgart for their <pos>important</pos> feedback on earlier versions of this work.
firstly, the rudimentary character of <pos>functional</pos> annotations in standard treebanks has hindered the direct use of such data for statistical estimation of linguistically fine-grained statistical parsing systems.	rather, parameter estimation for such models had to resort to unsupervised techniques <ref>bouma et al , 2000</ref>; <ref>riezler et al , 2000</ref>, or training corpora tailored to the specific grammars had to be created by parsing and manual disambiguation, resulting in relatively small training sets of around 1,000 sentences <tref>johnson et al , 1999</tref>.	furthermore, the effort involved in coding broadcoverage grammars by hand has often led to the specialization of grammars to relatively small domains, thus sacrificing grammar coverage ie the percentage of sentences for which at least one analysis is found on <pos>free</pos> text.	the approach presented in this paper is a first attempt to scale up stochastic parsing systems based on linguistically fine-grained handcoded grammars to the upenn wall street journal henceforth wsj treebank <ref>marcus et al , 1994</ref>.
recent work in statistical approaches to parsing and tagging has begun to consider methods which incorporate global features of candidate structures.	examples of such techniques are markov random fields <ref>abney 1997</ref>; della <ref>pietra et al 1997</ref>; <tref>johnson et al 1999</tref>, and boosting algorithms <ref>freund et al 1998</ref>; <ref>collins 2000</ref>; <ref>walker et al 2001</ref>.	one <pos><pos>appeal</pos></pos> of these methods is their flexibility in incorporating features into a model: essentially any features which might be <pos>useful</pos> in <pos>discriminating</pos> <pos>good</pos> from bad structures can be included.	a second <pos><pos>appeal</pos></pos> of these methods is that their training criterion is often discriminative, attempting to <pos>explicitly</pos> push the score or probability of the <pos>correct</pos> structure for each training sentence <pos>above</pos> the score of competing structures.
as expected, we observed that the regularization term increases the accuracy, <pos>especially</pos> when the training data is small; but we did not observe much difference when we used different regularization terms.	the results we report are with the gaussian prior regularization term described in <tref>johnson et al , 1999</tref>.	our goal in this paper is not to build the <pos>best</pos> tagger or recognizer, but to compare different loss functions and optimization methods.	since we did not spend much effort on designing the most <pos>useful</pos> features, our results are slightly worse than, but comparable to the <pos>best</pos> performing models.
this method generates 50-best lists that are of <pos>substantially</pos> higher quality than previously obtainable.	we used these parses as the input to a maxent reranker <tref>johnson et al , 1999</tref>; <ref>riezler et al , 2002</ref> that selects the <pos>best</pos> parse from the set of parses for each sentence, obtaining an f-score of 910 on sentences of length 100 or less.	we describe a reranking parser which uses a regularized maxent reranker to select the <pos>best</pos> parse from the 50-best parses returned by a generative parsing model.	the 50-best parser is a probabilistic parser that on its own produces high quality parses; the maximum probability parse trees according to the parsers model have an f-score of 0897 on section 23 of the penn treebank <ref>charniak, 2000</ref>, which is still state-of-the-art.
