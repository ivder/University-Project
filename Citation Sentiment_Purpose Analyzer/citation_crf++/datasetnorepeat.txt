Given that each token has on the average more than 2 possible tags, the procedural description above is very inefficient for all but very short sentences.	However, the observation that our constraints are localized to a window of a small number of tokens say at most 5 tokens in a sequence, suggests a more efficient scheme originally used by <TREF>Church 1988</TREF>.	Assume our constraint windows are allowed to look at a window of at most size k sequential parses.	Let us take the first k tokens of a sentence and generate all possible paths of k arcs spanning k  1 nodes, and apply all constraints to these short paths.	2
The experimental results show the proposed method significantly outperforms both hand-crafted and conventional statistical methods.	The last few years have seen the great success of stochastic part-of-speech POS taggers <TREF>Church, 1988</TREF>: <REF>Kupiec, 1992</REF>; Charniak et M , 1993; <REF>Brill, 1992</REF>; <REF>Nagata, 1994</REF>.	The stochastic approach generally attains 94 to 96 accuracy and replaces the labor-intensive compilation of linguistics rules by using an automated learning algorithm.	However, 1NTT is an abbreviation of Nippon Telegraph and Telephone Corporation.	2
1.	A recent trend in natural language processing has been toward a greater emphasis on statistical approaches, beginning with the success of statistical part-of-speech tagging programs <TREF>Church 1988</TREF>, and continuing with other work using statistical part-of-speech tagging programs, such as BBN PLUM <REF>Weischedel et al 1993</REF> and NYU Proteus <REF>Grishman and Sterling 1993</REF>.	More recently, statistical methods have been applied to domain-specific semantic parsing <REF>Miller et al 1994</REF>, and to the more difficult problem of wide-coverage syntactic parsing <REF>Magerman 1995</REF>.	Nevertheless, most natural language systems remain primarily rule based, and even systems that do use statistical techniques, such as ATT Chronus <REF>Levin and Pieraccini 1995</REF>, continue to require a significant rule based component.	2
Disambiguation of capitalized words is usually handled by POS taggers, which treat capitalized words in the same way as other categories, that is, by accounting for the immediate syntactic context and using estimates collected from a training corpus.	<REF>As Church 1988</REF> rightly pointed out, however, Proper nouns and capitalized words are particularly problematic: some capitalized words are proper nouns and some are not.	Estimates from the Brown Corpus can be misleading.	For example, the capitalized word Acts is found twice in the Brown Corpus, both times as a proper noun in a title.	2
The morphological ambiguity will differ depending on the level of tagging used in each case, as shown in table 2.	There are two kinds of methods for morphological disambiguation: on one hand, statistical methods need little effort and obtain very good results <TREF>Church, 1988</TREF>; Cutting el al, 1992, at least when applied to English, but when we try to apply them to Basque we encounter additional problems; on the other hand, some rule-based systems <REF>Brill, 1992</REF>; <REF>Voutilainen et al, 1992</REF> are at least as good as statistical systems and are better adapted to free-order languages and agglutinative languages.	So, we 381 have selected one of each group: Constraint Grammar formalism <REF>Karlsson et al, 1995</REF> and the HMM based TATOO tagger <REF>Armstrong et al, 1995</REF>, which has been designed to be applied it to the output of a morphological analyser and the tagset can be switched easily without changing the input text.	second  third 70 ks I M M MCG MCG Figure 1-Initial ambiguity3.	2
We are tagging this material with a much simpler tagset than used by previous projects, as discussed at the Oct 1989 DARPA Workshop.	The material is first processed using Ken Churchs tagger <TREF>Church 1988</TREF>, which labels it as if it were Brown Corpus material, and then is mapped to our tagset by a SEDscript.	Because of fundamental differences in tagging strategy between the Penn Treebank Project and the Brown project, the resulting mapping is about 9 inaccurate, given the tagging guidelines of the Penn Treebank project as given in 40 pages of explicit tagging guidelines.	This material is then hand-corrected by our annotators; the result is consistent within annotators to about 3 cf.	2
H90-1055:17.	Deducing Linguistic Structure from the Statistics of Large Corpora Eric Brill David Magerman Mitchell Marcus Beatrice Santorini Department of Computer and Information Science University of Pennsylvania Philadelphia, PA 19104 1 Introduction Within the last two years, approaches using both stochastic and symbolic techniques have proved adequate to deduce lexical ambiguity resolution rules with less than 3-4 error rate, when trained on moderate sized 500K word corpora of English text eg <TREF>Church, 1988</TREF>; <REF>Hindle, 1989</REF>.	The success of these techniques suggests that much of the grammatical structure of language may be derived automatically through distributional analysis, an approach attempted and abandoned in the 1950s.	We describe here two experiments to see how far purely distributional techniques can be pushed to automatically provide both a set of part of speech tags for English, and a grammatical analysis of free English text.	2
This line of research was motivated by a series of successful applications of mutual information statistics to other problems in natural language processing.	In the last decade, research in speech recognition <REF>Jelinek 1985</REF>, noun classification <REF>Hindle 1988</REF>, predicate argument relations <REF>Church  Hanks 1989</REF>, and other areas have shown that mutual information statistics provide a wealth of information for solving these problems.	22 Mutual Information Statistics The mutual information statistic <REF>Fano 1961</REF> is a measure of the interdependence of two signals in a message.	It is a function of the probabilities of the two events: Mz, u  log u xzPvy In this paper, the events x and y will be part-of-speech n-grams instead of single parts-of-speech, as in some earlier work.	2
4 Concluding remarks Though there can be little doubt that the ruling system of bakeoffs actively encourages a degree of oneupmanship, our paper and our software are not offered in a competitive spirit.	As we said at the out211 set, we dont necessarily believe HunPos to be in any way better than TnT, and certainly the main ideas have been pioneered by <REF>DeRose 1988</REF>, <TREF>Church 1988</TREF>, and others long before this generation of HMM work.	But to improve the results beyond what a basic HMM can achieve one needs to tune the system, and progress can only be made if the experiments are end to end replicable.	There is no doubt many other systems could be tweaked further and improve on our results what matters is that anybody could now also tweak HunPos without any restriction to improve the state of the art.	2
In practice, computational limitations do not allow the enumeration of all possible assignments for long sentences, and smoothing is required for infrequent events.	This is described in more detail in the original publication <TREF>Church, 1988</TREF>.	Although more sophisticated algorithms for unsupervised learning which can be trained on plain text instead on manually tagged corpora are well established see eg <REF>Merialdo, 1994</REF>, we decided not to use them.	The main reason is that with large tag sets, the sparse-data-problem can become so severe that unsupervised training easily ends up in local minima, which can lead to poor results without any indication to the user.	3
Much research has been donc Oll knowledge acquisition fiom large-scalc annotated corpora as a rich source of linguistic knowledge.	Mtior works done to create English POS taggers henceforth, taggers, for example, include <TREF>Church 1988</TREF>, <REF>Kupicc 1992</REF>, <REF>Brill 1992</REF>and <REF>Voutilaincn et al 1992</REF>.	The problem with this framework, however, is that such reliable corpora are hardly awdlable duc to a huge amount of the labor-intensive work required.	In case of the acquisition of non-core knowledge, such as specific, lexically or dolnain dependent knowledge, preparation of annotated corpora becomes more serious problem.	3
But dictionaries of technical terminology have many one-word terms.	Simplex or complex NPs eg , <TREF>Church 1988</TREF>; <REF>Hindle and Rooth 1991</REF>; <REF>Wacholder 1998</REF> identify simplex or base NPs  NPs which do not have any component NPs -at least in part because this bypasses the need to solve the quite difficult attachment problem, ie, to determine which simpler NPs should be combined to output a more complex NP.	But if people find complex NPs more useful than simpler ones, it is important to focus on improvement of techniques to reliably identify more complex terms.	Semantic and syntactic terms variants.	3
Thus, Examples 3-5 illustrate how the syntactic context of a word can help determine its meaning.	22 Motivation from previous work 221 Parsing In recent years, the success of statistical parsing techniques can be attributed to several factors, such as the increasing size of computing machinery to accommodate larger models, the availability of resources such as the Penn Treebank <REF>Marcus et al , 1993</REF> and the success of machine learning techniques for lowerlevel NLP problems, such as part-of-speech tagging <TREF>Church, 1988</TREF>; <REF>Brill, 1995</REF>, and PPattachment <REF>Brill and Resnik, 1994</REF>; <REF>Collins and Brooks, 1995</REF>.	However, perhaps even more significant has been the lexicalization of the grammar formalisms being probabilistically modeled: crucially, all the recent, successful statistical parsers have in some way made use of bilexical dependencies.	This includes both the parsers that attach probabilities to parser moves <REF>Magerman, 1995</REF>; <REF>Ratnaparkhi, 1997</REF>, but also those of the lexicalized PCFG variety <REF>Collins, 1997</REF>; <REF>Charniak, 1997</REF>.	2
Recent research advances may lead to the development of viable book indexing methods for Chinese books.	These include the availability of efficient and high precision word segmentation methods for Chinese text <REF>Chang et al , 1991</REF>; <REF>Sproat and Shih, 1990</REF>; <REF>Wang et al , 1990</REF>, the availability of statistical analysis of a Chinese corpus <REF>Liu et al , 1975</REF> and large-scale electronic Chinese dictionaries with partof-speech information <REF>Chang et al , 1988</REF>; BDC, 1992, the corpus-based statistical part-of-speech tagger <TREF>Church, 1988</TREF>; <REF>DeRose, 1988</REF>; <REF>Beale, 1988</REF>, as well as phrasal and clausal analyzers <TREF>Church 1988</TREF>; <REF>Ejerhed 1990</REF> 2.	Problem description As being pointed out in <REF>Salton, 1988</REF>, back-of-book indexes may consist of more than one word that are derived from a noun phrase.	Given the text of a book, an indexing system, must perform some kind of phrasal and statistical analysis in order to produce a list of candidate indexes and their occurrence statistics in order to generate indexes as shown in Figure 1 which is an excerpt from the reconstruction of indexes of a book on transformational grammar for Mandarin Chinese <REF>Tang, 1977</REF>.	2
Much recent research in the field of natural language processing NLP has focused on an empirical, corpus-based approach <REF>Church and Mercer, 1993</REF>.	The high accuracy achieved by a corpus-based approach to part-of-speech tagging and noun phrase parsing, as demonstrated by <TREF>Church, 1988</TREF>, has inspired similar approaches to other problems in natural language processing, including syntactic parsing and word sense disambiguation WSD.	The availability of large quantities of part-ofspeech tagged and syntactically parsed sentences like the Penn Treebank corpus <REF>Marcus, Santorini, and Marcinkiewicz, 1993</REF> has contributed greatly to the development of robust, broad coverage partof-speech taggers and syntactic parsers.	The Penn Treebank corpus contains a sufficient number of partof-speech tagged and syntactically parsed sentences to serve as adequate training material for building broad coverage part-of-speech taggers and parsers.	2
One method of handling large vocabularies is simply increasing the size of the lexicon.	Research efforts at IBM Chodorow, et al 1988; Neff, et al 1989, Bell Labs Church, et al 1989, New Mexico State University <REF>Wilks 1987</REF>, and elsewhere have used mechanical processing of on-line dictionaries to infer at least minimal syntactic and semantic information from dictionary definitions.	However, even assuming a very large lexicon already exists, it can never be complete.	Systems aiming for coverage of unrestricted language in broad domains must continually deal with new words and novel word senses.	3
In order to make these improvements, we need access to word-class information Pos information <REF>Johansson et al 1986</REF>; <REF>Black, Garside, and Leech 1993</REF> or semantic information <REF>Beckwith et al 1991</REF>, which is usually obtained in three main ways: Firstly, we can use corpora that have been manually tagged by linguistically informed experts <REF>Derouault and Merialdo 1986</REF>.	Secondly, we can construct automatic part-ofspeech taggers and process untagged corpora <REF>Kupiec 1992</REF>; <REF>Black, Garside, and Leech 1993</REF>; this method boasts a high degree of accuracy, although often the construction of the automatic tagger involves a bootstrapping process based on a core corpus which has been manually tagged <TREF>Church 1988</TREF>.	The third option is to derive a fully automatic word-classification system from untagged corpora.	Some advantages of this last approach include its applicability to any natural language for which some corpus exists, independent of the degree of development of its grammar, and its parsimonious commitment to the machinery of modern linguistics.	3
If an external resource is used in the form of a morphological analyzer MA, this will almost always overgenerate, yielding false ambiguity.	But even if the MA is tight, a considerable proportion of ambiguous tokens will come from legitimate but rare analyses of frequent types <TREF>Church, 1988</TREF>.	For example the word nem, can mean both not and gender, so both ADV and NOUN are valid analyses, but the adverbial reading is about five orders of magnitude more frequent than the noun reading, 12596 vs 4 tokens in the 1 m word manually annotated Szeged Korpusz <REF>Csendes et al , 2004</REF>.	Thus the difficulty of the task is better measured by the average information required for disambiguating a token.	3
Part-of-speech tagging is required to detect new terms formed through conversion.	This is quite feasible using statistical taggers like those of <REF>Garside 1987</REF>, <TREF>Church 1988</TREF> or <REF>Foster 1991</REF> which achieve performance upwards of 97 on unrestricted text.	Terms formed through semantic drift are the wolves in sheeps clothing stealing through terminological pastures.	They are well enough conceMcd to allude at times even the human reader and no automatic term-recognition system has attempted to distinguish such terms, despite the prevalence ofpolysemy in such fields as the social sciences Riggs, 1993 and the importance for purposes of terminological standardization that deviant usage be tracked.	2
On sentences with <40 words, the former model performs at 69 precision, 75 recall, and the latter at 77 precision and 78 recall.	Ever since the success of HMMs application to part-of-speech tagging in <TREF>Church, 1988</TREF>, machine learning approaches to natural language processing have steadily become more widespread.	This increase has of course been due to their proven efficacy in many tasks, but also to their engineering effiCacy.	Many machine learning approaches let the data speak for itself data ipsa loquuntur, as it were, allowing the modeler to focus on what features of the data are important, rather than on the complicated interaction of such features, as had often been the case with hand-crafted NLP systems.	2
The program can be trained even with a relatively small amount of treebank data; then it can be J used for parsing unrestricted pre-tagged text.	As far as coverage is concerned, our parser can handle recursive structures, which is an advantage compared to simpler techniques such as that described by <TREF>Church 1988</TREF>.	On the other hand, the Markov assumption underlying our approach means that only strictly local dependencies are recognised.	For full parsing, one would probably need non-local contextual information, such as the long-range trigrams in Link Grammar Della <REF>Pietra et al , 1994</REF>.	3
Purely rule-based techniques seemed too brittle for dealing with the variety of constructions, the long sentences averaging 29 words per sentence, and the degree of unexpected input.	Statistical models based on local information eg , <REF>DeRose 1988</REF>; <TREF>Church 1988</TREF> might operate effectively in spite of sentence length and unexpected input.	To see whether our four hypotheses in italics above effectively addressed the four concerns above, we chose to test the hypotheses on two well-known problems: ambiguity both at the structural level and at the part-of-speech level and inferring syntactic and semantic information about unknown words.	Guided by the past success of probabilistic models in speech processing, we have integrated probabilistic models into our language processing systems.	2
We report in Section 2 on our experiments on the assignment of part of speech to words in text.	The effectiveness of such models is well known <REF>DeRose 1988</REF>; <TREF>Church 1988</TREF>; <REF>Kupiec 1989</REF>; <REF>Jelinek 1985</REF>, and they are currently in use in parsers eg de <REF>Marcken 1990</REF>.	Our work is an incremental improvement on these models in three ways: 1 Much less training data than theoretically required proved adequate; 2 we integrated a probabilistic model of word features to handle unknown words uniformly within the probabilistic model and measured its contribution; and 3 we have applied the forward-backward algorithm to accurately compute the most likely tag set.	In Section 3, we demonstrate that probability models can improve the performance of knowledge-based syntactic and semantic processing in dealing with structural ambiguity and with unknown words.	2
This was expanded upon by <REF>Gale et al , 1992</REF>, and in a class-based variant by <REF>Yarowsky, 1992</REF>.	Decision trees <REF>Brown, 1991</REF> have been usefully applied to word-sense ambiguities, and HMM part-of-speech taggers <REF>Jelinek 1985</REF>, <TREF>Church 1988</TREF>, <REF>Merialdo 1990</REF> have addressed the syntactic ambiguities presented here.	<REF>Hearst 1991</REF> presented an effective approach to modeling local contextual evidence, while <REF>Resnik 1993</REF> gave a classic treatment of the use of word classes in selectional constraints.	An algorithm for combining syntactic and semantic evidence in lexical ambiguity resolution has been realized in <REF>Chang et al , 1992</REF>.	2
Preprocessing the Corpus with a Part of Speech Tagger Phrasal verbs involving the preposition to raise an interesting problem because of the possible confusion with the infinitive marker to.	We have found that if we first tag every word in the corpus with a part of speech using a method such as <TREF>Church 1988</TREF> or <REF>DeRose 1988</REF>, and then measure associations between tagged words, we can identify interesting contrasts between verbs associated with a following preposition toin and verbs associated with a following infinitive marker toto.	Part of speech notation is borrowed from <REF>Francis and Kucera 1982</REF>; m  preposition; to  infinitive marker; vb  bare verb; vbg  verb  ing; vbd  verb  ed; vbz  verb  s; vbn  verb  en.	The score identifies quite a number of verbs associated in an interesting way with to; restricting our attention to pairs with a score of 30 or more, there are 768 verbs associated with the preposition toin and 551 verbs with the infinitive marker toto.	2
In the partof-speech tagging field, the disambiguation of capitalized words is treated similarly to the disambiguation of common words.	However, as <TREF>Church 1988</TREF> rightly pointed out Proper nouns and capitalized words are particularly problematic: some capitalized words are proper nouns and some are not.	Estimates from the Brown Corpus can be misleading.	For example, the capitalized word Acts is found twice in Brown Corpus, both times as a proper noun in a title.	2
Given that each token has on the average more than 2 possible tags, the procedural description above is very inefficient for M1 but very short sentences.	However, the observation that our constraints are localized to a window of a small number of tokens say at most 5 tokens in a sequence, suggests a more efficient scheme originally used by <TREF>Church 1988</TREF>.	Assume our constraint windows are allowed to look at a window of at most size k sequential parses.	Let us take the first k tokens of a sentence and generate all possible paths of k arcs spanning k  1 nodes, and apply all constraints to these short paths.	2
1; they closely replicate Brills results 1993b, page 96, allowing for the fact that his tests used more templates, including templates like if one of the three previous tags is A.	Brills results demonstrate that this approach can outperform the Hidden Markov Model approaches that are frequently used for part-of-speech tagging <REF>Jelinek, 1985</REF>; <TREF>Church, 1988</TREF>; <REF>DeRose, 1988</REF>; <REF>Cutting et al , 1992</REF>; <REF>Weischedel et al , 1993</REF>, as well as showing promise for other applications.	The resulting model, encoded as a list of rules, is also typically more compact and for some purposes more easily interpretable than a table of HMM probabilities.	An Incremental Algorithm It is worthwhile noting first that it is possible in some circumstances to significantly speed up the straightforward algorithm described above.	3
We rewrite this term as follows: PrW1,ND1,N N  I-IPrWiDilWl,ilDl,il i1 N  l-I PrWilWl,i-lDl,i PrDilWl,i-lDl,i-1 i1 7 Equation 7 involves two probability distributions that need to be estimated.	These are the same distributions that are needed by previous POS-based language models Equation 5 and POS taggers <TREF>Church 1988</TREF>; <REF>Charniak et al 1993</REF>.	However, these approaches simplify the context so that the lexical probability is just conditioned on the POS category of the word, and the POS probability is conditioned on just the preceding POS tags, which leads to the following two approximations.	PrWiIWl,ilDl,i  PrWilDi 8 PrDiIWulDl,il  PrDiIDul 9 However, to successfully incorporate POS information, we need to account for the full richness of the probability distributions, as will be demonstrated in Section 344.	3
1992 circumvent this problem by training their taggers on untagged data using tile Itaum-Welch algorithm also know as the forward-backward algorithm.	They report rates of correctly tagged words which are comparable to that presented by <TREF>Church 1988</TREF> and <REF>Kempe 1993</REF>.	A third and rather new approach is tagging with artificial neural networks.	In the area of speech recognition neural networks have been used for a decade r, ow.	2
In practice, computational limitations do not allow the enumeration of all possible assignments for long sentences, and smoothing is required for infrequent events.	This is described in more detail in the original publication <TREF>Church, 1988</TREF>.	Although more sophisticated algorithms for unsupervised learning which can be trained on plain text instead on manually tagged corpora are well established see eg <REF>Merialdo, 1994</REF>, we decided not to use them.	The main reason is that with large tag sets, the sparse-data-problem can become so severe that unsupervised training easily ends up in local minima, which call lead to poor results without any indication to the user.	3
Indeed, recent increased interest in the problem of disambiguating lexical category in English has led to significant progress in developing effective programs for assigning lexical category in unrestricted text.	The most successful and comprehensive of these are based on probabilistic modeling of category sequence and word category <REF>Church 1987</REF>; <REF>Garside, Leech and Sampson 1987</REF>; <REF>DeRose 1988</REF>.	These stochastic methods show impressive performance: Church reports a success rate of 95 to 99, and shows a sample text with an error rate of less than one percent.	What may seem particularly surprising is that these methods succeed essentially without reference to syntactic structure; purely surface lexical patterns are involved.	2
We redistribute the probability mass of low count sequences to unseen sequences.	Generalized Forward Backward Reestimation Generalization of the Forward and Viterbi Algorithm In English part of speech taggers, the maximization of Equation 1 to get the most likely tag sequence, is accomplished by the Viterbi algorithm <TREF>Church, 1988</TREF>, and the maximum likelihood estimates of the parameters of Equation 2 are obtained from untagged corpus by the ForwardBackward algorithm <REF>Cutting et al , 1992</REF>.	However, it is impossible to apply the Viterbi algorithm and the Forward-Backward algorithm for word segmentation of those languages that have no delimiter between words, such as Japanese and Chinese, because word segmentation hypotheses overlap one another.	Figure 3 shows an example of overlapping word hypotheses and possible word segmentations for the string Ntig-f all prefectures in the nation.	3
problem.	Excellent methods have been developed for part-of-speech POS tagging using stochastic models trained on partially tagged corpora <TREF>Church, 1988</TREF>; Cutting, <REF>Kupiec, Pedersen  Sibun, 1992</REF>.	Semantic issues have been addressed, particularly for sense disambiguation, by using large contexts, eg, 50 nearby words <REF>Gale, Church  Yarowsky, 1992</REF> or by reference to on-line dictionaries <REF>Krovetz, 1991</REF>; <REF>Lesk, 1986</REF>; <REF>Liddy  Paik, 1992</REF>; <REF>Zernik, 1991</REF>.	More recently, methods to work with entirely untagged corpora have been developed which show great promise <REF>Brill  Marcus, 1992</REF>; <REF>Finch  Chater, 1992</REF>; <REF>Myaeng  Li, 1992</REF>; <REF>Schutze, 1992</REF>.	2
to appear, <REF>Hearst 1991</REF>, <REF>Lesk 1986</REF>, <REF>Smadja and McKeown 1990</REF>, <REF>Walker 1987</REF>, <REF>Veronis and Ide 1990</REF>, <REF>Yarowsky 1992</REF>, Zemik 1990, 1991.	Much of this work offers the prospect that a disambiguation system might be able to input unrestricted text and tag each word with the most likely sense with fairly reasonable accuracy and efficiency, just as part of speech taggers eg , <TREF>Church 1988</TREF> can now input unrestricted text and assign each word with the most likely part of speech with fairly reasonable accuracy and efficiency.	The availability of massive lexicographic databases offers a promising route to overcoming the knowledge acquisition bottleneck.	More than thirty years ago, BarI-<REF>Iillel 1960</REF> predicted that it would be futile to write expert-system-like rules by-hand as they had been doing at Georgetown at the time because there would be no way to scale up such rules to cope with unrestricted input.	2
It shows the descriptive power of low-level morphology-based constraints.	The most successful achievements so far in the domain of large-scale morphological disambiguation of running text have been those for English reported by <REF>Garside, Leech, and Sampson 1987</REF>, on tagging the LOB corpus, and <TREF>Church 1988</TREF>, on assigning part-of-speech labels and parsing noun phrases.	Success rates ranging between 95-99 are reported, depending on how success is defined.	These approaches are probabilistic and based on transitional probabilities calculated from extensive pretagged corpora.	2
Additionally, there is a slight but not significant improvement of tagging accuracy.	Statistical part-of-speech disambiguation can be efficiently done with n-gram models <TREF>Church, 1988</TREF>; <REF>Cutting et al , 1992</REF>.	These models are equivalent to Hidden Markov Models HMMs <REF>Rabiner, 1989</REF> of order n 1.	The states represent parts of speech categories, tags, there is exactly one state for each category, and each state outputs words of a particular category.	2
In the past decade, the speech recognition community has had huge successes in applying hidden Markov models, or HMMs to their problems.	More recently, the natural language processing community has effectively employed these models for part-ofspeech tagging, as in the seminal <TREF>Church, 1988</TREF> and other, more recent efforts <REF>Weischedel et al , 1993</REF>.	We would now propose that HMMs have successfully been applied to the problem of name-finding.	We have built a named-entity NE recognition system using a slightly-modified version of an HMM; we call our system Nymble.	2
The computational tools available for studying machinereadable corpora are at present still rather primitive.	These are concordancing programs see Figure 1, which are basically KWIC key word in context; <REF>Aho et al 1988</REF> indexes with additional features such as the ability to extend the context, sort leftward as well as rightward, and so on.	There is very little interactive software.	In a typical situation in the lexicography of the 1980s, a lexicographer is giwen the concordances for a word, marks up the printout with colored pens to identify the salient senses, and then writes syntactic descriptions and definitions.	3
2 Relation to Previous Works Quite a few works have dealt with extending a given POS tagger, mainly by smoothing it using extra-information about untreated words.	For example, <TREF>Church, 1988</TREF> uses the simple heuristic of predicting proper nouns from capitalization.	This method is not applicable to Arabic and Hebrew, which lack typographical marking of proper nouns.	More advanced methods like those described by Weischedel et al.	3
Work on the use of synchronous TAGs to capture quantifier scoping possibilities makes use of so-called multicomponent TAGs.	Finally, the base TAGs may be lexicalized <TREF>Schabes et al , 1988</TREF> or not.	Once the base formalism has been decided upon we currently are using lexicalized multi-component TAGs with substitution and adjunction, a simple translation strategy from a source string to a target is to parse the string using an appropriate TAG parser for the base formalism.	Each derivation of the source string can be mapped according to the synchronizing links in the grammar to a target derivation.	2
Also, the provision of conceptual entities which are incrementally generated by the semantic interpretation process supplies the necessary anchoring points for the continuous resolution of textual anaphora and ellipses <REF>Strube  Hahn, 1995</REF>; <REF>Hahn et al , 1996</REF>.	The lexical distribution of grammatical knowledge one finds in many lexiealized grammar formalisms eg , LTAGS <TREF>Schabes et al , 1988</TREF> or HPSG <REF>Pollard  Sag, 1994</REF> is still constrained to declarative notions.	Given that the control flow of text understanding is globally unpredictable and, also, needs to be purposefully adapted to critical states of the analysis eg , cases of severe extragrammaticality, we drive lexicalization to its limits in that we also incorporate procedural control knowledge at the lexical gr,unmar level.	The specification of lexiealized communication primitives allows heterogeneous and local lorms of interaction among groups of lexical items.	3
Formally, a derivation tree is represented as a set of dependencies: D   i,  j,r i , where  i is an elementary tree,   i represents a node in  j where substitution/adjunction has occurred, and r i is a label of the applied rule, ie, adjunction or substitution.	A probability of derivation tree D   i,  j,r i  is generally defined as follows <TREF>Schabes et al , 1988</TREF>; <REF>Chiang, 2000</REF>.	pD productdisplay i p i   j,r i  Note that each probability on the right represents the syntactic/semantic preference of a dependency of two lexical items.	We can readily see that the model is very similar to LPCFG models.	2
That is, the models are still based on decomposition into primitive lexical dependencies.	Derivation trees, the structural description in LTAG <TREF>Schabes et al , 1988</TREF>, represent the association of lexical items ie, elementary trees.	In LTAG, all syntactic constraints of words are described in an elementary tree, and the dependencies of elementary trees, ie, a derivation tree, describe the semantic relations of words more directly than lexicalized parse trees.	For example, Figure 3 has a derivation tree corresponding to the parse tree in Figure 1 2.	2
A more linguistically motivated approach is to expand the domain of productions downward to incorporate more tree structures.	The Lexicalized Tree-Adjoining Grammar LTAG formalism <TREF>Schabes et al , 1988</TREF>, <REF>Schabes, 1990</REF>, although not context-free, is the most well-known instance in this category.	PLTIGs belong to this third category and generate only context-free languages.	LTAGs and LTIGs are tree-rewriting systems, consisting of a set of elementary trees combined by tree operations.	2
Thus CCG assigns the following two groupings to John likes apples: 2 John likes apples 3 John likes apples The work on CCG was presented by Mark Steedman in an earlier DARPA SLS Workshop <REF>Steedman, 1989</REF>.	In this paper, we show how a CCG-like account for coordination can be constructed in the framework of lexicalized tree-adjoining grammars TAGs <REF>Joshi, 1987</REF>; <TREF>Schabes et al , 1988</TREF>; <REF>Schabes, 1990</REF>.	2.	In particular, we show how a fixed constituency can be maintained at the level of the elementary trees of lexicalized TAGs and yet be able to achieve the kind of flexibility needed for dealing with the so-called non-constituents.	2
which cannot be felicitously uttered except in a context where there is something in the discourse that a restriction could apply to.	Conventional approaches to subcategorization, such as Definite Clause Grammar <REF>Pereira and Warren, 1980</REF>, Categorial Grammar <REF>Ades and Steedman, 1982</REF>, PATR-II <REF>Shieber, 1986</REF>, and lexicalized TAG <TREF>Schabes et al, 1988</TREF> all deal with complementation by including in one form or another a notion of subcategorization frame that specifies a sequence of complement phrases and constraints on them.	Handling all the possible variations in complement distribution in such formalisms inevitably leads to an explosion in the number of such frames, and a correspondingly more difficult task in porting to a new domain.	In our approach, on the other hand, it becomes possible to view subcategorization of a lexical item as a set of constraints on the outgoing arcs of its semantic graph node.	3
Recently there has been a gain in interest in the so-called mildly context-sensitive formalisms Vijay-<REF>Shanker, 1987</REF>; <REF>Weir, 1988</REF>; <REF>Joshi, VijayShanker, and Weir, 1991</REF>; Vijay-<REF>Shanker and Weir, 1993a</REF> that generate only a small superset of context-free languages.	One such formalism is lexicalized tree-adjoining grammar LTAG Schabes, Abeill, and <REF>Joshi, 1988</REF>; <REF>Abeillfi et al , 1990</REF>; <REF>Joshi and Schabes, 1992</REF>, which provides a number of attractive properties at the cost of decreased efficiency, On6-time in the worst case <REF>VijayShanker, 1987</REF>; <REF>Schabes, 1991</REF>; <REF>Lang, 1990</REF>; <REF>VijayShanker and Weir, 1993b</REF>.	An LTAG lexicon consists of a set of trees each of which contains one or more lexical items.	These elementary trees can be viewed as the elementary clauses including their transformational variants in which the lexical items participate.	2
Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation.	At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar LFG <REF>Bresnan, 1982</REF>, Lexicalized Tree Adjoining Grammar LTAG <TREF>Schabes et al , 1988</TREF>, Headdriven Phrase Structure Grammar HPSG <REF>Pollard and Sag, 1994</REF> and Combinatory Categorial Grammar CCG <REF>Steedman, 2000</REF>, which represent deep syntactic structures that cannot be expressed in a shallower formalism designed to represent only aspects of surface syntax, such as the dependency formalism used in current mainstream dependency parsing.	We present a novel framework that combines strengths from surface syntactic parsing and deep syntactic parsing, specifically by combining dependency and HPSG parsing.	We show that, by using surface dependencies to constrain the application of wide-coverage HPSG rules, we can benefit from a number of parsing techniques designed for high-accuracy dependency parsing, while actually performing deep syntactic analysis.	2
This paper will concentrate on context-free grammars CFG and their associated parsers.	However, virtually all Tree Adjoining Grammars TAG, see eg, <TREF>Schabes et al , 1988</TREF> used in NLP applications can almost be seen as lexicalized Tree Insertion Grammars TIG, which can be converted into strongly equivalent CFGs <REF>Schabes and Waters, 1995</REF>.	Hence, the parsing techniques and tools described here can be applied to most TAGs used for NLP, with, in the worst case, a light over-generation which can be easily and efficiently eliminated in a complementary pass.	This is indeed what we have achieved with a TAG automatically extracted from Villemonte de <REF>La Clergerie, 2005</REF>s large-coverage factorized French TAG, as we will see in Section 4.	2
We can thus define lexical transfer rules that avoid the defects of a mere word-to-word approach but still benefit from the simplicity and elegance of a lexical approach.	We rely on the French and English LTAG grammars Abeille 1988, Abeille 1990 b, Abeilld et al 1990, Abeill6 and Schabes 1989, 1990 that have been designed over the past two years jointly at University of Pennsylvania and University of Paris 7-Jussieu.	1 Strategy for Machine Translation with LTAGs The idea of using grammars written with lexicalist formalisms for machine translation is not new This research was partially ftmded by ARO grant DAAG29-84-K-0061, DARPA grant N00014-85-K0018, and NSF grant MCS-82-19196 at the University of Pen nsylvania.	We are indebted to Stuart Shieber for his valuable comments.	2
3 A TAG Analysis The TAG formalism for a recent introduction, see <REF>Joshi 1987a</REF> is well suited for linguistic description because 1 it provides a larger domain of locality than a CFG or other augmented CFG-based formalisms such as tlPSG or LFG, and 2 it allows factoring of recursion from the domain of dependencies.	This extended domain of locality, provided by the elementary trees of TAG, allows us to lexicalize a TAG grammar: we can associate each tree in a grammar with a lexical item <TREF>Schabes et al 1988</TREF>, <REF>Schabes 1990</REF> 4.	The tree will contain the lexical item, and all of its syntac3Some verbs allow scrambling out of their Complements more freely than others.	It appears that all subject-control verbs and most object-control verbs governing the dative allow scrambling fairly fely, while scrambling with objectcontrol verbs governing the accusative is more restricted cir.	2
178 6 Comparison to PSG Approaches One feature of word order domains is that they factor ordering alternatives from the syntactic tree, much like feature annotations do for morphological alternatives.	Other lexicalized grammars collapse syntactic and ordering information and are forced to represent ordering alternatives by lexical ambiguity, most notable L-TAG <TREF>Schabes et al , 1988</TREF> and some versions of CG <REF>Hepple, 1994</REF>.	This is not necessary in our approach, which drastically reduces the search space for parsing.	This property is shared by the proposal of <REF>Reape 1993</REF> to associate HPSG signs with sequences of constituents, also called word order domains.	3
We also investigate the reason for that difference.	Various parsing techniques have been developed for lexicalized grammars such as Lexicalized Tree Adjoining Grammar LTAG <TREF>Schabes et al , 1988</TREF>, and Head-Driven Phrase Structure Grammar HPSG <REF>Pollard and Sag, 1994</REF>.	Along with the independent development of parsing techniques for individual grammar formalisms, some of them have been adapted to other formalisms <TREF>Schabes et al , 1988</TREF>; van <REF>Noord, 1994</REF>; <REF>Yoshida et al , 1999</REF>; <REF>Torisawa et al , 2000</REF>.	However, these realizations sometimes exhibit quite different performance in each grammar formalism <REF>Yoshida et al , 1999</REF>; <REF>Yoshinaga et al , 2001</REF>.	3
Various parsing techniques have been developed for lexicalized grammars such as Lexicalized Tree Adjoining Grammar LTAG <TREF>Schabes et al , 1988</TREF>, and Head-Driven Phrase Structure Grammar HPSG <REF>Pollard and Sag, 1994</REF>.	Along with the independent development of parsing techniques for individual grammar formalisms, some of them have been adapted to other formalisms <TREF>Schabes et al , 1988</TREF>; van <REF>Noord, 1994</REF>; <REF>Yoshida et al , 1999</REF>; <REF>Torisawa et al , 2000</REF>.	However, these realizations sometimes exhibit quite different performance in each grammar formalism <REF>Yoshida et al , 1999</REF>; <REF>Yoshinaga et al , 2001</REF>.	If we could identify an algorithmic difference that causes performance difference, it would reveal advantages and disadvantages of the different realizations.	2
The parser achieves an OGn6-time worst case behavior, OG2n4-time for unambiguous grammars and linear time for a large class of grammars.	The parser uses the following two-pass parsing strategy originally defined for lexicalized grammars <TREF>Schabes et al , 1988</TREF> which improves its performance in practice <REF>Schabes and Joshi, 1990</REF>:  In the first step the parser will select, the set of structures corresponding to each word in the sentence.	Each structure can be considered as encoding a set of rules.	In the second step, the parser tries to see whether these structures can be combined to obtain a wellformed structure.	2
This information is particularly useful for a top-down component of the parser <REF>Schabes and Joshi, 1990</REF>.	XTAG provides all the utilities required for designing a lexicalized TAG structured as in Schabes et al 1988.	All the syntactic concepts of lexicalized TAG such as the grouping of the trees in tree families which represents the possible variants on a basic subcategorization frame are accessible through mouse-sensitive items.	Also, all the operations required to build a grammar such as load trees, define tree families, load syntactic and morphological lexicon can be predefined with a macro-like language whose instructions can be loaded from a file See Figure 5.	2
See the introduction by Joshi 1987 for an introduction to tree-adjoining grammar.	We refer the reader to Joshi 1985, Joshi 1987, Kroch and Joshi 1985, Abeill et al 1990a, Abeill 1988 and to Joshi and Schabes 1991 for more information on the linguistic characteristics of TAG such as its lexicalization and factoring recursion out of dependencies.	2The TAG derivation tree is the basis for semantic interpretation <REF>Shieber and Schabes, 1990b</REF>, generation <REF>Shieber and Schabes, 1991</REF> and machine translation Abeill et al , 1990b since the information given in this data-structure is richer than the one found in the derived tree.	Furthermore, it is at the level of the derivation tree that ambiguity must be defined.	2
Interestingly, several studies suggested that the identification of PropBank annotations would require linguistically-motivated features that can be obtained by deep linguistic analysis <REF>Gildea and Hockenmaier, 2003</REF>; <REF>Chen and Rambow, 2003</REF>.	They employed a CCG <REF>Steedman, 2000</REF> or LTAG <TREF>Schabes et al , 1988</TREF> parser to acquire syntactic/semantic structures, which would be passed to statistical classifier as features.	That is, they used deep analysis as a preprocessor to obtain useful features for training a probabilistic model or statistical classifier of a semantic argument identifier.	These results imply the superiority of deep linguistic analysis for this task.	2
All errors are of course our own.	As for Lexicalized TAGs, in <TREF>Schabes et al , 1988</TREF> a two step algorithm has been presented: during the first step the trees corresponding to the input string are selected and in the second step the input string is parsed with respect to this set of trees.	Another paper by <REF>Schabes and Joshi 1989</REF> shows how parsing strategies can take advantage of lexicalization in order to improve parsers performance.	Two major advantages have been discussed in the cited work: grammar filtering the parser can use only a subset of the entire grammar and bottom-up information further constraints are imposed on the way trees can be combined.	2
In <REF>Kroch and Joshi, 1985</REF> a detailed discussion of the linguistic relevance of TAGs can be found.	Lexicalized Tree Adjoining Grammars <TREF>Schabes et al , 1988</TREF> are a refinement of TAGs such that each elementary tree is associated with a lexieal item, called the anchor of the tree.	Therefore, Lexicalized TAGs conform to a common tendency in modem theories of grammar, namely the attempt to embed grammatical information within lexical items.	Notably, the association between elementary trees and anchors improves also parsing performance, as will be discussed below.	2
A more linguistically motivated approach is to expand the domain of productions downward to incorporate more tree structures.	The Lexicalized Tree-Adjoining Grammar LTAG formalism <TREF>Schabes et al, 1988</TREF>, <REF>Schabes, 1990</REF> , although not context-free, is the most well-known instance in this category.	PLTIGs belong to this third category and generate only context-free languages.	LTAGs and LTIGs are tree-rewriting systems, consisting of a set of elementary trees combined by tree operations.	2
E87-1042:140.	TEMPORAL REASONING IN NATURAL LANGUAGE UNDERSTANDING: THE TEMPORAL STRUCTURE OF THE NARRATIVE Alexander Nakhimovsky Department of Computer Science Colgate University Hamilton, NY 13346 USA CSNet: sashacolgate Abstract This paper proposes a new framework for discourse analysis, in the spirit of <TREF>Grosz and Sidner 1986</TREF>, Webber 1987a,b but differentiated with respect to the type or genre of discourse.	It is argued that different genres call for different representations and processing strategies; particularly important is the distinction between subjective, pefformative discourse and objective discourse, of which narrative is a primary example.	This paper concentrates on narratives and introduces the notions of temporal focus proposed also in <REF>Webber 1987b</REF> and narrative move.	3
1; they closely replicate Brills results 1993b, page 96, allowing for the fact that his tests used more templates, including templates like if one of the three previous tags is A.	Brills results demonstrate that this approach can outperform the Hidden Markov Model approaches that are frequently used for part-of-speech tagging <REF>Jelinek, 1985</REF>; <REF>Church, 1988</REF>; <TREF>DeRose, 1988</TREF>; <REF>Cutting et al , 1992</REF>; <REF>Weischedel et al , 1993</REF>, as well as showing promise for other applications.	The resulting model, encoded as a list of rules, is also typically more compact and for some purposes more easily interpretable than a table of HMM probabilities.	An Incremental Algorithm It is worthwhile noting first that it is possible in some circumstances to significantly speed up the straightforward algorithm described above.	3
Indeed, recent increased interest in the problem of disambiguating lexical category in English has led to significant progress in developing effective programs for assigning lexical category in unrestricted text.	The most successful and comprehensive of these are based on probabilistic modeling of category sequence and word category <REF>Church 1987</REF>; <REF>Garside, Leech and Sampson 1987</REF>; <TREF>DeRose 1988</TREF>.	These stochastic methods show impressive performance: Church reports a success rate of 95 to 99, and shows a sample text with an error rate of less than one percent.	What may seem particularly surprising is that these methods succeed essentially without reference to syntactic structure; purely surface lexical patterns are involved.	2
4 Concluding remarks Though there can be little doubt that the ruling system of bakeoffs actively encourages a degree of oneupmanship, our paper and our software are not offered in a competitive spirit.	As we said at the out211 set, we dont necessarily believe HunPos to be in any way better than TnT, and certainly the main ideas have been pioneered by <TREF>DeRose 1988</TREF>, <REF>Church 1988</REF>, and others long before this generation of HMM work.	But to improve the results beyond what a basic HMM can achieve one needs to tune the system, and progress can only be made if the experiments are end to end replicable.	There is no doubt many other systems could be tweaked further and improve on our results what matters is that anybody could now also tweak HunPos without any restriction to improve the state of the art.	2
54 29 3 43 40 4 97 69 7 These results are remarkably good, in spite of the fact that many other systems are reported to reach an accuracy of 9697.	<REF>Garside 1987</REF>, <REF>Marshall 1987</REF>, <TREF>DeRose 1988</TREF>, <REF>Church 1988</REF>, <REF>Ejerhed 1987</REF>, O<REF>Shaughnessy 1989</REF>.	Those systems, however, all use heavier artillery than MorP, that has been deliberately restricted in accordance with the hypotheses presented above.	This restrictiveness concerns both the size of the lexicon and the ways of carrying out disambiguation.	3
Although finite-state machines have been used for part-of-speech tagging <REF>Tapanainen and Voutilainen 1993</REF>; <REF>Silberztein 1993</REF>, none of these approaches has the same flexibility as stochastic techniques.	Unlike stochastic approaches to part-of-speech tagging <REF>Church 1988</REF>; <REF>Kupiec 1992</REF>; <REF>Cutting et al 1992</REF>; <REF>Merialdo 1990</REF>; <TREF>DeRose 1988</TREF>; <REF>Weischedel et al 1993</REF>, up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired.	<REF>Recently, Brill 1992</REF> described a rule-based tagger that performs as well as taggers based upon probabilistic models and overcomes the limitations common in rule-based approaches to language processing: it is robust and the rules are automatically ac Mitsubishi Electric Research Laboratories, 201 Broadway, Cambridge, MA 02139.	E-mail: rocbe/schabesmerlcom.	2
Unknown Words Unknown Common Words Unknown Proper Nouns Tagger Guesser Metrics Error Error Coverage Error Coverage HMM Xerox mean 17851643 30022169 37567270 10785563 63797113 s-error 0484710 0469922 1687396 0613745 1714969 HMM Cascade mean 12378716 21266264 36507909 7776456 64795969 s-error 0917656 0403957 2336381 0853958 2206457 Brill Brill mean 14688501 27411736 38998687 6439525 62160917 s-error 0908172 0539634 2627234 0501082 4010992 Brill Cascade mean 11327863 20986240 37933048 5548990 63816586 s-error 0761576 0480798 2353510 0561009 3775991 the Brown Corpus, we obtained the error rate mean 0 4003093 with the standard error deB0155599.	This agrees with the results on the closed dictionary ie , without unknown words obtained by other researchers for this class of the model on the same corpus <REF>Kupiec 1992</REF>; <TREF>DeRose 1988</TREF>.	The Brill tagger showed some better results: error rate mean 0 3327366 with the standard error deBO 123903.	Although our primary goal was not to compare the taggers themselves but rather their performance with the guessing components, we attribute the difference in their performance to the fact that Brills tagger uses the information about the most likely tag for a word whereas the HMM tagger did not have this information and instead used the priors for a set of POS-tags ambiguity class.	2
for evaluation at word level, choose the most probable tag for each word in the sentence argmax argmax Wi  t pti  t/W  t  pW, T T:tit where Wi is the tag assigned to word wi by the tagging procedure b in the context of the sentence W, We call this procedure Maximum Likelihood ML tagging.	It is interesting to note that the most commonly used method is Viterbi tagging see <TREF>DeRose 1988</TREF>; <REF>Church 1989</REF> although it is not the optimal method for evaluation at word level.	The reasons for this preference are presumably that:  Viterbi tagging is simpler to implement than ML tagging and requires less computation although they both have the same asymptotic complexity  Viterbi tagging provides the best interpretation for the sentence, which is linguistically appealing  ML tagging may produce sequences of tags that are linguistically impossible because the choice of a tag depends on all contexts taken together.	However, in our experiments, we will show that Viterbi and ML tagging result in very similar performance.	2
Although methods for unsupervised training of HMMs do exist, training is usually done in a supervised way by estimation of the above probabilities from relative frequencies in the training data.	The HMM approach to tagging is by far the most studied and applied <REF>Church 1988</REF>; <TREF>DeRose 1988</TREF>; <REF>Charniak 1993</REF>.	In van <REF>Halteren, Zavrel, and Daelemans 1998</REF> we used a straightforward implementation of HMMs, which turned out to have the worst accuracy of the four competing methods.	In the present work, we have replaced this by the TnT system we will refer to this tagger as HMM below.	2
In this paper, we report on experimental work dealing with the part-of-speech tagging of a corpus of transcribed spoken Swedish.	The tagger used implements a standard probabilistic biclass model see, e g, <TREF>DeRose 1988</TREF> trained on a tagged subset of the Stockhohn-Ume Corpus of written Swedish <REF>Ejerhed et al 1992</REF>.	Given that the transcriptions contain many modifications of standard orthography in order to capture spoken language variants, reductions, etc.	a special lexicon had to be developed to map spoken langnage variants onto their canonical written language forms.	2
Purely rule-based techniques seemed too brittle for dealing with the variety of constructions, the long sentences averaging 29 words per sentence, and the degree of unexpected input.	Statistical models based on local information eg , <TREF>DeRose 1988</TREF>; <REF>Church 1988</REF> might operate effectively in spite of sentence length and unexpected input.	To see whether our four hypotheses in italics above effectively addressed the four concerns above, we chose to test the hypotheses on two well-known problems: ambiguity both at the structural level and at the part-of-speech level and inferring syntactic and semantic information about unknown words.	Guided by the past success of probabilistic models in speech processing, we have integrated probabilistic models into our language processing systems.	2
We report in Section 2 on our experiments on the assignment of part of speech to words in text.	The effectiveness of such models is well known <TREF>DeRose 1988</TREF>; <REF>Church 1988</REF>; <REF>Kupiec 1989</REF>; <REF>Jelinek 1985</REF>, and they are currently in use in parsers eg de <REF>Marcken 1990</REF>.	Our work is an incremental improvement on these models in three ways: 1 Much less training data than theoretically required proved adequate; 2 we integrated a probabilistic model of word features to handle unknown words uniformly within the probabilistic model and measured its contribution; and 3 we have applied the forward-backward algorithm to accurately compute the most likely tag set.	In Section 3, we demonstrate that probability models can improve the performance of knowledge-based syntactic and semantic processing in dealing with structural ambiguity and with unknown words.	2
However, at least 30 of the dates and times in the MUC test were fixed-format ones occurring in document headers, trailers, and copyright notices.	 Finally, there is a large body of work, eg, <TREF>Moens and Steedman 1988</TREF>, <REF>Passoneau 1988</REF>, <REF>Webber 1988</REF>, <REF>Hwang 1992</REF>, <REF>Song and Cohen 1991</REF>, that has focused on a computational analysis of tense and aspect.	While the work on event chronologies is based on some of the notions developed in that body of work, we hope to further exploit insights from previous work.	Conclusion We have developed a temporal annotation specification, and an algorithm for resolving a class of time expressions found in news.	2
Other aspects of our ontology are designed following proposals by <REF>Jackendoff 1990</REF>, in particular his analysis of movement events.	2 <TREF>Moens and Steedman 1988</TREF> also use this term, but they restrict it to momentaneous events.	Unfortunately, the terminology used in the literature for these kinds of categories varies so much that a standardization seems out of reach.	404 Stede Verb Alternations event1 fill conf---- > not-full -state-1    f > pa>btination  fill-state-2   water-1  value > full Figure 2 SitSpec representing a fill-event.	3
As their central feature we take them to always involve some change of state: the building loses its integrity, the book comes into existence, or gets finished.	<REF>While Bach 1986</REF> did not investigate the internal structure of events, others suggested that this needs to be done eg , <TREF>Moens and Steedman 1988</TREF>; <REF>Parsons 1990</REF>.	<REF>Pustejovsky 1991</REF> treated Vendlerian accomplishments and achievements as transitions from a state Qy to NOT-Qy, and suggested that accomplishments in addition have an intrinsic agent performing an activity that brings about the change of state.	We follow this line, but modify it in some ways.	3
In this paper, we show how one can find and exploit approximate solutions for both of these problems by capitalizing on the occurrences of certain lexicogrammatical constructs.	Such constructs can include tense 96 and aspect <TREF>Moens and Steedman, 1988</TREF>; <REF>Webber, 1988</REF>; <REF>Lascarides and Asher, 1993</REF>, certain patterns of pronominalization and anaphoric usages <REF>Sidner, 1981</REF>; <REF>Grosz and Sidner, 1986</REF></REF>; <REF>Sumita et al , 1992</REF>; <REF>Grosz, Joshi, and Weinstein, 1995</REF>,/t-clefts <REF>Delin and Oberlander, 1992</REF>, and discourse markers or cue phrases <REF>Ballard, Conrad, and Longacre, 1971</REF>; <REF>Halliday and Hasan, 1976</REF>; <REF>Van Dijk, 1979</REF>; <REF>Longacre, 1983</REF>; <REF>Grosz and Sidner, 1986</REF></REF>; <REF>Schiffrin, 1987</REF>; <REF>Cohen, 1987</REF>; <REF>Redeker, 1990</REF>; <REF>Sanders, Spooren, and Noordman, 1992</REF>; <REF>Hirschberg and Litman, 1993</REF>; <REF>Knott, 1995</REF>; <REF>Fraser, 1996</REF>; <REF>Moser and Moore, 1997</REF>.	In the work described here, we investigate how far we can get by focusing our attention only on discourse markers and lexicogrammatical constructs that can be detected by a shallow analysis of natural language texts.	The intuition behind our choice relies on the following facts:  Psycholinguistic and other empirical research <REF>Kintsch, 1977</REF>; <REF>Schiffrin, 1987</REF>; <REF>Segal, Duchan, and Scott, 1991</REF>; <REF>Cahn, 1992</REF>; <REF>Sanders, Spooren, and Noordman, 1992</REF>; <REF>Hirschberg and Litman, 1993</REF>; <REF>Knott, 1995</REF>; <REF>Costermans and Fayol, 1997</REF> has shown that discourse markers are consistently used by human subjects both as cohesive ties between adjacent clauses and as macroconnectors between larger textual units.	2
The feature specification of this ompositionally derived accomplishment is therefore identical to that of a sentence containing a telic accomplishment verb, such as destroy.	According to many researchers, knowledge of lexical aspect--how verbs denote situations as developing or holding in time-may be used to interpret event sequences in discourse <REF>Dowty, 1986</REF>; <TREF>Moens and Steedman, 1988</TREF>; <REF>Passoneau, 1988</REF>.	In particular, Dowty suggests that, absent other cues, a relic event is interpreted as completed before the next event or state, as with ran into lhe room in 4a; in contrast, atelic situations, such as run, was hungry in 4b and 4 are interpreted as contemporaneous with the following situations: fell and made a pizza, respectively.	4 a Mary ran into the room.	2
In 10, s is the consequent state of the event of John greeting Max, and it holds at the time t which precedes now.	So our semantics of the perfect is like that in <TREF>Moens and Steedman 1988</TREF>: a perfect transforms an event into a consequent state, and asserts that the consequent state holds.	The pluperfect of a state, such as 11, therefore, is assumed to first undergo a transformation into an event.	11 John had loved Mary.	2
The imperfective point of view brought by IMP imposes a change of point of view on the term of the eventuality.	As for accomplishments, we can assume that they can be decomposed into several stages, according to <TREF>Moens and Steedman, 1988</TREF>: first a preparatory phase, second a culmination or achievement we are not concerned here with the result state.	We can then say that IMP refers only to the preparatory phase, so that the term of the eventuality loses all relevance.	This explains the so-called imperfective paradox: it is possible to use IMP even though the eventuality never reaches its term: 6 a I1 traversait la rue quand la voiture la 6cras6 He was crossing the street when the car hit him b  I1 traversa la rue quand la voiture la 6cras6 He crossed the street when the car hit him As for achievements, we can assume that they are reduced to a culmination.	2
Weneedamuchbettermodelofhowtocommunicate time, and how this communication depends on the semantics and linguistic expression of the events being described.	An obvious first step, which we are currently working on, is to include a linguisticallymotivatedtemporalontology <REF>MoensandSteedman, 1988</REF>, which will be separate from the existing domain ontology.	We also need better techniques for communicating the temporal relationships between events in cases where they are not listed in chronological order <REF>Oberlander and Lascarides, 1992</REF>.	6 Discussion Two discourse analysts from Edinburgh University, Dr Andy McKinlay and Dr Chris McVittie, kindly examined and compared some of the human and BT45 texts.	2
The domain is limited to trajectoryof-motion events specified by the verbs run, jog, sit is worth noting that as an alternative to positing a lexical ambiguity, one could just as easily invoke a coercion operator on an event predicate Pz mapping it to the process predicate he.	plurPx  e, which would bring the present treatment more in line with <TREF>Moens and Steedman 1988</TREF> and <REF>Jackendoff 1991</REF>.	plod, and walk; the locative prepositions to, towards, from, away from, along, eastwards, westwards, and to and back; various landmarks; the distance adverbials n miles; the frequency adverbials twice and n times; and finally the temporal adverbials for and in.	Trajectory-of-motion events are modeled as continuous constant rate changes of location in one dimension of the TRAJECTOR relative to one or more LANDMARKS following <REF>Regier 1992</REF> in his use of Langackers 1987 terminology.	2
2 3 Theory 31 Ontology Various authors including <REF>Link, 1983</REF>, <REF>Bach, 1986</REF>, <REF>Krifka, 1989</REF>, <REF>Eberle, 1990</REF> have proposed modeltheoretic treatments in which a parallel ontological distinction is made between substances and things, processes and events, etc A similarly parallel distinction is employed here, but in a rather different way: unlike the above treatments, the present account models substances, processes, and other such entities as abstract kinds whose realizations vary in amount.	As such, the approach developed here may be seen as building upon the work of <REF>Carlson 1977</REF> and his successors; it also represents one way to further formalize the intuitions found in <TREF>Moens and Steedman 1988</TREF> and <REF>Jackendoff 1991</REF>.	<REF>Following Schubert and Pelletier 1987</REF>, the present account distinguishes individuals from kinds, but not from stages or quantities.	Extending their ontology, the same distinction is assumed to hold not only in the domain of materials but also in the domain of eventualities, and derivatively in the domains of space and time as well.	2
Note that in a terminal DRS ready for an embedding test, all the auxiliary Rpts disappear do not participate in the embedding.	The perfect is analyzed by using the notion of a nucleus <TREF>Moens and Steedman, 1988</TREF> to account for the inner structure of an eventuality.	A nucleus is defined as a structure containing a preparatory process, culmination and consequent state.	The categorization of verb phrases into different aspectual classes can be phrased in terms of which part of the nucleus they refer to.	2
For example, I made a fire is culminated, whereas, I gazed at the sunset is non-culminated.	Aspectual classification is necessary for interpreting temporal modifiers and assessing temporal entailments <TREF>Moens and Steedman, 1988</TREF>; <REF>Dorr, 1992</REF>; <REF>Klavans, 1994</REF>, and is therefore a necessary component for applications that perform certain language interpretation, summarization, information retrieval, and machine translation tasks.	Aspectual classification is a diflqcult problem because many verbs, like have, are aspectually ambiguous.	In this paper, I demonstrate that verbs can be disambiguated according to aspect by the semantic category of the direct object.	2
In 26i, the event is associated with the features d,t,-a, whereas, in 26ii the event is associated with the features d,t,a.	According to <REF>Bennett et al , 1990</REF> in the spirit of <TREF>Moens and Steedman, 1988</TREF>, predicates are allowed to undergo an atomicity coercion in which an inherently non-atomic predicate such as dio may become atomic under certain conditions.	These conditions are language-specific in nature, ie, they depend on the lexical-semantic structure of the predicate in question.	Given the featural scheme that is imposed on top of the lexical-semantic framework, it is easy to specify coercion functions for each language.	2
In light of these observations, the lexicM-semantic structure adopted for UNITRAN is an augmented form of Jackendoffs representation in which events are distinguished from states as before, but they are further subdivided into activities, achievements, and accomplishments.	The subdivision is achieved by means of three features proposed by <REF>Bennett et al , 1990</REF> following the framework of <TREF>Moens and Steedman, 1988</TREF> in the spirit of <REF>Dowty, 1979</REF> and <REF>Vendler, 1967</REF>: dynamic ie , events vs states, as in the Jackendoff framework, :t:telic i e, culminative events transitions vs nonculminative events activities, and atomic ie , point events vs extended events.	This featural system is imposed on top of the lexical-semantic framework proposed by Jackendoff.	For example, the primitive GO would be annotated with the features d,t,-a for the verb destroy, but d,t,a for the verb obliterate, thus providing the appropriate distinction for cases such as 12.	2
Figure 2 relates the four types of lexical-semantic frameworks outlined above.	Note that the system of features proposed by <REF>Bennett et al , 1990</REF> and <TREF>Moens and Steedman, 1988</TREF> provide the finest tuning given that five distinct categories of predicates are identified by the feature settings.	This system is essentially equivalent to the Dowty/Vendler proposal, but features are used to distinguish the categories more precisely.	In the next section, we will see how the tense and aspect structure described in section 21 and the lexicM-semantic representation described in this section are combined to provide the framework for generating a target-language surface form.	2
2b John finished drawing the circle.	<REF>Dowty 1986</REF> and <TREF>Moens and Steedman 1988</TREF> decisively questioned the coherence of the class of achievement verbs, arguing that not all of them are non-durative.	As noted above, Vendler identifies punctual events through the conjunction of the positive at and negative finish tests.	However, they do not always yield comparable results : 3a 3b 4a 4b Karpov beat Kasparov at 1000 PM The Allies beat Germany at I000 PM  Karpov finished beating Kasparov The Allies finished beating Germany.	2
They trigger a change-of-state COS, henceforth, result states RSs, henceforth being entailments of CoSs.	<TREF>Moens and Steedman 1988</TREF>, <REF>Smith 1991</REF>, <REF>Pustejovsky 1995</REF>, and others argue that it is a defining property of telic events.	They should therefore include an undergoer argument, whose CoS determines the telicity of the event ie , it acts as a measuring-out argument.	<REF>Tenny 1987</REF> thus claims that telic events require such an argument, which she calls an affected argument.	2
The subdivision is achieved by means of three features proposed by Bennett etal.	1990 following the framework of <TREF>Moens and Steedman 1988</TREF>: -t-dynamic ie , events vs states, as in the Jackendoff framework, telic ie , culminative events transitions vs noneulminative events activities, and -I-atomic ie , point events vs extended events.	We impose this system of features on top of the current lexical-semantic framework.	For example, the lexical entry for all three verbs, ransack, obliterate, and destroy, would contain the following lexical-semantic representation: 6 Event CAUSE Thing X, Event GOLoc Thing X, Position TOLoc X John, Property DESTROYED The three verbs would then be distinguished by annotating this representation with the aspectual features d,t,-a for the verb ransack, d,t,-a for the verb destroy, and d,t,a for the verb obliterate, thus providing the appropriate distinction for cases such as 4.	2
S: Juan le dio una puflaJada a Marls John gave a knife-wound to Mary S: Juan le dio pufialadas a Marls John gave knife-wounds to Mary b Duratlve Divergence, E: John met/knew Mary 4 S: Juan coaoci6 a Marls John met Mary S: Juan conoci a Mrfa John knew Merit Figure 1: Three Levels of MT Divergences et el.	1990 have examined aspect and verb semantics within the context of machine translation in the spirit of <TREF>Moens and Steedman 1988</TREF>.	This paper borrows from, and extends, these ideas by demonstrating how this theoretical framework might be adapted for crosslinguistic applicability.	The framework has been tested within the context of an interlingual machine translation system and is currently being used as the basis for extraction of aspectual information from corpora.	2
Although there have been quite a few studies on individual aspects of sentence planning, little attention has been paid to the interaction between the various tasks--exceptions are <REF>Rambow and Korelsky 1992</REF> and <REF>Wanner and Hovy 1996</REF>--and in particular to the role of marker choice in the overall sentence planning process.	There exists a large body of research in NLU on analysing the temporal structure of texts, including the role of temporal markers, though again restricted to English <TREF>Moens and Steedman 1988</TREF>; <REF>Lascarides and Oberlander 1993</REF>; <REF>Hitzeman et al 1995</REF>.	We turn to these studies when it comes to identifying the information that needs to be assembled for representing temporal markers.	3 Linguistic perspective: Describing temporal markers Selecting an appropriate German temporal marker given two events in a temporal relationship requires detailed knowledge of the semantic, pragmatic and syntactic properties that characterize temporal markers.	3
edu Abstract Verbal and compositional lexical aspect provide the underlying temporal structure of events.	Knowledge of lexical aspect, eg, atelicity, is therefore required for interpreting event sequences in discourse <REF>Dowty, 1986</REF>; <TREF>Moens and Steedman, 1988</TREF>; <REF>Passoneau, 1988</REF>, interfacing to temporal databases <REF>Androutsopoulos, 1996</REF>, processing temporal modifiers <REF>Antonisse, 1994</REF>, describing allowable alternations and their semantic effects <REF>Resnik, 1996</REF>; <REF>Tenny, 1994</REF>, and selecting tense and lexical items for natural language generation <REF>Dorr and Olsen, 1996</REF>; <REF>Klavans and Chodorow, 1992</REF>, cf.	<REF>Slobin and Bocaz, 1988</REF>.	We show that it is possible to represent lexical aspect--both verbal and compositional--on a large scale, using Lexical Conceptual Structure LCS representations of verbs in the classes cataloged by <REF>Levin 1993</REF>.	2
Finally, we illustrate how knowledge of lexical aspect facilitates the interpretation of events in NLP applications.	Knowledge of lexical aspect--how verbs denote situations as developing or holding in time--is required for interpreting event sequences in discourse <REF>Dowty, 1986</REF>; <TREF>Moens and Steedman, 1988</TREF>; <REF>Passoneau, 1988</REF>, interfacing to temporal databases <REF>Androutsopoulos, 1996</REF>, processing temporal modifiers <REF>Antonisse, 1994</REF>, describing allowable alternations and their semantic effects <REF>Resnik, 1996</REF>; <REF>Tenny, 1994</REF>, and for selecting tense and lexical items for natural language generation Dorr and Olsen.	1996: <REF>Klavans and Chodorow, 1992</REF>, cf.	<REF>Slobin and Bocaz, 1988</REF>.	2
How does 37b get its interpretation.	As with 36d, the relevant elements of 37b can be represented as   then R   after S  turn right on County Line   e 3 :turn-rightyou, county line and the unresolved interpretation of 37b is thus  xafterx, EVe 3  aftere 3, EV 560 Computational Linguistics Volume 29, Number 4 As for resolving EV, in a well-known article, <TREF>Moens and Steedman 1988</TREF> discuss several ways in which an eventuality of one type eg , a process can be coerced into an eventuality of another type eg , an accomplishment, which Moens and Steedman call a culminated process.	In this case, the matrix argument of then the eventuality of turning right on County Line can be used to coerce the process eventuality in 37b into a culminated process of going west on Lancaster Avenue until County Line.	We treat this coercion as a type of associative or bridging inference, as in the examples discussed in section 31.	2
The alternatives arise when more than one event can be used.	The temporal ontology is based on a recent theory of temporal semantics developed by <TREF>Moens and Steedman 1988</TREF>.	This allows a modular representation of the semantics of temporal adverbials like until and by, and also aids in the generation of tense and aspect.	This system looks at the mechanics of how the alternatives can be generated from the initial data, but we will have less to say about choosing between them.	2
21 Aspectual Categories of Verbs A number of aspectually oriented lexical-semantic representations have been proposed.	Ve adopt and extend the feature-based framework proposed by <REF>Bennett et al , 1990</REF> in the spirit of <TREF>Moens and Steedman, 1988</TREF>.	They uses three features: dynamic, telic, and atomic.	We add two more features: process and gradual.	2
In principle, it is possible for this second module to detect aspectual transformations that apply to any input clause, independent of the manner in which the core constituents interact to produce its fundamental aspectual class.	600 Siegel and McKeown Improving Aspectual Classification 26 Applications of Aspectual Classification Aspectual classification is a required component of applications that perform natural language interpretation, natural language generation, summarization, information retrieval, and machine translation tasks <TREF>Moens and Steedman 1988</TREF>; <REF>Klavans and Chodorow 1992</REF>; <REF>Klavans 1994</REF>; <REF>Dorr 1992</REF>; <REF>Wiebe et al 1997</REF>.	These applications require the ability to reason about time, ie, temporal reasoning.	Assessing temporal relationships is a prerequisite for inferring sequences of medical procedures in medical domains.	2
598 Siegel and McKeown Improving Aspectual Classification Table 3 Several aspectual entailments.	If a clause occurring: necessarily entails: then it must be: in past progressive tense as argument of stopped in simple present tense past tense reading past tense reading the habitual reading Nonculminated Event Nonculminated Event or State Event 23 Interpreting Temporal Connectives and Modifiers Several researchers have developed models that incorporate aspectual class to assess temporal constraints between connected clauses <REF>Hwang and Schubert 1991</REF>; <REF>Schubert and Hwang 1990</REF>; <REF>Dorr 1992</REF>; <REF>Passonneau 1988</REF>; <TREF>Moens and Steedman 1988</TREF>; <REF>Hitzeman, Moens, and Grover 1994</REF>.	For example, stativity must be identified to detect temporal constraints between clauses connected with when.	For example, in interpreting, 7 She had good strength when objectively tested.	2
An understanding system can recognize the aspectual transformations that have affected a clause only after establishing the clauses fundamental aspectual category.	Linguistic models motivate the division between a module that first detects fundamental aspect and a second that detects aspectual transformations <REF>Hwang and Schubert 1991</REF>; <REF>Schubert and Hwang 1990</REF>; <REF>Dorr 1992</REF>; <REF>Passonneau 1988</REF>; <TREF>Moens and Steedman 1988</TREF>; <REF>Hitzeman, Moens, and Grover 1994</REF>.	In principle, it is possible for this second module to detect aspectual transformations that apply to any input clause, independent of the manner in which the core constituents interact to produce its fundamental aspectual class.	600 Siegel and McKeown Improving Aspectual Classification 26 Applications of Aspectual Classification Aspectual classification is a required component of applications that perform natural language interpretation, natural language generation, summarization, information retrieval, and machine translation tasks <TREF>Moens and Steedman 1988</TREF>; <REF>Klavans and Chodorow 1992</REF>; <REF>Klavans 1994</REF>; <REF>Dorr 1992</REF>; <REF>Wiebe et al 1997</REF>.	2
Some aspectual auxiliaries also perform an aspectual transformation of the clause they modify, eg, 11 I finished staring at it culminated process.	Aspectual coercion, a second type of aspectual transformation, can take place when a clause is modified by an aspectual marker that violates an aspectual constraint <TREF>Moens and Steedman 1988</TREF>; <REF>Pustejovsky 1991</REF>.	In this case, an alternative interpretation of the clause is inferred which satisfies the aspectual constraint.	For example, the progressive marker is constrained to appear with an extended event.	2
E-mail: evscscolumbiaedu t Computer Science Dept , 1214 Amsterdam Ave , New York, NY 10027.	E-mail: kathycscolumbiaedu  2001 Association for Computational Linguistics Computational Linguistics Volume 26, Number 4 Aspectual classification is necessary for interpreting temporal modifiers and assessing temporal entailments <TREF>Moens and Steedman 1988</TREF>; <REF>Dorr 1992</REF>; <REF>Klavans 1994</REF> and is therefore a required component for applications that perform certain natural language interpretation, generation, summarization, information retrieval, and machine translation tasks.	Each of these applications requires the ability to reason about time.	A verbs aspectual category can be predicted by co-occurrence frequencies between the verb and linguistic phenomena such as the progressive tense and certain temporal modifiers <REF>Klavans and Chodorow 1992</REF>.	2
Aspectual classification is also a necessary prerequisite for interpreting certain adverbial adjuncts, as well as identifying temporal constraints between sentences in a discourse <TREF>Moens and Steedman 1988</TREF>; <REF>Dorr 1992</REF>; <REF>Klavans 1994</REF>.	In addition, it is crucial for lexical choice and tense selection in machine translation <TREF>Moens and Steedman 1988</TREF>; <REF>Klavans and Chodorow 1992</REF>; <REF>Klavans 1994</REF>; <REF>Dorr 1992</REF>.	Table 1 sunnarizes the three aspectual distinctions, which compose five aspectual categories.	In addition to the two distinctions described in the previous section, atomicity distinguishes punctual events eg , She noticed the picture on the wall from extended events, which have a time duration eg , She ran to the store.	2
Therefore, if it appears with an atomic event, eg, 12 He hiccupped point, the event is transformed to an extended event, eg, 13 He was hiccupping process.	in this case with the iterated reading of the clause <TREF>Moens and Steedman 1988</TREF>.	25 The First Problem: Fundamental Aspect We define fundamental aspectual class as the aspectual class of a clause before any aspectual transformations or coercions.	That is, the fundamental aspectual category is the category the clause would have if it were stripped of any and all aspectual markers that induce an aspectual transformation, as well as all components of the clauses pragmatic context that induce a transformation.	2
Aspect in Natural Language Because, in general, the sequential order of clauses is not enough to determine the underlying chronological order, aspectual classification is required for interpreting 596 Siegel and McKeown Improving Aspectual Classification Table 1 Aspectual classes.	This table is adapted from Moens and Steedman 1988, p 17.	Culminated Nonculminated EVENTS punctual extended CULMINATION CULMINATED PROCESS recognize build a house POINT PROCESS hiccup run, swim, walk STATES understand even the simplest narratives in natural language.	For example, consider: 1 Sue mentioned Miami event.	2
THE IMPERFECTIVE PARADOX AND TRAJECTORY-OF-MOTION EVENTS  Michael White Department of Computer and Information Science University of Pennsylvania Philadelphia, PA, USA mwhit el inc c is upenn, edu Abstract In the first part of the paper, I present a new treatment of THE IMPERFICTIVE PARADOX <REF>Dowty 1979</REF> for the restricted case of trajectoryof-motion events.	This treatment extends and refines those of <TREF>Moens and Steedman 1988</TREF> and <REF>Jackendoff 1991</REF>.	In the second part, I describe an implemented algorithm based on this treatment which determines whether a specified sequence of such events is or is not possible under certain situationally supplied constraints and restrictive assumptions.	Bach 1986:12 summarizes THE IMPERFECTIVE PARADOX <REF>Dowty 1979</REF> as follows: how can we characterize the meaning of a progressive sentence like la 17 on the basis of the meaning of a simple sentence like lb 18 when la can be true of a history without lb ever being true.	2
<REF>White 1993</REF>.	5Much as in <TREF>Moens and Steedman 1988</TREF> and <REF>Jackendoff 1991</REF>, the introduction of gr is necessary to avoid having an ill-sorted formula.	284 the manner of motion supplied by a verb, it does nevertheless permit one to consider factors such as the normal speed as well as the meanings of the prepositions 10, lowards, etc By making two additional restrictive assumptions, namely that these events be of constant velocity and in one dimension, I have been able to construct and implement an algorithm which determines whether a specified sequence of such events is or is not possible under certain situationally supplied constraints.	These constraints include the locations of various landmarks assumed to remain stationary and the minimum, maximum, and normal rates associated with various manners of motion eg running, jogging for a given individual.	2
Capitalizing on Bachs insight, I present in the first part of the paper a new treatment of the imperfective paradox which relies on the possibility of having actual events standing in the part-of relation to hypothetical super-events.	This treatment extends and refines those of <TREF>Moens and Steedman 1988</TREF> and <REF>Jackendoff 1991</REF>, at least for the restricted case of trajectory-of-motion events.	1 In particular, the present treatment correctly accounts not only for what 2a fails to entail -namely, that John eventually reaches the museum -but also for what 2a does in fact entail -namely, that John follows by jogging at least an initial part of a path that leads to the museum.	In the second part of the paper, I briefly describe an implemented algorithm based on this theoretical treatment which determines whether a specified sequence of trajectory-of-motion is or is not possible under certain situationally supplied constraints and restrictive assumptions.	2
The imperfective point of view brought by IMP imposes a change of point of view on the term of the eventuality.	As for accomplishments, we can assume that they can be decomposed into several stages, according to <TREF>Moens and Steedman, 1988</TREF>: first  preparatory phase, second a cuhnination or achievement we are not concerned here with the result state.	We can then say that IMP refers only to the preparatory phase, so that the term of the eventuality loses all relevance.	This explains the so-called imperfective paradox: it is possible to use IMP even though the eventnality never reaches its term: 6 a I1 traversait la rue quand la voiture la ras6 He was crossing the street when the car hit him b  I1 traversa la rue quand la voiture la 6cras6 Ile crossed the street when the car hit him As for achievements, we can assume that they are reduced to a culmination.	2
Furthermore, stativity is the first of three fundamental temporal distinctions that compose the aspectual class of a clause.	Aspectual classification is a necessary component for a system that analyzes temporal constraints, or performs lexical choice and tense selection in machine translation <TREF>Moens and Steedman, 1988</TREF>; <REF>Passonneau, 1988</REF>; <REF>Doff, 1992</REF>; <REF>Klavans, 1994</REF>.	Researchers have used empirical analysis of corpora to develop linguistically-based numerical indicators that aid in aspectual classification <REF>Klavans and Chodorow, 1992</REF>; <REF>Siegel and McKeown, 1996</REF>.	Specifically, this technique takes advantage of linguistic constraints that pertain to aspect, eg, only clauses that describe an event can appear in the progressive.	2
By using TAGs we get the additional benefit of an existing parser that yields derivations and derived trees fiom which we can construct the compositional semantics of a given sentence.	We decompose each event E into a tripartite structure in a manner similar to <TREF>Moens and Steedman 1988</TREF>, introducing a time function for each predicate to specify whether the predicate is true in the preparatory dringE, cuhnination erdE, or consequent resll:E stage of an event.	hfitial trees capture tile semantics of the basic senses of verbs in each class.	For example, many IThese restrictions are more like preferences that generate a preferred reading of a sentence.	2
For example, I made a fire is culminated, since a new state is introduced something is made, whereas, I gazed at the sunset is non-culminated.	Aspectual classification is necessary for interpreting temporal modifiers and assessing temporal entailments <REF>Vendler, 1967</REF>; <REF>Dowty, 1979</REF>; <TREF>Moens and Steedman, 1988</TREF>; <REF>Dorr, 1992</REF>, and is therefore a necessary component for applications that perform certain natural language interpretation, natural language generation, summarization, information retrieval, and machine translation tasks.	Aspect introduces a large-scale, domaindependent lexical classification problem.	Although an aspectual lexicon of verbs would suffice to classify many clauses by their main verb only, a verbs primary class is often domaindependent <REF>Siegel, 1998b</REF>.	2
112 Table 1: Aspectual classes.	This table comes from Moens and Steedman <TREF>Moens and Steedman, 1988</TREF>.	Culm EVENTS STATES punctual extended CULM CULM PROCESS recognize build a house NonPOINT PROCESS Culm hiccup run, swim understand 2 Aspect in Natural Language Table 1 summarizes the three aspectual distinctions, which compose five aspectual categories.	In addition to the two distinctions described in the previous section, atomicity distinguishes events according to whether they have a time duration punctual versus extended.	2
This can effect not only the semantic interpretation of the text itself, but also translation and the choice of adverb.	3 3Many of these issues are discussed in the CL Special Issue on Tense and Aspect <REF>June, 1988</REF> in articles by Hinniche, Moens and Steedman, Nakhimovsky, Passoneau, and Webber.	For example, Passoneau demonstrates how, without an ccurate specification of the pectual tendencies of the verb coupled with the effect of temporal and aspectual adjuncts, messages, which tend to be in the present tense, ttre not correctly understood nor generated in the PUNDIT system.	For instance, the pressure is low must be interpreted at statlve, whereas the pump operates must be interpreted as a process.	2
It is also clear that events are not undifferentiated masses, but rather have subparts that can be picked out by the choice of phrase type or the addition of adverbial phrases.	<TREF>Moens  Steedman 1988</TREF> identify three constituents to an event nucleus, a preparatory process, culmination, and consequent state, whereas <REF>Nakhimovsky 1988</REF> identifies five: preparatory, initial, body, final, result, exemplified by the following: 1 15.	When the children crossed the road, a they waited for the teacher to give a signal b they stepped onto its concrete surface as if it were about to swallow them up.	c they were nearly hit by a car d they reached the other side stricken with fear.	2
Weather would seem selfcontained, but change, creation and stative are not semantic fields at all.	Stative belongs to the Aktionsart categorisation of verbs distinguishing it from verbs of activity, achievement and accomplishment, which is orthogonal to the categorisation of verbs into semantic fields <REF>Vendler, 1967</REF>, <TREF>Moens  Steedman 1988</TREF>, <REF>Amaro, 2006</REF>.	Moreover, a verb can belong to more than one Aktionsart category, as these apply to verbs in contexts.	33 Suggested Revision of Categories Among verbs, the level of arbitrariness and incorrectness of the WordNet categories seems greater than that of the relations.	2
Differences in annotation could be due to the differences in interpretations of the event; however, we found that the vast majority of radically different judgments can be categorized into a relatively small number of classes.	Some of these correspond to aspectual features of events, which have been intensively investigated eg , <REF>Vendler, 1967</REF>; <REF>Dowty, 1979</REF>; <TREF>Moens and Steedman, 1988</TREF>; <REF>Passonneau, 1988</REF>.	We then developed guidelines to cover those cases see the next section.	22 Event Classes Action vs State: Actions involve change, such as those described by words like speaking, gave, and skyrocketed.	2
Events of type eat differ from those of type run in the way the result d is brought about.	This can be illustrated by means of the notion of a nucleus-structure Moens/<REF>Steedman 1988</REF>.	A nucleus-structure consists of three parts: the inception-point IP, the development-portion DP and the culmination-point CP.	Nucleus-Structure e S S  IP DP CP Figure 1 The result  is evaluated at each part of the nucleus-structure.	2
This is, to our knowledge, the first implementation of Webbers DE generation ideas.	We designed the 243 algorithms and structures necessary to generate discourse entities from our logical representation of the meaning of utterances, and from pointing gestures, and currently use them in Januss <REF>Weischedel et al , 1987</REF>, BSN, 1988 pronoun resolution component, which applies centering techniques <TREF>Grosz et al , 1983</TREF>, <REF>Sidner 1981, 1983</REF>, <REF>Brennan et al 1987</REF> to track and constrain references.	Janus has been demonstrated in the Navy domain for DARPAs Fleet Command Center Battle Management Program FCCBMP, and in the Army domain for the Air Land Battle Management Program ALBM.	2 Meaninq Representation for DE Generation Webber found that appropriate discourse entities could be generated from the meaning representation of a sentence by applying rules to the representation that are strictly structural in nature, as long as the representation reflects certain crucial aspects of the sentence.	2
So, top-down constraints must be weakened in order for parsing to be guaranteed to terminate.	In order to solve the nontermination problem, <TREF>Shieber 1985</TREF> proposes restrictor, a statically predefined set of features to consider in propagation, and restriction, a filtering function which removes the features not in restrictor from top-down expectation.	However, not only does this approach fail to provide a method to automatically generate the restrictor set, it may weaken the predicative power of top-down expectation more than necessary: a globally defined restrictor can only specify the least common features for all propagation paths.	In this paper, a general method of maximizing top-down constraints is proposed.	3
Manual detection is also problematic: when a grammar is large, particularly if semantic features are included, complete detection is nearly impossible.	As for the techniques developed so far which partially solve prediction nontermination eg , <TREF>Shieber 1985</TREF>; <REF>Haas 1989</REF>; <REF>Samuelsson 1993</REF>, they do not apply to nonminimal derivations because nonminimal derivations may arise without left recursion or recursion in general s One way is to define p to filter out all features except the context-free backbone of predictions.	However, this severely restricts the range of possible instantiations of Shiebers algorithm.	9 A third possibility is to manually fix the grammar so that nonminimal derivations do not occur, as we noted in Section 4.	3
However, simple subsumption may filter out valid parses for some grammars, thus sacrificing completeness.	7 Another possibility is to filter out problematic features in the Prediction step by using the function p However, automatic detection of such features ie , automatic derivation of p is undecidable for the same reason as the prediction nontermination problem caused by left recursion for unification grammars <TREF>Shieber 1985</TREF>.	Manual detection is also problematic: when a grammar is large, particularly if semantic features are included, complete detection is nearly impossible.	As for the techniques developed so far which partially solve prediction nontermination eg , <TREF>Shieber 1985</TREF>; <REF>Haas 1989</REF>; <REF>Samuelsson 1993</REF>, they do not apply to nonminimal derivations because nonminimal derivations may arise without left recursion or recursion in general s One way is to define p to filter out all features except the context-free backbone of predictions.	3
It is more so because 1 there is no restriction such as that there should be only one zero morpheme within an S clause, and 2 the stack is useless because zero morphemes are independent morphemes and are not bound to other morphemes comparable to wh-words.	<TREF>Shieber 1985</TREF> proposes a more efficient approach to gaps in the PATR-II formalism, extending Earleys algorithm by using restriction to do top-down filtering.	While an approach to zero morphemes similar to Shiebers gap treatment is possible, we can see one advantage of ours.	That is, our approach does not depend on what kind of parsing algorithm we choose.	3
For the experiments discussed in the final section all goal-weakening operators were chosen by hand, based on small experiments and inspection of the goal table and item table.	Even if goal weakening is reminiscent of Shiebers 1985 restriction operator, the rules of the game are quite different: in the case of goal weakening, as much information as possible is removed without risking nontermination of the parser, whereas in the case of Shiebers restriction operator, information is removed until the resulting parser terminates.	For the current version of the grammar of OVIS, weakening the goal category in such a way that all information below a depth of 6 is replaced by fresh variables eliminates the problem caused by the absence of the occur check; moreover, this goal-weakening operator reduces parsing times substantially.	In the latest version, we use different goal-weakening operators for each different functor.	3
Depending on the properties of a particular grammar, it may, for example, be worthwhile to restrict a given category to its syntactic features before attempting to solve the parse-goal of that category.	Shiebers 1985 restriction operator can be used here.	Thus we essentially throw some information away before an attempt is made to solve a memorized goal.	For example, the category xA, B, f A, B, gA,hB, i C   may be weakened into: xA,B,f ,,g, If we assume that the predicate weaken/2 relates a term t to a weakened version tw, such that tw subsumes t, then 15 is the improved version of the parse predicate: parsewithweakening Cat, P0, P, E0, E  15 weakenCat,WeakenedCat, parseWeakenedCat,P0,P,E0,E, CatWeakenedCat.	2
Some additional penalty may also have been incurred by not using dotted grammar rules to generate reductions, as in standard leftcorner parsing algorithms.	2 There are important differences between the technique for limited prediction in this parser, and other techniques for limited prediction such as Shiebers notion of restriction <TREF>Shieber, 1985</TREF> which we also use.	In methods such as Shiebers, predictions are weakened in ways that can result in an overall gain in efficiency, but predictions nevertheless must be dynamically generated for every phrase that is built bottom-up.	In our log version 314.	2
In addition, the parser maintains a skeletal copy of the chart in which edges are labeled only by the nonterminal symbols contained in their context-free backbone, which gives us more efficient indexing of the full grammar rules.	Other optimizations include using one-word look-ahead before adding new predictions, and using restrictors <TREF>Shieber, 1985</TREF> to increase the generality of the predictions.	Comparison with Other Parsers Table 1 compares the average number of edges, average number of predictions, and average parse times 1 in seconds per utterance for the limited 1All parse times given in this paper were produced on a Sun SPARCstation 10/51, running Quintus Pro111 For grammar with start symbol , phrase structure rules P, lexicon L, context-independent categories CI, and context-dependent categories CD; and for word string w  wlwn: Variant Edges Preds Secs Bottom-Up 1191 0 146 Limited Left-Context 203 25 10 Left-Corner 112 78 40 Table h Comparison of Syntax-Only Parsers if  E CD, predictT, 0; addemptycategories 0 ; for i from I to n do foreach C such that C--wi EL do addedgetochartC, i-i, i ; makenewpredictionsC, ii, i ; findnew-reductionsC, il,i end addemptycategories i ; end sub findmew-reductionsB, j, k  foreach A and a such that A- B 6 P do foreach i such that i  match, j do if A 6 CD and predictedA,i or A 6 CI addedgetochartA, i, k; makenewpredictionsA, i, k ; findnewreductionsA, i, k ; end end  sub addemptycategoriesi  foreach A such that A - e E P do if A 6 CD and predictedA,/ or A 6 CI addedgetochartA, i, i ; makenewpredictionsA, i, i ; findnewreductionsA, i, i ; end  sub makenewpredictionsA, i, j  foreach Aft E Predictionsi do predict fl, j end foreach H - ABfl 6 P such that H 6 CI and B E CD and fl 6 CI do predict B, j end foreach H -- AB 6 P such that H E CD and B E CD and fl E CI and predictedH, i or H left-corner-of C and predictedC, i do predict B, j end Figure 1: Limited Left-Context Algorithm left-context parser with those for a variant equivalent to a bottom-up parser when all categories are context independent and for a variant equivalent to a left-corner parser when all categories are context dependent.	The tests were performed on a set of 194 utterances chosen at random from the ARPA ATIS corpus MADCOW, 1992, using a broad-coverage syntactic grammar of English having 84 coverage of the test set.	2
The situation becomes more complicated when we move to unification-based grammars, since there may be an unbounded number of different categories appearing in the accessible stack states.	In the system implemented here we used restriction <TREF>Shieber, 1985</TREF> on the stack states to restrict attention to a finite number of distinct stack states for any given stack depth.	Since the restriction operation maps a stack state to a more general one, it produces a finite-state approximation which accepts a superset of the language generated by the original unification grammar.	Thus for general constraint-based grammars the language accepted by our finite-state approximation is not guaranteed to be either a superset or a subset of the language generated by the input grammar.	2
However, their particular realization of the technique is severely restricted for NLP applications, since it uses a deterministic one-path LR algorithm, applicable only to semantically unambiguous grammars.	<REF>Pereira and Warren 1983</REF> and <TREF>Shieber 1985</TREF> present v6rsions of Earleys algorithm for unification grammars, in which unification is the sole operation responsible for attribute evaluation.	However, given the high computational cost of unification, important differences between attribute and unification grammars in their respective attribution domains and functions Correa, forthcoming, and the more general nature of attribute grammars in this regard, it is of interest to investigate the extension of Earleys algorithm directly to the main subclasses of attribute grammar.	The paper is organized as follows: Section 2 presents pieliminary elements, including a definition of attribute grammar and Earleys algorithm.	3
The parsing problem for offline parsable grammars ts solvable.	Yet these grammars apparently have enough formal power to describe natural language at least, they can describe the crossed-serial dependencies of Dutch and Swiss German, which are presently the most widely accepted example of a construction that goes beyond context-free grammar <TREF>Shieber 1985a</TREF>.	Suppose that the variable M ranges over integers, and the function letter s denotes the successor function.	Consider the rule 1 pM --- psM A grammar containing this rule cannot be offline parsable, because erasing the arguments of the top-level terms in the rule gives 2 p --- p which immediately leads to infinite ambiguity.	2
This does not deny that compilation methods may be able to convert a grammar into a program that generates without termination problems.	In fact, the partial execution techniques described by two of us <REF>Pereira and Shieber 1985</REF> could form the basis of a compiler built by partial execution of the new algorithm we propose below relative to a grammar.	However, the compiler will not generate a program that generates top-down, as Strzalkowskis does.	v c,k,mj V mj I zag V k,m V e,k saw  helpen voeren help feed Figure 2 Schematic of Verb Subeategorization Lists for Dutch Example.	3
But even this ad hoc solution is problematic, as there may be no principled bound on the size of the subcategorization list.	For instance, in analyses of Dutch cross-serial verb constructions <REF>Evers 1975</REF>; <REF>Huybrechts 1984</REF>, subcategorization lists may be concatenated by syntactic rules Moort32 Computational Linguistics Volume 16, Number 1, <REF>March 1990</REF> Shieber et al Semantic Head-Driven Grammar gat 1984; <REF>Fodor et al 1985</REF>; <REF>Pollard 1988</REF>, resulting in arbitrarily long lists.	Consider the Dutch sentence dat Jan Marie de oppasser de olifanten zag helpen that John Mary the keeper the elephants saw help voeren feed that John saw Mary help the keeper feed the elephants The string of verbs is analyzed by appending their subcategorization lists as in Figure 2.	Subcategorization lists under this analysis can have any length, and it is impossible to predict from a semantic structure the size of its corresponding subcategorization list merely by examining the lexicon.	3
Though theoretically very attractive, codescription has its price: i the grammar is difficult to modularize due to the fact that the levels constrain each other mutually and ii there is a computational overhead when parsers use the complete descriptions.	Problems of these kinds which were already noted by <TREF>Shieber, 1985</TREF> motivated tile research described here.	The goal was to develop more flexible ways of using codescriptive grammars than having them applied by a parser with full informational power.	The underlying observation is that constraints in such grammars can play different roles:  Genuine constraints which relate directly to tile grammaticality wellformedness of the input.	2
Furthermore, the need to perform nondestructive unification means that a large proportion of the processing time is spent copying feature structures.	One approach to this problem is to refine parsing algorithms by developing techniques such as restrictions, structure-sharing, and lazy unification that reduce the amount of structure that is stored and hence the need for copying of features structures <TREF>Shieber, 1985</TREF>; <REF>Pereira, 1985</REF>; <REF>Karttunen and Kay, 1985</REF>; <REF>Wroblewski, 1987</REF>; <REF>Gerdemann, 1989</REF>; <REF>Godden, 1990</REF>; <REF>Kogure, 1990</REF>; <REF>Emele, 1991</REF>; <REF>Tomabechi, 1991</REF>; <REF>Harrison and Ellison, 1992</REF>.	While these techniques can yield significant improvements in performance, the generality of unification-based grammar formalisms means that there are still cases where expensive processing is unavoidable.	This approach does not address the fundamental issue of the tradeoff between the descriptive capacity of a formalism and its computational power.	2
Original Earley prediction works on category symbols.	An answer to these problems was presented by <TREF>Shieber 1985</TREF> who proposed to do Earley prediction on the basis of some finite quotient of all constituent DAGs which can be specified by the grammar writer.	Another example for the influence of the CUG efforts on the development of PATR is a new template notation introduced by Lauri Karttunen in his Interlisp-D version of PATR.	Since categorial grammars exhibit an extensive embedding of categories within other categories, it is useful to unify templates not only with the whole lexical DAG but also with its categorial subgraphs.	2
The sohltion to this problem is to define a finite number of equivalence classes into which the infinite uumber of nnnterminals inay be sorted.	Fhese ,lasses may be established in a number of ways; the one we have adopted in that presented by Harrison and Ellison,  992 which builds on l;he work of <TREF>Shieber, 1985</TREF>: it introduces the nol;ion of a negative restrictor to define equivalence classes.	In this solution a predefined portion of a category a specific set of paths is discarded when determining whether a category belongs to an equivalence :lass or not.	For instance, in the above example we could define the negative restrictor to be orth.	2
In terms of parse times the two algorithms are almost equivalent.	Comparing our results with those of <TREF>Shieber 1985</TREF> and <REF>Haas 1989</REF>, we see that in all cases top-down filtering may reduce the size of the chart significantly.	<REF>Whereas Haas 1989</REF> found that top-down filtering never helps to actually decrease parse times in a bottom-up parser, we have found at least one example German where top-down filtering is useful.	183 7 Conclusions There is a trend in modern linguistics to replace grammars that are completely language specific by grammars which combine universal rules and principles with language specific parameter settings, lexicons, etc This trend can be observed in such diverse frameworks as Lexical Functional Grammar, Government-Binding Theory, Head-driven Phrase Structure Grammar and Categorial Grammar.	3
Contrary to bottomup parsing, however, the adaptation of a top-down algorithm for UG requires some special care.	For UGs which lack a so-called context-free back-bone, such as CUG, the top-down prediction step can only be guaranteed to terminate if we make use of restriction, as defined in <TREF>Shieber 1985</TREF>.	Top-down prediction with a restrictor R where R is a finite set of paths through a feature-structure amounts to the following: Restriction The restriction of a feature-structure F relative to a restrictor R is the most specific feature-structure F  E F, such that every path in F j has either an atomic value or is an element of R Predictor Step For each item, End, LHS, Parsed, Next I ToParse such that Rjve, is the restriction of Next relative to R, and each rule RNe:t  RHS, add itemi,i, Rge:t, , RHS.	Restriction can be used to develop a top-down chart parser for CUG in which the top-down prediction step terminates.	2
In particular, in order to derive a finite CF grammar, we will need to consider only those features that have a finite number of possible values, or at least consider only finitely many of the possible values for infinitely valued features.	We can use the technique of restriction <TREF>Shieber 1985</TREF> to remove these features from our feature structures.	Removing these features may give us a more permissive language model, but it will still be a sound approximation.	The experimental results reported in this paper are based on a grammar under development at RIACS for a spoken dialogue interface to a semi-autonomous robot, the Personal Satellite Assistant PSA.	2
The situation becomes more complicated when we move to unification-based grammars, since there may be an unbounded number of different categories appearing in the accessible stack states.	In the system implemented here we used restriction <TREF>Shieber, 1985</TREF> on the stack states to restrict attention to a finite number of distinct stack states for any given stack depth.	Since the restriction operation maps a stack state to a more general one, it produces a finite-state approximation which accepts a superset of the language generated by the original unification grammar.	Thus for general constraint-based grammars the language accepted by our finite-state aptroximation is not guaranteed to be either a superset or a subset of the language generated by the input grammar.	2
More specifically, magic generation falls prey to non-termination in the face of head recursion, ie, the generation analog of left recursion in parsing.	This necessitates a dynamic processing strategy, ie, memoization, extended with an abstraction function like, eg, restriction <TREF>Shieber, 1985</TREF>, to weaken filtering and a subsumption check to discard redundant results.	It is shown that for a large class of grammars the subsumption check which often influences processing efficiency rather dramatically can be eliminated through fine-tuning of the magic predicates derived for a particular grammar after applying an abstraction function in an off-line fashion.	Unfolding can be used to eliminate superfluous filtering steps.	2
24 Top-Down Predictive Linking The aim of our proposal is to define equivalence relations that keep the linking relation finite while also preventing it from being too restrictive; this turns the linking relation into a weakpredietion table in the sense of Haas 1989: 227ff.	Like Shieber 1985, 1992 with the notion of restriction, we confine our attention to a subset of specifications; in particular, we can define a feature structure that subsumes all VP-type feature structures of Shiebers recursive subcategorization rules.	But unlike Shieber, our restrictors are computed automatically by building the generalization of the occurrences ofleftrecursive categories in a grammar.	The intuitive idea is that we consider categories to be left recursive if their tokens can be unified rather than being identical, as in the case of atoms; we then use their generalization, or greatest lower bound, as a common denominator defining an equivalence relation.	3
1990 have discussed similar techniques in the context of semantichead-driven generation, we are concerned here with parsing.	We view the linking relation not simply as a filter to increase efficiency within the domain of syntactic analysis--this aspect is stressed by <TREF>Shieber 1985</TREF> and other investigators such as <REF>Bouma 1991</REF>--but rather as a device for the top-down predictive instantiation of information, as Shieber et al.	1990 have shown for semantic-head-driven generation.	In this paper we are concerned especially with morphosyntactic information and illustrate the relevance of predictive linking for morphological analysis and for the analysis of unknown or new lexical items.	2
Whatever term one takes, an important aspect of the relation is that it can be used to reduce the search space of possible syntactic analyses at an earlier point in parsing and thus serves to improve the efficiency of a parser.	Shieber 1985, 1992 follows established terminology in speaking of top-down filtering in connection with the prediction step of the Earley algorithm.	His central notion of restriction, whereby a restrictor is a finite subset of the paths specified in a feature structure, is related to the technique we introduce here, since both guarantee the finiteness of an otherwise possibly infinite domain of complex categories, but Shiebers restrictors are specified manually.	We propose a general algorithmic method of compilation that avoids manual specification.	3
In particular, declaring certain category-valued features so that they cannot take variable values may lead to nontermination in the backbone construction for some grammars.	However, it should be possible to restrict the set of features that are considered in category-valued features in an analogous way to Shiebers 1985 restrictors for Earleys 1970 algorithm, so that a parse table can still be constructed.	36 Ted Briscoe and John Carroll Generalized Probabilistic LR Parsing 4.	Building LR Parse Tables for Large NL Grammars The backbone grammar generated from the ANLT grammar is large: it contains almost 500 distinct categories and more than 1600 productions.	2
This requirement places a greater load on the grammar writer and is inconsistent with most recent unification-based grammar formalisms, which represent grammatical categories entirely as feature bundles eg <REF>Gazdar et al 1985</REF>; <REF>Pollard and Sag 1987</REF>; <REF>Zeevat, Calder, and Klein 1987</REF>.	In addition, it violates the principle that grammatical formalisms should be declarative and defined independently of parsing procedure, since different definitions of the CF portion of the grammar will, at least, effect the efficiency of the resulting parser and might, in principle, lead to nontermination on certain inputs in a manner similar to that described by <TREF>Shieber 1985</TREF>.	In what follows, we will assume that the unification-based grammars we are considering are represented in the ANLT object grammar formalism <REF>Briscoe et al 1987</REF>.	This formalism is a notational variant of Definite Clause Grammar eg <REF>Pereira and Warren 1980</REF>, in which rules consist of a mother category and one or more daughter categories, defining possible phrase structure configurations.	3
In contrast to the symbols in context-free grammars, feature structures in unification-based grammars often include information encoding part of the derivation history, most notably semantics.	In order to achieve successful packing rates, feature restriction <TREF>Shieber, 1985</TREF> is used to remove this information during creation of the packed parse forest.	During the unpacking phase, which operates only on successful parse trees, these features are unified back in again.	For their experiments with efficient subsumptionbased packing, <REF>Oepen and Carroll, 2000</REF> experimented with different settings of the packing restrictor for the English Resource Grammar ERG <REF>Copestake and Flickinger, 2000</REF>: they found that good packing rates, and overall good performance during forest creation and unpacking were achieved, for the ERG, with partial restriction of the semantics, eg keeping index features unrestricted, since they have an impact on external combinatorial potential, but restricting most of the internal MRS representation, including the list of elementary predications and scope constraints.	2
Attention is restricted here to approximations of context-free grammars because context-free languages are the smallest class of formal language that can realistically be applied to the analysis of natural language.	Techniques such as restriction <TREF>Shieber, 1985</TREF> can be used to construct context-free approximations of many unification-based formalisms, so techniques for constructing finite-state approximations of context-free grammars can then be applied to these formalisms too.	2 Finite-state calculus A finite-state calculus or finite automata toolkit is a set of programs for manipulating finite-state automata and the regular languages and transducers that they describe.	Standard operations include intersection, union, difference, determinisation and minimisation.	2
Problems in the prediction step of the Earley parser used for unification-based formalisms no longer exist.	The use of restrictors as proposed by <TREF>Shieber 1985</TREF> is no longer necessary and the difficulties caused by treating subcategorization as a feature is no longer a problem.	By assuming that the number of structures associated with a lexical item is finite, since each structure has a lexical item attached to it, we implicitly make the assumption that an input string of finite length cannot be syntactically infinitely ambiguous.	Since the trees are produced by the input string, the parser can use information that might be non-local to guide the search.	3
The resulting structures form equivalence classes, since they abstract from word-specific information, such as FORM or STEM.	The abstraction is specified by means of a restrictor <TREF>Shieber, 1985</TREF>, the so-called lexicon restrictor.	After that, the grammar rules are instantiated by unification, using the abstracted lexicon entries and resulting in derivation trees of depth 1.	The rule restrictor is applied to each resulting feature structure FS, removing all information contained only in the daughters of a rule.	2
However, these models are inadequate for language interpretation, since they cannot express the relevant syntactic and semantic regularities.	Augmented phrase structure grammar APSG formalisms, such as unification-based grammars <TREF>Shieber, 1985a</TREF>, can express many of those regularities, but they are computationally less suitable for language modeling, because of the inherent cost of computing state transitions in APSG parsers.	The above problems might be circumvented by using separate grammars for language modeling and language interpretation.	Ideally, the recognition grammar should not reject sentences acceptable by the interpretation grammar and it should contain as much as reasonable of the constraints built into the interpretation grammar.	3
That is, instantiation of productions introduces the nontermination problem of left-recursive productions to the procedure, as well as to the Predictor Step of Earleys algorithm.	To overcome this problem, <TREF>Shieber 1985</TREF> proposes restrictor, which specifies a maximum depth of feature-based categories.	When the depth of a category in a predicted item exceeds the limit imposed by a restrictor, further instantiation of the category in new items is prohibited.	The Predictor Step eventually halts when it starts creating a new item whose feature specification within the depth allowed by the resuictor is identical to, or subsumed by, a previous one.	2
The situation is different for active chart items since daughters can affect their siblings.	To be independent from a-certain grammatical theory or implementation, we use restrictors similar to <TREF>Shieber, 1985</TREF> as a flexible and easyto-use specification to perform this deletion.	A positive restrictor is an automaton describing the paths in a feature structure that will remain after restriction the deletion operation, 3There are refinements of the technique which we have implemented and which in practice produce additional benefits; we will report these in a subsequent paper.	Briefly, they involve an improvement to th e path collection method, and the storage of other information besides types in the vectors.	2
That is, for the reference determination, the subject roles of the candidates referent within a discourse segment will be checked intheflrstplace.	Thisflndingsupportswell the suggestion in centering theory that the grammaticalrelationsshouldbeusedasthe key criteria to rank forward-looking centers in the process of focus tracking <TREF>Brennan et al , 1987</TREF>; <REF>Grosz et al , 1995</REF>.	3.	candi Pron and candi NoAntecedent are to be examined in the cases when the subject-role checking fails, which conflrms the hypothesis in the S-List model by <REF>Strube 1998</REF> that co-refereing candidates would have higher preference than other candidates in the pronoun resolution.	2
One of the most unusual features of centering theory is that the notions of utterance, previous utterance, ranking, and realization used in the definitions above are left unspecified, to be appropriately defined on the basis of empirical evidence, and possibly in a different way for each language.	As a result, centering theory is best viewed as a cluster of theories, each of which specifies the parameters in a different ways: eg, ranking has been claimed to depend on grammatical function <REF>Kameyama, 1985</REF>; <TREF>Brennan et al , 1987</TREF>, on thematic roles <REF>Cote, 1998</REF>, and on the discourse status of the CFs <REF>Strube and Hahn, 1999</REF>; there are at least two definitions of what counts as previous utterance <REF>Kameyama, 1998</REF>; <REF>Suri and McCoy, 1994</REF>; and realization can be interpreted either in a strict sense, ie, by taking a CF to be realized in an utterance only if an NP in that utterance denotes that CF, or in a looser sense, by also counting a CF as realized if it is referred to indirectly by means of a bridging reference <REF>Clark, 1977</REF>, ie, an anaphoric expression that refers to an object which wasnt mentioned before but is somehow related to an object that already has, as in the vase the handle see, eg, the discussion in <REF>Grosz et al , 1995</REF>; <REF>Walker et al , 1998b</REF>.	3 METHODS The fact that so many basic notions of centering theory do not have a completely specified definition makes empirical verification of the theory rather difficult.	Because any attempt at directly annotating a corpus for utterances and their CBs is bound to force the annotators to adopt some specification of the basic notions of the theory, previous studies have tended to study a particular variant of the theory <REF>Di Eugenio, 1998</REF>; <REF>Kameyama, 1998</REF>; <REF>Passonneau, 1993</REF>; <REF>Strube and Hahn, 1999</REF>; <REF>Walker, 1989</REF>.	3
Similarly, we can assess other strategies of sentence ordering that have been proposed in the literature.	Hard-core centering approaches only deal with the last sentence <TREF>Brennan et al , 1987</TREF>.	In Negra, these approaches can consequently have at most a success rate of 442.	Performance is particularly low with possessive pronouns which often only have antecedents in the current sentence.	3
Pragmatic level Working together, surface patterns and possessive relationships can deal with many PPAs found in our corpus, but we still have two problems to be solved: semantic ambiguity among two or more acceptable candidates and abstract anaphors/antecedents, which cannot be solved by simply applying possessive relationship rules.	For these cases, and possibly for some other cases not included in previous rules, we suggest a pragmatic factor, adapted from S Brennans et al 1987 centering algorithm.	Although sentence center plays a crucial role in many works in anaphor resolution, usually limiting the number of candidates to be considered, we notice that, because PPAs can refer to almost any NP in the sentence rather than, for example, personal pronouns, which are often related to the sentence center, pragmatic knowledge plays only a secondary but still important role in our approach.	We have adapted basic aspects of center algorithm, considering subject/object preference, and domain concepts preference, 1012 suggested by R <REF>Mitkov 1996</REF>, aiming to estimate the most probable center for intrasentential PPAs.	2
Their approach has been proven as the point of departure for a new model which is valid for English as well.	The use of the centering transitions in Brennan et als 1987 algorithm prevents it from being applied incrementally cf.	<REF>Kehler 1997</REF>.	In my approach, I propose to replace the functions of the backward-looking center and the centering transitions by the order among the elements of the list of salient discourse entities S-list.	3
Although they report that their method estimates over 90 of zero subjects correctly, there are several difficulties including the fact that the test corpus is identical with the corpus from which the pragmatic constraints are extracted, and the fact that there are so many rules46 rules to estimate 175 sentences.	As for the identifying method available in general discourses, the centering theory<TREF>Brennan et al , 1987</TREF>; <REF>Walker et al , 1990</REF> and the property sharing theory<REF>Kameyama, 1988</REF> are proposed.	The important feature of these theories is the fact that it is independent of the type of discourse.	However, according to our experimental result, it seems that these kinds of theory do not estimate zero subjects in high precision for manual sentences 3.	3
Grosz et al list seven such costraints, three of which can be directly evaluated.	Even though we are not following here the distinction between constraints and rules introduced in <REF>Brennan, Friedman, and Pollard 1987</REF>, we will use for these three claims the names Brennan et al gave them, by which they are now best known: Constraint 1 Strong: All utterances of a segment except for the first have exactly one CB.	Rule 1 GJW95: If any CF is pronominalized, the CB is Rule 2 GJW95: Sequences of continuations are preferred over sequences of retains, which are preferred over sequences of shifts.	231 Constraint 1, Topic Uniqueness, and Entity Coherence.	2
162 In effect, we use this probability information to identify the topic of the segment with the belief that the topic is more likely to be referred to by a pronoun.	The idea is similar to that used in the centering approach <TREF>Brennan et al , 1987</TREF> where a continued topic is the highest-ranked candidate for pronominalization.	Given the above possible sources of informar tion, we arrive at the following equation, where Fp denotes a function from pronouns to their antecedents: Fp  argmaxP Ap  alp, h, l, t, l, so, d A where Ap is a random variable denoting the referent of the pronoun p and a is a proposed antecedent.	In the conditioning events, h is the head constituent above p, l r is the list of candidate antecedents to be considered, t is the type of phrase of the proposed antecedent always a noun-phrase in this study, I is the type of the head constituent, sp describes the syntactic structure in which p appears, dspecifies the distance of each antecedent from p and M is the number of times the referent is mentioned.	2
This distinction underlies my proposals about the attentional consequences of pitch accents when applied to pronominals, in particular, that while most pitch accents may weaken or reinforce a cospecifiers status as the center of attention, a contrastively stressed pronominal may force a shift, even when contraindicated by textual features.	To predict and track the center of attention in discourse, theories of centering <REF>Grosz et al , 1983</REF>; <TREF>Brennan et al , 1987</TREF>; <REF>Grosz et al , 1989</REF> and immediate focus <REF>Sidner, 1986</REF> rely on syntactic and grammatical features of the text such as pronominalization and surface sentence position.	This may be sufficient for written discourse.	For oral discourse, however, we must also consider the way intonation affects the interpretation of a sentence, especially the cases in which it alters the predictions of centering theories.	3
3.	Processing Complex Sentences: A Reason for Extending Focusing Algorithms Although complex sentences are prevalent in written English, most other local focusing research focusing: Sidner 1979 and Carter 1987; centering: Grosz, Joshi, and Weinstein 1983, 1995, Brennan, Friedman, and Pollard 1987, Walker 1989, 1993, Kameyama 1986 2, Walker, Iida, and Cote 1994, Brennan 1998, Kameyama, Passonneau, and Poesio 1993, Linson 1993 and Hoffman 1998; and PUNDIT: Dahl 1986, Palmer et al 1986, and Dahl and Ball 1990 did not explicitly and/or adequately address how to process complex sentences.	Thus, there is a need to extend focusing algorithms.	An exception to this rule is the work of <REF>Strube 1996</REF> which applies functionalinformation-structure-based criteria on a per-clause basis, <REF>Kameyama 1998</REF>, and <REF>Strube 1998</REF>.	3
Together this work suggests that the interlocutors shared visual context has a major impact on their patterns of referring behavior.	Yet, a number of discourse-based models of reference primarily rely on linguistic information without regard to the surrounding visual environment eg , see <TREF>Brennan et al , 1987</TREF>; <REF>Hobbs, 1978</REF>; <REF>Poesio et al , 2004</REF>; <REF>Strube, 1998</REF>; <REF>Tetreault, 2005</REF>.	Recently, multi-modal models have emerged that integrate visual information into the resolution process.	However, many of these models are restricted by their simplifying assumption of communication via a command language.	3
Since the constraints are eflective in the lifferent target from ours, the accuracy of identifying the referents of zero pronouns would be improved much more by using both of his constraints and the constraint we proposed.	As for the identifying method available in general discourses, the centering theory<TREF>Brennan et al , 1987</TREF>; <REF>Walker et al , 1990</REF> and the property sharing theory<REF>Kameyama, 1988</REF> are proposed.	Although this kind of theory has a good point that it is independent of the type o17 discourse, the linguistic constraints specitic to expressions like the pragmatic constraints l/roposed by Dohsaka or us are more accurate than theirs when the speeitlc constraints are applicable.	3 General ontology in manuals and prinmry constraints In this section, we consider the general ontology which can be used in,dl types of manuals.	3
It is often claimed in current work on in natural language generation that the constraints on felicitous text proposed by the theory are useful to guide text structuring, in combination with other factors see <REF>Karamanis, 2003</REF> for an overview.	However, how successful Centerings constraints are on their own in generating a felicitous text structure is an open question, already raised by the seminal papers of the theory <TREF>Brennan et al , 1987</TREF>; <REF>Grosz et al , 1995</REF>.	In this work, we explored this question by developing an approach to text structuring purely based on Centering, in which the role of other factors is deliberately ignored.	In accordance with recent work in the emerging field of text-to-text generation <REF>Barzilay et al , 2002</REF>; <REF>Lapata, 2003</REF>, we assume that the input to text structuring is a set of clauses.	2
P99-1079:56.	Analysis of Syntax-Based Pronoun Resolution Methods Joel R Tetreault University of Rochester Department of Computer Science Rochester, NY, 14627 tetreaulcs, rochester, edu Abstract This paper presents a pronoun resolution algorithm that adheres to the constraints and rules of Centering Theory <REF>Grosz et al , 1995</REF> and is an alternative to Brennan et als 1987 algorithm.	The advantages of this new model, the Left-Right Centering Algorithm LRC, lie in its incremental processing of utterances and in its low computational overhead.	The algorithm is compared with three other pronoun resolution methods: Hobbs syntax-based algorithm, Strubes S-list approach, and the BFP Centering algorithm.	3
The noteworthy results were that Hobbs and LRC performed the best.	The aim of this project is to develop a pronoun resolution algorithm which performs better than the <TREF>Brennan et al 1987</TREF> algorithm 1 as a cognitive model while also performing well empirically.	A revised algorithm Left-Right Centering was motivated by the fact that the BFP algorithm did not allow for incremental processing of an utterance and hence of its pronouns, and also by the fact that it occasionally imposes a high computational load, detracting from its psycholinguistic plausibility.	A second motivation for the project is to remedy the dearth of empirical results on pronoun resolution methods.	3
5.	Identify Transition with the Cb and Cf resolved, use the criteria from <TREF>Brennan et al , 1987</TREF> to assign the transition.	It should be noted that BFP makes use of Centering Rule 2 <REF>Grosz et al , 1995</REF>, LRC does not use the transition generated or Rule 2 in steps 4 and 5 since Rule 2s role in pronoun resolution is not yet known see <REF>Kehler 1997</REF> for a critique of its use by BFP.	Computational overhead is avoided since no anchors or auxiliary data structures need to be produced and filtered.	2
Cheapness is satisfied by a transition pair Un-1, Un, Un, Unl if the preferred center of Un is the Cb of Unl For example, this test is satisfied by a RETAIN-SHIFT sequence but not by CONTINUE-SHIFT, so it is predicted that the former pattern will be used to introduce a new center.	This claim is consistent with the findings of <REF>Brennan 1998</REF>, <TREF>Brennan et al 1987</TREF>.	If we consider examples la-e below, the sequence cd-e , including a RETAIN-SHIFT sequence, reads more fluently than c-d-e even though the latter scores better according to the canonical ranking.	a John has had trouble arranging his vacation.	2
PFNOCB, a second baseline, which enhances MNOCB with a global constraint on coherence that <REF>Karamanis, 2003</REF> calls the PageFocus PF.	PFBFP which is based on PF as well as the original formulation of CT in <TREF>Brennan et al , 1987</TREF>.	PFKP which makes use of PF as well as the recent reformulation of CT in <REF>Kibble and Power, 2000</REF>.	<REF>Karamanis et al , 2004</REF> report that PFNOCB outperformed MNOCB but was overtaken by PFBFP and PFKP.	2
This is in contrast to theories of global coherence, which can consider relations between larger chunks of the discourse and eg structures them into a tree <REF>Mann and Thompson, 1988</REF>; <REF>Marcu, 1997</REF>; <REF>Webber et al , 1999</REF>.	Measures of local coherence specify which ordering of the sentences makes for the most coherent discourse, and can be based eg on Centering Theory <REF>Walker et al , 1998</REF>; <TREF>Brennan et al , 1987</TREF>; <REF>Kibble and Power, 2000</REF>; <REF>Karamanis and Manurung, 2002</REF> or on statistical models <REF>Lapata, 2003</REF>.	But while formal models of local coherence have made substantial progress over the past few years, the question of how to efficiently compute an ordering of the sentences in a discourse that maximises local coherence is still largely unsolved.	The fundamental problem is that any of the factorial number of permutations of the sentences could be the optimal discourse, which makes for a formidable search space for nontrivial discourses.	3
One more filtering criterion is mutual information MI, which reflects the relatedness of two terms in their combination , kj ww  To keep a relation  kji wwwP, we require , kj ww be a meaningful combination.	We use the following pointwise MI <TREF>Church and Hanks 1989</TREF>:  ,log, kj kj kj wPwP wwPwwMI  We only keep meaningful combinations such that 0, >kj wwMI  By these filtering criteria, we are able to reduce considerably the number of biterms and triterms.	For example, on a collection of about 200MB, with a vocabulary size of about 148K, we selected only about 27M useful biterms and about 137M triterms, which remain tractable.	33 Probability of Biterms In LM used in IR, each query term is attributed the same weight.	2
Afterward, if a target adjective sense was not resolved, semantic indicator attributes were applied; no individual indicator nouns were used.	The semantic attributes that were applied were animate, body part, color, concrete, human, and text type; <TREF>Church and Hanks 1989</TREF> had pointed to two of these attributes, person and body part also time, previously mentioned above in a seemingly casual listing of just five attributes potentially useful for describing the lexico-syntactic regularities of noun-verb relations.	Table 1 shows that these few, general attributes cover almost three-quarters of all instances of the target adjectives.	Disambiguation by these syntactic and semantic attributes is effectively as reliable as disambiguation using significant indicator nouns: having three apparent errors in disambiguation is not significantly worse than the errorless performance of the significant indicator nouns in the 100-sentence samples.	2
Content words that have a close syntactic relation to one another are useful candidates for examination and are intuitively more likely to bear a close semantic relation than words that are near one another but are not related syntactically.	One much-studied example is the semantic relation between a verb and its arguments eg , <REF>Boguraev et al 1989</REF>; <TREF>Church and Hanks 1989</TREF>; Braden-<REF>Harder 1991</REF>; <REF>Hindle and Rooth 1991</REF>.	Discrimination among senses of adjectives based on the nouns they modify or of which they are predicated has been the subject of less intensive and systematic study.	Determining the potential of this line of evidence is the focus of this paper.	2
Like path coreference, semantic compatibility can be considered a form of world knowledge needed for more challenging pronoun resolution instances.	We encode the semantic compatibility between a noun and its parse tree parent and grammatical relationship with the parent using mutual information MI <TREF>Church and Hanks, 1989</TREF>.	Suppose we are determining whether ham is a suitable antecedent for the pronoun it in eat it.	We calculate the MI as: MIeat:obj, ham  log Preat:obj:hamPreat:objPrham Although semantic compatibility is usually only computed for possessive-noun, subject-verb, and verb-object relationships, we include 121 different kinds of syntactic relationships as parsed in our news corpus3 We collected 488 billion parent:rel:node triples, including over 327 million possessive-noun values, 129 billion subject-verb and 877 million verb-direct object.	2
This line of research was motivated by a series of successful applications of mutual information statistics to other problems in natural language processing.	In the last decade, research in speech recognition <REF>Jelinek 1985</REF>, noun classification <REF>Hindle 1988</REF>, predicate argument relations <TREF>Church  Hanks 1989</TREF>, and other areas have shown that mutual information statistics provide a wealth of information for solving these problems.	22 Mutual Information Statistics The mutual information statistic <REF>Fano 1961</REF> is a measure of the interdependence of two signals in a message.	It is a function of the probabilities of the two events: Mz, u  log u xzPvy In this paper, the events x and y will be part-of-speech n-grams instead of single parts-of-speech, as in some earlier work.	2
In the past, for this purpose a number of measures have been proposed.	They were based on mutual information <TREF>Church  Hanks, 1989</TREF>, conditional probabilities <REF>Rapp, 1996</REF>, or on some standard statistical tests, such as the chi-square test or the loglikelihood ratio <REF>Dunning, 1993</REF>.	For the purpose of this paper, we decided to use the loglikelihood ratio, which is theoretically well justified and more appropriate for sparse data than chi-square.	In preliminary experiments it also led to slightly better results than the conditional probability measure.	3
Finally, methods and strategies for handling low-frequency data are suggested.	The measures2  Mutual Information a0a2a1  <TREF>Church and Hanks, 1989</TREF>, the log-likelihood ratio test <REF>Dunning, 1993</REF>, two statistical tests: t-test and a3a5a4 -test, and co-occurrence frequency  are applied to two sets of data: adjective-noun AdjN pairs and preposition-noun-verb PNV triples, where the AMs are applied to PN,V pairs.	See section 3 for a description of the base data.	For evaluation of the association measures, a6 -best strategies section 41 are supplemented with precision and recall graphs section 42 over the complete data sets.	2
41 EIIR: Expected Independent Information Ranking Model Baseline Model Recall the task definition from Section 3.	Finding a property r that most reduces the uncertainty in a query set Q can be modeled by measuring the strength of association between r and Q <REF>Following Pantel and Lin 2002</REF>, we use pointwise mutual information pmi to measure the association strength between two events q and r, where q is a term in Q and r is syntactic dependency, as follows <TREF>Church and Hanks 1989</TREF>:      N fqc N rwc N rqc Ff Ww rqpmi       , , , log,  41 where cq,r is the frequency of r in the feature vector of q as defined in Section 32, W is the set of all words in our corpus, F is the set of all syntactic dependencies in our corpus, and N   WwFf fwc , is the total frequency count of all features of all words.	We estimate the association strength between a property r and a set of terms Q by taking the expected pmi between r and each term in Q as:       Qq rqpmiqPrQpmi ,,  42 where Pq is the probability of q in the corpus.	Finally, the EIIR model chooses an n-best list by selecting the n properties from R that have highest pmiQ, r.	2
RB, RBR, or RBS VB, VBD, VBN, or VBG anything The second step is to estimate the semantic orientation of the extracted phrases, using the PMI-IR algorithm.	This algorithm uses mutual information as a measure of the strength of semantic association between two words <TREF>Church  Hanks, 1989</TREF>.	PMI-IR has been empirically evaluated using 80 synonym test questions from the Test of English as a Foreign Language TOEFL, obtaining a score of 74 <REF>Turney, 2001</REF>.	For comparison, Latent Semantic Analysis LSA, another statistical measure of word association, attains a score of 64 on the 3 http://wwwcsjhuedu/brill/RBT114tarZ 4 <REF>See Santorini 1995</REF> for a complete description of the tags.	2
Thus we introduce d-bigram which is a bigram cooccurrence information concerning the distance<REF>Tsutsumi et al , 1993</REF>.	Expression 1 calculates the score between two neighboring letters; UKi  E E Mwj,wid;d  x,qd 1 dl j-i--d--1 where wl as an eveN;, d as the distance between two eveN;s, dmax as the maximum distance used in the processing we set drnax - 5, and gd as the weight fimction on distance for this system gd  d-2<REF>Sano et al , 1996</REF>, to decrease tile influence of tile d-bigrams when the distance get longer <TREF>Church and Hanks, 1989</TREF>.	When calculating the linking score between the letters wi and Wil, tile d-bigram information of the letter pairs around tim target two such as wi-l, wi2; 3 are added.	Expression 2 calculates the mutual information between two events with d-bigram data; v; d d -2 where x, y as events, d for the distance between two events, and Px as the probability.	2
Word alignments that are shared by many different words are most probably mismatches.	For this experiment we used Pointwise Mutual Information I <TREF>Church and Hanks, 1989</TREF>.	IW, f  log PW, fPWPf,where W is the target word PW is the probability of seeing the word Pf is the probability of seeing the feature PW,f is the probability of seeing the word and the feature together.	33 Word Alignment The multilingual approach we are proposing relies on automatic word alignment of parallel corpora from Dutch to one or more target languages.	2
Hence, greater the frequency, the more is the likelihood of the expression to be a MWE.	612 Point-wise Mutual Information a16  Point-wise Mutual information of a collocation <TREF>Church and Hanks, 1989</TREF> is defined as, a16a18a17a19a11a21a20a23a22a25a24a27a26 a15a28a17a19a11a2a20a23a22a25a24a30a29a31a15a28a17a33a32a34a20a35a32a36a24 a15a28a17a19a11a2a20a35a32a36a24a30a29a37a15a28a17a33a32a34a20a23a22a25a24 where, a11 is the verb and a22 is the object of the collocation.	The higher the Mutual information of a collocation, the more is the likelihood of the expression to be a MWE.	613 Least mutual information difference with similar collocations a38  This feature is based on Lins work <REF>Lin, 1999</REF>.	2
Various statistical measures have been suggested for ranking expressions based on their compositionality.	Some of these are Frequency, Mutual Information <TREF>Church and Hanks, 1989</TREF>, distributed frequency of object <REF>Tapanainen et al , 1998</REF> and LSA model <REF>Baldwin et al , 2003</REF> <REF>Schutze, 1998</REF>.	In this paper, we define novel measures both collocation based and context based measures to measure the relative compositionality of MWEs of V-N type see section 6 for details.	Integrating these statistical measures should provide better evidence for ranking the expressions.	2
Mutual Information is attractive because it is not only easy to compute, but also takes into consideration corpus statistics and semantics.	The mutual information between two terms <TREF>Church and Hanks, 1989</TREF> can be calculated using Equation 2.	Ix,y  log nx,y N nx N ny N 2 nx,y is the number of times terms x and y occurred within a term window of 100 terms across the corpus, while nx and ny are the frequencies of x and y in the collection of size N terms.	To tackle the situation where we have an arbitrary number of variables terms we extend the twovariable case to the multivariate case.	2
AER  MergePos 054 045 049 05101  MergeMI 055 045 050 05045 Table 6: Results using the compositionality based features pressions of various types.	Some of them are Frequency, Point-wise mutual information <TREF>Church and Hanks, 1989</TREF>, Distributed frequency of object <REF>Tapanainen et al , 1998</REF>, Distributed frequency of object using verb information <REF>Venkatapathy and Joshi, 2005</REF></REF>, Similarity of object in verbobject pair using the LSA model <REF>Baldwin et al , 2003</REF>, <REF>Venkatapathy and Joshi, 2005</REF></REF> and Lexical and Syntactic fixedness <REF>Fazly and Stevenson, 2006</REF>.	These features have largely been evaluated by the correlation of the compositionality value predicted by these measures with the gold standard value suggested by human judges.	It has been shown that the correlation of these measures is higher than simple baseline measures suggesting that these measures represent compositionality quite well.	2
In the past, various measures have been suggested for measuring the compositionality of multi-word expressions.	Some of these are mutual information <TREF>Church and Hanks, 1989</TREF>, distributed frequency <REF>Tapanainen et al , 1998</REF> and Latent Semantic Analysis LSA model <REF>Baldwin et al , 2003</REF>.	Even though, these measures have been shown to represent compositionality quite well, compositionality itself has not been shown to be useful in any application yet.	In this paper, we explore this possibility of using the information about compositionality of MWEs verb based for the word alignment task.	2
More is to be learned from the fact that you can drink wine than from the fact that you can drink it even though there are more clauses in our sample with  as an object of drink than with wine.	To capture this intuition, we turn, following <TREF>Church and Hanks 1989</TREF>, to mutual information see <REF>Fano 1961</REF>.	The mutual information of two events lx y is defined as follows: Px y lxy  log2 Px Py where Px y is the joint probability of events x and y, and Px and Py axe the respective independent probabilities.	When the joint probability Px y is high relative to the product of the independent probabilities, I is positive; when the joint probability is relatively low, I is negative.	2
The aim of this measure is to indicate the relatedness between two elements composing a pair.	Mutual information has been positively used in many NLP tasks such as collocation analysis <TREF>Church and Hanks, 1989</TREF>, terminology extraction <REF>Damerau, 1993</REF>, and word sense disambiguation <REF>Brown et al , 1991</REF>.	3 Experimental Evaluation As many other corpus linguistic approaches, our entailment detection model relies partially on some linguistic prior knowledge the expected structure of the searched collocations, ie, the textual entailment patterns and partially on some probability distribution estimation.	Only a positive combination of both these two ingredients can give good results when applying and evaluating the model.	2
In the first stage, pairwise lexical relations are retrieved using only statistical information.	This stage is comparable to <TREF>Church and Hanks 1989</TREF> in that it evaluates a certain word association between pairs of words.	As in <TREF>Church and Hanks 1989</TREF>, the words can appear in any order and they can be separated by an arbitrary number of other words.	However, the statistics we use provide more information and allow us to have more precision in our output.	2
This stage is comparable to <TREF>Church and Hanks 1989</TREF> in that it evaluates a certain word association between pairs of words.	As in <TREF>Church and Hanks 1989</TREF>, the words can appear in any order and they can be separated by an arbitrary number of other words.	However, the statistics we use provide more information and allow us to have more precision in our output.	The output of this first stage is then passed in parallel to the next two stages.	3
This limitation is intrinsic to the technique used since mutual information scores are defined for two items.	The second limitation is that many collocations identified in <TREF>Church and Hanks 1989</TREF> do not really identify true collocations, but simply pairs of words that frequently appear together such as the pairs doctor-nurse, doctor-bill, doctor-honorary, doctors-dentists, doctors-hospitals, etc These co-occurrences are mostly due to semantic reasons.	The two words are used in the same context because they are of related meanings; they are not part of a single collocational construct.	The work we describe in the rest of this paper is along the same lines of research.	3
The two words are often used together because they are associated with the same context rather than for pure structural reasons.	Many collocations retrieved in <TREF>Church and Hanks 1989</TREF> were of this type, as they retrieved doctors-dentists, doctors-nurses, doctorbills, doctors-hospitals, nurses-doctor, etc , which are not collocations in the sense defined above.	Such collocations are not of interest for our purpose, although they could be useful for disambiguation or other semantic purposes.	Condition C2 filters out exactly this type of collocations.	3
In this case, we are interested in collocations between the head of a PP complement, a preposition and the head of the phrase being postmodified.	In general, these words will not be adjacent in the text, so it will not be possible to use existing approaches unmodified eg <TREF>Church and Hanks 1989</TREF>, because these apply to adjacent words in unanalyzed text.	<REF>Hindle and Rooth 1991</REF> report good results using a mutual information measure of collocation applied within such a structurally defined context, and their approach should carry over to our framework straightforwardly.	One way of integrating structural collocational information into the system presented above would be to make use of the semantic component of the ANLT grammar This component pairs logical forms with each distinct syntactic analysis that represent, among other things, the predicate-argument structure of the input.	3
Since we use grammatical context, the feature set is considerably larger than the simple word based proximity feature set for the newspaper corpus.	43 Calculating Feature Vectors Having collected all nouns and their features, we now proceed to construct feature vectors and values for nouns from both corpora using mutual information <TREF>Church and Hanks, 1989</TREF>.	We first construct a frequency count vector Ce  ce1,ce2,,cek, where k is the total number of features and cef is the frequency count of feature f occurring in word e Here, cef is the number of times word e occurred in context f We then construct a mutual information vector MIe  mie1,mie2,,miek for each word e, where mief is the pointwise mutual information between word e and feature f, which is defined as: mief  log cef Nsummationtext n i1 cif N  summationtextk j1 cej N 6 where n is the number of words and N  5We perform this operation so that we can compare the performance of our system to that of <REF>Pantel and Lin 2002</REF>.	summationtextn i1 summationtextm j1 cij is the total frequency count of all features of all words.	2
The SO of a phrase is determined based upon the phrases pointwise mutual information PMI with the words excellent and poor.	PMI is defined by <TREF>Church and Hanks 1989</TREF> as follows: a0a2a1a4a3a6a5a8a7a10a9a12a11a13a7a15a14a17a16a19a18a21a20a23a22a25a24 a14a27a26a29a28 a5a8a7a10a9a19a30a31a7a15a14a17a16 a28 a5a8a7 a9 a16 a28 a5a8a7 a14 a16a33a32 1 where a28 a5a8a7a10a9a19a30a31a7a15a14a12a16 is the probability that a7a34a9 and a7a35a14 co-occur.	The SO for a a28a37a36a39a38a41a40a29a42a44a43 is the difference between its PMI with the word excellent and its PMI with the word poor The method used to derive these values takes advantage of the possibility of using the World Wide Web as a corpus, similarly to work such as <REF>Keller and Lapata, 2003</REF>.	The probabilities are estimated by querying the AltaVista Advanced Search engine1 for counts.	2
To solve the problem, we make use of a kind of cooperative evolution strategy to design an evolutionary algorithm.	Word compositions have long been a concern in lexicography<REF>Benson et al 1986</REF></REF>; <REF>Miller et al 1995</REF>, and now as a specific kind of lexical knowledge, it has been shown that they have an important role in many areas in natural language processing, eg, parsing, generation, lexicon building, word sense disambiguation, and information retrieving, etceg , <REF>Abney 1989, 1990</REF>; <REF>Benson et al 1986</REF></REF>; <REF>Yarowsky 1995</REF>; <TREF>Church and Hanks 1989</TREF>; Church, <REF>Gale, Hans, and Hindle 1989</REF>.	But due to the huge number of words, it is impossible to list all compositions between words by hand in dictionaries.	So an urgent problem occurs: how to automatically acquire word compositions.	2
While bound compositions are not predictable, ie, their reasonableness cannot be derived from the syntactic and semantic properties of the words in them<REF>Smadja 1993</REF>.	Now with the availability of large-scale corpus, automatic acquisition of word compositions, especially word collocations from them have been extensively studiedeg , <REF>Choueka et al 1988</REF>; <TREF>Church and Hanks 1989</TREF>; <REF>Smadja 1993</REF>.	The key of their methods is to make use of some statistical means, eg, frequencies or mutual information, to quantify the compositional strength between words.	These methods are more appropriate for retrieving bound compositions, while less appropriate for retrieving free ones.	3
Dictionaries produced by hand always substantially lag real language use.	The last two points do not argue against the use of existing dictionaries, but show that the incomplete information that they provide needs to be supplemented with further knowledge that is best collected automatically The desire to combine hand-coded and automatically learned knowledge 1A point made by <TREF>Church and Hanks 1989</TREF>.	Arbitrary gaps in listing can be smoothed with a program such as the work presented here.	For example, among the 27 verbs that most commonly cooccurred with from, Church and Hanks found 7 for which this 235 suggests that we should aim for a high precision learner even at some cost in coverage, and that is the approach adopted here.	2
They are estimated by using Table 2.	C8B4CRCYD4D3D7B5 BP CUB4CRBND4D3D7B5 CUB4CRBND4D3D7B5B7CUB4BMCRBND4D3D7B5 C8B4CRCYD2CTCVB5 BP CUB4CRBND2CTCVB5 CUB4CRBND2CTCVB5B7CUB4BMCRBND2CTCVB5 PMI based polarity value Using PMI, the strength of association between CR and positive sentences and negative sentences is defined as follows <TREF>Church and Hanks, 1989</TREF>.	C8C5C1B4CRBND4D3D7B5 BP D0D3CV BE C8B4CRBND4D3D7B5 C8B4CRB5C8B4D4D3D7B5 C8C5C1B4CRBND2CTCVB5 BP D0D3CV BE C8B4CRBND2CTCVB5 C8B4CRB5C8B4D2CTCVB5 PMI based polarity value is defined as their difference.	This idea is the same as <REF>Turney, 2002</REF>.	2
If the absolute value of the relative distance in a sentence for a feature and an opinion word is less than Minimum-Offset, they are considered contextdependent.	Many methods have been proposed to measure the co-occurrence relation between two words such as  2 Church and Mercer,1993 , mutual information <TREF>Church and Hanks, 1989</TREF></TREF>; <REF>Pantel and Lin, 2002</REF>, t-test <TREF>Church and Hanks, 1989</TREF></TREF>, and loglikelihood Dunning,1993.	In this paper a revised formula of mutual information is used to measure the association since mutual information of a lowfrequency word pair tends to be very high.	Table 1 gives the contingency table for two words or phrases w 1  and  w 2 , where A is the number of reviews where w 1  and w 2  co-occur; B indicates the number of reviews where w 1  occurs but does not co-occur with w 2 ; C denotes the number of reviews where w 2  occurs but does not co-occur with w 1 ; D is number of reviews where neither w 1  nor w 2  occurs; N  A  B  C  D With the table, the revised formula of mutual information is designed to calculate the association of w 1 with w 2  as formula 1.	2
Other types of phrases.	Many efficient techniques exist to extract multiword expressions, collocations, lexical units and idioms <TREF>Church and Hanks, 1989</TREF>; <REF>Smadja, 1993</REF>; <REF>Dias et al , 2000</REF>; <REF>Dias, 2003</REF>.	Unfortunately, very few have been applied to information retrieval with a deep evaluation of the results.	Maximal Frequent Sequences.	2
The definition can be easily extended to a set of expressions T Given a pair vt and vh we define the following entailment strength indicator Svt,vh.	Specifically, the measure Snomvt,vh is derived from point-wise mutual information <TREF>Church and Hanks, 1989</TREF>: Snomvt,vh  log pvt,vhnompv tpvhpers 3 where nom is the event of having a nominalized textual entailment pattern and pers is the event of having an agentive nominalization of verbs.	Probabilities are estimated using maximum-likelihood: pvt,vhnom  fCPnomvt,vhf C uniontextP nomvprimet,vprimeh, 852 pvt  fCFvt/fCuniontextFv, and pvhpers  fCFagentvh/fCuniontextFagentv.	Counts are considered useful when they are greater or equal to 3.	2
We then construct a mutual information vector MIe  mi e1, mi e2, , mi em  for each word e, where mi ef is the pointwise mutual information between word e and feature f, which is defined as: N c N c N c ef m j ej n i if ef mi       1 1 log 1 where n is the number of words and N    n i m j ij c 11 is the total frequency count of all features of all words.	Mutual information is commonly used to measure the association strength between two words <TREF>Church and Hanks 1989</TREF>.	A well-known problem is that mutual information is biased towards infrequent elements/features.	We therefore multiply mi ef with the following discounting factor: 1,min,min 1 11 11                        m j jf n i ei m j jf n i ei ef ef cc cc c c 2 32 Phase II Following <REF>Pantel and Lin 2002</REF>, we construct a committee for each semantic class.	2
In fact, corpus linguistics became a popular research field because of the claim that shallow techniques could overcome the lexical coverage bottleneck of traditional NLP techniques.	Among the applications of collocational analysis for lexical acquisition are: the derivation of syntactic disambiguation cues <REF>Basili et al 1991, 1993a</REF>; <REF>Hindle and Rooths 1991</REF>,1993; <REF>Sekine 1992</REF> <REF>Bogges et al 1992</REF>, sense preference <REF>Yarowski 1992</REF>, acquisition of selectional restrictions <REF>Basili et al 1992b, 1993b</REF>; <REF>Utsuro et al 1993</REF>, lexical preference in generation <REF>Smadjia 1991</REF>, word clustering <REF>Pereira 1993</REF>; <TREF>Hindle 1990</TREF>; <REF>Basili et al 1993c</REF>, etc In the majority of these papers, even though the precedent or subsequent statistical processing reduces the number of accidental associations, very large corpora 10,000,000 words are necessary to obtain reliable data on a large enough number of words.	In addition, most papers produce a performance evaluation of their methods but do not provide a measure of the coverage, ie the percentage of cases for which their method actually provides a right or wrong solution.	It is quite common that results are discussed only for 10-20 cases.	3
or the cooccurrence of two words within a limited distance in the context.	Statistical data about these various cooccurrence relations is employed for a variety of applications, such as speech recognition <REF>Jelinek, 1990</REF>, language generation <REF>Smadja and McKeown, 1990</REF>, lexicography <REF>Church and Hanks, 1990</REF>, machine translation Brown et al , ; <REF>Sadler, 1989</REF>, information retrieval <REF>Maarek and Smadja, 1989</REF> and various disambiguation tasks <REF>Dagan et al , 1991</REF>; <REF>Hindle and Rooth, 1991</REF>; <REF>Grishman et al , 1986</REF>; <REF>Dagan and Itai, 1990</REF>.	A major problem for the above applications is how to estimate the probability of cooccurrences that were not observed in the training corpus.	Due to data sparseness in unrestricted language, the aggregate probability of such cooccurrences is large and can easily get to 25 or more, even for a very large training corpus <REF>Church and Mercer, 1992</REF>.	3
Semantic variation is rarely studied in specialized domains.	Works on word similarity and word sense disambiguation are generally based on statistical methods designed for large or even very large corpora <TREF>Hindle, 1990</TREF>; <REF>Agirre and Rigau, 1996</REF>.	Therefore, they cannot be applied for technical documents which usually are medium size corpora.	However, dealing with already linguistic filtered data, <REF>Assadi, 1997</REF> aims at statistically build rough clusters supposing that similar candidate terms have similar expansions.	3
Distributional similarity represents the relatedness of two words by the commonality of contexts the words share, based on the distributional hypothesis <REF>Harris, 1985</REF>, which states that semantically similar words share similar contexts.	A number of researches which utilized distributional similarity have been conducted, including <TREF>Hindle, 1990</TREF>; <REF>Lin, 1998</REF>; <REF>Geffet and Dagan, 2004</REF> and many others.	Although they have been successful in acquiring related words, various parameters such as similarity measures and weighting are involved.	As Weeds et al.	3
The hypothesis states that words that occur in the same contexts tend to have similar meaning.	Researchers have mostly looked at representing words by their surrounding words <REF>Lund and Burgess 1996</REF> and by their syntactical contexts <TREF>Hindle 1990</TREF>; <REF>Lin 1998</REF>.	However, these representations do not distinguish between the different senses of words.	Our framework utilizes these principles and representations to induce disambiguated feature vectors.	3
Automatic exploration of a sublanguage corpus constitutes a first step towards identifying the semantic classes and relationships which are relevant for this sublanguage.	In the past five years, important research on the automatic acquisition of word classes based on lexical distribution has been published <REF>Church and Hanks, 1990</REF>; <TREF>Hindle, 1990</TREF>; <REF>Smadja, 1993</REF>; Greinstette, 1994; <REF>Grishman and Sterling, 1994</REF>.	Most of these approaches, however, need large or even very large corpora in order for word classes to be discovered 1 whereas it is often the case that the data to be processed are insufficient to provide reliable lexical intbrmation.	In other words, it is not always possible to resort to statistical methods.	3
Among many kinds of lexical relations, synonyms are especially useful ones, having broad range of applications such as query expansion technique in information retrieval and automatic thesaurus construction.	Various methods <TREF>Hindle, 1990</TREF>; <REF>Lin, 1998</REF>; <REF>Hagiwara et al , 2005</REF> have been proposed for synonym acquisition.	Most of the acquisition methods are based on distributional hypothesis <REF>Harris, 1985</REF>, which states that semantically similar words share similar contexts, and it has been experimentally shown considerably plausible.	However, whereas many methods which adopt the hypothesis are based on contextual clues concerning words, and there has been much consideration on the language models such as Latent Semantic Indexing <REF>Deerwester et al , 1990</REF> and Probabilistic LSI <REF>Hofmann, 1999</REF> and synonym acquisition method, almost no attention has been paid to what kind of categories of contextual information, or their combinations, are useful for word featuring in terms of synonym acquisition.	3
Typical feature weighting functions include the logarithm of the frequency of word-feature cooccurrence <REF>Ruge, 1992</REF>, and the conditional probability of the feature given the word within probabilistic-based measures <REF>Pereira et al , 1993</REF>, <REF>Lee, 1997</REF>, <REF>Dagan et al , 1999</REF>.	Probably the most widely used association weight function is point-wise Mutual Information MI <REF>Church et al , 1990</REF>, <TREF>Hindle, 1990</TREF>, <REF>Lin, 1998</REF>, <REF>Dagan, 2000</REF>, defined by:  ,log, 2 fPwP fwPfwMI  A known weakness of MI is its tendency to assign high weights for rare features.	Yet, similarity measures that utilize MI showed good performance.	In particular, a common practice is to filter out features by minimal frequency and weight thresholds.	3
The similarity is usually calculated from a thesaurus.	Since a handmade thesaurus is not slfitahle for machine use, and expensive to compile, automatical construction ofa thesaurus has been attempted using corpora <TREF>Hindle, 1990</TREF>.	llowever, the thesaurus constructed by such ways does not contain so many nouns, and these nouns are specified by the used corpus.	In other words, we cannot construct the general thesaurus from only a corpus.	3
In previous research, word meanings have been statistically modeled based on syntactic information derived from a corpus.	<TREF>Hindle 1990</TREF> used noun-verb syntactic relations, and <REF>Hatzivassiloglou and McKeown 1993</REF> used coordinated adjective-adjective modifier pairs.	These methods are useful for the organization of words deep within a hierarchy, but do not seem to provide a solution for the top levels of the hierarchy.	To find an objective hierarchical word structure, we utilize the complementary similarity measure CSM, which estimates a one-to-many relation, such as superordinatesubordinate relations <REF>Hagita and Sawaki 1995</REF>, <REF>Yamamoto and Umemura 2002</REF>.	3
In NLP, representing verb semantics with their thematic roles is a consolidated practice, even though theoretical researches <REF>Pustejovski 1991</REF> propose more rich and formal representation frameworks.	More recent papers <TREF>Hindle 1990</TREF>, <REF>Pereira and Tishby 1992</REF> proposed to cluster nouns on the basis of a metric derived from the distribution of subject, verb and object in the texts.	Both papers use as a source of information large corpora, but differ in the type of statistical approach used to determine word similarity.	These studies, though valuable, leave several open problems: 70 1 A metric of conceptual closeness based on mere syntactic similarity is questionable, particularly if applied to verbs.	3
However, many studies investigate synonym extraction from only one resource.	The most frequently used resource for synonym extraction is large monolingual corpora <TREF>Hindle, 1990</TREF>; <REF>Crouch and Yang, 1992</REF>; <REF>Grefenstatte, 1994</REF>; <REF>Park and Choi, 1997</REF>; <REF>Gasperin et al , 2001</REF> and <REF>Lin, 1998</REF>.	The methods used the contexts around the investigated words to discover synonyms.	The problem of the methods is that the precision of the extracted synonymous words is low because it extracts many word pairs such as cat and dog, which are similar but not synonymous.	3
The underlying idea is based largely on the central claim of the distributional hypothesis <REF>Harris 1968</REF>, that is: The meaning of entities, and the meaning of grammatical relations among them, is related to the restriction of combinations of these entities relative to other entities.	This hypothesized relationship between distributional similarity and semantic similarity has given rise to a large body of work on automatic thesaurus generation <TREF>Hindle 1990</TREF>; <REF>Grefenstette 1994</REF>; <REF>Lin 1998a</REF>; <REF>Curran and Moens 2002</REF>; <REF>Kilgarriff 2003</REF>.	There are inherent problems in evaluating automatic thesaurus extraction techniques, and much research assumes a gold standard that does not exist see Kilgarriff 2003 and Weeds 2003 for more discussion of this.	A further problem for distributional similarity methods for automatic thesaurus generation is that they do not offer any obvious way to distinguish between linguistic relations such as synonymy, antonymy, and hyponymy see Caraballo 1999 and Lin et al 2003 for work on this.	3
First, most of theln assume that the input corpora me aligned sentence by sentence, which reduces their applicability remarkably.	Although a number of automatic sentence alignment methods have been proposed <TREF>Brown et al 1991</TREF> ; <REF>Gale  Church 1991</REF> b; <REF>Kay  Roscheisen 1993</REF>; <REF>Chen 1993</REF>, they are not very reliable for real noisy bilingual texts.	Second, the statistical methods usually require a very large corpus as their input.	However, it is not easy to obtain a very large corpus.	3
Several automatic methods have been proposed for this task in recent years.	However, most of these methods address only the sub-problem of alignment <REF>Catizone et al 1989</REF>, <TREF>Brown et al 1991</TREF>, <REF>Gale  Church 1991</REF>, <REF>Debili  Sammouda 1992</REF>, <REF>Simard et al 1992</REF>, Kay  R<REF>Sscheisen 1993</REF>, <REF>Wu 1994</REF>.	Alignment algorithms assume the availability of text unit boundary information and their output has less expressive power than a general bitext map.	The only published solution to the more difficult general bitext mapping problem <REF>Church 1993</REF> can err by several typeset pages.	3
There are several papers in the literature about bitext alignment.	The algorithms that seem to work best rely on the high correlation between the lengths of corresponding sentences <TREF>Brown et al 1991</TREF>, <REF>Gale  Church 1991</REF>.	However, these algorithms can fumble in bitext sections that contain many sentences of very similar length, like this vote record: English French Mr McInnis.	Yes.	3
Many software products which aid human translators now contain sentence alignment tools as an aid to speeding up editing and terminology searching.	Various methods have been developed for sentence alignment which we can categorise as either lexical such as <REF>Chen, 1993</REF>, based on a large-scale bilingual lexicon; statistical such as <TREF>Brown et al , 1991</TREF> <REF>Church, 1993</REF><REF>Gale and Church, 1903</REF>Kay and R<REF>Ssheheisen, 1993</REF>, based on distributional regularities of words or byte-length ratios and possibly inducing a bilingual lexicon as a by-product, or hybrid such as <REF>Utsuro et al , 1994</REF> <REF>Wu, 1994</REF>, based on some combination of the other two.	Neither of the pure approaches is entirely satisfactory for the following reasons:  Text volume limits the usefulness of statistical approaches.	We would often like to be able to align small amounts of text, or texts from various domains which do not share the same statistical properties.	3
To support machine translation, parallel sentences should be extracted from the mined parallel documents.	However, current sentence alignment models, <TREF>Brown et al 1991</TREF>; <REF>Gale  Church 1991</REF>; <REF>Wu 1994</REF>; Chen 489 1993; <REF>Zhao and Vogel, 2002</REF>; etc.	are targeted on traditional textual documents.	Due to the noisy nature of the web documents, parallel web pages may consist of non-translational content and many out-of-vocabulary words, both of which reduce sentence alignment accuracy.	3
If we reinterpret the Viterbi alignment to mean the most probable alignment that we can find rather than the most probable alignment that exists, then a similarly reinterpreted Viterbi training algorithm still converges.	We have already used this algorithm successfully as a part of a system to assign senses to English and French words on the basis of the context in which they appear <REF>Brown et al 1991a, 1991b</REF>.	We expect to use it in models that we develop beyond Model 5.	293 Computational Linguistics Volume 19, Number 2 63 Multi-Word Cepts In Models 1-5, we restrict our attention to alignments with cepts containing no more than one word each.	2
P93-1001:25.	Charalign: A Program for Aligning Parallel Texts at the Character Level Kenneth Ward Church ATT Bell Laboratories 600 Mountain Avenue Murray Hill NJ, 07974-0636 kwc researchattcom Abstract There have been a number of recent papers on aligning parallel texts at the sentence level, eg, <TREF>Brown et al 1991</TREF>, Gale and Church to appear, <REF>Isabelle 1992</REF>, Kay and R/Ssenschein to appear, <REF>Simard et al 1992</REF>, <REF>WarwickArmstrong and Russell 1990</REF>.	On clean inputs, such as the Canadian Hansards, these methods have been very successful at least 96 correct by sentence.	Unfortunately, if the input is noisy due to OCR and/or unknown markup conventions, then these methods tend to break down because the noise can make it difficult to find paragraph boundaries, let alone sentences.	3
In general, the statistical approach does not use existing hand-written bilingual dictionaries, and depends solely upon statistics.	For example, sentence alignment of bilingual texts are performed just by measuring sentence lengths in words or in characters <TREF>Brown et al , 1991</TREF>; <REF>Gale and Church, 1993</REF>, or by statistically estimating word level correspondences <REF>Chen, 1993</REF>; Kay and R<REF>Sscheisen, 1993</REF>.	The statistical approach analyzes unstructured sentences in bilingual texts, and it is claimed that the results are useful enough in real applications such as machine translation and word sense disambiguation.	However, structured bilingual sentences are undoubtedly more informative and important for future natural language researches.	3
<REF>As Chen 1993</REF> points out, dynamic programming is particularly susceptible to deletions occurring in one of the two languages.	Thus, dynamic programming based sentence alignment algorithms rely on paragraph anchors <TREF>Brown et al 1991</TREF> or lexical information, such as cognates <REF>Simard 1992</REF>, to maintain a high accuracy rate.	These methods are not robust with respect to non-literal translations and large deletions <REF>Simard 1996</REF>.	This paper presents a new approach based on image processing IP techniques, which is immune to such predicaments.	3
However, since Chinese text consists of an unsegmented character stream without marked word boundaries, it would not be possible to count the number of words in a sentence without first parsing it.	Although it has been suggested that lengthbased methods are language-independent <REF>Gale  Church 1991</REF>; <TREF>Brown et al 1991</TREF>, they may in fact rely to some extent on length correlations arising from the historical relationships of the languages being aligned.	If translated sentences share cognates, then the character lengths of those cognates are of course correlated.	Grammatical similarities between related languages may also produce correlations in sentence lengths.	3
In other words, the only feature of Lt and L2 that affects their alignment probability is their length.	Note that there are other length-based alignment methods 81 that measure length in number of words instead of characters <TREF>Brown et al 1991</TREF>.	However, since Chinese text consists of an unsegmented character stream without marked word boundaries, it would not be possible to count the number of words in a sentence without first parsing it.	Although it has been suggested that lengthbased methods are language-independent <REF>Gale  Church 1991</REF>; <TREF>Brown et al 1991</TREF>, they may in fact rely to some extent on length correlations arising from the historical relationships of the languages being aligned.	3
Existing lexicon compilation methods <REF>Kupiec 1993</REF>; <REF>Smadja  McKeown 1994</REF>; <REF>Kumano  Hirakawa 1994</REF>; <REF>Dagan et al 1993</REF>; <REF>Wu  Xia 1994</REF> all attempt to extract pairs of words or compounds that are translations of each other from previously sentencealigned, parallel texts.	However, sentence alignment <TREF>Brown et al 1991</TREF>; Kay  R<REF>Sscheisen 1993</REF>; <REF>Gale  <REF>Church 1993</REF></REF>; <REF>Church 1993</REF>; <REF>Chen 1993</REF>; <REF>Wu 1994</REF> is not always practical when corpora have unclear sentence boundaries or with noisy text segments present in only one language.	Our proposed algorithm for bilingual lexicon acquisition bootstraps off of corpus alignment procedures we developed earlier <REF>Fung  Church 1994</REF>; <REF>Fung  McKeown 1994</REF>.	Those procedures attempted to align texts by finding matching word pairs and have demonstrated their effectiveness for Chinese/English and Japanese/English.	3
It can resolve the alignment problem on real bilingual text.	There have been a number of papers on aligning parallel texts at the sentence level in the last century, eg, <TREF>Brown et al 1991</TREF>; <REF>Gale and Church, 1993</REF>; <REF>Simard et al 1992</REF>; <REF>Wu DeKai 1994</REF>.	On clean inputs, such as the Canadian Hansards and the Hong Kang Hansards, these methods have been very successful.	Church, Kenneth W, 1993; <REF>Chen, Stanley, 1993</REF> proposed some methods to resolve the problem in noisy bilingual texts.	2
Therefore, sentence mapping algorithms need not worry about crossing correspondences.	<REF>In 1991</REF>, two teams of researchers independently discovered that sentences can be accurately aligned by matching sequences 310 with similar lengths <REF>Gale  Church, 1991a</REF>; <TREF>Brown et al , 1991</TREF>.	Soon thereafter, <REF>Church 1993</REF> found that bitext mapping at the sentence level is not an option for noisy bitexts found in the real world.	Sentences are often difficult to detect, especially where punctuation is missing due to OCR errors.	3
bank were surrendered by banc.	SENT fair Although there has been some previous work on the sentence alignment, eg, <TREF>Brown, Lai, and Mercer, 1991</TREF>, <REF>Kay and Rtscheisen, 1988</REF>, Catizone et al , to appear, the alignment task remains a significant obstacle preventing many potential users from reaping many of the benefits of bilingual corpora, because the proposed solutions are often unavailable, unreliable, and/or computationally prohibitive.	The align program is based on a very simple statistical model of character lengths.	The model makes use of the fact that longer sentences in one language tend to be translated into longer sentences in the other language, and that shorter sentences tend to be translated into shorter sentences.	3
The result shows that the use of dependency relation helps to acquire interesting translation patterns.	Since the advent of statistical methods in Machine qhanslation, the bilingual sentence alignmerit <TREF>Brown et al , 1991</TREF> or word alignment <REF>Dagan et al , 1992</REF> have been explored and achieved numerous success over the last decade.	In coN;rasl,, fewer resull;s are reported in phraselevel correspondence.	As word sequences are not translated literally a word for a word, acquiring phraseqevel correspondence still remains an important problem to be exploited.	2
To do such kinds of researches, the most impmlant task is to align the bilingual texts.	Many length-based alignment algorithms have been proposed <TREF>Brown et al , 1991</TREF>; <REF>Gale and Church, 1991a</REF>.	The correct rates are good.	However, the languages they processed belong to occidental family.	3
Therefore, we decided to extract NE translation pairs from content-aligned corpora, such as multilingual broadcast news articles including new NEs daily, which are not literally translated.	Sentential alignment <TREF>Brown et al , 1991</TREF>; <REF>Gale and Church, 1993</REF>; <REF>Kay and Roscheisen, 1993</REF>; <REF>Utsuro et al , 1994</REF>; <REF>Haruno and Yamazaki, 1996</REF> is commonly used as a starting point for finding the translations of words or expressions from bilingual corpora.	However, it is not always possible to correspond non-parallel corpora in sentences.	Past statistical methods for non-parallel corpora <REF>Fung and Yee, 1998</REF> are not valid for finding translations of words or expressions with low frequency.	3
Many software products which aid human translators now contain sentence alignment tools as an aid to speeding up editing and terminology searching.	Various methods have been developed for sentence alignment which we can categorise as either lexical such as <REF>Chen, 1993</REF>, based on a large-scale bilingual lexicon; statistical such as <TREF>Brown et al, 1991</TREF> <REF>Church, 1993</REF>Gale and <REF>Church, 1993</REF><REF>Kay and Rhshcheisen, 1993</REF>, based on distributional regularities of words or byte-lengdl ratios and possibly inducing a bilingual lexicon as a by-product, or hybrid such as <REF>Vtsuro et al, 1994</REF> <REF>Wu, 1994</REF>, based on some combination of the other two.	Neither of the pure approaches is entirely satisfactory for the following reasons:  Text volume limits the usefulness of statistical approaches.	We would often like to be able to align small amounts of text, or texts from various domains which do not share the same statistical properties.	3
1.	Motivation There have been quite a number of recent papers on parallel text: Brown et al 1990, 1991, 1993, <REF>Chen 1993</REF>, <REF>Church 1993</REF>, <REF>Church et al 1993</REF>, <REF>Dagan et al 1993</REF>, Gale and Church 1991, 1993, <REF>Isabelle 1992</REF>, <REF>Kay and Rgsenschein 1993</REF>, <REF>Klavans and Tzoukermann 1990</REF>, <REF>Kupiec 1993</REF>, <REF>Matsumoto 1991</REF>, <REF>Ogden and Gonzales 1993</REF>, <REF>Shemtov 1993</REF>, <REF>Simard et al 1992</REF>, <REF>WarwickArmstrong and Russell 1990</REF>, Wu to appear.	Most of this work has been focused on European language pairs, especially English-French.	It remains an open question how well these methods might generalize to other language pairs, especially pairs such as English-Japanese and EnglishChinese.	3
This is compared with the previous rate of about 30 terms per hour using charaligns output, and an extremely lower rate before alignment tools were available.	4 Conclusions Compared with other word alignment algorithms <REF>Brown et al , 1993</REF>; <REF>Gale and Church, 1991a</REF>, wordalign does not require sentence alignment as input, and was shown to produce useful alignments for small and noisy corpora.	Its robustness was achieved by modifying Brown et als Model 2 to handle an initial rough alignment, reducing the number of parameters and introducing a dependency between alignments of adjacent words.	Taking the output of charalign as input, wordalign produces significantly better, word7 Offset from correct alignment 0 1 2 3 4 Percentage 605 108 75 52 16 Accumulative percentage 605 713 788 84 856 Table 2: Wordaligns precision on noisy input, scanned by an OCR device.	2
To deal with these robustness issues, <REF>Church 1993</REF> developed a character-based alignment method called charalign.	The method was intended as a replacement for sentence-based methods eg , <TREF>Brown et al , 1991a</TREF>; <REF>Gale and Church, 1991b</REF>; <REF>Kay and Rosenschein, 1993</REF>, which are very sensitive to noise.	This paper describes a new program, called wordalign, that starts with an initial rough alignment eg , the output of charalign or a sentence-based alignment method, and produces improved alignments by exploiting constraints at the word-level.	The alignment algorithm consists of two steps: 1 estimate translation probabilities, and 2 use these probabilities to search for most probable alignment path.	3
20 The method of acquiring parameters from ambiguous occurrences in a corpus, relying on the spreading of noise, can be used in many contexts.	For example, it was used for acquiring statistics for disambiguating prepositional phrase attachments, counting ambiguous occurrences of prepositional phrases as representing both noun-pp and verb-pp constructs <TREF>Hindle and Rooth 1991</TREF>.	588 Ido Dagan and Alon Itai Word Sense Disambiguation vides a useful source of a sense tagged corpus.	<REF>Gale, Church, and Yarowsky 1992a</REF> have also exploited this resource for achieving large amounts of testing and training materials.	2
The use of such relations mainly relations between verbs or nouns and their arguments and modifiers for various purposes has received growing attention in recent research <REF>Church and Hanks 1990</REF>; <REF>Zernik and Jacobs 1990</REF>; <REF>Hindle 1990</REF>; <REF>Smadja 1993</REF>.	More specifically, two recent works have suggested using statistical data on lexical relations for resolving ambiguity of prepositional phrase attachment <TREF>Hindle and Rooth 1991</TREF> and pronoun references <REF>Dagan and Itai 1990, 1991</REF>.	Clearly, statistics on lexical relations can also be useful for target word selection.	Consider, for example, the Hebrew sentence extracted from the foreign news section of the daily Ha-<REF>Aretz, September 1990</REF> transcripted to Latin letters: 1 Nose ze mana mi-shtei ha-mdinot mi-lahtom al hoze shalom.	2
or the cooccurrence of two words within a limited distance in the context.	Statistical data about these various cooccurrence relations is employed for a variety of applications, such as speech recognition <REF>Jelinek, 1990</REF>, language generation <REF>Smadja and McKeown, 1990</REF>, lexicography <REF>Church and Hanks, 1990</REF>, machine translation Brown et al , ; <REF>Sadler, 1989</REF>, information retrieval <REF>Maarek and Smadja, 1989</REF> and various disambiguation tasks <REF>Dagan et al , 1991</REF>; <TREF>Hindle and Rooth, 1991</TREF>; <REF>Grishman et al , 1986</REF>; <REF>Dagan and Itai, 1990</REF>.	A major problem for the above applications is how to estimate the probability of cooccurrences that were not observed in the training corpus.	Due to data sparseness in unrestricted language, the aggregate probability of such cooccurrences is large and can easily get to 25 or more, even for a very large training corpus <REF>Church and Mercer, 1992</REF>.	3
Some of our initial data suggest that the hypothesis of deep semantic selection may in fact be correct, as well as indicating what the nature of the coercion rules may be.	Using techniques described in <REF>Church and Hindle 1990</REF>, <REF>Church and Hanks 1990</REF>, and <TREF>Hindle and Rooth 1991</TREF>, below are some examples of the most frequent V-O pairs from the AP corpus.	Counts for objects of begin/V: 205 begin/V career/O 176 begin/V day/O 159 begin/V work/O 140 begin/V talk/O 120 begin/V campaign/O 113 begin/V investigation/O 106 begin/V process/O 92 begin/V program/O 8S begin/V operation/O 86 begin/V negotiation/O 66 begin/V strike/O 64 begin/V production/O 59 begin/V meeting/O 89 begin/V term/O 50 begin/V visit/O 45 begin/V test/O 39 begin/V construction/O 31 begin/V debate/O 29 begin/V trial/O Corpus studies confirm similar results for weakly intensional contexts <REF>Pustejovsky 1991</REF> such as the complement of coercive verbs such as veto.	These are interesting because regardless of the noun type appearing as complement, it is embedded within a semantic interpretation of the proposal to, thereby clothing the complement within an intensional context.	2
Some of our initial data suggest that the hypothesis of deep semantic selection may in fact be correct, as well as indicating what the nature of the coercion rules may be.	Using techniques described in <REF>Church and Hindle 1990</REF>, <REF>Church and Hanks 1990</REF>, and <TREF>Hindle and Rooth 1991</TREF>, Figure 4 shows some examples of the most frequent V-O pairs from the AP corpus.	Corpus studies confirm similar results for weakly intensional contexts such as the complement of coercive verbs such as veto.	These are interesting because regardless of the noun type appearing as complement, it is embedded within a semantic interpretation of the proposal to, thereby clothing the complement within an intensional context.	2
There has not been a general method proposed to date, however, that learns dependencies between case slots.	Methods of resolving ambiguities have been based, for example, on the assumption that case slots are mutually independent <TREF>Hindle and Rooth 1991</TREF>, or at most two case slots are dependent <REF>Collins and Brooks 1995</REF>.	In this article, we propose an efficient and general method of learning dependencies between case frame slots.	2.	2
We use the definition of lexical likelihood described above to avoid this problem.	4 32 The data sparseness problem Hindle  Rooth have previously proposed resolving pp-attachment ambiguities with two-word probabilities <TREF>Hindle and Rooth, 1991</TREF>, eg, Pwithlicecream,Pwithleat, but these are not accurate enough to represent lexical preference.	For example, in the sentences, Britain reopened the embassy in December, Britain reopened the embassy in Teheran, 10 the pp-attachment sites of the two prepositional phrases are different.	The attachment sites would be determined to be the same, however, if we were to use two-word probabilities c:f<REF>Resnik, 1993</REF>, and thus the ambiguity of only one of the sentences can be resolved.	3
There have been a number of methods proposed to perform structural disambiguation using probability models, many of which have proved to be quite effective <REF>Alshawi and Carter, 1995</REF>; <REF>Black et al , 1992</REF>; Briscoe and Carroll.	1993; <REF>Chang et al , 1992</REF>; <REF>Collins and Brooks, 1995</REF>; <REF>Fujisaki, 1989</REF>; <TREF>Hindle and Rooth, 1991</TREF>; <REF>Hindle and Rooth, 1993</REF>; <REF>Jelinek et al , 1990</REF>; <REF>Magerman and Marcus, 1991</REF>; <REF>Magerman, 1995</REF>; <REF>Ratnaparkhi et al , 1994</REF>; <REF>Resnik, 1993</REF>; <REF>Su and Chang, 1988</REF>.	Although each of the disambiguation methods proposed to date has its merits, none resolves the disambiguation problem completely satisfactorily.	We feel that it is necessary to devise a new method that unifies the above two approaches, ie, to implement psycholinguistic principles of disambiguation on the basis of a probabilistic methodology.	3
Thus our problem involves the following three subproblems: a resolving structural ambiguities based on LPR in terms of probabilistic representations, b resolving structural ambiguities based on RAP and ALPP in terms of probabilistic representations, and c combining the two.	For subproblem a, we have devised a new method, based on LPR, which has some good properties not shared by the methods proposed so far <REF>Alshawi and Carter, 1995</REF>; <REF>Chang et al , 1992</REF>; <REF>Collins and Brooks, 1995</REF>; <TREF>Hindle and Rooth, 1991</TREF>; <REF>Ratnaparkhi et al , 1994</REF>; <REF>Resnik, 1993</REF>.	In <REF>Li and Abe, 1995</REF>, we have described this method in detail.	In the present paper, we mainly describe our solutions to subproblems b and c.	3
We expect that the knowledge to be extracted will not only be useful for disambiguating sentences but also will contribute to discovering ontological classes in given subject domains.	Though several studies with similar objectives have been reported <REF>Church, 1988</REF>, <REF>Zernik and Jacobs, 1990</REF>, <REF>Calzolari and Bindi, 1990</REF>, <REF>Garside and Leech, 1985</REF>, <TREF>Hindle and Rooth, 1991</TREF>, <REF>Brown et al , 1990</REF>, they require that sample corpora be correctly analyzed or tagged in advance.	It must be a training corpus, which is tagged or parsed by human or it needs correspondence between two language corpora.	Because their preparation needs a lot of manual assistance or an unerring tagger or parser, this requirement makes their algorithm, troublesome in actual application environments.	3
In general, these words will not be adjacent in the text, so it will not be possible to use existing approaches unmodified eg <REF>Church and Hanks 1989</REF>, because these apply to adjacent words in unanalyzed text.	<TREF>Hindle and Rooth 1991</TREF> report good results using a mutual information measure of collocation applied within such a structurally defined context, and their approach should carry over to our framework straightforwardly.	One way of integrating structural collocational information into the system presented above would be to make use of the semantic component of the ANLT grammar This component pairs logical forms with each distinct syntactic analysis that represent, among other things, the predicate-argument structure of the input.	In the resolution of PP attachment and similar ambiguities, it is collocation at this level of representation that appears to be most relevant.	2
2 Acquiring syntactic associations Clustered association data are collected by first extracting from the corpus all the syntactically related word pairs.	Combining statistical and parsing methods has been done by <REF>Hindle, 1990</REF>; Hindle and Rooths,1991 and <REF>Smadja and McKewon, 1990</REF>; Smadja,1991.	The novel aspect of our study is that we collect not only operational pairs, but triples, such as Nprep N, VprepN etc In fact, the preposition convey important information on the nature of the semantic link between syntactically related content words.	By looking at the preposition, it is possible to restrict the set of semantic relations underlying a syntactic relation eg forpurpose,beneficiary.	2
Much of the overhead and inefficiency comes from the fact that the lexical and structural ambiguity of natmal language input can only be dealt with using limited context information available to the parser.	Partial parsing techniques have been used with a considerable success in processing large volumes of text, for example ATTs Fidditch <TREF>Hindle and Rooth, 1991</TREF> parsed 13 million words of Associated Press news messages, while MITs parser de <REF>Marcken, 1990</REF> was used to process the 1 million word Lancaster/Oslo/Bergen LOB corpus.	In both cases, the parsers were designed to do partial processing only, that is, they would never attempt a complete analysis of certain constructions, such as the attachment of pp-adjuncts, subordinate clauses, or coordinations.	This kind of partial analysis may be sufficient in some applications because of a relatively high precision of identifying correct syntactic dependencies.	2
FUTURE DIRECTIONS This paper presented one method of learning subcategorizations, but there are other approaches one might try.	For disambiguating whether a PP is subcategorized by a verb in the V NP PP environment, <TREF>Hindle and Rooth 1991</TREF> used a t-score to determine whether the PP has a stronger association with the verb or the preceding NP.	This method could be usefully incorporated into my parser, but it remains a special-purpose technique for one particular ease.	Another research direction would be making the parser stochastic as well, rather than it being a categorical finite state device that runs on the output of a stochastic tagger.	2
The generalization step is needed in order to represent the input case frame instances more compactly as well as to judge the degree of acceptability of unseen case frame instances.	For the extraction problem, there have been various methods proposed to date, which are quite adequate <TREF>Hindle and Rooth 1991</TREF>; <REF>Grishman and Sterling 1992</REF>; <REF>Manning 1992</REF>; <REF>Utsuro, Matsumoto, and Nagao 1992</REF>; <REF>Brent 1993</REF>; <REF>Smadja 1993</REF>; <REF>Grefenstette 1994</REF>; <REF>Briscoe and Carroll 1997</REF>.	The generalization problem, in contrast, is a more challenging one and has not been solved completely.	A number of methods for generalizing values of a case frame slot for a verb have been  CC Media Res.	2
236 Li and Abe Generalizing Case Frames Table 13 Results of PP-attachment disambiguation.	Coverage Accuracy Default 100 562 MDL  Default 100 822 SA  Default 100 767 LA  Default 100 807 LAt  Default 100 781 TEL 100 824 We also implemented the exact method proposed by <TREF>Hindle and Rooth 1991</TREF>, which makes disambiguation judgement using the t-score.	Figure 10 shows the result as LAt, where the threshold for t-score is set to 128 significance level of 90 percent.	From Figure 10 we see that with respect to accuracy-coverage curves, MDL outperforms both SA and LA throughout, while SA is better than LA Next, we tested the method of applying a default rule after applying each method.	2
We estimate Pnoun2 I verb, prep and Pnoun2 I nount, prep from training data consisting of triples, and compare them: If the former exceeds the latter by a certain margin we attach it to verb, else if the latter exceeds the former by the same margin we attach it to noun1.	In our experiments, described below, we compare the performance of our proposed method, which we refer to as MDL, against the methods proposed by <TREF>Hindle and Rooth 1991</TREF>, <REF>Resnik 1993b</REF>, and <REF>Brill and Resnik 1994</REF>, referred to respectively as LA, SA, and TEL.	Data Set.	We used the bracketed corpus of the Penn Treebank Wall Street Journal corpus <REF>Marcus, Santorini, and Marcinkiewicz 1993</REF> as our data.	3
Sense disambiguation thus resembles many other decision tasks, and not surprisingly, several common decision algorithms were employed in different works.	These include a Bayesian classifier <REF>Gale, Church, and Yarowsky 1993</REF> and a distance 589 Computational Linguistics Volume 20, Number 4 metric between vectors <REF>Schiitze 1993</REF>, both inspired from methods in information retrieval; the use of the flip-flop algorithm for ordering possible informants about the preferred sense, trying to maximize the mutual information between the informant and the ambiguous word <TREF>Brown et al 1991</TREF>; and the use of confidence intervals to establish the degree of confidence in a certain preference, combined with a constraint propagation algorithm the current paper.	At the present stage of research on sense disambiguation, it is difficult to judge whether a certain decision algorithm is significantly superior to others.	21 Yet, these decision models can be characterized by several criteria, which clarify the similarities and differences between them.	2
It seems, however, that Brown et al expect that target word selection would be determined mainly by translation probabilities the second factor in the above term, which should be derived from a bilingual corpus <REF>Brown et al 1990</REF>, p 79.	This view is reflected also in their elaborate method for target word selection <TREF>Brown et al 1991</TREF>, in which better estimates of translation probabilities are achieved as a result of word sense disambiguation.	Our method, on the other hand, incorporates only 592 Ido Dagan and Alon Itai Word Sense Disambiguation target language probabilities and ignores any notion of translation probabilities.	It thus demonstrates a possible trade-off between these two types of probabilities: using more informative statistics of the target language may compensate for the lack of translation probabilities.	3
Model parameters are automatically estimated using a corpus of translation pairs.	TMs have been used for statistical machine translation <REF>Berger et al , 1996</REF>, word alignment of a translation corpus <REF>Melamed, 2000</REF>, multilingual document retrieval <REF>Franz et al , 1999</REF>, automatic dictionary construction <REF>Resnik and Melamed, 1997</REF>, and data preparation for word sense disambiguation programs <TREF>Brown et al , 1991</TREF>.	Developing a better TM is a fundamental issue for those applications.	Researchers at IBM first described such a statistical TM in <REF>Brown et al , 1988</REF>.	3
A third difference concerns the granularity of WSD attempted, which one can illustrate in terms of the two levels of semantic distinctions found in many dictionaries: homograph and sense see Section 31.	Like <REF>Cowie, Guthrie, and Guthrie 1992</REF>, we shall give results at both levels, but it is worth pointing out that the targets of, say, work using translation equivalents eg , <TREF>Brown et al 1991</TREF>; <REF>Gale, Church, and <REF>Yarowsky 1992</REF>c</REF>; and see Section 23 and Roget categories <REF>Yarowsky 1992</REF>; <REF>Masterman 1957</REF> correspond broadly to the wider, homograph, distinctions.	In this paper we attempt to show that the high level of results more typical of systems trained on many instances of a restricted vocabulary can also be obtained by large vocabulary systems, and that the best results are to be obtained from an optimization of a combination of types of lexical knowledge see Section 2.	11 Lexical Knowledge and WSD Syntactic, semantic, and pragmatic information are all potentially useful for WSD, as can be demonstrated by considering the following sentences: 1 2 3 4 John did not feel well.	2
Using the definitionbased conceptual co-occurrence data collected from the relatively small Brown corpus, our sense disambiguation system achieves an average accuracy comparable to human performance given the same contextual information.	Previous corpus-based sense disambiguation methods require substantial amounts of sense-tagged training data <REF>Kelly and Stone, 1975</REF>; <REF>Black, 1988</REF> and <REF>Hearst, 1991</REF> or aligned bilingual corpora <TREF>Brown et al , 1991</TREF>; <REF>Dagan, 1991</REF> and <REF>Gale et al 1992</REF>.	<REF>Yarowsky 1992</REF> introduces a thesaurus-based approach to statistical sense disambiguation which works on monolingual corpora without the need for sense-tagged training data.	By collecting statistical data of word occurrences in the context of different thesaurus categories from a relatively large corpus 10 million words, the system can identify salient words for each category.	3
Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in learning procedure with the requirement of predefined sense inventory for target words.	They roughly fall into three categories according to what is used for supervision in learning process: 1 using external resources, eg, thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, <REF>Lesk, 1986</REF>; <REF>Lin, 1997</REF>; <REF>McCarthy et al , 2004</REF>; <REF>Seo et al , 2004</REF>; <REF>Yarowsky, 1992</REF>, 2 exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora eg parallel corpora or untagged monolingual corpora in two languages <TREF>Brown et al , 1991</TREF>; <REF>Dagan and Itai, 1994</REF>; <REF>Diab and Resnik, 2002</REF>; <REF>Li and Li, 2004</REF>; <REF>Ng et al , 2003</REF>, 3 bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data <REF>Hearst, 1991</REF>; <REF>Karov and Edelman, 1998</REF>; <REF>Mihalcea, 2004</REF>; <REF>Park et al , 2000</REF>; <REF>Yarowsky, 1995</REF>.	As a commonly used semi-supervised learning method for WSD, bootstrapping algorithm works by iteratively classifying unlabeled examples and adding confidently classified examples into labeled dataset using a model learned from augmented labeled dataset in previous iteration.	It can be found that the affinity information among unlabeled examples is not fully explored in this bootstrapping process.	2
Later work from the AI community relied heavily upon selectional restrictions for verbs, although primarily in terms of features exhibited by their arguments such as DRINKABLE rather than in terms of individual words or word classes.	More recent work <TREF>Brown et al 1991</TREF><REF>Hearst 1991</REF> has utilized a set of discrete local questions such as word-to-the-right in the development of statistical decision procedures.	However, a strong trend in recent years is to treat a reasonably wide context window as an unordered bag of independent evidence points.	This technique from information retrieval has been used in neural networks, Bayesian discriminators, and dictionary definition matching.	3
Hence, we use a slightly different framework.	We view a bilingual corpus as a sequence of sentence beads <TREF>Brown et al , 1991b</TREF>, where a sentence bead corresponds to an irreducible group of sentences that align with each other.	For example, the correct alignment of the bilingual corpus in Figure 2 consists of the sentence bead El; F1 followed by the sentence bead E2; ;2, F3.	We can represent an alignment 4 of a corpus as a sequence of sentence beads Epl; Fpl, Ep2; F,, where the E and F can be zero, one, or more sentences long.	2
In this way, we can ignore the context of the collocations and their translations, and base our decisions only on the patterns of co-occurrence of each collocation and its candidate translations across the entire corpus.	This approach is quite different from those adopted for the translation of single words <REF>Klavans and Tzoukermann 1990</REF>; <REF>Dorr 1992</REF>; <REF>Klavans and Tzoukermann 1996</REF>, since for single words polysemy cannot be ignored; indeed, the problem of sense disambiguation has been linked to the problem of translating ambiguous words <TREF>Brown et al 1991</TREF>; <REF>Dagan, Itai, and Schwall 1991</REF>; <REF>Dagan and Itai 1994</REF>.	The assumption of a single meaning per collocation was based on our previous experience with English collocations <REF>Smadja 1993</REF>, is supported for less opaque collocations by the fact that their constituent words tend to have a single sense when they appear in the collocation <REF>Yarowsky 1993</REF>, and was verified during our evaluation of Champollion Section 7.	We construct a mathematical model of the events we want to correlate, namely, the appearance of any word or group of words in the sentences of our corpus, as follows: To each group of words G, in either the source or the target language, we map a binary random variable Xc that takes the value 1 if G appears in a particular sentence and 0 if not.	2
If sense labeling is part of the task, an outside source of knowledge is necessary to define the senses.	Regardless of whether it takes the form of dictionaries <REF>Lesk 1986</REF>; <REF>Guthrie et al 1991</REF>; <REF>Dagan, Itai, and Schwall 1991</REF>; <REF>Karov and Edelman 1996</REF>, thesauri <REF>Yarowsky 1992</REF>; <REF>Walker and Amsler 1986</REF>, bilingual corpora <TREF>Brown et al 1991</TREF>; <REF>Church and Gale 1991</REF>, or hand-labeled training sets <REF>Hearst 1991</REF>; <REF>Leacock, Towell, and Voorhees 1993</REF>; <REF>Niwa and Nitta 1994</REF>; <REF>Bruce and Wiebe 1994</REF>, providing information for sense definitions can be a considerable burden.	What makes our approach unique is that, since we narrow the problem to sense discrimination, we can dispense of an outside source of knowledge for defining senses.	 Xerox Palo Alto Research Center, 3333 Coyote Hill Road, Palo Alto, CA 94304 Q 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 1 We therefore call our approach automatic word sense discrimination, since we do not require manually constructed sources of knowledge.	3
It thrives on raw, unannotated monolingual corpora the more the merrier.	Although there is some hope from using aligned bilingual corpora as training data for supervised algorithms <TREF>Brown et al , 1991</TREF>, this approach suffers from both the limited availability of such corpora, and the frequent failure of bilingual translation differences to model monolingual sense differences.	The use of dictionary definitions as an optional seed for the unsupervised algorithm stems from a long history of dictionary-based approaches, including <REF>Lesk 1986</REF>, Guthrie et al.	1991, <REF>Veronis and Ide 1990</REF>, and <REF>Slator 1991</REF>.	3
It is also crucial in cross-language text processing including cross-language information retrieval and abstraction.	Due to the recent availability of large text corpora, various statistical approaches have been tried including using 1 parallel corpora <REF>Brown et al , 1990</REF>, <TREF>Brown et al , 1991</TREF>, <REF>Brown, 1997</REF>, 2 non-parallel bilingual corpora tagged with topic area <REF>Yamabana et al , 1998</REF> and 3 un-tagged mono-language corpora in the target language <REF>Dagan and Itai, 1994</REF>, <REF>Tanaka and Iwasaki, 1996</REF>, <REF>Kikui, 1998</REF>.	A problem with the first two approaches is that it is not easy to obtain sufficiently large parallel or manually tagged corpora for the pair of languages targeted.	Although the third approach eases the problem of preparing corpora, it suffers from a lack of useful information in the source language.	3
Although this requirement may seem trivial, most corpus-based methods do not, in fact, allow such flexibility.	For example, defining the senses by the possible translations of the word <REF>Dagan, Itai and Schwall 1991</REF>; <TREF>Brown et al 1991</TREF>; <REF>Gale, Church, and <REF>Yarowsky 1992</REF></REF>, by the Rogets categories <REF>Yarowsky 1992</REF>, or by clustering Schitze 1992, yields a grouping that does not always conform to the desired sense distinctions.	In comparison to these approaches, our reliance on the MRD for the definition of senses in the initialization of the learning process guarantees the required flexibility in setting the sense distinctions.	Specifically, the user of our system may choose a certain dictionary definition, a combination of definitions from several dictionaries, or manually listed seed words for every sense that needs to be defined.	3
Traditionally, the problem of sparse data is approached by estimating the probability of unobserved co-occurrences using the actual co-occurrences in the training set.	This can be done by smoothing the observed frequencies 7 <REF>Church and Mercer 1993</REF> or by class-based methods <TREF>Brown et al 1991</TREF>; <REF>Pereira and Tishby 1992</REF>; <REF>Pereira, Tishby, and Lee 1993</REF>; <REF>Hirschman 1986</REF>; <REF>Resnik 1992</REF>; <REF>Brill et al 1990</REF>; <REF>Dagan, Marcus, and Markovitch 1993</REF>.	In comparison to these approaches, we use similarity information throughout training, and not merely for estimating co-occurrence statistics.	This allows the system to learn successfully from very sparse data.	3
Traditionally, the problem of sparse data is approached by estimating the probability of unobserved cooccurrences using the actual cooccurrences in the training set.	This can be done by smoothing the observed frequencies <REF>Church and Mercer, 1993</REF>, or by class-based methods <TREF>Brown et al , 1991</TREF>; <REF>Pereira and Tishby, 1992</REF>; Pereira et ah, 1993; <REF>Hirschman, 1986</REF>; <REF>Resnik, 1992</REF>; Brill et ah, 1990; <REF>Dagan et al , 1993</REF>.	In comparison to these approaches, we use similarity information throughout training, and not merely for estimating cooccurrence statistics.	This allows the system to learn successfully from very sparse data.	3
The aim of this measure is to indicate the relatedness between two elements composing a pair.	Mutual information has been positively used in many NLP tasks such as collocation analysis <REF>Church and Hanks, 1989</REF>, terminology extraction <REF>Damerau, 1993</REF>, and word sense disambiguation <TREF>Brown et al , 1991</TREF>.	3 Experimental Evaluation As many other corpus linguistic approaches, our entailment detection model relies partially on some linguistic prior knowledge the expected structure of the searched collocations, ie, the textual entailment patterns and partially on some probability distribution estimation.	Only a positive combination of both these two ingredients can give good results when applying and evaluating the model.	3
Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in the learning procedure with the need of predened sense inventories for target words.	The information for semi-supervised sense disambiguation is usually obtained from bilingual corpora eg parallel corpora or untagged monolingual corpora in two languages <TREF>Brown et al , 1991</TREF>; <REF>Dagan and Itai, 1994</REF>, or sense-tagged seed examples <REF>Yarowsky, 1995</REF>.	Some observations can be made on the previous supervised and semi-supervised methods.	They always rely on hand-crafted lexicons eg , WordNet as sense inventories.	2
By repeating the above processes, it can create an accurate classifier for word translation disambiguation.	For other related work, see, for example, <TREF>Brown et al 1991</TREF>; <REF>Dagan and Itai 1994</REF>; <REF>Pedersen and Bruce 1997</REF>; <REF>Schutze 1998</REF>; <REF>Kikui 1999</REF>; <REF>Mihalcea and Moldovan 1999</REF>.	3 Bilingual Bootstrapping 31 Overview Instead of using Monolingual Bootstrapping, we propose a new method for word translation disambiguation using Bilingual Bootstrapping.	In translation from English to Chinese, for instance, BB makes use of not only unclassified data in English, but also unclassified data in Chinese.	2
To deal with these robustness issues, <REF>Church 1993</REF> developed a character-based alignment method called charalign.	The method was intended as a replacement for sentence-based methods eg , <TREF>Brown et al , 1991a</TREF>; <REF>Gale and Church, 1991b</REF>; <REF>Kay and Rosenschein, 1993</REF>, which are very sensitive to noise.	This paper describes a new program, called wordalign, that starts with an initial rough alignment eg , the output of charalign or a sentence-based alignment method, and produces improved alignments by exploiting constraints at the word-level.	The alignment algorithm consists of two steps: 1 estimate translation probabilities, and 2 use these probabilities to search for most probable alignment path.	2
More importantly, because wordalign and charalign were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at ATT Language Line Services, a commercial translation service, to help them with difficult terminology.	Aligning parallel texts has recently received considerable attention <REF>Warwick et al , 1990</REF>; <TREF>Brown et al , 1991a</TREF>; <REF>Gale and Church, 1991b</REF>; <REF>Gale and Church, 1991a</REF>; <REF>Kay and Rosenschein, 1993</REF>; <REF>Simard et al , 1992</REF>; <REF>Church, 1993</REF>; <REF>Kupiec, 1993</REF>; <REF>Matsumoto et al , 1993</REF>.	These methods have been used in machine translation <REF>Brown et al , 1990</REF>; <REF>Sadler, 1989</REF>, terminology research and translation aids <REF>Isabelle, 1992</REF>; <REF>Ogden and Gonzales, 1993</REF>, bilingual lexicography <REF>Klavans and Tzoukermann, 1990</REF>, collocation studies <REF>Smadja, 1992</REF>, word-sense disambiguation <TREF>Brown et al , 1991b</TREF>; <REF>Gale et al , 1992</REF> and information retrieval in a multilingual environment <REF>Landauer and Littman, 1990</REF>.	The information retrieval application may be of particular relevance to this audience.	2
Aligning parallel texts has recently received considerable attention <REF>Warwick et al , 1990</REF>; <TREF>Brown et al , 1991a</TREF>; <REF>Gale and Church, 1991b</REF>; <REF>Gale and Church, 1991a</REF>; <REF>Kay and Rosenschein, 1993</REF>; <REF>Simard et al , 1992</REF>; <REF>Church, 1993</REF>; <REF>Kupiec, 1993</REF>; <REF>Matsumoto et al , 1993</REF>.	These methods have been used in machine translation <REF>Brown et al , 1990</REF>; <REF>Sadler, 1989</REF>, terminology research and translation aids <REF>Isabelle, 1992</REF>; <REF>Ogden and Gonzales, 1993</REF>, bilingual lexicography <REF>Klavans and Tzoukermann, 1990</REF>, collocation studies <REF>Smadja, 1992</REF>, word-sense disambiguation <TREF>Brown et al , 1991b</TREF>; <REF>Gale et al , 1992</REF> and information retrieval in a multilingual environment <REF>Landauer and Littman, 1990</REF>.	The information retrieval application may be of particular relevance to this audience.	It would be highly desirable for users to be able to express queries in whatever language they chose and retrieve documents that may or may not have been written in the same language as the query.	2
Similar results are reported in <REF>Nakatani, Hirschberg, and Grosz 1995</REF> and <REF>Hirschberg and Nakatani 1996</REF> for spontaneous speech as well.	<REF>Grosz and Hirschberg 1992</REF> also use the classification and regression tree system CART <REF>Brieman et al 1984</REF> to automatically construct and evaluate decision trees for classifying aspects of discourse structure from intonational feature values.	The studies of <REF>Swerts 1995</REF> and <REF>Swerts and Ostendorf 1995</REF> also investigate the prosodic structuring of discourse.	<REF>In Swerts 1995</REF>, paragraph boundaries are empirically obtained as described above.	2
SEMCOR contains 91,808 words tagged with WordNet synsets, 6,071 of which are proper names, which we ignored, leaving 85,737 words which could potentially be translated.	The translation contains only 36,869 words tagged with LDOCE senses; however, this is a reasonable size for an evaluation corpus for the task, and it is several orders of magnitude larger than those used by other researchers working in large vocabulary WSD, for example <REF>Cowie, Guthrie, and Guthrie 1992</REF>, <REF>Harley and Glennon 1997</REF>, and Mahesh et al.	1997.	This corpus was also constructed without the excessive cost of additional hand-tagging and does not introduce any of the inconsistencies that can occur with a poorly controlled tagging strategy.	2
Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction <REF>Smadja, 1993</REF>; <REF>Fung and Wu, 1994</REF>, phrasal translation <REF>Smadja et al , 1996</REF>; <REF>Kupiec, 1993</REF>; <REF>Wu, 1995</REF>; <REF>Dagan and Church, 1994</REF>, target word selection <REF>Liu and Li, 1997</REF>; <REF>Tanaka and Iwasaki, 1996</REF>, domain word translation <REF>Fung and Lo, 1998</REF>; <REF>Fung, 1998</REF>, sense disambiguation <REF>Brown et al , 1991</REF>; <REF>Dagan et al , 1991</REF>; <REF>Dagan and Itai, 1994</REF>; <TREF>Gale et al , 1992a</TREF>; <TREF>Gale et al , 1992b</TREF>; <TREF>Gale et al , 1992c</TREF>; <REF>Shiitze, 1992</REF>; <REF>Gale et al , 1993</REF>; <REF>Yarowsky, 1995</REF>, and even recently for query translation in cross-language IR as well <REF>Ballesteros and Croft, 1998</REF>.	Co-occurrence statistics is collected from either bilingual parallel and 334 non-parallel corpora <REF>Smadja et al , 1996</REF>; <REF>Kupiec, 1993</REF>; <REF>Wu, 1995</REF>; <REF>Tanaka and Iwasaki, 1996</REF>; <REF>Fung and Lo, 1998</REF>, or monolingual corpora <REF>Smadja, 1993</REF>; <REF>Fung and Wu, 1994</REF>; <REF>Liu and Li, 1997</REF>; <REF>Shiitze, 1992</REF>; <REF>Yarowsky, 1995</REF>.	As we noted in <REF>Fung and Lo, 1998</REF>; <REF>Fung, 1998</REF>, parallel corpora are rare in most domains.	We want to devise a method that uses only monolingual data in the primary language to train co-occurrence information.	2
pruning translation alternatives for query translation.	Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction <REF>Smadja, 1993</REF>; <REF>Fung and Wu, 1994</REF>, phrasal translation <REF>Smadja et al , 1996</REF>; <REF>Kupiec, 1993</REF>; <REF>Wu, 1995</REF>; <REF>Dagan and Church, 1994</REF>, target word selection <REF>Liu and Li, 1997</REF>; <REF>Tanaka and Iwasaki, 1996</REF>, domain word translation <REF>Fung and Lo, 1998</REF>; <REF>Fung, 1998</REF>, sense disambiguation <REF>Brown et al , 1991</REF>; <REF>Dagan et al , 1991</REF>; <REF>Dagan and Itai, 1994</REF>; <TREF>Gale et al , 1992a</TREF>; <TREF>Gale et al , 1992b</TREF>; <TREF>Gale et al , 1992c</TREF>; <REF>Shiitze, 1992</REF>; <REF>Gale et al , 1993</REF>; <REF>Yarowsky, 1995</REF>, and even recently for query translation in cross-language IR as well <REF>Ballesteros and Croft, 1998</REF>.	Co-occurrence statistics is collected from either bilingual parallel and 334 non-parallel corpora <REF>Smadja et al , 1996</REF>; <REF>Kupiec, 1993</REF>; <REF>Wu, 1995</REF>; <REF>Tanaka and Iwasaki, 1996</REF>; <REF>Fung and Lo, 1998</REF>, or monolingual corpora <REF>Smadja, 1993</REF>; <REF>Fung and Wu, 1994</REF>; <REF>Liu and Li, 1997</REF>; <REF>Shiitze, 1992</REF>; <REF>Yarowsky, 1995</REF>.	As we noted in <REF>Fung and Lo, 1998</REF>; <REF>Fung, 1998</REF>, parallel corpora are rare in most domains.	2
In this paper we use many examples from the RSD.	21 Morphology The morphologic analyzer <REF>Marziali, 1992</REF> derives from the work on a generative approach to the Italian morphology <REF>Russo, 1987</REF>, first used in DANTE, a NLP system for analysis of short narrative texts in the financial domain <REF>Antonacci et al 1989</REF>.	Tile analyzer includes over 7000 elementary lemmata stems without affixes, eg flex is the elementary lemma for de448 flex, in-flex, re-fiex anti has been experimented since now on economic, financial, commercial and legal domains.	Elementary lemmata cover much more than 700 words, since many words have an affix.	2
A second problem with the Fidditch parser is poor performances: tilt recall and precision at detecting word collocations are declared to be as low as 50, I-iowever it is unclear if this value applies only to SVO triples, and how it has been derived.	The recall is low because tile Fidditch parser, as other partial parsers <REF>Sekine et al, 1992</REF>; Resnik and Hearst, i993, only detect links between adjacent or near-adjacent words.	Thougll a 50/,, precision and recall might be 447 reasonable for human assisted tasks, like in lexicography, supervised translation, etc , it is not fair enough if collocational analysis must serve a fully automated system.	In fact, corpus linguistics became a popular research field because of the claim that shallow techniques could overcome the lexical coverage bottleneck of traditional NLP techniques.	3
The parser The SSA syntactic analysis is a rewriting procedure of a single sentence into a set of 1 meme-yy ijgjin esl.	The SSA is based on a discontinuous grammar, described more formally in <REF>Basili et al 1992a</REF>.	In tiffs section we provide a qualitative clescription of the rules by which esls are generated.	Examples of esls generated by the parser are: NV the subject-verb relation, V N the direct objectverb relation, N P N noun preposition noun, V P N verb preposition noun, NAdj adjective noun, N N conqound etc Overall, we identify over 20 different esls.	2
In fact, corpus linguistics became a popular research field because of the claim that shallow techniques could overcome the lexical coverage bottleneck of traditional NLP techniques.	Among the applications of collocational analysis for lexical acquisition are: the derivation of syntactic disambiguation cues <REF>Basili et al 1991, 1993a</REF>; <REF>Hindle and Rooths 1991</REF>,1993; <REF>Sekine 1992</REF> <REF>Bogges et al 1992</REF>, sense preference <REF>Yarowski 1992</REF>, acquisition of selectional restrictions <REF>Basili et al 1992b, 1993b</REF>; <REF>Utsuro et al 1993</REF>, lexical preference in generation <REF>Smadjia 1991</REF>, word clustering <REF>Pereira 1993</REF>; <REF>Hindle 1990</REF>; <REF>Basili et al 1993c</REF>, etc In the majority of these papers, even though the precedent or subsequent statistical processing reduces the number of accidental associations, very large corpora 10,000,000 words are necessary to obtain reliable data on a large enough number of words.	In addition, most papers produce a performance evaluation of their methods but do not provide a measure of the coverage, ie the percentage of cases for which their method actually provides a right or wrong solution.	It is quite common that results are discussed only for 10-20 cases.	3
7 SUBJECTS a, b, c, d, e, f, g 161 I0 Hei falls over, Figure 2: Portion of Segntentation from Narrative 6 Subject Annotation of Narratrs httention Digression to describe suml track No verbal communication ie , speaker describes lack thereof Describes that it is a silent movie with only nature sounds Speaker describes sound techniques used in tnovie Explain that there is no speaking ill nlovie Figure 3: Segment spanning 1,12 through 154 3 Discourse Segment Boundaries In <REF>Passonneau and Litman, 1993</REF>, we show that our subjects agree with one another at levels that are statistically significant, thus demonstrating the reliability of intention as a segmentation criterion.	Percent agreement is defined in <TREF>Gale et al , 1992</TREF> as the ratio of observed agreements with the majority opinion to possible agreements with the majority opinion.	We use percent agreement to measure the ability of subjects to agree with one anotlter on whether there is  segment boundary between two adjacent prosodic phrases.	We find that the average agreement across the 20 narratives on the status of all potential boundary locations is 89 with a range from 82-92.	2
An artificial ambiguous word can be coined with the monosemous words in table 1.	This process is similar to the use of general pseudowords <TREF>Gale et al , 1992b</TREF>; <REF>Gaustad, 2001</REF>; <REF>Nakov and Hearst, 2003</REF>, but has some essential differences.	This artificial ambiguous word need to simulate the function of the real ambiguous word, and to acquire semantic knowledge as the real ambiguous word does.	Thus, we call it an equivalent pseudoword EP for its equivalence with the real ambiguous word.	2
That is, LEXAS is only concerned with disambiguating senses of a word in a given POS.	Making such an assumption is reasonable since POS taggers that can achieve accuracy of 96 are readily available to assign POS to unrestricted English sentences <REF>Brill, 1992</REF>; <REF>Cutting et al , 1992</REF>.	In addition, sense definitions are only available for root words in a dictionary.	These are words that are not morphologically inflected, such as interest as opposed to the plural form interests, fall as opposed to the other inflected forms like fell, fallen, falling, falls, etc The sense of a morphologically inflected content word is the sense of its uninflected form.	2
9 Related work Using vector space model and similarity measures for ranking is a common approach in IR for query/text and text/text comparisons <REF>Salton and Buckley, 1988</REF>; <REF>Salton and Yang, 1973</REF>; <REF>Croft, 1984</REF>; <REF>Turtle and Croft, 1992</REF>; <REF>Bookstein, 1983</REF>; <REF>Korfhage, 1995</REF>; <REF>Jones, 1979</REF>.	This approach has also been used by <REF>Dagan and Itai, 1994</REF>; <TREF>Gale et al , 1992</TREF>; <REF>Shiitze, 1992</REF>; <REF>Gale et al , 1993</REF>; <REF>Yarowsky, 1995</REF>; Gale and Church, 1Lunar is not an unknown word in English, Yeltsin finds its translation in the 4-th candidate.	Table 5: tion out score 0008421 0007895 0007669 0007588 0007283 0006812 0006430 0006218 0005921 0005527 0005335 0005335 0005221 0004731 0004470 0004275 0003878 0003859 0003859 0003784 0003686 0003550 0003519 0003481 0003407 0003407 0003338 0003324 Some Chinese ut English Teng-hui SAR flu Lei poultry SAR hijack poultry Tung Diaoyu PrimeMinister President China Lien poultry China flu PrimeMinister President poultry Kalkanov poultry SAR Zhuhai PrimeMinister President flu apologise unknown word translaChinese  Weng-hui  u Lei j Poultry  Chee-hwa  Teng-hui  SAR  Chee-hwa : Teng-hui  Weng-hui W Weng-hui CLam  Teng-hui - Chee-hwa  Teng-hui Lei  Chee-hwa  Chee-hwa  Leung  Zhuhai I Lei J Yeltsin - Chee-hwa  Lam Lam j Poultry W Teng-hui 0003250 DPP 0003206 Tang 0003202 Tung 0003040 Leung 0003033 China 0002888 Zhuhai 0002886 Tung  Teng-hui Tang Leung Leung  SAR  Lunar Tung 1994 for sense disambiguation between multiple usages of the same word.	Some of the early statistical terminology translation methods are <REF>Brown et al , 1993</REF>; <REF>Wu and Xia, 1994</REF>; <REF>Dagan and Church, 1994</REF>; <REF>Gale and Church, 1991</REF>; <REF>Kupiec, 1993</REF>; <REF>Smadja et al , 1996</REF>; Kay and R<REF>Sscheisen, 1993</REF>; <REF>Fung and Church, 1994</REF>; <REF>Fung, 1995b</REF>.	2
It remains to be seen how we can also make use of the multilingual texts as NLP resources.	In the years since the appearance of the first papers on using statistical models for bilingual lexicon compilation and machine translation<REF>Brown et al , 1993</REF>; <REF>Brown et al , 1991</REF>; <REF>Gale and <REF>Church, 1993</REF></REF>; <REF>Church, 1993</REF>; <REF>Simard et al , 1992</REF>, large amount of human effort and time has been invested in collecting parallel corpora of translated texts.	Our goal is to alleviate this effort and enlarge the scope of corpus resources by looking into monolingual, comparable texts.	This type of texts are known as nonparallel corpora.	2
The same system used in a validation mode, can be used to check and spot alignment errors in multilingually aligned wordnets as BalkaNet and EuroWordNet.	Word Sense Disambiguation WSD is wellknown as one of the more difficult problems in the field of natural language processing, as noted in <TREF>Gale et al, 1992</TREF>; <REF>Kilgarriff, 1997</REF>; <REF>Ide and Vronis, 1998</REF>, and others.	The difficulties stem from several sources, including the lack of means to formalize the properties of context that characterize the use of an ambiguous word in a given sense, lack of a standard and possibly exhaustive sense inventory, and the subjectivity of the human evaluation of such algorithms.	To address the last problem, <TREF>Gale et al, 1992</TREF> argue for upper and lower bounds of precision when comparing automatically assigned sense labels with those assigned by human judges.	2
The difficulties stem from several sources, including the lack of means to formalize the properties of context that characterize the use of an ambiguous word in a given sense, lack of a standard and possibly exhaustive sense inventory, and the subjectivity of the human evaluation of such algorithms.	To address the last problem, <TREF>Gale et al, 1992</TREF> argue for upper and lower bounds of precision when comparing automatically assigned sense labels with those assigned by human judges.	The lower bound should not drop below the baseline usage of the algorithm in which every word that is disambiguated is assigned the most frequent sense whereas the upper bound should not be too restrictive when the word in question is hard to disambiguate even for human judges a measure of this difficulty is the computation of the agreement rates between human annotators.	Identification and formalization of the determining contextual parameters for a word used in a given sense is the focus of WSD work that treats texts in a monolingual settingthat is, a setting where translations of the texts in other languages either do not exist or are not considered.	2
The summed deviation for perfect performance is thus 0.	Finally, to interpret our quantitative results, we use the performance of our human subjects as a target goal for the performance of our algorithms <TREF>Gale et al , 1992</TREF>.	Table 1 shows the average human performance for both the training and test sets of narratives.	Note that human performance is basically the same for both sets of narratives.	2
Researchers have begun to investigate the ability of humans to agree with one another on segmen108 tation, and to propose methodologies for quantifying their findings.	Several studies have used expert coders to locally and globally structure spoken discourse according to the model of <REF>Grosz and Sidnet 1986</REF>, including <REF>Grosz and Hirschberg, 1992</REF>; <REF>Hirschberg and Grosz, 1992</REF>; <REF>Nakatani et al , 1995</REF>; <REF>Stifleman, 1995</REF>.	<REF>Hearst 1994</REF> asked subjects to place boundaries between paragraphs of expository texts, to indicate topic changes.	<REF>Moser and Moore 1995</REF> had an expert coder assign segments and various segment features and relations based on RST.	2
<REF>Moser and Moore 1995</REF> had an expert coder assign segments and various segment features and relations based on RST.	To quantify their findings, these studies use notions of agreement <TREF>Gale et al , 1992</TREF>; <REF>Moset and Moore, 1995</REF> and/or reliability <REF>Passonneau and Litman, 1993</REF>; Passonneau and Litman, to appear; <REF>Isard and Carletta, 1995</REF>.	By asking subjects to segment discourse using a non-linguistic criterion, the correlation of linguistic devices with independently derived segments can then be investigated in a way that avoids circularity.	Together, <REF>Grosz and Hirschberg, 1992</REF>; <REF>Hirschberg and Grosz, 1992</REF>; <REF>Nakatani et al , 1995</REF> comprise an ongoing study using three corpora: professionally read AP news stories, spontaneous narrative, and read and spontaneous versions of task-oriented monologues.	2
By using 10-fold cross validation <REF>Kohavi and John, 1995</REF> to automatically pick the best number of nearest neighbors to use, the performance of LSXAS has improved.	4 Word Sense Disambiguation in the Large In <TREF>Gale et al , 1992</TREF>, it was argued that any wide coverage WSD program must be able to perform significantly better than the most-frequent-sense classifier to be worthy of serious consideration.	The performance of LEXAS as indicated in Table 1 is significantly better than the most-frequent-sense classifier for the set of 191 words collected in our corpus.	Figure 1 and 2 also confirm that all the training examples collected in our corpus are effectively utilized by LEXAS to improve its WSD performance.	2
1.	Word sense disambiguation has long been one of the major concerns in natural language processing area eg , <REF>Bruce et al , 1994</REF>; <REF>Choueka et al , 1985</REF>; <REF>Gale et al , 1993</REF>; <REF>McRoy, 1992</REF>; <REF>Yarowsky 1992, 1994, 1995</REF>, whose aim is to identify the correct sense of a word in a particular context, among all of its senses defined in a dictionary or a thesaurus.	Undoubtedly, effective disambiguation techniques are of great use in many natural language processing tasks, eg, machine translation and information retrieving <REF>Allen, 1995</REF>; <REF>Ng and Lee, 1996</REF>; <REF>Resnik, 1995</REF>, etc Previous strategies for word sense disambiguation mainly fall into two categories: statistics-based method and exemplar-based method.	Statistics-based method often requires large-scale corpora eg , <REF>Hirst, 1987</REF>; <REF>Luk, 1995</REF>, sense-tagging or not, monolingual or aligned bilingual, as training data to specify significant clues for each word sense.	2
H2s,  max Ps, l ew ,n, E EW where EWi  ew I si  synsetew  1 P  si I ewj  - -nj where si  synsetewj, nj  Isyr et w,l In this formula, n is the number of synsets of the translation et.	23 Heuristic 3: Sense Ordering <TREF>Gale et al , 1992</TREF> reports that word sense disambiguation would be at least 75 correct if a system assigns the most frequently occurring sense.	<REF>Miller et al , 1994</REF> found that automatic I We use English WordNet version 16 L 143 assignment of polysemous words in Brown Corpus to senses in WordNet was 58 correct with a heuristic of most frequently occurring sense.	We adopt these previous results to develop sense ordering heuristic.	2
In the area of word sense disambiguation, <REF>Black 1988</REF> developed a model based on decision trees using a corpus of 22 million tokens, after manually sense-tagging approximately 2,000 concordance lines for five test words.	Since then, supervised learning from sense-tagged corpora has since been used by several researchers: Zernik 1990, 1991, <REF>Hearst 1991</REF>, <REF>Leacock, Towell, and Voorhees 1993</REF>, Gale, Church, and Yarowsky 1992d, 1993, <REF>Bruce and Wiebe 1994</REF>, Miller et al.	1994, <REF>Niwa and Nitta 1994</REF>, <REF>Lehman 1994</REF>, among others.	However, despite the availability of increasingly large corpora, two major obstacles impede the acquisition of lexical knowledge from corpora: the difficulties of manually sense-tagging a training corpus, and data sparseness.	2
<REF>Sutcliffe and Slater 1995</REF> replicated this method on full text samples from Orwells Animal Farm and found similar results 72 correct sense assignment, compared with a 33 chance baseline, and 40 using Lesks method.	Several authors for example, Krovetz and Croft 1989, Guthrie et al 1991, Slator 1992, Cowie, Guthrie, and Guthrie 1992, Janssen 1992, Braden-Harder 1993, Liddy and Paik 1993 have attempted to improve results by using supplementary fields of information in the electronic version of the Longman Dictionary of Contemporary English LDOCE, in particular, the box codes and subject codes provided for each sense.	Box codes include primitives such as ABSTRACT, ANIMATE, HUMAN, etc , and encode type restrictions on nouns and adjectives and on the arguments of verbs.	Subject codes use another set of primitives to classify senses of words by subject ECONOMICS, ENGINEERING, etc.	2
<REF>Atkins 1987</REF> and Kilgarriff forthcoming also implicitly adopt the view of <REF>Harris 1954</REF>, according to which each sense distinction is reflected in a distinct context.	A similar view underlies the class-based methods cited in Section 243 <REF>Brown et al 1992</REF>; <REF>Pereira and Tishby 1992</REF>; <REF>Pereira, Tishby, and Lee 1993</REF>.	In this volume, Schiitze continues in this vein and proposes a technique that avoids the problem of sense distinction altogether: he creates sense clusters from a corpus rather than relying on a pre-established sense list.	324 Enumeration or generation.	2
Furthermore, our percent agreement figures are comparable with the results of other segmentation studies discussed above.	While studies of other tasks have achieved stronger results eg , 968 in a word-sense disambiguation study <TREF>Gale et al , 1992</TREF>, the meaning of percent agreement in isolation is unclear.	For example, a percent agreement figure of less than 90 could still be very meaningful if the probability of obtaining such a figure is low.	In the next section we demonstrate the significance of our findings.	2
AGREEMENT AMONG SUBJECTS We measure the ability of subjects to agree with one another, using a figure called percent agreement.	Percent agreement, defined in <TREF>Gale et al , 1992</TREF>, is the ratio of observed agreements with the majority opinion to possible agreements with the majority opinion.	Here, agreement among four, five, six, or seven subjects on whether or not there is a segment boundary between two adjacent prosodic phrases constitutes a majority opinion.	Given a transcript of length n prosodic phrases, there are n-1 possible boundaries.	2
RELIABILITY The correspondence between discourse segments and more abstract units of meaning is poorly understood see <REF>Moore and Pollack, 1992</REF>.	A number of alternative proposals have been presented which directly or indirectly relate segments to intentions <REF>Grosz and Sidner, 1986</REF>, RST relations <REF>Mann et al , 1992</REF> or other semantic relations <REF>Polanyi, 1988</REF>.	We present initial results of an investigation of whether naive subjects can reliably segment discourse using speaker intention as a criterion.	Our corpus consists of 20 narrative monologues about the same movie, taken from <REF>Chafe 1980</REF> N14,000 words.	2
Answer from five human subjects.	By this experiment, we try to clarify the upper bound of the performance of the text segmentation task, which can be considered to indicate the degree of the difficulty of the task<REF>Passonneau and Litman, 1993</REF>; <TREF>Gale et al , 1992</TREF>.	Figure 1,2 and table 1 show the results of the experiments.	Two figures show the systems mean performance of 14 texts.	2
Answer from five human subjects.	By this experiment, we try to clarify the upper bound of the performance of the text segmentation task, which can be considered to indicate the degree of the difficulty of the task<REF>Passonneau and Litman, 1993</REF>; <TREF>Gale et al, 1992</TREF>.	Figure 1,2 and table 1 show the results of the experiments.	Two figures show the systems mean performance of 14 texts.	2
587 752 Naive-Bayes 582 745 Table 1: Experimental Results ures are those of <REF>Ng and Lee, 1996</REF>.	The default strategy of picking the most frequent sense has been advocgted as the baseline performance for evaluating WSD programs <TREF>Gale et al , 1992b</TREF>; <REF>Miller et al , 1994</REF>.	There are two instantiations of this strategy in our current evaluation.	Since WORDNET orders its senses such that sense 1 is the most frequent sense, one possibility is to always pick sense 1 as the best sense assignment.	2
This is in spite of the conditional independence assumption made by the Naive-Bayes algorithm, which may be unjustified in the domains tested.	Gale, Church and Yarowsky <TREF>Gale et al , 1992a</TREF>; <REF>Gale et al , 1995</REF>; <REF>Yarowsky, 1992</REF> have also successfully used the Naive-Bayes algorithm and several extensions and variations for word sense disambiguation.	On the other hand, our past work on WSD <REF>Ng and Lee, 1996</REF> used an exemplar-based or nearest neighbor learning approach.	Our WSD program, LEXAS, extracts a set of features, including part of speech and morphological form, surrounding words, local collocations, and verb-object syntactic relation from a sentence containing the word to be disambiguated.	2
Much recent research on word sense disambiguation WSD has adopted a corpus-based, learning approach.	Many different learning approaches have been used, including neural networks <REF>Leacock et al , 1993</REF></REF>, probabilistic algorithms <REF>Bruce and Wiebe, 1994</REF>; <TREF>Gale et al , 1992a</TREF>; <REF>Gale et al , 1995</REF>; <REF>Leacock et al , 1993</REF></REF>; <REF>Yarowsky, 1992</REF>, decision lists <REF>Yarowsky, 1994</REF>, exemplar-based learning algorithms <REF>Cardie, 1993</REF>; <REF>Ng and Lee, 1996</REF>, etc In particular, <REF>Mooney 1996</REF> evaluated seven state-of-the-art machine learning algorithms on a common data set for disambiguating six senses of the word line.	The seven algorithms that he evaluated are: a Naive-Bayes classifier <REF>Duda and Hart, 1973</REF>, a perceptron <REF>Rosenblatt, 1958</REF>, a decisiontree learner <REF>Quinlan, 1993</REF>, a k nearest-neighbor classifier exemplar-based learner <REF>Cover and Hart, 1967</REF>, logic-based DNF and CNF learners <REF>Mooney, 1995</REF>, and a decision-list learner <REF>Rivest, 1987</REF>.	His results indicate that the simple Naive-Bayes algorithm gives the highest accuracy on the line corpus tested.	2
9 Related work Using vector space model and similarity measures for ranking is a common approach in IR for query/text and text/text comparisons <REF>Salton and Buckley, 1988</REF>; <REF>Salton and Yang, 1973</REF>; <REF>Croft, 1984</REF>; <REF>Turtle and Croft, 1992</REF>; <REF>Bookstein, 1983</REF>; <REF>Korflmge, 1995</REF>; <REF>Jones, 1979</REF>.	This approach has also been used by <REF>Dagan and Itai, 1994</REF>; <TREF>Gale et al, 1992</TREF>; <REF>Shiitze, 1992</REF>; <REF>Gale et al, 1993</REF>; <REF>Yarowsky, 1995</REF>; Gale and Church, 1Lunar is not an unknown word in English, Yeltsin finds its translation in the 4-th candidate.	Table 5: Some Chinese unknown word translation output score 0008421 0007895 0007669 0007588 0007283 0006812 0006430 0006218 0005921 0005527 0005335 0005335 0005221 0004731 0004470 0004275 0003878 0003859 0003859 0003784 0003686 0003550 0003519 0003481 0003407 0003407 0OO3338 0003324 0003250 0003206 0003202 0003040 0003033 0002888 0002886 English Chinese Teng-hui  Teng-hui SAR , SAR flu N m, Lei  Lei poultry j Poultry SAR  Chee-hwa hijack  Teng-hui poultry  SAR ,ng  Chee-hw Diaoyu  Teng-hui PrimeMinister  Teng-hui President  Teng-hui China  Lava Lien  Teng-hui poultry  Chee-hwa China  Teng-hui flu  Lei PrivaeMinister -I Chee-hwa President 1; Chee-hwa poultry  Leung Kalkanov i Zhuhai poultry I Lei SAR 1 J l Yeltsin Zhuhai -1 l Chee-hwa PrimeMinister  Lain President  Lava flu  Poultry apologise  Teng-hui Dee  Teng-hui Tang J Tang iSlng  Leung Leung : Leung China tN SAR Zhuhai  Lunar Ttulg  Tung 1994 for sense disambiguation between multiple usages of the same word.	Some of the early statistical terminology translation methods are <REF>Brown et al, 1993</REF>; <REF>Wu and Xia, 1994</REF>; <REF>Dagan and Church, 1994</REF>; <REF>Gale and Church, 1991</REF>; <REF>Kupiec, 1993</REF>; <REF>Smadja et al, 1996</REF>; Kay and R<REF>Sscheisen, 1993</REF>; <REF>Fung and Church, 1994</REF>; <REF>Fhmg, 1995b</REF>.	2
It remains to be seen how we can also make use of the multilingual texts as NLP resources.	In the years since the appearance of the first papers on using statistical models for bilingual lexicon compilation and machine translation<REF>Brown et al, 1993</REF>; <REF>Brown et al, 1991</REF>; <REF>Gale and <REF>Church, 1993</REF></REF>; <REF>Church, 1993</REF>; <REF>Simard et al, 1992</REF>, large amount of human effort and time has been invested in collecting parallel corpora of translated texts.	Our goal is to alleviate this effort and enlarge the scope of corpus resources by looking into monolingual, comparable texts.	This type of texts are known as nonparallel corpora.	2
The PARADISE model posits that performance can be correlated with a meaningful external criterion such as usability, and thus that the overall goal of a spoken dialogue agent is to maximize an objective related to usability.	User satisfaction ratings <REF>Kamm, 1995</REF>; <REF>Shriberg, Wade, and Price, 1992</REF>; <REF>Polifroni et al , 1992</REF> have been frequently used in the literature as an external indicator of the usability of a dialogue agent.	The model further posits that two types of factors are potential relevant contributors to user satisfaction namely task success and dialogue costs, and that two types of factors are potential relevant contributors to costs <REF>Walker, 1996</REF>.	In addition to the use of decision theory to create this objective structure, other novel aspects of PARADISE include the use of the Kappa coefficient <REF>Carletta, 1996</REF>; <REF>Siegel and Castellan, 1988</REF> to operationalize task success, and the use of linear regression to quantify the relative contribution of the success and cost factors to user satisfaction.	2
An agents responses to a query are compared with a predefined key of minimum and maximum reference answers; performance is the proportion of responses that match the key.	This approach has many widely acknowledged limitations <REF>Hirschman and Pao, 1993</REF>; <REF>Danieli et al , 1992</REF>; <REF>Bates and Ayuso, 1993</REF>, eg, although there may be many potential dialogue strategies for carrying out a task, the key is tied to one particular dialogue strategy.	In contrast, agents using different dialogue strategies can be compared with measures such as inappropriate utterance ratio, turn correction ratio, concept accuracy, implicit recovery and transaction success Danieli LWe use the term agent to emphasize the fact that we are evaluating a speaking entity that may have a personality.	Readers who wish to may substitute the word system wherever agent is used.	2
The problems arise because most sense distinctions are not as clear as the distinction between river bank and money bnk, so it is not always straightforward for a person to say what the correct answer is Thus we do not always know what it would mean to say that a computer program got the right answer.	The issue is discussed in detail by <TREF>Gale et al , 1992</TREF> who identify the problem as one of identifying the upper bound for the performance of a WSD program.	If people can only agree on the correct answer x of the time, a claim that a program achieves more than x accuracy is hard to interpret, and x is the upper bound for what the program can meaningfully achieve.	There have been some discussions as to what this upper bound might be.	2
Although this methodology could be valid for certain NLP problems, such as English Part-of-Speech tagging, we think that there exists reasonable evidence to say that, in WSD, accuracy results cannot be simply extrapolated to other domains contrary to the opinion of other authors <REF>Ng, 1997b</REF>: On the aSupervised approaches, also known as data-driven or corpus-dmven, are those that learn from a previously semantically annotated corpus.	172 one hand, WSD is very dependant to the domain of application <TREF>Gale et al , 1992b</TREF> --see also <REF>Ng and Lee, 1996</REF>; <REF>Ng, 1997a</REF>, in which quite different accuracy figures are obtained when testing an exemplar-based WSD classifier on two different corpora.	Oi1 the other hand, it does not seem reasonable to think that the training material is large and representative enough to cover all potential types of examples.	To date, a thorough study of the domain dependence of WSD --in the style of other studies devoted to parsing <REF>Sekine, 1997</REF>-has not been carried out.	2
In order to corroborate the previous hypotheses, this paper explores the portability and tuning of four different ML algorithms previously applied to WSD by training and testing them on different corpora.	Additionally, supervised methods suffer from the knowledge acquisition bottleneck <TREF>Gale et al , 1992a</TREF>.	<REF>Ng, 1997b</REF> estimates that the manual annotation effort necessary to build a broad coverage semantically annotated English corpus is about 16 personyears.	This overhead for supervision could be much greater if a costly tuning procedure is required before applying any existing system to each new domain.	2
Word sense disambiguation is a potentially crucial task in many NLP applications, such as machine translation Brown, Della Pietra, and <REF>Della Pietra 1991</REF>, parsing <REF>Lytinen 1986</REF>; <REF>Nagao 1994</REF> and text retrieval <REF>Krovets and Croft 1992</REF>; <REF>Voorhees 1993</REF>.	Various corpus-based approaches to word sense disambiguation have been proposed <REF>Bruce and Wiebe 1994</REF>; <REF>Charniak 1993</REF>; <REF>Dagan and Itai 1994</REF>; <REF>Fujii et al 1996</REF>; <REF>Hearst 1991</REF>; <REF>Karov and Edelman 1996</REF>; <REF>Kurohashi and Nagao 1994</REF>; <REF>Li, Szpakowicz, and Matwin 1995</REF>; <REF>Ng and Lee 1996</REF>; <REF>Niwa and Nitta 1994</REF>; Schitze 1992; <REF>Uramoto 1994b</REF>; <REF>Yarowsky 1995</REF>.	The use of corpus-based approaches has grown with the use of machine-readable text, because unlike conventional rule-based approaches relying on hand-crafted selectional rules some of which are reviewed, for example, by Hirst 1987, corpus-based approaches release us from the task of generalizing observed phenomena through a set of rules.	Our verb sense disambiguation system is based on such an approach, that is, an example-based approach.	2
Alignment at other levels of resolution is obviously useful.	A section, paragraph, sentence, phrase, collocation, or word can be aligned to its translation <TREF>Kupiec 1993</TREF>; <REF>Smadja, McKeown, and Hatzivassiloglou 1996</REF>.	Other logical approaches involve aligning parse trees of a sentence and its translation <REF>Matsumoto, Ishimoto, and Utsuro 1993</REF>; <REF>Meyers, Yangarber, and Grishman 1996</REF>, or simultaneously generating parse trees and alignment arrangements <REF>Wu 1995</REF>.	Department of Computer Science, National Tsing Hua University, Hsinchu, 30043, Taiwan, ROC.	2
Prec.	100 97 200 94 Table 2: Multiword notion results As a comparison, <TREF>Kupiec, 1993</TREF> obtained a precision of 90 for the first hundred associations between English and French noun phrases, using the EM algorithm.	Our experiments with a similar method showed a precision around 92 for the first hundred associations on a set of aligned sentences comprising the one used for the above experiment.	Art evaluation on single words, showed a precision of 98 for the first hundred and 97 for the first two hundred.	3
3 Multilingual terminology extraction Several works describe methods to extract terms, or candidate terms, in English and/or French <REF>Justeson and Katz, 1995</REF>; <REF>Daille, 1994</REF>; Nkwenti-<REF>Azeh, 1992</REF>.	Some more specific works describe methods to align noun phrases within parallel corpora <TREF>Kupiec, 1993</TREF>.	The underlying assumption beyond these works is that the monolingually extracted units correspond to each other cross-lingually.	Unfortunately, this is not always the case, and the above methodology suffers flom the weaknesses pointed out by <REF>Wu, 1997</REF> concerning parse-parse-match procedures.	3
In addition, new collocations are produced one after another and most of them are technical jargons.	There has been a growing interest in corpus-based approaches which retrieve collocations from large corpora <REF>Nagao and Mori, 1994</REF>, <REF>Ikehara et al , 1996</REF> <TREF>Kupiec, 1993</TREF>, <REF>Fung, 1995</REF>, <REF>Kitamura and Matsumoto, 1996</REF>, <REF>Smadja, 1993</REF>, <REF>Smadja et al , 1996</REF>, <REF>Haruno et al , 1996</REF>.	Although these approaches achieved good results for the task considered, most of them aim to extract fixed collocations, mainly noun phrases, and require the information which is dependent on each language such as dictionaries and parts of speech.	From a practical point of view, however, a more robust and flexible approach is desirable.	3
This highlights the need for finding multi-word translation correspondences.	Previous works that focus on multi-word translation correspondences from parallel corpora include noun phrase correspondences <TREF>Kupiec, 1993</TREF>, fixed/flexible collocations <REF>Smadja et al , 1996</REF>, n-gram word sequences of arbitrary length <REF>Kitamura and Matsumoto, 1996</REF>, non-compositional compounds <REF>Melamed, 2001</REF>, captoids <REF>Moore, 2001</REF>, and named entities 1.	In all of these approaches, a common problem seems to be an identification of meaningful multi-word translation units.	There are a number of factors which make handling of multi-word units more complicated than it appears.	3
It can now be assumed that a parallel bilingual corpus may be aligned to the sentence level with reasonable accuracy Kay and Ri3cheisen 1988; <REF>Catizone, Russel, and Warwick 1989</REF>; <REF>Gale and Church 1991</REF>; <REF>Brown, Lai, and Mercer 1991</REF>; <REF>Chen 1993</REF>, even for languages as disparate as Chinese and English <REF>Wu 1994</REF>.	Algorithms for subsentential alignment have been developed as well as granularities of the character <REF>Church 1993</REF>, word <REF>Dagan, Church, and Gale 1993</REF>; <REF>Fung and Church 1994</REF>; <REF>Fung and McKeown 1994</REF>, collocation <REF>Smadja 1992</REF>, and specially segmented <TREF>Kupiec 1993</TREF> levels.	However, the identification of subsentential, nested, phrasal translations within the parallel texts remains a nontrivial problem, due to the added complexity of dealing with constituent structure.	Manual phrasal matching is feasible only for small corpora, either for toy-prototype testing or for narrowly restricted applications.	3
4 GENERATING TRANSLATION CANDIDATES 41 Extraction of Japanese Terms Errors in the extraction of terms and phrases from parallel texts eventually lead to a failure in acquiring the correct term/phrase correspondences.	<REF>In Kupiec 1993</REF> and <REF>Yamamoto 1993</REF>, term and phrase extraction is applied to both of parallel texts.	In contrast, we extract from units only Japanese terms, thereby reducing the errors caused by term/phrase recognizer.	Japanese NPs can be recognized more accurately than English NPs because Japanese has considerably less multi-category words.	3
Automatic bilingual lexicon construction based on bilingual corpora has become an important first step for many studies and applications of natural language processing NLP, such as machine translation MT, crosslanguage information retrieval CLIR, and bilingual text alignment.	As noted in <REF>Tsuji 2002</REF>, many previous methods <REF>Dagan et al , 1993</REF>; <TREF>Kupiec, 1993</TREF>; <REF>Wu and Xia, 1994</REF>; <REF>Melamed, 1996</REF>; <REF>Smadja et al , 1996</REF> deal with this problem based on frequency of words appearing in the corpora, which can not be effectively applied to lowfrequency words, such as transliterated words.	These transliterated words are often domain-specific and created frequently.	Many of them are not found in existing bilingual dictionaries.	3
<REF>Tiedemann, 1993</REF> ; <REF>Boutsis  Piperidis, 1996</REF> ; <REF>Piperidis et al , 1997</REF> combine statistical and linguistic information for the same task.	Some methods make alignment suggestions at an intermediate level between sentence and word 271 and word <REF>Smadja, 1992</REF> ; <REF>Smadja et al , 1996</REF> ; <TREF>Kupiec, 1993</TREF> ; <REF>Kumano  Hirakawa, 1994</REF> ; <REF>Boutsis  Piperidis, 1998</REF>.	A common problem is the delimitation and spotting of the units to be matched.	This is not a real problem for methods aiming at alignments at a high level of granularity paragraphs, sentences where unit delimiters are clear.	3
We have been studying robust lexicon compilation methods which do not rely on sentence alignment.	Existing lexicon compilation methods <TREF>Kupiec 1993</TREF>; <REF>Smadja  McKeown 1994</REF>; <REF>Kumano  Hirakawa 1994</REF>; <REF>Dagan et al 1993</REF>; <REF>Wu  Xia 1994</REF> all attempt to extract pairs of words or compounds that are translations of each other from previously sentencealigned, parallel texts.	However, sentence alignment <REF>Brown et al 1991</REF>; Kay  R<REF>Sscheisen 1993</REF>; <REF>Gale  <REF>Church 1993</REF></REF>; <REF>Church 1993</REF>; <REF>Chen 1993</REF>; <REF>Wu 1994</REF> is not always practical when corpora have unclear sentence boundaries or with noisy text segments present in only one language.	Our proposed algorithm for bilingual lexicon acquisition bootstraps off of corpus alignment procedures we developed earlier <REF>Fung  Church 1994</REF>; <REF>Fung  McKeown 1994</REF>.	3
Compilation of translation lexicons is a crucial process for machine translation MT <REF>Brown et al , 1990</REF> and cross-language information retrieval CLIR systems <REF>Nie et al , 1999</REF>.	A lot of effort has been spent on constructing translation lexicons from domain-specific corpora in an automatic way <REF>Melamed, 2000</REF>; <REF>Smadja et al , 1996</REF>; <TREF>Kupiec, 1993</TREF>.	However, such methods encounter two fundamental problems: translation of regional variations and the lack of up-to-date and high-lexical-coverage corpus source, which are worthy of further investigation.	The first problem is resulted from the fact that the translations of a term may have variations in different dialectal regions.	3
Prec.	100 97 200 94 Table 2: Multiword notion results As a comparison, <TREF>Kupiec, 1993</TREF> obtained a precision of 90 for the first hundred associations between English and French noun phrases, using the EM algorithm.	Our experiments with a similar method showed a precision around 92 for the first hundred associations on a set of aligned sentences comprising the one used for the above experiment.	An evaluation on single words, showed a precision of 9870 for the first hundred and 97 for the first two hundred.	3
3 Multilingual terminology extraction Several works describe methods to extract terms, or candidate terms, in English and/or French <REF>Justeson and Katz, 1995</REF>; <REF>Daille, 1994</REF>; Nkwenti-<REF>Azeh, 1992</REF>.	Some more specific works describe methods to align noun phrases within parallel corpora <TREF>Kupiec, 1993</TREF>.	The underlying assumption beyond these works is that the monolingually extracted units correspond to each other cross-lingually.	Unfortunately, this is not always the case, and the above methodology suffers from the weaknesses pointed out by <REF>Wu, 1997</REF> concerning parse-parse-match procedures.	3
We present an algorithm in finding word correlation statistics for automatic bilingual lexicon compilation from a non-parallel corpus in Chinese and English.	Most previous automatic lexicon compilation techniques require a sentence-aligned clean parallel bilingual corpus <TREF>Kupiec 1993</TREF>; <REF>Smadja  McKeown 1994</REF>; <REF>Kumano  Hirakawa 1994</REF>; <REF>Dagan et al 1993</REF>; <REF>Wu  Xia 1994</REF>.	We have previously shown an algorithm which extracts a bilingual lexicon from noisy parallel corpus without sentence alignment <REF>Fung  McKeown 1994</REF>; <REF>Fung 1995</REF>.	Although bilingual parallel corpora have been available in recent years, they are still relatively few in comparison to the large amount of monolingual text.	3
<REF>Smadja et al , 1996</REF>; <REF>Gao et al , 2002</REF>; <REF>Wu and Zhou, 2003</REF>.	Some studies have been done for acquiring collocation translations using parallel corpora <REF>Smadja et al, 1996</REF>; <TREF>Kupiec, 1993</TREF>; Echizen-ya et al , 2003.	These works implicitly assume that a bilingual corpus on a large scale can be obtained easily.	However, despite efforts in compiling parallel corpora, sufficient amounts of such corpora are still unavailable.	3
The minimum score derived from any of the criteria applied is deemed initially to be the score of the constituent.	That is, an assumption of full statistical dependence <TREF>Yarowsky, 1994</TREF>, rather than the more common full independence, is made3 When llf events El, E2,, E, are fully independent, then the joint probability PE1 A A En is the product of PEIPEn, but if they are maximally dependent, it is the minimum of these values.	Of course, neither assumption is any more than an approximation to the truth; but assuming dependence has the advantage that the estimate of the joint probability depends much less strongly on n, and so estimates for alternative joint events can be directly compared, without any possibly tricky normalization, even if they are composed of different numbers of atomic events.	This property is desirable: different sub-paths through a chart may span different numbers of edges, and one can imagine evaluation criteria which are only defined for some kinds of edge, or which often duplicate information supplied by other criteria.	2
In this paper, we propose a method of detecting Japanese homophone errors in Japanese texts.	Our method is based on a decision list proposed by Yarowsky <TREF>Yarowsky, 1994</TREF>; <REF>Yarowsky, 1995</REF>.	We improve the original decision list by using written words in the default evidence.	The improved decision list can raise the F-measure of error detection.	2
We also use the special evidence default, frqwl, default is defined as the frequency of wl.	step5 Pick the highest strength estwh,ej among 5As in this paper, the addition of a small value is an easy and effective way to avoid the unsatisfactory case, as shown in <TREF>Yarowsky, 1994</TREF>.	estwl, , eaw, e,   , e e, and set the word wk as the answer for the evidence ej.	In this case, the identifying strength is estwk, ej.	2
Following commonpracticeinfeatureextractioneg.	<TREF>Yarowsky, 1994</TREF>, and using the mxpost1 part of speech tagger and WordNets lemmatization, the following feature set was used: bag of word lemmas for the context words in the preceding, current and following sentence; unigrams of lemmas and parts of speech in a window of /three words, where each position provides a distinct feature; and bigrams of lemmas in the same window.	The SVMLight <REF>Joachims, 1999</REF> classifier was used in the supervised settings with its default parameters.	To obtain a multi-class classifier we used a standard one-vs-all approach of training a binary SVM for each possible sense and then selecting the highest scoring sense for a test example.	2
It has been amply demonstrated that a wide assortment of machine learning algorithms are quite effective at extracting linguistic information from manually annotated corpora.	Among the machine learning algorithms studied, rule based systems have proven effective on many natural language processing tasks, including part-of-speech tagging <REF>Brill, 1995</REF>; <REF>Ramshaw and Marcus, 1994</REF>, spelling correction <REF>Mangu and Brill, 1997</REF>, word-sense disambiguation <REF>Gale et al , 1992</REF>, message understanding <REF>Day et al , 1997</REF>, discourse tagging <REF>Samuel et al , 1998</REF>, accent restoration <TREF>Yarowsky, 1994</TREF>, prepositional-phrase attachment <REF>Brill and Resnik, 1994</REF> and base noun phrase identification Ramshaw and Marcus, In Press; <REF>Cardie and Pierce, 1998</REF>; <REF>Veenstra, 1998</REF>; <REF>Argamon et al , 1998</REF>.	Many of these rule based systems learn a short list of simple rules typically on the order of 50-300 which are easily understood by humans.	Since these rule-based systems achieve good performance while learning a small list of simple rules, it raises the question of whether peoand Woman.	2
Unsupervised learning holds great promise for breakthroughs in natural language processing.	In cases like <REF>Yarowsky, 1995</REF>, unsupervised methods offer accuracy results than rival supervised methods <TREF>Yarowsky, 1994</TREF> while requiring only a fraction of the data preparation effort.	Such methods have also been a key driver of progress in statistical machine translation, which depends heavily on unsupervised word alignments <REF>Brown et al , 1993</REF>.	There are also interesting problems for which supervised learning is not an option.	3
In contrast, we used exemplar-based learning, where the contributions of all features are summed up and taken into account in coming up with a classification.	We also include verb-object syntactic relation as a feature, which is not used in <TREF>Yarowsky, 1994</TREF>.	Although the work of Yarowsky, i994 can be applied to WSD, the results reported in <TREF>Yarowsky, 1994</TREF> only dealt with accent restoration, which is a much simpler problem.	It is unclear how Yarowskys method will fare on WSD of a common test data set like the one we used, nor has his method been tested on a large data set with highly ambiguous words tagged with the refined senses of WORDNET.	3
In contrast, LEXAS uses supervised learning from tagged sentences, which is also the approach taken by most recent work on WSD, including <REF>Bruce and Wiebe, 1994</REF>; <REF>Miller et al , 1994</REF>; <REF>Leacock et al , 1993</REF>; <TREF>Yarowsky, 1994</TREF>; <REF>Yarowsky, 1993</REF>; <REF>Yarowsky, 1992</REF>.	The work of <REF>Miller et al , 1994</REF>; <REF>Leacock et al , 1993</REF>; <REF>Yarowsky, 1992</REF> used only the unordered set of surrounding words to perform WSD, and they used statistical classifiers, neural networks, or IR-based techniques.	The work of <REF>Bruce and Wiebe, 1994</REF> used parts of speech POS and morphological form, in addition to surrounding words.	However, the POS used are abbreviated POS, and only in a window of -b2 words.	2
One line of research focuses on the use of the knowledge contained in a machine-readable dictionary to perform WSD, such as <REF>Wilks et al , 1990</REF>; <REF>Luk, 1995</REF>.	In contrast, LEXAS uses supervised learning from tagged sentences, which is also the approach taken by most recent work on WSD, including <REF>Bruce and Wiebe, 1994</REF>; <REF>Miller et al , 1994</REF>; <REF>Leacock et al , 1993</REF>; <TREF>Yarowsky, 1994</TREF>; <REF>Yarowsky, 1993</REF>; <REF>Yarowsky, 1992</REF>.	The work of <REF>Miller et al , 1994</REF>; <REF>Leacock et al , 1993</REF>; <REF>Yarowsky, 1992</REF> used only the unordered set of surrounding words to perform WSD, and they used statistical classifiers, neural networks, or IR-based techniques.	The work of <REF>Bruce and Wiebe, 1994</REF> used parts of speech POS and morphological form, in addition to surrounding words.	2
We also include verb-object syntactic relation as a feature, which is not used in <TREF>Yarowsky, 1994</TREF>.	Although the work of Yarowsky, i994 can be applied to WSD, the results reported in <TREF>Yarowsky, 1994</TREF> only dealt with accent restoration, which is a much simpler problem.	It is unclear how Yarowskys method will fare on WSD of a common test data set like the one we used, nor has his method been tested on a large data set with highly ambiguous words tagged with the refined senses of WORDNET.	The work of <REF>Miller et al , 1994</REF> is the only prior work we know of which attempted to evaluate WSD on a large data set and using the refined sense distinction of WORDNET.	3
That local collocation knowledge provides important clues to WSD is pointed out in <REF>Yarowsky, 1993</REF>, although it was demonstrated only on performing binary or very coarse sense disambiguation.	The work of <TREF>Yarowsky, 1994</TREF> is perhaps the most similar to our present work.	However, his work used decision list to perform classification, in which only the single best disambiguating evidence that matched a target context is used.	In contrast, we used exemplar-based learning, where the contributions of all features are summed up and taken into account in coming up with a classification.	2
Methods can typically be delineated along two dimensions, corpns-based vs dictionary-based approaches.	Corpus-based word sense disambignation algorjthm such as <REF>Ng and Lee, 1996</REF>; <REF>Bruce and Wiebe, 1994</REF>; <TREF>Yarowsky, 1994</TREF> relied on supervised learning fzom annotated corpora.	The main drawback of these approaches is their requirement of a sizable sense-tagged corpus.	Attempts to alleviate this tagbottleneck ilude tmotstrias Te ot ill,, 1996; <REF>Hearst, 1991</REF> and unsupervised algorith Yarowsky, 199s Dictionary-based approaches rely on linguistic knowledge sources such as mali,e-readable dictionaries <REF>Luk, 1995</REF>; <REF>Veronis and Ide, 1990</REF> and WordNet <REF>Agirre and Rigau, 1996</REF>; <REF>Resnik, 1995</REF> and e0ploit these for word sense disaznbiguation.	3
Moreover, word trigrams are ineffective at capturing longdistance properties such as discourse topic and tense.	Feature-based approaches, such as Bayesian classifters <REF>Gale, Church, and Yarowsky, 1993</REF>, decision lists <TREF>Yarowsky, 1994</TREF>, and Bayesian hybrids <REF>Golding, 1995</REF>, have had varying degrees of success for the problem of context-sensitive spelling correction.	However, we report experiments that show that these methods are of limited effectiveness for cases such as their, there, theyre and than, then, where the predominant distinction to be made among the words is syntactic.	71 Confusion set Train Test Most freq.	3
1995, we formalize the problem of deciding dependency preference of subordinate clauses by utilizing the correlation of scope embedding preference and dependency preference of Japanese subordinate clauses.	Then, as a statistical learning method, we employ the decision list learning method of <TREF>Yarowsky 1994</TREF>, where optimal combination of those features are selected and sorted in the form of decision rules, according to the strength of correlation between those features and the dependency preference of the two subordinate clauses.	We evaluate the proposed method through the experiment on learning dependency preference of Japanese subordinate clauses from the EDR bracketed corpus section 4.	We show that the proposed method outperforms other related methods/models.	2
The head vp chunk of Clause1 does not modify that of Clause2, but modifies that of another subordinate clause or the matrix clause which follows Clause2.	Roughly speaking, the first corresponds to the case where Clause2 inherently has a scope of the same or a broader breadth compared with that of Clause1, while the second corresponds to the case where Clause2 inherently has a narrower scope compared with that of Clause17 32 Decision List Learning A decision list <TREF>Yarowsky, 1994</TREF> is a sorted list of the decision rules each of which decides the value of a decision D given some evidence E Each decision rule in a decision list is sorted TOur modeling is slightly different from those of other standard approaches to statistical dependency analysis <REF>Collins, 1996</REF>; <REF>Fujio and Matsumoto, 1998</REF>; <REF>Haruno et al , 1998</REF> which simply distinguish the two cases: the case where dependency relation holds between the given two vp chunks or clauses, and the case where dependency relation does not hold.	In contrast to those standard approaches, we ignore the case where the head vp chunk of Clause1 modifies that of another subordinate clause which precedes Clause2.	This is because we assume that this case is more loosely related to the scope embedding preference of subordinate clauses.	2
We formalize the problem of deciding scope embedding preference as a classification problem, in which various types of linguistic information of each subordinate clause are encoded as features and used for deciding which one of given two subordinate clauses has a broader scope than the other.	As a statistical learning method, we employ the decision list learning method of <TREF>Yarowsky 1994</TREF>.	113 Table 2: Features of Japanese Subordinate Clauses Feature Type  of Feat  Each Binary Feature Punctuation 2 with-comma, without-comma Grammatical adverb, adverbial-noun, formal-noun, temporal-noun, some features have distinction 17 quoting-particle, copula, predicate-conjunctive-particle, of chunk-final/middle topic-marking-particle, sentence-final-particle 12 Conjugation form of chunk-final conjugative word Lexical lexicalized forms of Grammatical features, with more than 9 occurrences in EDR corpus 235 stem, base, mizen, renyou, rental, conditional, imperative, ta, tari, re, conjecture, volitional adverb eg , ippou-de, irai, adverbial-noun eg , tame, baai topic-marking-particle eg , ha, mo, quoting-particle to, predicate-conjunctive-particle eg , ga, kara, temporal-noun eg , ima, shunkan, formal-noun eg , koto, copula dearu, sentence-final-particle eg , ka, yo 31 The Task Definition Considering the dependency preference of Japanese subordinate clauses described in section 24, the following gives the definition of our task of deciding the dependency of Japanese subordinate clauses.	Suppose that a sentence has two subordinate clauses Clause1 and Clause2, where the head vp chunk of Clausel precedes that of Clause2.	2
For each piece of evidence, calculate the likelihood ratio of the conditional probability of a decision D  xl given the presence of that piece of evidence to the conditional probability of the rest of the decisions D -,xl: PDxl I EI lg2 PDxl EI Then, a decision list is constructed with pieces of evidence sorted in descending order with respect to their likelihood ratios, s 2.	The final line of a decision list is defined as a default, where the likelihood ratio is calculated as the ratio of the largest marginal probability of the decision D  xl to the marginal proba<REF>Syarowsky 1994</REF> discusses several techniques for avoiding the problems which arise when an observed count is 0.	Among those techniques, we employ the simplest one, ie, adding a small constant c 01 < c < 025 to the numerator and denominator.	With this modification, more frequent evidence is preferred when there exist several evidences for each of which the conditional probability PDx  EI equals to 1.	2
a7 Decision List DL are lists of weighted classification rules involving the evaluation of one single feature.	At classification time, the algorithm applies the rule with the highest weight that matches the test example <TREF>Yarowsky, 1994</TREF>.	The provider is IXA and they also applied smoothing to generate more robust decision lists.	a7 In the Vector Space Model method cosVSM, each example is treated as a binary-valued feature vector.	2
To construct classifiers using supervised methods, we need classified data such as those in Figure 1.	22 Decision Lists Let us first consider the use of decision lists, as proposed in <TREF>Yarowsky 1994</TREF>.	Let f  denote a feature of the context of .	A feature can be, for example, a words occurrence immediately to the left of .	2
While the basic idea of our model is similar to trigger models, they handle co-occurrences of word pairs independently and do not use a representation of the whole context.	This omission is also done in applications such as word sense dismnbiguation Yarowsky: 1994; FUNG et al , 1999.	Our model is the most related to Coccaro mad <REF>Jurafsky 1998</REF>, in that a reduced vector space approach was taken and context is represented by the accumulation of word cooccurrence vectors.	Their model was reported to decrease the test set perplexity by 12, compared to the bigram nmdel.	2
Exemplar-based method makes use of typical contexts exemplars of a word sense, eg, verbnoun collocations or adjective-noun collocations, and identifies the correct sense of a word in a particular context by comparing the context with the exemplars <REF>Ng and Lee, 1996</REF>.	Recently, some kinds of learning techniques have been applied to cumulatively acquire exemplars form large corpora <REF>Yarowsky, 1994, 1995</REF>.	But ideal resources from which to learn exemplars are not generally available for any languages.	Moreover, the effectiveness of this method on disambiguating words in large-scale corpora into fine-grained sense distinctions needs to be further investigated <REF>Ng and Lee, 1996</REF>.	3
STEP 3a: Train the supervised classification algorithm on the SENSE-A/SENSE-B seed sets.	The decision-list algorithm used here <TREF>Yarowsky, 1994</TREF> identifies other collocations that reliably partition the seed training data, ranked by the purity of the distribution.	Below is an abbreviated example of the decision list trained on the plant seed data.	9 Initial decision list for plant abbreviated LogL 810 758 739 720 627 470 439 430 410 352 348 345 Collocation Sense plant life  A manufacturing plant  B life within 4-2-10 words  A manufacturing in 4-2-10 words  B animal within -I-2-10 words  A equipment within -1-2-10 words , B employee within 4-2-10 words  B assembly plant  B plant closure  B plant species  A automate within 4-2-10 words :: B microscopic plant  A 9Note that a given collocate such as life may appear multiple times in the list in different collocations1 relationships, including left-adjacent, right-adjacent, cooccurrence at other positions in a k-word window and various other syntactic associations.	2
Our probabilistic decision lists can thus be thought of as a competitive way to probabilize TBLs, with the advantage of preserving the list-structure and simplicity of TBL, and the possible disadvantage of losing the dependency on the current state.	<TREF>Yarowsky 1994</TREF> suggests two improvements to the standard algorithm.	First, he suggests an optional, more complex smoothing algorithm than the one we applied.	His technique involves estimating both a probability based on the global probability distribution for a question, and a local probability, given that no questions higher in the list were TRUE, and then interpolating between the two probabilities.	2
In the system presented here, the classifiers built for each ambiguous word are based on its lemma instead.	Lemmatization allows for more compact and generalizable data by clustering all inflected forms of an ambiguous word together, an effect already commented on by <TREF>Yarowsky 1994</TREF>.	The more inflection in a language, the more lemmatization will help to compress and generalize the data.	In the case of our WSD system this means that less classifiers have to be built therefore adding up the training material available to the algorithm for each ambiguous wordform.	2
Alternatively, we chose for a model constructing classifiers based on lemmas therefore reducing the number of classifiers that need to be made.	As has already been noted by <TREF>Yarowsky 1994</TREF>, using lemmas helps to produce more concise and generic evidence than inflected forms.	Therefore building classifiers based on lemmas increases the data available to each classifier.	We make use of the advantage of clustering all instances of eg one verb in a single classifier instead of several classifiers one for each inflected form found in the data.	2
For instance, governing body and governing bodies are different collocations for the sake of this paper.	4 Adaptation of decision lists to n-way ambiguities Decision lists as defined in <REF>Yarowsky, 1993</REF>; 1994 are simple means to solve ambiguity problems.	They have been successfully applied to accent restoration, word sense disambiguation 209 and homograph disambiguation <TREF>Yarowsky, 1994</TREF>; 1995; 1996.	In order to build decision lists the training examples are processed to extract the features each feature corresponds to a kind of collocation, which are weighted with a log-likelihood measure.	2
4 Adaptation of decision lists to n-way ambiguities Decision lists as defined in <REF>Yarowsky, 1993</REF>; 1994 are simple means to solve ambiguity problems.	They have been successfully applied to accent restoration, word sense disambiguation 209 and homograph disambiguation <TREF>Yarowsky, 1994</TREF>; 1995; 1996.	In order to build decision lists the training examples are processed to extract the features each feature corresponds to a kind of collocation, which are weighted with a log-likelihood measure.	The list of all features ordered by log-likelihood values constitutes the decision list.	2
The context within which the ambiguous word occurs is typically represented by a set of linguistically motivated features from which a learning algorithm induces a representative model that performs the disambiguation.	A variety of classifiers have been employed for this task see Mooney 1996 and Ide and Veronis 1998 for overviews, the most popular being decision lists <REF>Yarowsky 1994, 1995</REF> and naive Bayesian classifiers <REF>Pedersen 2000</REF>; <REF>Ng 1997</REF>; <REF>Pedersen and Bruce 1998</REF>; <REF>Mooney 1996</REF>; <REF>Cucerzan and Yarowsky 2002</REF>.	We employed a naive Bayesian classifier <REF>Duda and Hart 1973</REF> for our experiments, as it is a very convenient framework for incorporating prior knowledge and studying its influence on the classification task.	In Section 51 we describe a basic naive Bayesian classifier and show how it can be extended with informative priors.	2
At present we have chosen one algorithm which does not combine features Decision Lists and another which does combine features AdaBoost.	Despite their simplicity, Decision Lists Dlist for short as defined in <TREF>Yarowsky 1994</TREF> have been shown to be very effective for WSD <REF>Kilgarriff  Palmer, 2000</REF>.	Features are weighted with a log-likelihood measure, and arranged in an ordered list according to their weight.	In our case the probabilities have been estimated using the maximum likelihood estimate, smoothed adding a small constant 01 when probabilities are zero.	2
Machine learning methods have become the most popular technique in a variety of classification problems of these sort, and have shown significant success.	A partial list consists of Bayesian classifiers <REF>Gale et al , 1993</REF>, decision lists <TREF>Yarowsky, 1994</TREF>, Bayesian hybrids <REF>Golding, 1995</REF>, HMMs <REF>Charniak, 1993</REF>, inductive logic methods <REF>Zelle and Mooney, 1996</REF>, memorya3 This research is supported by NSF grants IIS-9801638, IIS0085836 and SBR-987345.	based methods <REF>Zavrel et al , 1997</REF>, linear classifiers <REF>Roth, 1998</REF>; <REF>Roth, 1999</REF> and transformationbased learning <REF>Brill, 1995</REF>.	In many of these classification problems a significant source of difficulty is the fact that the number of candidates is very large  all words in words selection problems, all possible tags in tagging problems etc Since general purpose learning algorithms do not handle these multi-class classification problems well see below, most of the studies do not address the whole problem; rather, a small set of candidates typically two is first selected, and the classifier is trained to choose among these.	2
, 2005a and WordSenseDisambiguation WSD <REF>Strapparava et al , 2004</REF>.	In general, the strategy adopted to model syntagmatic relations is to provide bigrams and trigrams of collocated words as features to describe local contexts <TREF>Yarowsky, 1994</TREF>, and each word is regarded as a different instance to classify.	For instance, occurrences of a given class of named entities such as names of persons can be discriminated in texts by recognizing word patterns in their local contexts.	For example the token Rossi, whenever is preceded by the token Prof , often represents the name of a person.	2
Obviously, how to measure the confidence of features is a very important issue for the decision list.	We use the metric described in <TREF>Yarowsky, 1994</TREF>; <REF>Golding, 1995</REF>.	Provided that 1  0Ps f > for all i :  max    i i confidence f P s f 1 This value measures the extent to which the context is unambiguously correlated with one particular slot i s  24 Slot-value merging and semantic reclassification The slot-value merger is to combine the slots assigned to the concepts in an input sentence.	Another simultaneous task of the slot-value merger is to check the consistency among the identified slot-values.	2
In particular, they may involve performing speech recognition on speech data, parsing on text data, application of hand-coded rules to the results of parsing, or some combination of these.	Statistics are then compiled to estimate the probability pa j f of each semantic atom a given each separate feature f, using the standard formula pa j f  Naf  1Nf  2 where Nf is the number of occurrences in the training data of utterances with feature f, and N af is the number of occurrences of utterances with both feature f and semantic atom a The decoding process follows <TREF>Yarowsky, 1994</TREF> in assuming complete dependence between the features.	Note that this is in sharp contrast with the Naive Bayes classifier <REF>Duda et al , 2000</REF>, which assumes complete independence.	Of course, neither assumption can be true in practice; however, as argued in <REF>Carter, 2000</REF>, there are good reasons for preferring the dependence alternative as the better option in a situation where there are many features extracted in ways that are likely to overlap.	2
There is typically no corpus data available at the start of a project, but considerable amounts at the end: the intention behind ALTERF is to allow us to shift smoothly from an initial version of the system which is entirely rule-based, to a final version which is largely data-driven.	ALTERF characterises semantic analysis as a task slightly extending the decision-list classification algorithm <TREF>Yarowsky, 1994</TREF>; <REF>Carter, 2000</REF>.	We start with a set of semantic atoms, each representing a primitive domain concept, and define a semantic representation to be a non-empty set of semantic atoms.	For example, in the procedure assistant domain we represent the utterances please speak up show me the sample syringe set an alarm for five minutes from now no i said go to the next step respectively as fincrease volumeg fshow, sample syringeg fset alarm, 5, minutesg fcorrection, next stepg where increase volume, show, sample syringe, set alarm, 5, minutes, correction and next step are semantic atoms.	2
Left  Context   ML2MII ll,ight Named Entity  Context  M   Mm<3  1 2 Current Position 4 Supervised Learning for Japanese Named Entity Recognition This section describes how to apply tile decision list learning method to chunking/tagging named entities.	41 Decision List Learning A decision list <REF>Rivest, 1987</REF>; <TREF>Yarowsky, 1994</TREF> is a sorted list of decision rules, each of which decides the wflue of a decision D given some evidence E Each decision rule in a decision list is sorted in descending order with respect to some preference value, and rules with higher preference values are applied first when applying the decision list to some new test; data.	First, the random variable D representing a decision w, ries over several possible values, and the random wriable E representing some evidence varies over 1 and 0 where 1 denotes the presence of the corresponding piece of evidence, 0 its absence.	Then, given some training data in which the correct value of the decision D is annotated to each instance, the conditional probabilities PD  x I E  1 of observing the decision D  x under the condition of the presence of the evidence E E  1 are calculated and the decision list is constructed by the tbllowing procedure.	2
In general, creating training data tbr supervised learning is somewhat easier than creating pattern matching rules by hand.	Next, we apply Yarowskys method tbr supervised decision list learning I <TREF>Yarowsky, 1994</TREF> to 1VVe choose tile decision list learning method as the 705 Table 1: Statistics of NE Types of IREX NE Type ORGANIZATION PERSON LOCATION ARTIFACT DATE TIME MONEY PERCENT Total frequency  Training 3676 197 3840 206 5463 292 747 40 3567 191 502 27 390 21 492 26 18677 Test 361 239 338 224 413 274 48 32 260 172 54 35 15 10 21 14 1510 Japanese named entity recognition, into which we incorporate several noun phrase chunking techniques sections 3 and 4 and experimentally evaluate their performance on the IREX, workshops training and test data section 5.	As one of those noun phrase chunking techniques, we propose a method for incorporating richer contextual information as well as patterns of constituent morphemes within a named entity, compared with those considered in tire previous research <REF>Sekine et al , 1998</REF>; <REF>Borthwick, 1999</REF>, and show that the proposed method outperlbrms these approaches.	2 Japanese Named Entity Recognition 21 Task of the IREX Workshop The task of named entity recognition of the IREX workshop is to recognize eight named entity types in Table 1 IREX <REF>Conmfittee, 1999</REF>.	2
This small number has almost no effect on more frequent words, but boosts the score of less common, yet potentially equally informative, words.	222 Decision List The decision list classifier uses the log-likelihood of correspondence between each context feature and each sense, using additive smoothing <TREF>Yarowsky, 1994</TREF>.	The decision list was created by ordering the correspondences from strongest to weakest.	Instances that did not match any rule in the decision list were assigned the most frequent sense, as calculated from the training data.	2
It must be noted however that the LDOCE homograph level is far more rough-grained than the CIDE guideword level, let alone the sub-sense level, and that Wilks and Stevensons approach on its own would, by its very nature, not transfer down to more fine-grained distinctions.	Other research, such as Yarowskys into accent restoration in <REF>Spanish and French 1994</REF>, which reports accuracy levels of 9099, is again at a more rough-grained level, in this case that of distinguished unaccented and accented word forms.	While the sense tagging results are fairly encouraging, the part of speech tagging results arc at present relatively poor.	It thus secrns sensible, especially noting Wilks and Stevensons analysis mentioned above, to first run a sentence through a traditional part of speech tagger before trying to disambiguate the senses.	2
From this perspective, either accent identification can be extended to truecasing or truecasing can be extended to incorporate accent restoration.	<TREF>Yarowsky, 1994</TREF> reports good results with statistical methods for Spanish and French accent restoration.	Truecasing is also a specialized method for spelling correction by relaxing the notion of casing to spelling variations.	There is a vast literature on spelling correction <REF>Jones and Martin, 1997</REF>; <REF>Golding and Roth, 1996</REF> using both linguistic and statistical approaches.	2
Syncretism and related morphological ambiguities present a problem for many NL applications where lexical disambiguation is important; cases where the orthographic form is identical but the pronunciations of the various functions differ are particularly important for speech applications, such as text-to-speech, since appropriate word pronunciations must be computed from orthographic forms that underspecify the necessary information.	Ideally one would like to build models that use contextual information to perform lexical disambiguation <REF>Yarowsky 1992, 1994</REF>, but such models must be trained on specialized tagged corpora either hand-generated or semi-automatically generated and such training corpora are often not available, at least in the early phases of constructing a particular application.	Lacking good contextual models, one is forced to fall back on estimates of the lexical prior probabilities for the various functions of a form.	Following standard terminology, a lexical prior can be defined as follows: Imagine that a given form is n-ways ambiguous; the lexical prior probability of sense i of this form is simply the probability of sense i independent of the context in which the particular instantiation of the form occurs.	3
A feature expression F of the named entity can be any possible subset of the full feature expression mlength, NEtag,POS , or the set indicating that the system outputs no named entity within the segment.	F         any subset of braceleftBig mlength, NEtag,POS bracerightBig braceleftBig class sys no outputs bracerightBig In the training and testing phases, within each segment SegEv j of event expression, a class is assigned to each system, where each class class i sys for the i-th system is represented as a list of the classes of the named entities output by the system: class i sys  braceleftbigg /,  , / no output i 1,,n 34 Learning Algorithm We apply a simple decision list learning method to the task of learning a classifier for combining outputs of named entity chunkers 4  A decision list <TREF>Yarowsky, 1994</TREF> is a sorted list of decision rules, each of which decides the value of class given some features f of an event.	Each decision rule in a decision list is sorted in descending order with respect to some preference value, and rules with higher preference values are applied first when applying the decision list to some new test data.	In this paper, we simply sort the decision list according to the conditional probability Pclass i  f of the class i of the i-th systems output given a feature f 4 Experimental Evaluation We experimentally evaluate the performance of the proposed system combination method using the IREX workshops training and test data.	2
32 Nave Bayes The second system used was a nave Bayes classifier where the similarity between an instance, I, and a sense class, Sj, is defined as: SimI,Sj  PI,Sj  PSjPISj We then choose the sense class, Sj, which maximized the similarity function above, making standard independence assumptions.	33 Decision List The final system was a decision list classifier that found the log-likelihoods of the correspondence beAssociation for Computational Linguistics for the Semantic Analysis of Text, Barcelona, <REF>Spain, July 2004</REF> SENSEVAL-3: Third International Workshop on the Evaluation of Systems tween features and senses, using plus-one smoothing <TREF>Yarowsky, 1994</TREF>.	The features were ordered from most to least indicative to form the decision list.	A separate decision list was constructed for each set of lexical samples in the training data.	2
The ordering of rules employed in a decision list in order to simplify the representation and perform conflict resolution apparently gives it an advantage over other symbolic methods on this task.	In addition to the results reported by <TREF>Yarowsky 1994</TREF> and <REF>Mooney and Califf 1995</REF>, it provides evidence for the utility of this representation for natural-language problems.	With respect to training time, the symbolic methods are significantly slower since they are searching for a simple declarative representation of the concept.	Empirically, the time complexity for most methods are growing somewhat worse than linearly in the number of training examples.	2
Finally, decision lists <REF>Rivest, 1987</REF> are ordered lists of conjunctive rules, where rules are tested in order and the first one that matches an instance is used to classify it.	A number of effective concept-learning systems have employed decision lists Clark 84 <REF>Niblett, 1989</REF>; <REF>Quinlan, 1993</REF>; <REF>Mooney  Califf, 1995</REF> and they have already been successfully applied to lexical disambiguation <TREF>Yarowsky, 1994</TREF>.	All of the logic-based methods are variations of the FOIL algorithm for induction of first-order function-free Horn clauses <REF>Quinlan, 1990</REF>, appropriately simplified for the propositional case.	They are called PFoIL-DNF, PFOlL-CNF, and PFoIL-DLIsT.	2
32 The Syntagmatic Kernel Syntagmatic aspects are probably the most important evidence for recognizing lexical entailment.	In general, the strategy adopted to model syntagmatic relations in WSD is to provide bigrams and trigrams ofcollocatedwordsasfeaturestodescribelocalcontexts <TREF>Yarowsky, 1994</TREF>.	The main drawback of this approach is that non contiguous or shifted collocations cannot be identified, decreasing the generalization power of the learning algorithm.	For example, suppose that the word job has to be disambiguated into the sentence permanent academic job in, and that the occurrence We offer permanent positions is provided for training.	3
In this study we used ill the decision-list method the same 152 types of patterns that were used in/;lie maximuln-entropy method.	To determine the priority order of the rules, we referred to Yarowskys method <TREF>Yarowsky, 1994</TREF> and Nishiokwamas method <REF>Nishiokaymna et al , 1998</REF> and used the probability and frequency of each rule as measures of this priority order.	When nnlltiple rifles had the same probability, the rules were arranged in order of their frequency.	Suppose, for example, that Pattern A Noun: Normal Noun; Particle: Case-Particle: none: wo; Verb: Normal Form: 217; Symhol: Punctuatioif occurs 13 times in a learlfing set and that tell of the occurrences include the inserted partition Inal:k Suppose also thai; Pattern B Noun; Particle; Verb; Symbol occurs 12a times in a learning set and that 90 of the occurrences include the mark.	2
As we do not use multiple classifiers our approach is quite far from cotraining.	But it is close to the paradigm described by <TREF>Yarowsky 1995</TREF> and <REF>Turney 2002</REF> as it also employs self-training based on a relatively small seed data set which is incrementally enlarged with unlabelled samples.	But our approach does not use point-wise mutual information.	Instead we use relative frequencies of newly found features in a training subcorpus produced by the previous iteration of the classifier.	2
Finally, I present a simpler and more efcient approach to training dependency parsers by applying a boosting-like procedure to standard training methods.	Over the past decade, there has been tremendous progress on learning parsing models from treebank data <TREF>Magerman, 1995</TREF>; <REF>Collins, 1999</REF>; <REF>Charniak, 1997</REF>; <REF>Ratnaparkhi, 1999</REF>; <REF>Charniak, 2000</REF>; <REF>Wang et al , 2005</REF>; <REF>McDonald et al , 2005</REF>.	Most of the early work in this area was based on postulating generative probability models of language that included parse structures <TREF>Magerman, 1995</TREF>; <REF>Collins, 1997</REF>; <REF>Charniak, 1997</REF>.	Learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation tricks to cope with the sparse data problems <REF>Collins, 1997</REF>; <REF>Bikel, 2004</REF>.	2
Over the past decade, there has been tremendous progress on learning parsing models from treebank data <TREF>Magerman, 1995</TREF>; <REF>Collins, 1999</REF>; <REF>Charniak, 1997</REF>; <REF>Ratnaparkhi, 1999</REF>; <REF>Charniak, 2000</REF>; <REF>Wang et al , 2005</REF>; <REF>McDonald et al , 2005</REF>.	Most of the early work in this area was based on postulating generative probability models of language that included parse structures <TREF>Magerman, 1995</TREF>; <REF>Collins, 1997</REF>; <REF>Charniak, 1997</REF>.	Learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation tricks to cope with the sparse data problems <REF>Collins, 1997</REF>; <REF>Bikel, 2004</REF>.	Subsequent research began to focus more on conditional models of parse structure given the input sentence, which allowed discriminative training techniques such as maximum conditional likelihood ie maximum entropy  to be applied <REF>Ratnaparkhi, 1999</REF>; <REF>Charniak, 2000</REF>.	2
A recent trend in natural language processing has been toward a greater emphasis on statistical approaches, beginning with the success of statistical part-of-speech tagging programs <REF>Church 1988</REF>, and continuing with other work using statistical part-of-speech tagging programs, such as BBN PLUM <REF>Weischedel et al 1993</REF> and NYU Proteus <REF>Grishman and Sterling 1993</REF>.	More recently, statistical methods have been applied to domain-specific semantic parsing <REF>Miller et al 1994</REF>, and to the more difficult problem of wide-coverage syntactic parsing <TREF>Magerman 1995</TREF>.	Nevertheless, most natural language systems remain primarily rule based, and even systems that do use statistical techniques, such as ATT Chronus <REF>Levin and Pieraccini 1995</REF>, continue to require a significant rule based component.	Development of a complete end-to-end statistical understanding system has been the focus of several ongoing research efforts, including <REF>Miller et al 1995</REF> and <REF>Koppelman et al 1995</REF>.	2
In the second phase, these distributions are smoothed by mixing together distributions of various nodes in the decision tree.	As in <TREF>Magerman 1995</TREF>, mixture weights are determined by deleted interpolation on a separate block of training data.	43 Searching the Semantic Interpretation Model Searching the interpretation model proceeds in two phases.	In the first phase, every parse T received from the parsing model is rescored for every possible frame type, computing PT I FT our current model includes only a half dozen different types, so this computation is tractable.	2
In empirical approaches to parsing, lexical/semantic collocation extracted from corpus has been proved to be quite useful for ranking parses in syntactic analysis.	For example, <TREF>Magerman 1995</TREF>, <REF>Collins 1996</REF>, and <REF>Charniak 1997</REF> proposed statistical parsing models which incorporated lexical/semantic information.	In their models, syntactic and lexical/semantic features are dependent on each other and are combined together.	This paper also proposes a method of utilizing lexical/semantic features for the purpose of applying them to ranking parses in syntactic analysis.	2
We will discuss the maximum accuracy of 8433.	Compared to recent stochastic English parsers that yield 86 to 87 accuracy <REF>Collins, 1996</REF>; <TREF>Magerman, 1995</TREF>, 8433 seems unsatisfactory at the first glance.	The main reason behind this lies in the difference between the two corpora used: Penn Treebank <REF>Marcus et al , 1993</REF> and EDR corpus EDR, 1995.	Penn Treebank<REF>Marcus et al , 1993</REF> was also used to induce part-of-speech POS taggers because the corpus contains very precise and detailed POS markers as well as bracket, annotations.	3
A broader range of information, in particular lexical information, was found to be essential in disambiguating the syntactic structures of real-world sentences.	SPATTER <TREF>Magerman, 1995</TREF> augmented the pure PCFG by introducing a number of lexical attributes.	The parser controlled applications of each rule by using the lexical constraints induced by decision tree algorithm <REF>Quinlan, 1993</REF>.	The SPATTER parser attained 87 accuracy and first made stochastic parsers a practical choice.	2
Head-lexicalized stochastic grammars have recently become increasingly popular see <REF>Collins 1997, 1999</REF>; <REF>Charniak 1997, 2000</REF>.	These grammars are based on Magermans headpercolation scheme to determine the headword of each nonterminal <TREF>Magerman 1995</TREF>.	Unfortunately this means that head-lexicalized stochastic grammars are not able to capture dependency relations between words that according to Magermans head-percolation scheme are nonheadwords -eg between more and than in the WSJ construction carry more people than cargo where neither more nor than are headwords of the NP constituent more people than cargo.	A frontier-lexicalized DOP model, on the other hand, captures these dependencies since it includes subtrees in which more and than are the only frontier words.	3
In the first pass, it tags words as either head words or non-head words.	Training data for this pass is obtained using a head percolation table <TREF>Magerman 1995</TREF> on bracketed Penn Treebank sentences.	After training, head tagging is performed according to Equation 1, where 15 is the estimated probability and Hi is a characteristic function which is true iff word i is a head word.	n H  argmaxH HwilHiHilHi-1Hi-2 i1 1 The second pass then takes the words with this head information and supertags them according to Equation 2, where tHio is the supertag of the ePart of speech tagging models have not used heads in this manner to achieve variable length contexts.	2
There have been two main robust parsing paradigms: Finite State Grammar-based approaches such as <REF>Abney 1990</REF>, <REF>Grishman 1995</REF>, and Hobbs et al.	1997 and Statistical Parsing such as <REF>Charniak 1996</REF>, <TREF>Magerman 1995</TREF>, and <REF>Collins 1996</REF>.	<REF>Srinivas 1997a</REF> has presented a different approach called supertagging that integrates linguistically motivated lexical descriptions with the robustness of statistical techniques.	The idea underlying the approach is that the computation of linguistic structure can be localized if lexical items are associated with rich descriptions Supertags that impose complex constraints in a local context.	2
Second, one might propagate lexical information upward through the productions.	Examples of formalisms using this approach include the work of <TREF>Magerman 1995</TREF>, <REF>Charniak 1997</REF>, <REF>Collins 1997</REF>, and <REF>Goodman 1997</REF>.	A more linguistically motivated approach is to expand the domain of productions downward to incorporate more tree structures.	The Lexicalized Tree-Adjoining Grammar LTAG formalism <REF>Schabes et al , 1988</REF>, <REF>Schabes, 1990</REF>, although not context-free, is the most well-known instance in this category.	2
Standard symbolic machine learning techniques have been successfully applied to a number of tasks in natural language processing NLP.	Examples include the use of decision trees for syntactic analysis <TREF>Magerman, 1995</TREF>, coreference <REF>Aone and Bennett, 1995</REF>; <REF>McCarthy and Lehnert, 1995</REF>, and cue phrase identification <REF>Litman, 1994</REF>; the use of inductive logic programming for learning semantic grammars and building prolog parsers 113 <REF>Zelle and Mooney, 1994</REF>; <REF>Zelle and Mooney, 1993</REF>; the use of conceptual clustering algorithms for relative pronoun resolution <REF>Cardie, 1992a</REF>; <REF>Cardie 1992b</REF>, and the use of case-based learning techniques for lexical tagging tasks <REF>Cardie, 1993a</REF>; Daelemans et al , submitted.	In theory, both statistical and machine learning techniques can significantly reduce the knowledge-engineering effort for building large-scale NLP systems: they offer an automatic means for acquiring robust heuristics for a host of lexical and structural disambiguation tasks.	It is well-known in the machine learning community, however, that the success of a learning algorithm depends critically on the representation used to describe the training and test instances <REF>Almuallim and Dietterich, 1991</REF>, Langley and Sage, in press.	2
1994, and <TREF>Magerman 1995</TREF>.	A strength of these models is undoubtedly the powerful estimation techniques that they use: maximum-entropy modeling in <REF>Ratnaparkhi 1997</REF> or decision trees in <REF>Jelinek et al 1994</REF> and <TREF>Magerman 1995</TREF>.	A weakness, we will argue in this section, is the method of associating parameters with transitions taken by bottom-up, shift-reduce-style parsers.	We give examples in which this method leads to the parameters unnecessarily fragmenting the training data in some cases or ignoring important context in other cases.	2
3 We find lexical heads in Penn Treebank data using the rules described in Appendix A of <REF>Collins 1999</REF>.	The rules are a modified version of a head table provided by David Magerman and used in the parser described in <TREF>Magerman 1995</TREF>.	593 Collins Head-Driven Statistical Models for NL Parsing Internal rules Lexical rules TOP  S JJ  Last S  NP NP VP NN  week NP  JJ NN NNP  IBM NP  NNP VBD  bought VP  VBD NP NNP  Lotus NP  NNP Figure 1 A nonlexicalized parse tree and a list of the rules it contains.	Internal Rules: TOP  Sbought,VBD Sbought,VBD  NPweek,NN NPIBM,NNP VPbought,VBD NPweek,NN  JJLast,JJ NNweek,NN NPIBM,NNP  NNPIBM,NNP VPbought,VBD  VBDbought,VBD NPLotus,NNP NPLotus,NNP  NNPLotus,NNP Lexical Rules: JJLast,JJ  Last NNweek,NN  week NNPIBM,NNP  IBM VBDbought,VBD  bought NNPLotus,NN  Lotus Figure 2 A lexicalized parse tree and a list of the rules it contains.	2
<REF>Charniak 2000</REF> describes a series of enhancements to the earlier model of <REF>Charniak 1997</REF>.	The precision and recall of the traces found by Model 3 were 938 and 901, respectively out of 437 cases in section 23 of the treebank, where three criteria must be met for a trace to be correct: 1 It must be an argument to the correct headword; 2 It must be in the correct position in relation to that headword preceding or following; 15 <TREF>Magerman 1995</TREF> collapses ADVP and PRT into the same label; for comparison, we also removed this distinction when calculating scores.	608 Computational Linguistics Volume 29, Number 4 Table 2 Results on Section 23 of the WSJ Treebank.	LR/LP  labeled recall/precision.	2
Another important differencethe ability of models 1, 2, and 3 to generalize to produce context-free rules not seen in training datawas described in section 74 Section 82 showed that the parsing models of <REF>Ratnaparkhi 1997</REF>, Jelinek et al.	1994, and <TREF>Magerman 1995</TREF> can suffer from very similar problems to the label bias or observation bias problem observed in tagging models, as described in <REF>Lafferty, McCallum, and Pereira 2001</REF> and <REF>Klein and Manning 2002</REF>.	635 Collins Head-Driven Statistical Models for NL Parsing Acknowledgments My PhD thesis is the basis of the work in this article; I would like to thank Mitch Marcus for being an excellent PhD thesis adviser, and for contributing in many ways to this research.	I would like to thank the members of my thesis committeeAravind Joshi, Mark Liberman, Fernando Pereira, and Mark Steedmanfor the remarkable breadth and depth of their feedback.	3
<REF>Charniaks 1997</REF> models will most likely perform quite differently with binarybranching trees for example, his current models will learn that rules such as VP  VSGPPare very rare, but with binary-branching structures, this context sensitivity will be lost.	The models of <TREF>Magerman 1995</TREF> and <REF>Ratnaparkhi 1997</REF> use contextual predicates that would most likely need to be modified given a different annotation style.	<REF>Goodmans 1997</REF> models are the exception, as he already specifies that the treebank should be transformed into his chosen representation, binary-branching trees.	731 Representation Affects Structural, not Lexical, Preferences.	3
22 Statistical Parsers Pioneered by the IBM natural language group <REF>Fujisaki et al 1989</REF> and later pursued by, for example, <REF>Schabes, Roth, and Osborne 1993</REF>, Jelinek et al.	1994, <TREF>Magerman 1995</TREF>, <REF>Collins 1996</REF>, and <REF>Charniak 1997</REF>, this approach decouples the issue of wellformedness of an input string from the problem of assigning a structure to it.	These systems attempt to assign some structure to every input string.	The rules to assign a structure to an input are extracted automatically from hand-annotated parses of large corpora, which are then subjected to smoothing to obtain reasonable coverage of the language.	2
During the last few years large treebanks have become available to many researchers, which has resulted in researches applying a range of new techniques for parsing systems.	Most of the methods that are being suggested include some kind of Machine Learning, such as history based grammars and decision tree models <REF>Black et al , 1993</REF>; <TREF>Magerman, 1995</TREF>, training or inducing statistical grammars <REF>Black, Garside and Leech, 1993</REF>; <REF>Pereira and Schabes, 1992</REF>; <REF>Schabes et al , 1993</REF>, or other techniques <REF>Bod, 1993</REF>.	Consequently, syntactical analysis has become an area with a wide variety of a algorithms and methods for learning and parsing, and b type of information used for learning and parsing sometimes referred to as feature set.	These methods only could become popular through evaluation methods for parsing systems, such as Bracket Accuracy, Bracket Recall, Sentence Accuracy and Viterbi Score.	2
Although we report promising results, parse selection that is sufficiently accurate for many practical applications will require a more lexicalised system.	Magermans 1995 parser is an extension of the history-based parsing approach developed at IBM <REF>Black et al , 1993</REF> in which rules are conditioned on lexical and other essentially arbitrary information available in the parse history.	In future work, we intend to explore a more restricted and semantically-driven version of this approach in which, firstly, probabilities are associated with different subcategorisation possibilities, and secondly, alternative predicateargument structures derived from the grammar are ranked probabilistically.	However, the massively increased coverage obtained here by relaxing subcategorisation constraints underlines the need to acquire accurate and complete subcategorisation frames in a corpus-driven fashion, before such constraints can be exploited robustly and effectively with free text.	2
In our experiments, the window starts at the sentence prior to that containing the token and extends back W the window size sentences.	The choice to use sentences as the unit of distance is motivated by our intention to incorporate triggers of this form into a probabilistie treebank-based parser and tagger, such as <REF>Black et al , 1998</REF>; <REF>Black et al , 1997</REF>; <REF>Brill, 1994</REF>; <REF>Collins, 1996</REF>; <REF>Jelinek et al , 1994</REF>; <TREF>Magerman, 1995</TREF>; <REF>Ratnaparkhi, 1997</REF>.	All such parsers and taggers of which we are aware use only intrasentential information in predicting parses or tags, and we wish to remove this information, as far as possible, from our results 7 The window was not allowed to cross a document boundary.	The perplexity of the task before taking the trigger-pair information into account for tags was 2240 and for rules was 570.	2
We describe several simple, linguistically motivated annotations which do much to close the gap between a vanilla PCFG and state-of-the-art lexicalized models.	Specifically, we construct an unlexicalized PCFG which outperforms the lexicalized PCFGs of <TREF>Magerman 1995</TREF> and <REF>Collins 1996</REF> though not more recent models, such as <REF>Charniak 1997</REF> or <REF>Collins 1999</REF>.	One benefit of this result is a much-strengthened lower bound on the capacity of an unlexicalized PCFG.	To the extent that no such strong baseline has been provided, the community has tended to greatly overestimate the beneficial effect of lexicalization in probabilistic parsing, rather than looking critically at where lexicalized probabilities are both needed to make the right decision and available in the training data.	3
This approach was congruent with the great success of word n-gram models in speech recognition, and drew strength from a broader interest in lexicalized grammars, as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such as PP attachments <REF>Ford et al , 1982</REF>; <REF>Hindle and Rooth, 1993</REF>.	In the following decade, great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized PCFG models <TREF>Magerman, 1995</TREF>; <REF>Charniak, 1997</REF>; <REF>Collins, 1999</REF>; <REF>Charniak, 2000</REF>; <REF>Charniak, 2001</REF>.	However, several results have brought into question how large a role lexicalization plays in such parsers.	<REF>Johnson 1998</REF> showed that the performance of an unlexicalized PCFG over the Penn treebank could be improved enormously simply by annotating each node by its parent category.	2
However, perhaps even more significant has been the lexicalization of the grammar formalisms being probabilistically modeled: crucially, all the recent, successful statistical parsers have in some way made use of bilexical dependencies.	This includes both the parsers that attach probabilities to parser moves <TREF>Magerman, 1995</TREF>; <REF>Ratnaparkhi, 1997</REF>, but also those of the lexicalized PCFG variety <REF>Collins, 1997</REF>; <REF>Charniak, 1997</REF>.	155 Even more crucially, the bilexical dependencies involve head-modifier relations hereafter referred to simply as head relations.	The intuition behind the lexicalization of a grammar formalism is to capture lexical items idiosyncratic parsing preferences.	2
In those research, extracted lexical/semantic collocation is especially useful in terms of ranking parses in syntactic analysis as well as automatic construction of lexicon for NLP.	For example, in the context of syntactic disambiguation, <REF>Black 1993</REF> and <TREF>Magerman 1995</TREF> proposed statistical parsing models based-on decisiontree learning techniques, which incorporated not only syntactic but also lexical/semantic information in the decision-trees.	As lexical/semantic information, <REF>Black 1993</REF> used about 50 semantic categories, while <TREF>Magerman 1995</TREF> used lexicai forms of words.	<REF>Collins 1996</REF> proposed a statistical parser which is based on probabilities of dependencies between head-words in the parse tree.	2
For example, in the context of syntactic disambiguation, <REF>Black 1993</REF> and <TREF>Magerman 1995</TREF> proposed statistical parsing models based-on decisiontree learning techniques, which incorporated not only syntactic but also lexical/semantic information in the decision-trees.	As lexical/semantic information, <REF>Black 1993</REF> used about 50 semantic categories, while <TREF>Magerman 1995</TREF> used lexicai forms of words.	<REF>Collins 1996</REF> proposed a statistical parser which is based on probabilities of dependencies between head-words in the parse tree.	In those works, lexical/semantic collocation are used for ranking parses in syntactic analysis.	2
Other researchers have proposed automatically selecting the conditioning information for various states of the model, thus potentially increasing greatly the space of possible features and selectively choosing the best predictors for each situation.	Decision trees have been applied for feature selection for statistical parsing models by <TREF>Magerman 1995</TREF> and Haruno et al.	1998.	Another example of automatic feature selection for parsing is in the context of a deterministic parsing model that chooses parse actions based on automatically induced decision structures over a very rich feature set <REF>Hermjakob and Mooney, 1997</REF>.	2
We grew the trees fully and we calculated final expansion probabilities at the leaves by linear interpolation with estimates one level above.	This is a similar, but more limited, strategy to the one used by <TREF>Magerman 1995</TREF>.	The features over derivation trees which we made available to the learner are shown in Table 1.	The node direction features indicate whether a node is a left child, a right child, or a single child.	2
Figure 1a shows a parse tree in the Penn Treebank style for the English translation in Ex 1.	Given a parse tree, we use a head percolation table <TREF>Magerman, 1995</TREF> to create the corresponding dependency structure.	Figure 2a shows the dependency structure derived from the parse tree in Figure 1a.	32 Word alignment Because most of the 700 languages in ODIN are low-density languages with no on-line bilingual dictionariesorlargeparallelcorpora, aligning the source sentence and its English translation directly would not work well.	2
Inparticular,weleveragethe increasing availability of off-the-shelf parsers such as <REF>Charniak, 2001</REF>; <REF>Charniak, 2005</REF> to automatically or semi-automatically assign syntactic analyses to a set of suggested output sentences.	We then draw on lexicalization techniques for statistical language models <TREF>Magerman, 1995</TREF>; <REF>Collins, 1999</REF>; <REF>Chiang, 2000</REF>; <REF>Chiang, 2003</REF> to induce a probabilistic, lexicalized tree-adjoining grammar that supports the derivation of all the suggested output sentences, and many others besides.	The final step is to use the training examples to learn an effective search policy so that our run-time generation component can find good output sentences in a reasonable time frame.	In particular, we use variants of existing search optimization <REF>Daum and Marcu, 2005</REF> and ranking algorithms <REF>Collins and Koo, 2005</REF> to train our run-time component to find good outputs within a specified time window; see also <REF>Stent et al, 2004</REF>; <REF>Walker et al, 2001</REF>.	2
In the first stage, a collection of rules is used to automatically decorate the training syntax with a number of features.	These include deciding the lexical anchors for each non-terminal constituent and assigning complement/adjunct status for nonterminals which are not on their parents lexicalization path; see <TREF>Magerman, 1995</TREF>; <REF>Chiang, 2003</REF>; <REF>Collins, 1999</REF>.	In addition, we deterministically add features to improve several grammatical aspects, including 1 enforcing verb inflectional agreement in derived trees, 2 enforcing consistency in the finiteness of VP and S complements, and 3 restricting subject/direct object/indirect object complements to play the same grammatical role in derived trees.	In the second stage, the complements and adjuncts in the decorated trees are incrementally re80 syntax: cat: SA fin:other,  cat: S cat: NP,  apr: VBP, apn: other pos: PRP we fin:yes,  cat: VP apn: other,  pos: VBP do pos: RB nt fin: yes,  cat: VP, gra: obj1 fin: yes,  cat: VP, gra: obj1 pos: VBP have cat: NP,  gra: obj1 operations: initial tree comp semantics: speech-actaction  assert speech-actcontentpolarity  negative speech-actcontentattribute  resourceAttribute syntax: cat: NP,  apr: VBP, gra: obj1,  apn: other pos: JJ medical pos: NNS supplies cat: ADVP,  gra: adj pos: RB here cat: NP,  apr: VBZ, gra: adj,  apn: 3ps pos: NN captain operations: comp left/right adjunction left/right adjunction semantics: speech-actcontentvalue  medical-supplies speech-actcontentobject-id  market addressee  captain-kirk dialogue-actaddressee  captain-kirk speech-actaddressee  captain-kirk Figure 2: The linguistic resources inferred from the training example in Figure 1.	2
Whats more, on top of the large undertaking of designing and implementing a statistical parsing model, the use of heuristics has required a further e ort, forcing the researcher to bring both linguistic intuition and, more often, engineering savvy to bear whenever moving to a new treebank.	For example, in the rule sets used by the parsers described in <TREF>Magerman, 1995</TREF>; <REF>Ratnaparkhi, 1997</REF>; <REF>Collins, 1999</REF>, the sets of rules for finding the heads of ADJP, ADVP, NAC, PP and WHPP include rules for picking either the rightmost or leftmost FW foreign word.	The apparently haphazard placement of these rules that pick out FW and the rarity of FW nodes in the data strongly suggest these rules are the result of engineering e ort.	Furthermore, it is not at all apparent that tree-transforming heuristics that are useful for one parsing model will be useful for another.	2
Once enriched, the data can be used as a bootstrap for tools such as taggers.	311 Enriching IGT In a previous study <REF>Xia and Lewis, 2007</REF>, we proposed a three-step process to enrich IGT data: 1 parse the English translation with an English parser and convert English phrase structures PS into dependency structures DS with a head percolation table <TREF>Magerman, 1995</TREF>, 2 align the target line and the English translation using the gloss line, and 3 project the syntactic structures both PS and DS from English onto the target line.	For instance, given the IGT example in Ex 1, the enrichment algorithm will produce the word alignment in Figure 1 and the syntactic structures in Figure 2.	The  teacher  gave  a  book  to    the    boy   yesterday Rhoddodd  yr   athro     lyfr     ir     bachgen  ddoe  Gloss line:  Translation: Target line: gave-3sg  the  teacher book  to-the  boy   yesterday Figure 1: Aligning the target line and the English translation with the help of the gloss line 533 gave a Projecting DS athro bachgen lyfr yr ddoeir  Rhoddodd S NP1 VP NN teacher VBD   gave NP2 DT a NP4PP NN the IN NP3 yesterday NN DT book NN boy DT to S NP NN VBD NP NPPP NN INDT NN NNDT   rhoddodd   gave yrthe    athro teacher lyfr book     ir to-the bachogen boy ddoe yesterday teacher a boy the book the yesterdayto The b Projecting PS Figure 2: Projecting syntactic structure from English to the target language We evaluated the algorithm on a small set of 538 IGT instances for several languages.	2
The extraction procedure determines the structure of each elementary tree by localizing dependencies through the use of heuristics.	Salient heuristics include the use of a head percolation table <TREF>Magerman, 1995</TREF>, and another table that distinguishes between complements and adjunct nodes in the tree.	For our current work, we use the head percolation table to determine heads of phrases.	Also, we treat a PropBank argument ARG0 : : : ARG9 as a complement and a PropBank adjunct ARGMs as an adjunct when such annotation is available1 Otherwise, we basically follow the approach of <REF>Chen, 2001</REF>2 Besides introducing one kind of TAG extraction 1The version of the PropBank we are using is not fully annotated with semantic role information, although the most common predicates are.	2
unlike MaxEnt, cannot be used as a probabilistic component in a larger model.	MaxEnt can provide a probability for each tagging decision, which can be used in the probability calculation of any structure that is predicted over the POS tags, such as noun phrases, or entire parse trees, as in <REF>Jelinek et al , 1994</REF>, <TREF>Magerman, 1995</TREF>.	Thus MaxEnt has at least one advantage over each of the reviewed POS tagging techniques.	It is better able to use diverse information than Markov Models, requires less supporting techniques than SDT, and unlike TBL, can be used in a probabilistic framework.	2
Since most realistic natural language applications must process words that were never seen before in training data, all experiments in this paper are conducted on test data that include unknown words.	Several recent papers<REF>Brill, 1994</REF>, <TREF>Magerman, 1995</TREF> have reported 965 tagging accuracy on the Wall St Journal corpus.	The experiments in this paper test the hypothesis that better use of context will improve the accuracy.	A Maximum Entropy model is well-suited for such experiments since it cornbines diverse forms of contextual information in a principled manner, and does not impose any distributional assumptions on the training data.	2
In contrast, the MaxEnt model combines diverse and non-local information sources without making any independence assumptions.	140 A POS tagger is one component in the SDT based statisticM parsing system described in <REF>Jelinek et al , 1994</REF>, <TREF>Magerman, 1995</TREF>.	The total word accuracy on Wall St Journal data, 965<TREF>Magerman, 1995</TREF>, is similar to that presented in this paper.	However, the aforementioned SDT techniques require word classes<REF>Brown et al , 1992</REF> to help prevent data fragmentation, and a sophisticated smoothing algorithm to mitigate the effects of any fragmentation that occurs.	2
6 Conclusion It is worth noting that while we have presented the use of edge-based best-first chart parsing in the service of a rather pure form of PCFG parsing, there is no particular reason to assume that the technique is so limited in its domain of applicability.	One can imagine the same techniques coupled with more informative probability distributions, such as lexicalized PCFGs <REF>Charniak, 1997</REF>, or even grammars not based upon literal rules, but probability distributions that describe how rules are built up from smaller components <TREF>Magerman, 1995</TREF>; <REF>Collins, 1997</REF>.	Clearly further research is warranted.	Be this as it may, the take-home lesson from this paper is simple: combining an edge-based agenda with the figure of merit from CC  is easy to do by simply binarizing the grammar  provides a factor of 20 or so reduction in the number of edges required to find a first parse, and  improves parsing precision and recall over exhaustive parsing.	2
uk black,eubank,kashiokaatritlcojp GLeechOcentllancsacuk 1 Introduction A treebank is a body of natural language text which has been grammatically annotated by hand, in terms of some previously-established scheme of grammatical analysis.	Treebanks have been used within the field of natural language processing as a source of training data for statistical part og speech taggers <REF>Black et al , 1992</REF>; <REF>Brill, 1994</REF>; <REF>Merialdo, 1994</REF>; <REF>Weischedel et al , 1993</REF> and for statistical parsers <REF>Black et al , 1993</REF>; <REF>Brill, 1993</REF>; aelinek et al , 1994; <TREF>Magerman, 1995</TREF>; <REF>Magerman and Marcus, 1991</REF>.	In this article, we present the ATR/Lancaster 7reebauk of American English, a new resource tbr natural-language-, processing research, which has been prepared by Lancaster University UKs Unit for Computer Research on the English Language, according to specifications provided by ATR Japans Statistical Parsing Group.	First we provide a static description, with a a discussion of the mode of selection and initial processing of text for inclusion in the treebank, and b an explanation of the scheme of grammatical annotation we then apply to the text.	2
That approach is based on a conversion from constituent structure to dependency structure by recursively defining a head for each constituent.	The same idea was used by <TREF>Magerman 1995</TREF>, who developed the first head table for the Penn Treebank <REF>Marcus et al , 1994</REF>, and <REF>Collins 1996</REF>, whose constituent parser is internally based on probabilities of bilexical dependencies, ie dependencies between two words.	<REF>Collins 1997</REF>s parser and its reimplementation and extension by <REF>Bikel 2002</REF> have by now been applied to a variety of languages: English <REF>Collins, 1999</REF>, Czech <REF>Collins et al , 1999</REF>, German <REF>Dubey and Keller, 2003</REF>, Spanish <REF>Cowan and Collins, 2005</REF>, French <REF>Arun and Keller, 2005</REF>, Chinese <REF>Bikel, 2002</REF> and, according to Dan Bikels web page, Arabic.	<REF>Eisner 1996</REF> introduced a data-driven dependency parser and compared several probability models on English Penn Treebank data.	2
Moreover, the results of a less-than-optimal version of DOP on the Wall Street Journal corpus suggest that the approach can be succesfully extended to larger domains.	As future research, we will apply the full DOP model on WSJ word strings in order to compare our results with the best known parsers on this domain <TREF>Magerman, 1995</TREF>; <REF>Collins, 1996</REF>.	Acknowledgements I am grateful to Remko Scha for many useful comments and additions.	I also thank three anonymous reviewers for their comments.	2
Stochastic parsing systems either use a closed lexicon, or use a two step approach where first the words are tagged 133 by a stochastic tagger, after which the p-o-s tags with or without the words are parsed by a stochastic parser.	The latter approach has become increasingly popular eg <REF>Schabes et al , 1993</REF>; <REF>Weischedel et al , 1993</REF>; <REF>Briscoe, 1994</REF>; <TREF>Magerman, 1995</TREF>; <REF>Collins, 1996</REF>.	Notice, however, that the tagger used in this two step approach often uses Good-Turing or a similar smoothing method to adjust the observed frequencies of n-grams.	So why not apply Good-Turing directly to the structural units of a stochastic grammar.	2
We calculate the precision, recall, and Fscore; however for brevitys sake we only report the F-score for most experiments in this section.	In addition to antecedent recovery, we also report parsing accuracy, using the bracketing F-Score, the combined measure of PARSEVAL-style labeled bracketing precision and recall <TREF>Magerman, 1995</TREF>.	44 Results The results of the experiments are summarized in Table 3.	UNLEX and LEX refer to the unlexicalized and lexicalized models, respectively.	2
Given sentence-aligned bi-lingual training data, we first use GIZA <REF>Och and Ney, 2003</REF> to generate word level alignment.	We use a statistical CFG parser to parse the English side of the training data, and extract dependency trees with Magermans rules 1995.	Then we use heuristic rules to extract transfer rules recursively based on the GIZA alignment and the target dependency trees.	The rule extraction procedure is as follows.	2
Figure 9 shows that the perfect scheme would achieve roughly 93 precision and recall, which is a dramatic increase over the top 1 accuracy of 87 precision and 86 recall.	Figure 10 shows that the Exact Match, which counts the percentage of times 2Results for SPATTER on section 23 are reported in <REF>Collins, 1996</REF></REF> Parser Precision Maximum Entropy  868 Maximum Entropy 875 <REF>Collins, 1996</REF></REF> 857 <TREF>Magerman, 1995</TREF> 843 Recall 856 863 853 840 Table 5: Results on 2416 sentences of section 23 0 to 100 words in length of the WSJ Treebank.	Evaluations marked with  ignore quotation marks.	Evaluations marked with  collapse the distinction between ADVP and PRT, and ignore all punctuation.	2
The PARSEVAL Black and others, 1991 measures compare a proposed parse P with the corresponding correct treebank parse T as follows:  correct constituents in P Recall   constituents in T  correct constituents in P Precision   constituents in P A constituent in P is correct if there exists a constituent in T of the same label that spans the same words.	Table 5 shows results using the PARSEVAL measures, as well as results using the slightly more forgiving measures of <REF>Collins, 1996</REF> and <TREF>Magerman, 1995</TREF>.	Table 5 shows that the maximum entropy parser performs better than the parsers presented in <REF>Collins, 1996</REF> and <TREF>Magerman, 1995</TREF> , which have the best previously published parsing accuracies on the Wall St Journal domain.	It is often advantageous to produce the top N parses instead of just the top 1, since additional information can be used in a secondary model that reorders the top N and hopefully improves the quality of the top ranked parse.	2
Table 5 shows results using the PARSEVAL measures, as well as results using the slightly more forgiving measures of <REF>Collins, 1996</REF> and <TREF>Magerman, 1995</TREF>.	Table 5 shows that the maximum entropy parser performs better than the parsers presented in <REF>Collins, 1996</REF> and <TREF>Magerman, 1995</TREF> , which have the best previously published parsing accuracies on the Wall St Journal domain.	It is often advantageous to produce the top N parses instead of just the top 1, since additional information can be used in a secondary model that reorders the top N and hopefully improves the quality of the top ranked parse.	Suppose there exists a perfect reranking scheme that, for each sentence, magically picks the best parse from the top N parses produced by the maximum entropy parser, where the best parse has the highest average precision and recall when compared to the treebank parse.	2
The algorithm exploits robust lexical, syntactic, and semantic knowledge sources.	I Introduction The application of decision-based learning techniques over rich sets of linguistic features has improved significantly the coverage and performance of syntactic and to various degrees semantic parsers <REF>Simmons and Yu, 1992</REF>; <TREF>Magerman, 1995</TREF>; <REF>Hermjakob and Mooney, 1997</REF>.	In this paper, we apply a similar paradigm to developing a rhetorical parser that derives the discourse structure of unrestricted texts.	Crucial to our approach is the reliance on a corpus of 90 texts which were manually annotated with discourse trees and the adoption of a shift-reduce parsing model that is well-suited for learning.	2
If for any tuple Ts, Tt, C> such a sequence of actions can be derived, it is then possible to use a corpus of Ts, Tt, C tuples in order to automatically learn to derive from an unseen tree Ts,, which has the same structural properties as the trees Ts, a tree Ttj, which has structural properties similar to those of the trees Tt.	In order to solve the problem in definition 31, we extend the shift-reduce parsing paradigm applied by <TREF>Magerman 1995</TREF>, <REF>Hermjakob and Mooney 1997</REF>, and <REF>MarcH 1999</REF>.	In this extended paradigm, the transfer process starts with an empty Stack and an Input List that contains a sequence of elementary discourse trees edts, one edt for each edu in the tree Ts given as input.	The status and rhetorical relation associated with each edt is undefined.	2
If an erroneous string is extracted, its errors will propagate through the rest of the input :trings.	:3 Our Approach 31 The C45 Learning Algorithm Decision tree induction algorithms have been successfully applied for NLP problems such as sentence boundary dismnbiguation <REF>Pahner et al 1997</REF>, parsing <TREF>Magerman 1995</TREF> and word segmentation <REF>Mekuavin et al 1997</REF>.	We employ the c45 <REF>Quinhln 1993</REF> decision tree induction program as the learning algorithm for word extraction.	The induction algorithm proceeds by evaluating content of a series of attributes and iteratively building a tree fiom the attribute values with the leaves of the decision tree being the value of the goal attribute.	2
By implementing our own version of the publicly available Collins parser <REF>Collins, 1996</REF>, we also learned a dependency model that enables the mapping of parse trees into sets of binary relations between the head-word of each constituent and its sibling-words.	For example, the parse tree of TREC-9 question Q210: How many dogs pull a sled in the Iditarod  is: JJ S Iditarod VP NP PP NP NNPDTINNN NP DTVBPNNS NP manyHow WRB dogs pull a sled in the For each possible constituent in a parse tree, rules first described in <TREF>Magerman, 1995</TREF> and <REF>Jelinek et al , 1994</REF> identify the head-child and propagate the head-word to its parent.	For the parse of question Q210 the propagation is: NP sled DT NN DTIN manyHow WRB dogs NNSJJ NP dogs VBP pull a sled in the Iditarod NNP Iditarod NP Iditarod PP Iditarod NP sled VP pull S pull When the propagation is over, head-modifier relations are extracted, generating the following dependency structure, called question semantic form in <REF>Harabagiu et al , 2000</REF>.	dogs IditarodCOUNT pull sled In the structure above, COUNT represents the expected answer type, replacing the question stem how many.	2
Two new heavyweight algorithms were developed in the last year.	One is a full parser of English, using a statistically learned decision procedure; SPATTER has achieved the highest scores yet reported on parsing English text <TREF>Magerman, 1995</TREF>.	Because the measurable improvement in parsing is so great compared to manually constructed parsers, it appears to offer a qualitatively better parser.	We are looking ormat De scription Message Message Reader I M orphologieal Analyzer  Lexieai Pattern Matcher  Fast Partial Parser  Semantic Interpreter  entene e-Level Pattern Matcher Discourse  Format  S GML Handling Initial Iden tification of Entities Grouping Words into Meaningful Phrases Establish Relationships within sentences Establish Relationships Overall ----Template/Annotation Generator I Output Entities and Relationships  Output Figure 2: PLUM System Architecture: Rectangles represent domain-independent, language-independent algorithms; ovals represent knowledge bases.	2
For example, statistical techniques may have suggested the importance of hire, a verb which many groups did not happen to define.	Second, since there has been a marked improvement in the quality of full parsers, now achieving an F in the high 80s <TREF>Magerman, 1995</TREF>, we believe it is now feasible to consider using full parsers again.	The rationale is straightforward: for full templates eg , ST scores have been mired with an F in the 50s ever since MUC-3 in 1991.	Pattern matching has given us very robust, very portable technology, but has not broken the performance barrier all systems have run up against.	2
B,.	I I  As in the case of the models of <REF>Black 1993</REF>, <TREF>Magerman 1995</TREF>, and <REF>Collins 1996</REF>, this paper proposes a method of utilizing lexical/semantic features for the purpose of applying them to ranking parses in syntactic analysis.	However, unlike the models of <REF>Black 1993</REF>, <TREF>Magerman 1995</TREF>, and <REF>Collins 1996</REF>, we put an assumption that syntactic and lexical/semantie features are independent.	Then, we focus on extracting lexical/semantic collocational knowledge of verbs which is useful in syntactic analysis.	2
In those research, extracted lexical/semantic collocation is especially useful in terms of ranking parses in syntactic analysis as well as automatic construction of lexicon for NLP.	For example, in the context of syntactic disambiguation, <REF>Black 1993</REF> and <TREF>Magerman 1995</TREF> proposed statistical parsing models based-on decision-tree learning techniques, which incorporated not only syntactic but also lexical/semantic information in the decision-trees.	As lexical/semantic information, <REF>Black 1993</REF> used about 50 semantic categories, while <TREF>Magerman 1995</TREF> used lexical forms of words.	<REF>Collins 1996</REF> proposed a statistical parser which is based on probabilities of dependencies between head-words in the parse tree.	2
The key to extraction of the relations is that any phrase can be substituted by the corresponding tree head-word links marked bold in Figure 1.	To determine the tree head-word we used a set of rules similar to that described by <TREF>Magerman, 1995</TREF><REF>Jelinek et al , 1994</REF> and also used by <REF>Collins, 1996</REF>, which we modified in the following way:  The head of a prepositional phrase PP-IN NP was substituted by a function the name of which corresponds to the preposition, and its sole argument corresponds to the head of the noun phrase NP.	The head of a subordinate clause was changed to a function named after the head of the first element in the subordinate clause usually that or a NULL element and its sole argument corresponds to the head of its second element usually head of a sentence.	Because we assumed that the relations within the same phrase are independent, all the relations are between the modifier constituents and the head of a phrase only.	2
When inspecting manually, the binary word tree representation appears to be the most easy to understand.	A second application of the binary word tree can be found in decision-tree based systems such as the SPATTER parser <TREF>Magerman, 1995</TREF> or the ATR Decision-Tree Part-Of-Speech Tagger, as described by Ushioda <REF>Ushioda, 1996</REF>.	In this case it is necessary to use a hard-clustering method, such that a binary word tree can be constructed by the clustering process, as we did in the example in the previous sections.	A decision tree classifies data according to its properties by asking successive often binary questions.	2
The mentioned studies use word-clusters for interpolated n-gram language models.	Another application of hard clustering methods in particular bottom-up variants is that they can also produce a binary tree, which can be used for decision-tree based systems such as the SPATTER parser <TREF>Magerman, 1995</TREF> or the ATR Decision-Tree Part-OfSpeech Tagger <REF>Black et al , 1992</REF>, <REF>Ushioda, 1996</REF>.	Hogenhout  Matsumoto 18 Word Clustering from Syntactic Behavior In this case a decision tree contains binary questions to decide the properties of a word.	We present a hard clustering algorithm, in the sense that every word belongs to exactly one cluster or is one leaf in the binary word-tree of a particular part of speech.	2
These results have important implications for crosslinguistic parsing research, as they allow us to tease apart language-specific and annotationspecific effects.	Previous work for English eg , <TREF>Magerman, 1995</TREF>; <REF>Collins, 1997</REF> has shown that lexicalization leads to a sizable improvement in parsing performance.	English is a language with nonflexible word order and with a treebank with a nonflat annotation scheme see Table 2.	Research on German <REF>Dubey and Keller, 2003</REF> showed that lexicalization leads to no sizable improvement in parsing performance for this language.	2
342 Decision Tree.	Algorithms for decision tree induction <REF>Quinlan 1986</REF>; <REF>Bahl et al 1989</REF> have been successfully applied to NLP problems such as parsing <REF>Resnik 1993</REF>; <TREF>Magerman 1995</TREF> and discourse analysis <REF>Siegel and McKeown 1994</REF>; <REF>Soderland and Lehnert 1994</REF>.	We tested the Satz system using the c45 <REF>Quinlan 1993</REF> decision tree induction program as the learning algorithm and compared the results to those obtained previously with the neural network.	These results are discussed in Section 410.	2
In order to construct the etrees, which make such distinction, LexTract requires its user to provide additional information in the form of three tables: a Head Percolation Table, an Argument Table, and a Tagset Table.	A Head Percolation Table has previously been used in several statistical parsers <TREF>Magerman, 1995</TREF>; <REF>Collins, 1997</REF> to find heads of phrases.	Our strategy for choosing heads is similar to the one in <REF>Collins, 1997</REF>.	An Argument Table informs LexTract what types of arguments a head can take.	2
To start with performance values, Table 3 displays previous results on parsing Section 23 of the WSJ section of the Penn tree-bank.	Comparison indicates that our best model is already better than the early lexicalized model of <TREF>Magerman 1995</TREF>.	It is a bit worse than the unlexicalized PCFGs of <REF>Klein and Manning 2003</REF> and Matsuzaki et al.	2005, and of course, it is also worse than state-of-the-art lexicalized parsers experience shows that evaluation results on sections 22 and 23 do not differ much.	3
We emphasize that our task is to automatically induce a more refined grammar based on a few linguistic principles.	With automatic refinement it is harder to guarantee improved performance than with manual refinements <REF>Klein and Manning, 2003</REF> or with refinements based on direct lexicalization <TREF>Magerman 1995</TREF>, <REF>Collins 1996</REF>, <REF>Charniak 1997</REF>, etc.	If, however, our refinement provides improved performance then it has a clear advantage: it is automatically induced, which suggests that it is applicable across different domains, languages and tree-bank annotations.	Applying our method to the benchmark Penn treebank Wall-Street Journal, we obtain a refined probabilistic grammar that significantly improves over the original tree-bank grammar and that shows performance that is on par with early work on lexicalized probabilistic grammars.	3
2 Notation for context-free grammars The reader is assumed to be familiar with context-free grammars.	Our notation fol1Other relevant parsers simultaneously consider two or more words that are not necessarily in a dependency relationship <REF>Lafferty et al , 1992</REF>; <TREF>Magerman, 1995</TREF>; <REF>Collins and Brooks, 1995</REF>; <REF>Chelba and Jelinek, 1998</REF>.	457 lows <REF>Harrison, 1978</REF>; <REF>Hopcroft and Ullman, 1979</REF>.	A context-free grammar CFG is a tuple G  VN, VT, P, S, where VN and VT are finite, disjoint sets of nonterminal and terminal symbols, respectively, and S E VN is the start symbol.	3
They consider systematically a number of alternative probao bilistic formulations, including those of <REF>Resnik 1992</REF> and <REF>Schabes 1992</REF> and implemented systems based on other underlying grammatical frameworks, evaluating their adequacy from both a theoretical and empirical perspective in terms of their ability to model particular distributions of data that occur in existing treebanks.	<TREF>Magerman 1995</TREF>, <REF>Collins 1996</REF>, <REF>Ratnaparkhi 1997</REF>, <REF>Charniak 1997</REF> and others describe implemented systems with impressive accuracy on parsing unseen data from the Penn Treebank <REF>Marcus, Santorini  Marcinkiewicz, 1993</REF>.	These parsers model probabilistically the strengths of association between heads of phrases, and the configurations in which these lexical associations occur.	The accuracies reported for these systems are substantially better than their non-lexicalised probabilistic context-free grammar analogues, demonstrating clearly the value of lexico-statistical information.	2
In our experiments, the window starts al the sentence prior to that containing the token and extends back W the window size sentences.	The choice to use sentences as the unit of distance is motivated by our intention to incorporale triggers of this form into a probabilistie treebank based parser and tagger, sneh as <REF>Black et al, 1998</REF>; <REF>Black et al, 1997</REF>; <REF>Brill, 1994</REF>; <REF>Collins, 1996</REF>: aelinek et al, 1994; <TREF>Magerman, 1995</TREF>; blatnaparkhi, 1997.	All su<h parsers and taggers of which we are aware use only intrasentential information in predicting parses or tags, and we wish to remove this information, as far as possible, from our results ; The window was not allowed to cross a document bmndary.	The perplexity of lhe task before taking the trigger-pair information into account for tags was 2240 and for rules was 570.	2
Second, one might propagate lexical information upward through the productions.	Examples of formalisms using this approach include the work of <TREF>Magerman 1995</TREF>, <REF>Charniak 1997</REF>, <REF>Collins 1997</REF>, and <REF>Goodman 1997</REF>.	A more linguistically motivated approach is to expand the domain of productions downward to incorporate more tree structures.	The Lexicalized Tree-Adjoining Grammar LTAG formalism <REF>Schabes et al, 1988</REF>, <REF>Schabes, 1990</REF> , although not context-free, is the most well-known instance in this category.	2
Our model uses both lexical and syntactic features for determining the probability of inserting discourse boundaries.	We apply canonical lexical head projection rules <TREF>Magerman, 1995</TREF> in order to lexicalize syntactic trees.	For each word a45, the upper-most node with lexical head a45 which has a right sibling node determines the features on the basis of which we decide whether to insert a discourse boundary.	We denote such node a68a70a69, and the features we use are node a68a71a69, its parent a68a73a72, and the siblings of a68a74a69  In the example in Figure 2, we determine whether to insert a discourse boundary after the word says using as features node a68a75a72a65a43a77a76a6a78a79a21a10a80a33a81a33a82a34a80a33a24 and its children a68 a69 a43a83a76a22a84a86a85a34a21a14a80a33a81a26a82a34a80a32a24 and a68a74a87a88a43a90a89a33a84a92a91a6a93a79a21a95a94a8a96a32a97a6a97a22a24  We use our corpus to estimate the likelihood of inserting a discourse boundary between word a45 and the next word using formula 1, a57a58a21a35a59 a60a45a75a37a40a7a55a24a99a98a101a100 a5a8a7a29a21a35a68a73a72a75a102a103a50a52a50a52a50a40a68a104a69a74a42a27a68 a87 a50a52a50a52a50a38a24 a100 a5a8a7a29a21a35a68 a72 a102a105a50a52a50a4a50a55a68 a69 a68a104a87a62a50a4a50a52a50a38a24 1 where the numerator represents all the counts of the rule a68 a72 a102a105a50a52a50a4a50a40a68 a69 a68a104a87a51a50a52a50a52a50 for which a discourse boundary has been inserted after word a45, and the denominator represents all the counts of the rule.	2
Coming to this problem from the standpoint of tree transformation, we naturally view our work as a descendent of <REF>Johnson 1998</REF> and <REF>Klein and Manning 2003</REF>.	In retrospect, however, there are perhaps even greater similarities to that of <TREF>Magerman, 1995</TREF>; <REF>Henderson, 2003</REF>; <REF>Matsuzaki et al , 2005</REF>.	Consider the approach of Matsuzaki et al.	2005.	2
We would like to infer the number of annotations for each nonterminal automatically.	However, again in retrospect, it is in the work of <TREF>Magerman 1995</TREF> that we see the greatest similarity.	Rather than talking about clustering nodes, as we do, Magerman creates a decision tree, but the differences between clustering and decision trees are small.	Perhaps a more substantial difference is that by not casting his problem as one of learning phrasal categories Magerman loses all of the free PCFG technology that we can leverage.	2
Our experiment quantitatively analyzes several feature types power for syntactic structure prediction and draws a series of interesting conclusions.	In the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types <REF>Black, 1992</REF> <REF>Briscoe, 1993</REF> <REF>Brown, 1991</REF> <REF>Charniak, 1997</REF> <REF>Collins, 1996</REF> <REF>Collins, 1997</REF> <REF>Magerman, 1991</REF> <REF>Magerman, 1992</REF> <TREF>Magerman, 1995</TREF> <REF>Eisner, 1996</REF>.	How to evaluate the different feature types effects for syntactic parsing.	The paper proposes an information-theory-based feature types analysis model, which uses the measures of predictive information quantity, predictive information gain, predictive information redundancy and predictive information summation to quantitatively analyse the different contextual feature types or feature types combinations predictive power for syntactic structure.	2
Unfortunately, the state of knowledge in this regard is very limited.	Many probabilistic evaluation models have been published inspired by one or more of these feature types <REF>Black, 1992</REF> <REF>Briscoe, 1993</REF> <REF>Charniak, 1997</REF> <REF>Collins, 1996</REF> <REF>Collins, 1997</REF> <TREF>Magerman, 1995</TREF> <REF>Eisner, 1996</REF>, but discrepancies between training sets, algorithms, and hardware environments make it difficult, if not impossible, to compare the models objectively.	In the paper, we propose an information-theory-based feature type analysis model by which we can quantitatively analyse the predictive power of different feature types or feature type combinations for syntactic structure in a systematic way.	The conclusion is expected to provide reliable reference for feature type selection in the probabilistic evaluation modelling for statistical syntactic parsing.	3
PF model describes the probability of each feature in feature set FS taking on specific values when a CFG rule A ->  is given.	To make the model more practical in parameter estimation, we assume the features in feature set FS are independent from each other, thus:    FSFi AFiPAFSP ,,  5 Under this PCFGPF model, the goal of a parser is to choose a parse that maximizes the following score: ,maxarg 1 AFS i i i n i T PSTScore    6 Our model is thus a simplification of more sophisticated models which integrate PCFGs with features, such as those in <TREF>Magerman1995</TREF>, <REF>Collins1997</REF> and <REF>Goodman1997</REF>.	Compared with these models, our model is more practical when only small training data is available, since we assume the independence between features.	For example, in Goodmans probabilistic feature grammar PFG, each symbol in a PCFG is replaced by a set of features, so it can describe specific constraints on the rule.	2
14 Note that the humans did not have access to pause information.	Other studies have shown that when both speech and text are available to labelers, segmentation is clearer <REF>Swerts 1995</REF> and reliability improves <TREF>Hirschberg and Nakatani 1996</TREF>.	123 Computational Linguistics Volume 23, Number 1 Table 4 Evaluation for Tj > 4.	Recall Precision Fallout Error Summed Deviation PAUSE 92 18 54 49 193 CUE 72 15 53 50 216 NP 50 31 15 19 153 Humans 74 55 09 11 91 if cue1  true then boundary else nonboundary Figure 11 Cue word algorithm.	2
The types of discourse units being coded and the relations among them vary.	Several studies have used trained coders to locally and globally structure spontaneous or read speech using the model of <REF>Grosz and Sidner 1986</REF>, including <REF>Grosz and Hirschberg 1992</REF>; <REF>Nakatani, Hirschberg, and Grosz 1995</REF>; <REF>Stifleman 1995</REF>; <TREF>Hirschberg and Nakatani 1996</TREF>.	<REF>In Grosz and Hirschberg 1992</REF>, percent agreement see Section 32 among 7 coders on 3 texts under two conditions--text plus speech or text alone--is reported at levels ranging from 743 to 951.	<REF>In Hirschberg and Nakatani 1996</REF>, average reliability measured using the kappa coefficient discussed in Carletta 1996 of segmentinitial labels among 3 coders on 9 monologues produced by the same speaker, labeled using text and speech, is8 or above for both read and spontaneous speech; values of at least 8 are typically viewed as representing high reliability see Section 32.	2
<REF>Wilson and Wiebe 2005</REF> extend this basic annotation scheme to include different types of subjectivity, including Positive Sentiment, Negative Sentiment, Positive Arguing, and Negative Arguing.	Speech was found to improve inter-annotator agreement in discourse segmentation of monologs <TREF>Hirschberg and Nakatani 1996</TREF>.	Acoustic clues have been successfully employed for the reliable detection of the speakers emotions, including frustration, annoyance, anger, happiness, sadness, and boredom <REF>Liscombe et al 2003</REF>.	Devillers et al.	2
61 Reader Judgments There is a growing concern surrounding issues of intercoder reliability when using human judgments to evaluate discourse-processing algorithms <REF>Carletta 1996</REF>; <REF>Condon and Cech 1995</REF>.	Proposals have recently been made for protocols for the collection of human discourse segmentation data <REF>Nakatani et al 1995</REF> and for how to evaluate the validity of judgments so obtained <REF>Carletta 1996</REF>; <REF>Isard and Carletta 1995</REF>; Ros6 1995; <REF>Passonneau and Litman 1993</REF>; <REF>Litman and Passonneau 1995</REF>.	Recently, Hirschberg 52 <REF>Hearst TextTiling and Nakatani 1996</REF> have reported promising results for obtaining higher interjudge agreement using their collection protocols.	For the evaluation of the TextTiling algorithms, judgments were obtained from seven readers for each of 12 magazine articles that satisfied the length criteria between 1,800 and 2,500 words 9 and that contained little structural demarcation.	2
Proposals have recently been made for protocols for the collection of human discourse segmentation data <REF>Nakatani et al 1995</REF> and for how to evaluate the validity of judgments so obtained <REF>Carletta 1996</REF>; <REF>Isard and Carletta 1995</REF>; Ros6 1995; <REF>Passonneau and Litman 1993</REF>; <REF>Litman and Passonneau 1995</REF>.	Recently, Hirschberg 52 <REF>Hearst TextTiling and Nakatani 1996</REF> have reported promising results for obtaining higher interjudge agreement using their collection protocols.	For the evaluation of the TextTiling algorithms, judgments were obtained from seven readers for each of 12 magazine articles that satisfied the length criteria between 1,800 and 2,500 words 9 and that contained little structural demarcation.	The judges were asked simply to mark the paragraph boundaries at which the topic changed; they were not given more explicit instructions about the granularity of the segmentation.	2
We should expect to see, in grouping together paragraph-sized units instead of utterances, a decrease in the complexity of the feature set and algorithm needed.	The work described here makes use only of lexical distribution information, in lieu of prosodic cues such as intonational pitch, pause, and duration <TREF>Hirschberg and Nakatani 1996</TREF>, discourse markers such as oh, well, ok, however <REF>Schiffrin 1987</REF>; <REF>Litman and Passonneau 1995</REF>, pronoun reference resolution <REF>Passonneau and Litman 1993</REF>; <REF>Webber 1988</REF> and tense and aspect <REF>Webber 1987</REF>; <REF>Hwang and Schubert 1992</REF>.	From a computational viewpoint, deducing textual topic structure from lexical occurrence information alone is appealing, both because it is easy to compute, and because discourse cues are sometimes misleading with respect to the topic structure <REF>Brown and Yule 1983</REF>, Section 3.	4.	3
Metric F NM S noNM p  user turns 218 53 228 65 065  correct turns 72 18 67 22 059 AsrMis 37 27 46 28 046 SemMis 5 6 12 14 009 Table 2.	Average standard deviation for objective metrics in the first problem 6 Related work Discourse structure has been successfully used in non-interactive settings eg understanding specific lexical and prosodic phenomena <TREF>Hirschberg and Nakatani, 1996</TREF>, natural language generation <REF>Hovy, 1993</REF>, essay scoring <REF>Higgins et al , 2004</REF> as well as in interactive settings eg predictive/generative models of postural shifts <REF>Cassell et al , 2001</REF>, generation/interpretation of anaphoric expressions <REF>Allen et al , 2001</REF>, performance modeling <REF>Rotaru and Litman, 2006</REF>.	In this paper, we study the utility of the discourse structure on the user side of a dialogue system.	One related study is that of <REF>Rich and Sidner, 1998</REF>.	2
The main drawback of this corpus is that it comprises only read speech.	Prosody labeling on spontaneous speech corpora like Boston Directions corpus BDC, Switchboard SWBD has garnered attention in <TREF>Hirschberg and Nakatani, 1996</TREF>; <REF>Gregory and Altun, 2004</REF>.	Automatic prosody labeling has been achieved through various machine learning techniques, such as decision trees <REF>Hirschberg, 1993</REF>; <REF>Wightman and Ostendorf, 1994</REF>; <REF>Ma et al , 2003</REF>, rule-based systems <REF>Shimei and McKeown, 1999</REF>, bagging and boosting on CART <REF>Sun, 2002</REF>, hidden markov models <REF>Conkie et al , 1999</REF>, neural networks Hasegawa-<REF>Johnson et al , 2005</REF>,maximum-entropy models <REF>Brenier et al , 2005</REF> and conditional random fields <REF>Gregory and Altun, 2004</REF>.	Prosody labeling of the BU corpus has been reported in many studies <REF>Hirschberg, 1993</REF>; <REF>HasegawaJohnson et al , 2005</REF>; <REF>Ananthakrishnan and Narayanan, 2005</REF>.	2
For example, in Figure 1, if the student would have answered Tutor 2 correctly, the next tutor turn would have had the same content as Tutor 5 but the Advance label.	Also, while a human annotation of the discourse structure will be more complex but more time consuming <TREF>Hirschberg and Nakatani, 1996</TREF>; <REF>Levow, 2004</REF>, its advantages are outweighed by the automatic nature of our discourse structure annotation.	We would like to highlight that our transition annotation is domain independent and automatic.	Our transition labels capture behavior like starting a new dialogue NewTopLevel, crossing discourse segment boundaries Push, PopUp, PopUpAdv and local phenomena inside a discourse segment Advance, SameGoal.	2
Information status has generated large interest among researchers because of its complex interaction with other linguistic phenomena, thus affecting several Natural Language Processing tasks.	Since it correlates with word order and pitch accent <REF>Lambrecht, 1994</REF>; <TREF>Hirschberg and Nakatani, 1996</TREF>, for instance, incorporating knowledge on information status would be helpful for natural language generation, and in particular text-tospeech systems.	Stober and colleagues, for example, ascribe to the lack of such information the lower performance of text-to-speech compared to concept-to-speech generation, where such knowledge could be made directly available to the system <REF>Stober et al , 2000</REF>.	Another area where information status can play an important role is anaphora resolution.	2
Nine coders provided IU trees starting from identical CGUs.	Following the methodology in ttirschberg and <REF>Nakatani, 1996</REF>, we measured the reliability of coding for a linearized version of the IU tree, by calculating the reliability of coding of IU beginnings using the kappa metric.	We calculated the observed pairwise agreement of CGUs marked as the beginnings of IUs, and factored out the expected agreement estimated from the actual data, giving the pairwise kappa score.	Table 3 gives the raw data on coders marking of IU beginnings.	2
<REF>Rotondo 1984</REF> reported that hierarchical segmentation is impractical for naive subjects in discourses longer than 200 words <REF>Passonneau and Litman 1993</REF> conducted a pilot study in which subjects found it difficult and time-consuming to identify hierarchical relations in discourse.	Other attempts have had more success using improved annotation tools and more precise instructions <REF>Grosz and Hirschberg, 1992</REF>; <TREF>Hirschberg and Nakatani, 1996</TREF>.	Second, hierarchical segmentation of discourse is subjective.	While agreement among annotators regarding linear segmentation has been found to be higher than 80 <REF>Hearst, 1997</REF>, with respect to hierarchical segmentation it has been observed to be as low as 60 <REF>Flammia and Zue, 1995</REF>.	2
Let a28a30a40a30a11a14a28a21a40a30a28a16a31a36a3a41a34 be the set of senses of a3  For each sense of a3 a3a42a28a44a43a46a45a47a28a30a40a30a11a14a28a21a40a30a28a16a31a36a3a41a34  we obtain a ranking score by summing over the a27a48a28a21a28a16a31a32a3a6a15a33a11a50a49a30a34 of each neighbour a11a50a49a51a45a52a4a6a5  multiplied by a weight.	This weight is the WordNet similarity score a3a42a11a14a28a30a28  between the target sense a3a53a28a35a43  and the sense of a11a54a49 a11a14a28a35a55a56a45a57a28a30a40a30a11a14a28a21a40a30a28a16a31a36a11a54a49a16a34  that maximises this score, divided by the sum of all such WordNet similarity scores for a28a30a40a21a11a19a28a21a40a30a28a26a31a32a3a41a34 and a11a50a49  Thus we rank each sense a3a42a28 a43 a45a10a28a30a40a21a11a19a28a21a40a30a28a26a31a32a3a41a34 using:a58a41a59 a11a14a2a54a60a61a11a50a62a64a63a66a65a35a67a16a68a12a40a26a31a32a3a53a28 a43 a34a69a7 a70 a71a44a72a33a73a16a74a76a75 a27a48a28a21a28a16a31a32a3a6a15a33a11a50a49a30a34a30a77 a3a42a11a14a28a21a28a16a31a32a3a53a28a35a43a36a15a33a11a50a49a30a34 a78 a5a19a79a81a80a39a82 a73 a79a61a83 a71 a79a81a83a61a79a36a84a85a5a19a86 a3a53a11a14a28a30a28a26a31a32a3a42a28 a43 a82 a15a17a11 a49 a34 1 where: a3a42a11a14a28a21a28a16a31a32a3a53a28a35a43a87a15a17a11a54a49a21a34a66a7 a88a46a89a21a90 a71 a79a92a91 a73 a79a81a83 a71 a79a81a83a61a79a93a84 a71a44a72 a86 a31a36a3a42a11a14a28a30a28a26a31a32a3a53a28a35a43a36a15a33a11a14a28a35a55a29a34a87a34 22 Acquiring the Automatic Thesaurus There are many alternative distributional similarity measures proposed in the literature, for this work we used the measure and thesaurus construction method described by <TREF>Lin 1998</TREF>.	For input we used grammatical relation data extracted using an automatic parser <REF>Briscoe and Carroll, 2002</REF>.	For each noun we considered the co-occurring verbs in the direct object and subject relation, the modifying nouns in noun-noun relations and the modifying adjectives in adjective-noun relations.	2
Importantly, inter-dependence between links can still be accommodated by exploiting dynamic features in training features that take into account the labels of some of the surrounding components when predicting the label of a target component.	To cope with the sparse data problem, I use distributional word similarity <REF>Pereira et al , 1993</REF>; <REF>Grefenstette, 1994</REF>; <TREF>Lin, 1998</TREF> to generalize the observed frequency counts in the training corpus.	The experimental results on the Chinese Treebank 40 show that the accuracy of the conditional model is 136 higher than corresponding joint models, while similarity smoothing also allows the strictly lexicalised approach to outperform corresponding models based on part-of-speech tags.	4 Extensions to Large Margin Parsing The approach presented above has a limitation: it uses a local scoring function instead of a global scoring function to compute the score for a candidate tree.	2
Realword spelling correction is also referred to as context sensitive spelling correction, which tries to detect incorrect usage of valid words in certain contexts <REF>Golding and Roth, 1996</REF>; <REF>Mangu and Brill, 1997</REF>.	Distributional similarity between words has been investigated and successfully applied in many natural language tasks such as automatic semantic knowledge acquisition <TREF>Dekang Lin, 1998</TREF> and language model smoothing <REF>Essen and Steinbiss, 1992</REF>; <REF>Dagan et al , 1997</REF>.	An investigation on distributional similarity functions can be found in <REF>Lillian Lee, 1999</REF>.	3 Distributional Similarity-Based Models for Query Spelling Correction 31 Motivation Most of the previous work on spelling correction concentrates on the problem of designing better error models based on properties of character strings.	2
The formula is described in Equation 12.	,,, , 212 2 1 2 2 1 1 1 21 relrelsimeesimeesim eesim colcol  12 where ,, 21 iiiicol erelee   We assume that the relation type keeps the same, so 1, 21 relrelsim  The similarity of the words is calculated with the same method as described in <TREF>Lin, 1998</TREF>, which is rewritten in Equation 13.	The similarity of the words is calculated through the surrounding context words which have dependency relationships with the investigated words.	,,,, ,,,, , 2 2, 1 1, 21 21, 21 erelewerelew erelewerelew eeSim eTereleTerel eTeTerel  .	2
Up to now, there have been few researches which directly address the problem of extracting synonymous collocations.	However, a number of studies investigate the extraction of synonymous words from monolingual corpora <REF>Carolyn et al , 1992</REF>; <REF>Grefenstatte, 1994</REF>; <TREF>Lin, 1998</TREF>; <REF>Gasperin et al , 2001</REF>.	The methods used the contexts around the investigated words to discover synonyms.	The problem of the methods is that the precision of the extracted synonymous words is low because it extracts many word pairs such as cat and dog, which are similar but not synonymous.	3
In this light, the contributions of this paper are fourfold.	First, instead of separately addressing the tasks of collecting unlabeled sets of instances <TREF>Lin, 1998</TREF>, assigning appropriate class labels to a given set of instances <REF>Pantel and Ravichandran, 2004</REF>, and identifying relevant attributes for a given set of classes <REF>Pasca, 2007</REF>, our integrated method from Section 2 enables the simultaneous extraction of class instances, associated labels and attributes.	Second, by exploiting the contents of query logs during the extraction of labeled classes of instances from Web documents, we acquire thousands 4,583, to be exact of open-domain classes covering a wide range of topics and domains.	The accuracy reported in Section 32 exceeds 80 for both instance sets and class labels, although the extraction of classes requires a remarkably small amount of supervision, in the form of only a few commonly-used Is-A extraction patterns.	2
There have been many approachs to automatic detection of similar words from text.	Our method is similar to <REF>Hindle, 1990</REF>, <TREF>Lin, 1998</TREF>, and <REF>Gasperin, 2001</REF> in the use of dependency relationships as the word features.	Another approach used the words distribution to cluster the words <REF>Pereira, 1993</REF>, and Inoue <REF>Inoue, 1991</REF> also used the word distributional information in the Japanese-English word pairs to resolve the polysemous word problem.	Wu <REF>Wu, 2003</REF> shows one approach to collect synonymous collocation by using translation information.	2
The WMTS is well suited to LRA, because the WMTS scales well to large corpora one terabyte, in our case, it gives exact frequency counts unlike most Web search engines, it is designed for passage retrieval rather than document retrieval, and it has a powerful query syntax.	53 Thesaurus As a source of synonyms, we use <TREF>Lins 1998a</TREF> automatically generated thesaurus.	This thesaurus is available through an on-line interactive demonstration or it can be downloaded.	5 We used the on-line demonstration, since the downloadable version seems to contain fewer words.	2
The LRA algorithm consists of the following 12 steps: 1.	Find alternates: For each word pair A:B in the input set, look in <TREF>Lins 1998a</TREF> thesaurus for the top num sim words in the following experiments, num sim is 10 that are most similar to A For each A prime that is similar to A, make a new word pair A prime :B Likewise, look for the top num sim words that are most similar to B, and for each B prime , make a new word pair A:B prime  A:B is called the original pair and each A prime :B or A:B prime is an alternate pair.	The intent is that alternates should have almost the same semantic relations as the original.	For each input pair, there will now be 2  num sim alternate pairs.	2
For each input pair, there will now be 2  num sim alternate pairs.	When looking for similar words in <TREF>Lins 1998a</TREF> thesaurus, avoid words that seem unusual eg, hyphenated words, words with three characters or less, words with non-alphabetical characters, multiword phrases, and capitalized words.	The first column in Table 7 shows the alternate pairs that are generated for the original pair quart:volume.	2.	2
As a courtesy to other users of Lins on-line system, we insert a 20-second delay between each two queries.	Lins thesaurus was generated by parsing a corpus of about 5  10 7 English words, consisting of text from the Wall Street Journal, San Jose Mercury,andAP Newswire <TREF>Lin 1998a</TREF>.	The parser was used to extract pairs of words and their grammatical relations.	Words were then clustered into synonym sets, based on the similarity of their grammatical relations.	2
The first word class in the sequence, CL1, consists of words such as was, is, could, whereas the second class includes February, April, June, Aug , November and other similar words.	The classes of words are computed on the fly over all sequences of terms in the extracted patterns, on top of a large set of pairwise similarities among words <TREF>Lin, 1998</TREF> extracted in advance from around 50 million news articles indexed by the Google search engine over three years.	All digits in both patterns and sentences are replaced with a common marker, such 810 that any two numerical values with the same number of digits will overlap during matching.	Many methods have been proposed to compute distributional similarity between words, eg, <REF>Hindle, 1990</REF>, <REF>Pereira et al , 1993</REF>, <REF>Grefenstette, 1994</REF> and <TREF>Lin, 1998</TREF>.	2
Ageneralproblemforsuchclusteringtechniquesliesinthequestionofhowmany clustersoneshouldhave,ie howmanysenses areappropriateforaparticularwordinagiven domainManningandSch utze,1999,Ch14.	LinsapproachtothisproblemLin,1998is tobuildasimilaritytreeusingwhatisineffectahierarchicalclusteringmethodofwords relatedtoatargetwordinthiscasetheword dutyDi erentsensesofdutycanbediscerned asdi erentsub-treesofthissimilaritytreeWe presentanewmethodforword-sensediscriminationinSection6.	3 Building a Graph from a PoS-tagged Corpus Inthissectionwedescribehowagrapha collection of nodesand links  was built to representtherelationshipsbetweennounsThe modelwasbuiltusingtheBritishNationalCorpuswhichisautomaticallytaggedforpartsof speech.	Initially,grammaticalrelationsbetweenpairs ofwordswereextracted.	2
538 ture for NPi whose value is the most likely NE type.	7 NEIGHBOR: Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs see <TREF>Lin 1998a</TREF>.	Motivated by this observation, we create for each of NPis ten most semantically similar NPs a NEIGHBOR feature whose value is the surface string of the NP.	To determine the ten nearest neighbors, we use the semantic similarity values provided by Lins dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic de nition of similarity.	2
An example extraction would be <Eastern Airlines, the carrier>, where the rst entry is a proper noun labeled with either one of the seven MUC-style NE types4 or OTHERS5 and the second entry is a common noun.	We then infer the SC of a common noun as follows: 1 we compute the probability that the common noun co-occurs with each of the eight NE types6 based on the extracted appositive relations, and 2 if the most likely NE type has a co-occurrence probability above a certain threshold we set it to 07, we create a INDUCED CLASS fea1This is motivated by <TREF>Lins 1998c</TREF> observation that a coreference resolver that employs only the rst WordNet sense performs slightly better than one that employs more than one sense.	2The keywords are obtained via our experimentation with WordNet and the ACE SCs of the NPs in the ACE training data.	3We used 1 the BLLIP corpus 30M words, which consists of WSJ articles from 1987 to 1989, and 2 the Reuters Corpus 37GB data, which has 806,791 Reuters articles.	2
2 SUBJ VERB: If NPi is involved in a subjectverb relation, we create a SUBJ VERB feature whose value is the verb participating in the relation.	We use <TREF>Lins 1998b</TREF> MINIPAR dependency parser to extract grammatical relations.	Our motivation here is to coarsely model subcategorization.	3 VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NPi participates in a verb-object relation.	2
 6615 5779 913 172 3927 1551 Table 3: Coverage of verb association features by grammar/window resources.	were considered as verb features, such as <TREF>Lin 1998</TREF> and McCarthy et al.	2003.	Of the adverb associations, we nd only a small proportion among the parsed adverbs.	2
pointwise mutual information <REF>Church and Hanks, 1990</REF>, 3.	least mutual information difference with similar collocations, based on <REF>Lin, 1999</REF> and using Lins thesaurus <TREF>Lin, 1998</TREF> for obtaining the similar collocations.	4.	The distributed frequency of an object, which takes an average of the frequency of occurrence with an object over all verbs occurring with the object above a threshold.	2
We used three different types of probabilistic models, which vary in the classes selected for representation over which the probability distribution of the argument heads 2 is estimated.	Two use WordNet and the other uses the entries in a thesaurus of distributionally similar words acquired automatically following <TREF>Lin, 1998</TREF>.	The rst method is due to <REF>Li and Abe 1998</REF>.	The classes over which the probability distribution is calculated are selected according to the minimum description length principle MDL which uses the argument head tokens for nding the best classes for representation.	2
We cannot show the full WNPROTO due to lack of space, but we show some of the classes with higher probability which cover some typical nouns that occur as objects of park.	373 Algorithm 1 WNPROTO algorithm C  classes in WNPROTO D   disambiguated ty  TY  fD  0 frequency of disambiguated items TY  argument head types nouns occurring as objects of verb, with associated frequencies C1  WordNet where ty  TY occurring in c  C1 > 1 for all ty  TY do nd c  classesty  C1 where c  argmaxc typeratioc if c  c / C then add c to C add ty  c to D Disambiguated ty with c end if end for for all c  C do if ty  c  D > 1 then fD  fD  frequencytysum frequencies of types under classes to be used in model else remove c from C classes with less than two disambiguated nouns are removed end if end for for all c  C do pc  frequency-of-all-tys-disambiguated-to-classc,DfD calculating class probabilities end for Algorithm 2 DSPROTO algorithm C  classes in DSPROTO D   disambiguated ty  TY  fD  0 frequency of disambiguated items TY  argument head types nouns occurring as objects of verb, with associated frequencies C1  cty  TY where num-types-in-thesauruscty,TY  > 1 order C1 by num-types-in-thesauruscty,TY  classes ordered by coverage of argument head types for all cty  ordered C1 do Dcty   disambiguated for this class for all ty  TY where in-thesaurus-entrycty,ty do if ty / D then add ty to Dcty types disambiguated to this class only if not disambiguated by a class used already end if end for if Dcty > 1 then add cty to C for all ty  Dcty do add ty  cty to D Disambiguated ty with cty fD  fD  frequencyty end for end if end for for all cty  C do pcty  frequency-of-all-tys-disambiguated-to-classcty,DfD calculating class probabilities end for 374 33 DSPROTOs We use a thesaurus acquired using the method proposed by <TREF>Lin 1998</TREF>.	For input we used the grammatical relation data from automatic parses of the BNC.	For each noun we considered the cooccurring verbs in the object and subject relation, the modifying nouns in noun-noun relations and the modifying adjectives in adjective-noun relations.	2
One way to generalise the query is by using similarity classes, ie groups of words with lexically similar behaviour.	In his work on distributional similarity <TREF>Lin, 1998</TREF> designed a parser to identify grammatical relationships between words.	However, broad-coverage parsers suitable for processing BNC-like corpora are not available for many languages.	Another, resource-light approach treats the context as a bag of words BoW and detects the similarity of contexts on the basis of collocations in a window of a certain size, typically 3-4 words, eg.	3
Similarity and association measures can help for the cases of near-synonymy.	However, while similarity measures such as WordNet distance or Lins similarity metric only detect cases of semantic similarity, association measures such as the ones used by Poesio et al , or by Garera and Yarowsky also find cases of associative bridg497 Lin98 RFF TheY TheY:G2 PL03 Land country/state/land Staat Staat Kemalismus Regierung Kontinent state state Kemalism government continent Stadt Stadt Bauernfamilie Prasident Region city city agricultural family president region Region Landesregierung Bankgesellschaft Dollar Stadt region country government banking corporation dollar city Bundesrepublik Bundesregierung Baht Albanien Staat federal republic federal government Baht Albania state Republik Gewerkschaft Gasag Hauptstadt Bundesland republic trade union a gas company capital state Medikament medical drug Arzneimittel Pille RU Patient Arzneimittel pharmaceutical pill a drug patient pharmaceutical Praparat Droge Abtreibungspille Arzt Lebensmittel preparation drug non-medical abortion pill doctor foodstuff Pille Praparat Viagra Pille Praparat pill preparation Viagra pill preparation Hormon Pestizid Pharmakonzern Behandlung Behandlung hormone pesticide pharmaceutical company treatment treatment Lebensmittel Lebensmittel Praparat Abtreibungspille Arznei foodstuff foodstuff preparation abortion pill drug highest ranked words, with very rare words removed : RU 486, an abortifacient drug Lin98: Lins distributional similarity measure <TREF>Lin, 1998</TREF> RFF: Geffet and Dagans Relative Feature Focus measure <REF>Geffet and Dagan, 2004</REF> TheY: association measure introduced by <REF>Garera and Yarowsky 2006</REF> TheY:G2: similar method using a log-likelihood-based statistic see <REF>Dunning 1993</REF> this statistic has a preference for higher-frequency terms PL03: semantic space association measure proposed by <REF>Pado and Lapata 2003</REF> Table 1: Similarity and association measures: most similar items ing like 1a,b; the result of this can be seen in table 2: while the similarity measures Lin98, RFF list substitutable terms which behave like synonyms in many contexts, the association measures Garera and Yarowskys TheY measure, Pado and Lapatas association measure also find non-compatible associations such as countrycapital or drugtreatment, which is why they are commonly called relationfree.	For the purpose of coreference resolution, however we do not want to resolve the door to the antecedent the house as the two descriptions do not corefer, and it may be useful to filter out non-similar associations.	12 Information Sources Different resources may be differently suited for the recognition of the various relations.	2
While none of the information sources can match the precision of the hypernymy information encoded in GermaNet, or that of using a combination of high-precision patterns with the World Wide Web as a very large corpus, it is possible to achieve a considerable improvement in terms of recall without sacrificing too much precision by combining these methods.	Very interestingly, the distributional methods based on intra-sentence relations <TREF>Lin, 1998</TREF>; <REF>Pado and Lapata, 2003</REF> outperformed <REF>Garera and Yarowskys 2006</REF> association measure when used for ranking, which may due to sparse data problems or simply too much noise for the latter.	For the association measures, the fact that they are relation-free also means that they can profit from added semantic filtering.	The novel distance-bounded semantic similarity method where we use the most similar words in the previous discourse together with a semantic classbased filter and a distance limit comes near the precision of using surface patterns, and offers better accuracy than Gasperin and Vieiras method of using the globally most similar words.	2
Note, however, that <REF>McCarthy et al , 2004</REF> used the information about distributionally similar words to approximate corpus frequencies for word senses, whereas we target the estimation of a property of a given word sense the subjectivity.	Starting with a given ambiguous word w, we first find the distributionally similar words using the method of <TREF>Lin, 1998</TREF> applied to the automatically parsed texts of the British National Corpus.	Let DSW  dsw1, dsw2,  , dswn be the list of top-ranked distributionally similar words, sorted in decreasing order of their similarity.	Next, for each sense wsi of the word w, we determine the similarity with each of the words in the list DSW, using a WordNet-based measure of semantic similarity wnss.	2
41 Weight Tuning There are several motivations for learning the graph weights  in this domain.	First, some dependency relations  foremost, subject and object  are in general more salient than others <TREF>Lin, 1998</TREF>; <REF>Pado and Lapata, 2007</REF>.	In addition, dependency relations may have varying importance per different notions of word similarity eg, noun vs verb similarity <REF>Resnik and Diab, 2000</REF>.	Weight tuning allows the adaption of edge weights to each task ie, distribution of queries.	2
The learning methods described in this paper can be readily applied to 911 other directed and labelled entity-relation graphs7 The graph representation described in this paper is perhaps most related to syntax-based vector space models, which derive a notion of semantic similarity from statistics associated with a parsed corpus <REF>Grefenstette, 1994</REF>; <TREF>Lin, 1998</TREF>; <REF>Pado and Lapata, 2007</REF>.	In most cases, these models construct vectors to represent each word wi, where each element in the vector for wi corresponds to particular context c, and represents a count or an indication of whether wi occurred in context c A context can refer to simple co-occurrence with another word wj, to a particular syntactic relation to another word eg, a relation of direct object to wj, etc Given these word vectors, inter-word similarity is evaluated using some appropriate similarity measure for the vector space, such as cosine vector similarity, or Lins similarity <TREF>Lin, 1998</TREF>.	Recently, Pado and Lapata <REF>Pado and Lapata, 2007</REF> have suggested an extended syntactic vector space model called dependency vectors, in which rather than simple counts, the components of a word vector of contexts consist of weighted scores, which combine both co-occurrence frequency and the importance of a context, based on properties of the connecting dependency paths.	They considered two different weighting schemes: a length weighting scheme, assigning lower weight to longer connecting paths; and an obliqueness weighting hierarchy <REF>Keenan and Comrie, 1977</REF>, assigning higher weight to paths that include grammatically salient relations.	2
Instead, we include learning techniques to optimize the graphwalk based similarity measure.	The learning methods described in this paper can be readily applied to 911 other directed and labelled entity-relation graphs7 The graph representation described in this paper is perhaps most related to syntax-based vector space models, which derive a notion of semantic similarity from statistics associated with a parsed corpus <REF>Grefenstette, 1994</REF>; <TREF>Lin, 1998</TREF>; <REF>Pado and Lapata, 2007</REF>.	In most cases, these models construct vectors to represent each word wi, where each element in the vector for wi corresponds to particular context c, and represents a count or an indication of whether wi occurred in context c A context can refer to simple co-occurrence with another word wj, to a particular syntactic relation to another word eg, a relation of direct object to wj, etc Given these word vectors, inter-word similarity is evaluated using some appropriate similarity measure for the vector space, such as cosine vector similarity, or Lins similarity <TREF>Lin, 1998</TREF>.	Recently, Pado and Lapata <REF>Pado and Lapata, 2007</REF> have suggested an extended syntactic vector space model called dependency vectors, in which rather than simple counts, the components of a word vector of contexts consist of weighted scores, which combine both co-occurrence frequency and the importance of a context, based on properties of the connecting dependency paths.	2
To further enhance the quality of co-occurrence data, we search on the specific phrase a16 is measured in in which a16 is one of the related concepts of a14  This allows for the simultaneous discovery of unknown units and the retrieval of their co-occurrence counts.	Sentences in which the pattern occurs are parsed using Minipar <TREF>Lin, 1998b</TREF> so that we can obtain the word related to measured via the prepositional in relation.	This allows us to handle sentential constructions that may intervene between measured and a meaningful unit.	For each unit a17 that is related to measured via in, we increment the co-occurrence count a18a20a19a21a17a23a22a24a16a26a25, thereby collecting frequency counts for each a17 with a16  The patterns precision prevents incidental cooccurrence between a related concept and some unit that may occur simply because of the general topic of the document.	2
Thus, there is strong motivation to expand the list of units obtained from Google by automatically considering similar units.	519 We gather similar units from an automaticallyconstructed thesaurus of distributionally similar words <TREF>Lin, 1998a</TREF>.	The similar word expansion can add a term like gigs as a unit for size by virtue of its association with gigabytes, which is on the original list.	Unit similarity can be thought of as a mapping a18 a1 a6 a4 a0 a8 in which a6 is a set of units and a0 a8 is sets of related units.	2
CLUSTER CLUSTERED SIMILAR WORDS OF DUTY WITH SIMILARITY SCORE responsibility 016, obligation 0109, task 0101, function 0098, role 0091, post 0087, position 0086, job 0084, chore 008, mission 008, assignment 0079, liability 0077  tariff0091, restriction 0089, tax 0086, regulation 0085, requirement 0081, procedure 0079, penalty 0079, quota 0074, rule 007, levy 0061  fee 0085, salary 0081, pay 0064, fine 0058 personnel 0073, staff0073 training 0072, work 0064, exercise 0061 privilege 0069, right 0057, license 0056 22.	Corpus-based thesaurus Using the collocation database, Lin used an unsupervised method to construct a corpusbased thesaurus <TREF>Lin, 1998a</TREF> consisting of 11839 nouns, 3639 verbs and 5658 adjectives/adverbs.	Given a word w, the thesaurus returns a clustered list of similar words of w along with their similarity to w For example, the clustered similar words of duty are shown in Table 1.	23.	2
2.	Resources The input to our algorithm includes a collocation database <TREF>Lin, 1998b</TREF> and a corpus-based thesaurus <TREF>Lin, 1998a</TREF>, which are both available on the Interne0.	In addition, we require a bilingual thesaurus.	Below, we briefly describe these resources.	2
78 Table 1.	Clustered similar words of duty as given by <TREF>Lin, 1998a</TREF>.	CLUSTER CLUSTERED SIMILAR WORDS OF DUTY WITH SIMILARITY SCORE responsibility 016, obligation 0109, task 0101, function 0098, role 0091, post 0087, position 0086, job 0084, chore 008, mission 008, assignment 0079, liability 0077  tariff0091, restriction 0089, tax 0086, regulation 0085, requirement 0081, procedure 0079, penalty 0079, quota 0074, rule 007, levy 0061  fee 0085, salary 0081, pay 0064, fine 0058 personnel 0073, staff0073 training 0072, work 0064, exercise 0061 privilege 0069, right 0057, license 0056 22.	Corpus-based thesaurus Using the collocation database, Lin used an unsupervised method to construct a corpusbased thesaurus <TREF>Lin, 1998a</TREF> consisting of 11839 nouns, 3639 verbs and 5658 adjectives/adverbs.	2
It then assigns a score of 1 if the text contains a synonym, hyponym or derived form of the target word and a score of 0 otherwise.	42 Similarity As a second measure we used the distributional similarity measure of <TREF>Lin, 1998</TREF>.	For a text t and a word u we assign the max similarity score as follows: similarityt,u  maxvt simu,v 1 where simu,v is the similarity score for u and v4.	43 Alignment model <REF>Glickman et al , 2006</REF> was among the top scoring systems on the RTE-1 challenge and supplies a probabilistically motivated lexical measure based on word co-occurrence statistics.	2
We are going to extend the set of content bearing words and to include verbs.	We will take advantage of the flexibility provided by our framework and use syntax based measure of similarity in the computation of the verb vectors, following <TREF>Lin, 1998</TREF>.	Currently we are using string matching to compute the named entity based measure of similarity.	We are planning to integrate more sophisticated techniques in our framework.	2
2 Distributional Features In this section, we firstly describe how we extract contexts from corpora and then how distributional features are constructed for word pairs.	21 Context Extraction We adopted dependency structure as the context of words since it is the most widely used and wellperforming contextual information in the past studies <REF>Ruge, 1997</REF>; <TREF>Lin, 1998</TREF>.	In this paper the sophisticated parser RASP Toolkit 2 <REF>Briscoe et al, 2006</REF> was utilized to extract this kind of word relations.	We use the following example for illustration purposes: The library has a large collection of classic books by such authors as Herrick and Shakespeare.	2
Before explaining the following process Clustering 2, let us describe the measure used to calculate the similarity between syntactic positions.	We use a particular weighted version of the <TREF>Lin 1998</TREF> coefficient.	Our version, however, does not use pointwise mutual information to characterize the weight on position-word pairs.	As Manning and Schu tze 1999 argued, this does not seem to be a good measure of the strength of association between a word and a local position.	2
Inspired by <REF>Lin1999</REF>, weexamine the strength of association between the verb and noun constituents of the target combination and its variants, as an indirect cue to their idiomaticity.	We use the automatically-built thesaurus of <TREF>Lin 1998</TREF> to find similar words to the noun of the target expression, in order to automatically generate variants.	Only the noun constituent is varied, since replacing the verb constituent of a VNIC with a semantically related verb is more likely to yield another VNIC, as in keep/lose ones cool <REF>Nunberg et al , 1994</REF>.	Let a0a2a1a4a3a6a5a8a7a10a9a12a11a14a13a16a15a17a9a18a5a20a19a22a21a24a23a26a25a27a23a29a28a31a30 be the set of the a28 most similar nouns to the noun a9 of the target pair a32a34a33a36a35 a9a38a37.	2
4 Representing Syntactic Information Since both the Class-Word and the Class-Example methods work with syntactic features, the main source of information is a syntactically parsed corpus.	We parsed about half a gigabyte of a news corpus with MiniPar <TREF>Lin, 1998b</TREF>.	It is a statistically based dependency parser which is reported to reach 89 precision and 82 recall on press reportage texts.	MiniPar generates syntactic dependency structures directed labeled graphs whose 20 g1 g2 SyntNetg1,g2 loves1 s d15d15 o d37d37d74d74d74 d74d74d74d74 d74d74d74d74 loves4 o d47d47 s d15d15 Jane6 loves1,4 1,24,5 d15d15 4,6 o d47d47 1,3 o d42d42d84d84d84d84d84d84d84d84d84 d84d84d84d84d84d84 d84d84 Jane6 John2 Mary3 John5 John2,5 Mary3 Figure 2: Two syntactic graphs and their Syntactic Network.	2
Moreover, we used virtually all the words connected syntactically to a term, not only the modifiers.	A syntactic feature is a pair: word, syntactic relation <TREF>Lin, 1998a</TREF>.	We use two feature types: First order features, which are directly connected to the training or test examples in the dependency parse trees of Corpus; second order features, which are connected to the training or test instances indirectly byskipping one wordthe verbin the dependency tree.	As an example, lets consider two sentences: Edison invented the phonograph and Edison created the phonograph.	2
As a baseline, we implemented the non-referential it detector of <REF>Lappin and Leass 1994</REF>, labelled as LL in the results.	This is a syntactic detector, a point missed by <REF>Evans 2001</REF> in his criticism: the patterns are robust to intervening words and modi ers eg  it was never thought by the committee that  provided the sentence is parsed correctly7 We automatically parse sentences with Minipar, a broad-coverage dependency parser <TREF>Lin, 1998b</TREF>.	We also use a separate, extended version of the LL detector, implemented for large-scale nonreferential detection by <REF>Cherry and Bergsma 2005</REF>.	This system, also for Minipar, additionally detects instances of it labelled with Minipars pleonastic category Subj.	2
Our approach avoids hand-crafting a set of spe11 ci c indicator features; we simply use the distribution of the pronouns context.	Our method is thus related to previous work based on <REF>Harris 1985</REF>s distributional hypothesis2 It has been used to determine both word and syntactic path similarity <REF>Hindle, 1990</REF>; <TREF>Lin, 1998a</TREF>; <REF>Lin and Pantel, 2001</REF>.	Our work is part of a trend of extracting other important information from statistical distributions.	<REF>Dagan and Itai 1990</REF> use the distribution of a pronouns context to determine which candidate antecedents can  t the context.	2
These definitions are valid in the context of particular applications; however, in general, the correspondence between paraphrasing and types of lexical relations is not clear.	The same question arises with automatically constructed thesauri <REF>Pereira et al , 1993</REF>; <TREF>Lin, 1998</TREF>.	While the extracted pairs are indeed similar, they are not paraphrases.	For example, while dog and cat are recognized as the most similar concepts by the method described in <TREF>Lin, 1998</TREF>, it is hard to imagine a context in which these words would be interchangeable.	3
While the extracted pairs are indeed similar, they are not paraphrases.	For example, while dog and cat are recognized as the most similar concepts by the method described in <TREF>Lin, 1998</TREF>, it is hard to imagine a context in which these words would be interchangeable.	The first attempt to derive paraphrasing rules from corpora was undertaken by <REF>Jacquemin et al , 1997</REF>, who investigated morphological and syntactic variants of technical terms.	While these rules achieve high accuracy in identifying term paraphrases, the techniques used have not been extended to other types of paraphrasing yet.	3
For this reason, knowledge-poor approaches such as the distributional approach are particularly suited for this task.	Its previous applications eg , <REF>Grefenstette 1993</REF>, <REF>Hearst and Schuetze 1993</REF>, <REF>Takunaga et al 1997</REF>, <TREF>Lin 1998</TREF>, <REF>Caraballo 1999</REF> demonstrated that cooccurrence statistics on a target word is often sufficient for its automatical classification into one of numerous classes such as synsets of WordNet.	Distributional techniques, however, are poorly applicable to rare words, ie, those words for which a corpus does not contain enough cooccurrence data to judge about their meaning.	Such words are the primary concern of many practical NLP applications: as a rule, they are semantically focused words and carry a lot of important information.	3
We also extensively investigated other corpusbased features, such as the number of times the phrase occurred hyphenated or capitalized, and the 4We exclude counts from the training, development, and testing queries discussed in Section 41.	822 corpus-based distributional similarity <TREF>Lin, 1998</TREF> between a pair of tokens.	These features are not available from search-engine statistics because search engines disregard punctuation and capitalization, and collecting page-count-based distributional similarity statistics is computationally infeasible.	Unfortunately, none of the corpus-based features improved performance on the development set and are thus excluded from further consideration.	2
1 Thesaurus creation Over the last ten years, interest has been growing in distributional thesauruses hereafter simply thesauruses.	Following initial work by <REF>Sparck Jones, 1964</REF> and <REF>Grefenstette, 1994</REF>, an early, online distributional thesaurus presented in <TREF>Lin, 1998</TREF> has been widely used and cited, and numerous authors since have explored thesaurus properties and parameters: see survey component of <REF>Weeds and Weir, 2005</REF>.	A thesaurus is created by  taking a corpus  identifying contexts for each word  identifying which words share contexts.	For each word, the words that share most contexts according to some statistic which also takes account of their frequency are its nearest neighbours.	2
22 Compound Similarity As a critical technique, word similarity is generally used in the example-based models of semantic classification.	The measure of word similarity can be divided into two major approaches: taxonomy-based lexical approach <REF>Resnik 1995</REF>, <TREF>Lin 1998a</TREF>, <REF>Chen and Chen 1998</REF> and context-based syntactic approach <TREF>Lin 1998b</TREF>,<REF>Chen and You 2002</REF>, which is not the concern in this context-free model.	However, two problems arise here for the taxonomy-based lexical approach.	First, such similarity measures risk the failure to capture the similarity among some semantically highly related words, if they happen to be put under classes distant from each other according to a specific ontology4.	3
We then calculate normalized tuple similarity scores over the tuple pairs using a metric that accounts for similarities in both syntactic structure and content of each tuple.	A thesaurus constructed from corpus statistics <TREF>Lin, 1998</TREF> is utilized for the content similarity.	We utilize this metric to greedily pair together the most similar predicate argument tuples across Figure 2: System architecture sentences.	Any remaining unpaired tuples represent extra information and are passed to a dissimilarity classi er to decide whether such information is signi cant.	2
Although the 24 MSR corpus used strict means of resolving interrater disagreements during its construction, the annotators agreed with the MSR corpus labels only 935 187/200 of the time.	One weakness of our system is that we rely on a thesaurus <TREF>Lin, 1998</TREF> for word similarity information for predicate argument tuple pairing.	However, it is designed to provide similarity scores between pairs of individual words rather than phrases.	If a predicate argument tuples target or one argument is realized as a phrase borrow  check out, for instance, the thesaurus is unable to provide an accurate similarity score.	3
We collected the statistics on the grammatical relations contexts output by Minipar and used these as the feature vectors.	<REF>Following Lin 1998</REF>, we measure each feature f for a word e not by its frequency but by its pointwise mutual information, mi ef : 126    fPeP feP mi ef  , log 4 Inducing ontological features The resource described in the previous section yields lexical feature vectors for each word in a corpus.	We term these vectors lexical because they are collected by looking only at the lexicals in the text ie no sense information is used.	We use the term ontological feature vector to refer to a feature vector whose features are for a particular sense of the word.	2
The hypothesis states that words that occur in the same contexts tend to have similar meaning.	Researchers have mostly looked at representing words by their surrounding words <REF>Lund and Burgess 1996</REF> and by their syntactical contexts <REF>Hindle 1990</REF>; <TREF>Lin 1998</TREF>.	However, these representations do not distinguish between the different senses of words.	Our framework utilizes these principles and representations to induce disambiguated feature vectors.	3
ii Sentence splitting, using mxterminator <REF>Reynar and Ratnaparkhi, 1997</REF>.	iii Dependency parsing, using Minipar <TREF>Lin, 1998b</TREF>.	The proof search is implemented as a depth-first search, with maximal depth ie proof length of 4.	If the text contains more than one sentence, the prover aims to prove h from each of the parsed sentences, and entailment is determined based on the minimal cost.	2
summationtext lh Scorel OpenClassWordsh 2 2We set the threshold to 001 3The active verbal form with direct modifiers where Scorel is 1 if it appears in p, or if it is a derivation of a word in p according to WordNet.	Otherwise, Scorel is the maximal Lin dependency-basedsimilarityscorebetweenlandthe lemmas of p <TREF>Lin, 1998a</TREF> synonyms and hypernyms/hyponyms are handled by the lexical rules.	7 System Implementation Deriving the initial propositions t and h from the input text fragments consists of the following steps: i Anaphora resolution, using the MARS system <REF>Mitkov et al , 2002</REF>.	Each anaphor was replaced by its antecedent.	2
Figure 1: Application of inference rules.	POS and relation labels are based on Minipar <TREF>Lin, 1998b</TREF> If a complete proof is found h was generated, the prover concludes that entailment holds.	Otherwise, entailment is determined by comparing the minimal cost found during the proof search to some threshold .	3 Proof System Like logic-based systems, our proof system consists of propositions t, h, and intermediate premises, and inference entailment rules, which derive new propositions from previously established ones.	2
As a striking example, the 14 most syntactically similar verbs to believe in order are think, guess, hope, feel, wonder, theorize, fear, reckon, contend, suppose, understand, know, doubt, and suggest  all mental action verbs.	This observation further supports the distributional hypothesis of word similarity and corresponding technologies for identifying synonyms by similarity of lexical-syntactic context <TREF>Lin, 1998</TREF>.	Verb pairs instances Cosine bind 83 bound 95 0950 plunge 94 tumble 87 0888 dive 36 plunge 94 0867 dive 36 tumble 87 0866 jump 79 tumble 87 0865 fall 84 fell 102 0859 intersperse 99 perch 81 0859 assail 100 chide 98 0859 dip 81 fell 102 0858 buffet 72 embroil 100 0856 embroil 100 lock 73 0856 embroil 100 superimpose 100 0856 fell 102 jump 79 0855 fell 102 tumble 87 0855 embroil 100 whipsaw 63 0850 pluck 100 whisk 99 0849 acquit 100 hospitalize 99 0849 disincline 70 obligate 94 0848 jump 79 plunge 94 0848 dive 36 jump 79 0847 assail 100 lambaste 100 0847 festoon 98 strew 100 0846 mar 78 whipsaw 63 0846 pluck 100 whipsaw 63 0846 ensconce 101 whipsaw 63 0845 Table 2.	Top 25 most syntactically similar pairs of the 3257 verbs in PropBank.	2
Our approach is strictly empirical; the similarity of verbs is determined by examining the syntactic contexts in which they appear in a large text corpus.	Our approach is analogous to previous work in extracting collocations from large text corpora using syntactic information <TREF>Lin, 1998</TREF>.	In our work, we utilized the GigaWord corpus of English newswire text Linguistic <REF>Data Consortium, 2003</REF>, consisting of nearly 12 gigabytes of textual data.	To prepare this corpus for analysis, we extracted the body text from each of the 41 million entries in the corpus and applied a maximum-entropy algorithm to identify sentence boundaries <REF>Reynar and Ratnaparkhi, 1997</REF>.	2
There have been several attempts to group WordNet senses using various different types of information sources.	This paper describes work to automatically relate WordNet word senses using automatically acquired thesauruses <TREF>Lin, 1998</TREF> and WordNet similarity measures <REF>Patwardhan and Pedersen, 2003</REF>.	This work proposes using graded word sense relationships rather than fixed groupings clusters.	Previous research has focused on clustering WordNet senses into groups.	2
This is transformed from a distance measure in the WN-Similarity package by taking the reciprocal: jcns1,s2  1/Djcns1,s2 We use raw BNC data for calculating IC values.	DIST We use a distributional similarity measure <TREF>Lin, 1998</TREF> to obtain a fixed number 50 of the top ranked nearest neighbours for the target nouns.	For input we used grammatical relation data extracted using an automatic parser <REF>Briscoe and Carroll, 2002</REF>.	We used the 90 million words of written English from the British National Corpus BNC <REF>Leech, 1992</REF>.	2
To do this, many approaches to lexical acquisition employ the distributional model of word meaning induced from the distribution of the word across various lexical contexts of its occurrence found in the corpus.	The approach is now being actively explored for a wide range of semantics-related tasks including automatic construction of thesauri <TREF>Lin, 1998</TREF>; <REF>Caraballo, 1999</REF>, their enrichment <REF>Alfonseca and Manandhar, 2002</REF>; <REF>Pekar and Staab, 2002</REF>, acquisition of bilingual lexica from nonaligned <REF>Kay and Rscheisen, 1993</REF> and nonparallel corpora <REF>Fung and Yee, 1998</REF>, learning of information extraction patterns from un-annotated text <REF>Riloff and Schmelzenbach, 1998</REF>.	However, because of irregularities in corpus data, corpus statistics cannot guarantee optimal performance, notably for rare lexical items.	In order to improve robustness, recent research has attempted a variety of ways to incorporate external knowledge into the distributional model.	3
The number of unique collocations in the resulting database 2 is about 11 million.	Using the similarity measure proposed in <TREF>Lin, 1998</TREF>, we constructed a corpus-based thesaurus 3 consisting of 11839 nouns, 3639 verbs and 5658 adjective/adverbs which occurred in the corpus at least 100 times.	3 Mutual Information of a Collocation We define the probability space to consist of all possible collocation triples.	We use LH R M L to denote the 1 available at http://wwwcsumanitobaca/-lindek/miniparhtm/ 2available at http://wwwcsumanitobca/-lindek/nlldemohtm/ 3available at http://wwwcsumanitobaca/-lindek/nlldemohtm/ 317 frequency count of all the collocations that match the pattern H R M, where H and M are either words or the wild card  and R is either a dependency type or the wild card.	2
We briefly describe the process of obtaining this input.	More details about the construction of the collocation database and the thesaurus can be found in <TREF>Lin, 1998</TREF>.	We parsed a 125-million word newspaper corpus with Minipar, 1 a descendent of Principar <REF>Lin, 1993</REF>; <REF>Lin, 1994</REF>, and extracted dependency relationships from the parsed corpus.	A dependency relationship is a triple: head type modifier, where head and modifier are words in the input sentence and type is the type of the dependency relation.	2
From this tree structure, the similarity is obtained: simhill,coast  2355  06.	4 The similarity between word w with senses w1,,wn and word v with senses v1,,vm is defined as the maximum similarity between all the pairs of word senses: simw,v  maxi,j simwi,vj, 5 whose idea came from Lins method <TREF>Lin, 1998</TREF>.	42 Discrimination Rate The following two sections describe two evaluation measures based on the reference similarity.	The first one is discrimination rate DR.	2
The method is described in <REF>McCarthy et al , 2004</REF>, which we summarise here.	We acquire thesauruses for nouns, verbs, adjectives and adverbs based on the method proposed by <TREF>Lin 1998</TREF> using grammatical relations output from the RASP parser <REF>Briscoe and Carroll, 2002</REF>.	The grammatical contexts used are listed in table 3, but there is scope for extending or restricting the contexts for a given PoS.	We use the thesauruses for ranking the senses of the target words.	2
To measure the compositionality, semantically similar words are more suitable than synomys.	Hence, we choose to use Lins thesaurus <TREF>Lin, 1998</TREF> instead of Wordnet <REF>Miller et al , 1990</REF>.	902 614 Distributed Frequency of Object a0  The distributed frequency of object is based on the idea that if an object appears only with one verb or few verbs in a large corpus, the collocation is expected to have idiomatic nature <REF>Tapanainen et al , 1998</REF>.	For example, sure in make sure occurs with very few verbs.	2
The higher the value of a38, the more is the likelihood of the collocation to be a MWE.	2obtained from Lins <TREF>Lin, 1998</TREF> automatically generated thesaurus http://wwwcsualbertaca/a66 lindek/downloadshtm.	We obtained the best results section 8 when we substituted top-5 similar words for both the verb and the object.	To measure the compositionality, semantically similar words are more suitable than synomys.	2
As a representative of this approach we use Lins dependency-baseddistributionalsimilaritydatabase.	Lins database was created using the particular distributionalsimilaritymeasurein<TREF>Lin, 1998</TREF>, applied to a large corpus of news data 64 million words 4.	Two words obtain a high similarity score if they occur often in the same contexts, as captured by syntactic dependency relations.	For example, two verbs willbeconsideredsimilariftheyhavelargecommon sets of modifying subjects, objects, adverbs etc Distributional similarity does not capture directly meaning equivalence and entailment but rather a looser notion of meaning similarity <REF>Geffet and Dagan, 2005</REF>.	2
The setting allowed us to analyze different types of state of the art models and their behavior with respect to characteristic sub-cases of the problem.	The major conclusion that seems to arise from our experiments is the effectiveness of combining a knowledge based thesaurus such as WordNet with distributional statistical information such as <TREF>Lin, 1998</TREF>, overcoming the known deficiencies of each method alone.	Furthermore, modeling the a priori substitution likelihood captures the majority of cases in the evaluated setting, mostly because WordNet provides a rather noisy set of substitution candidates.	On the other hand, successfully incorporating local and global contextual information, as similar to WSD methods, remains a challenging task for future research.	2
The method we use to predict the rst sense is that of McCarthy et al.	2004, which was obtained using a thesaurus automatically created from the British National Corpus BNC applying the method of <TREF>Lin 1998</TREF>, coupled with WordNetbased similarity measures.	This method is fully unsupervised and completely unreliant on any annotations from our dataset.	In the case of SFs, we perform full synset WSD based on one of the above options, and then map the prediction onto the corresponding unique SF.	2
A noun, a4, is thus described by a set of co-occurrence triples a94 a4a7a14 a55 a14a32a95a97a96 and associated frequencies, where a55 is a grammatical relation and a95 is a possible cooccurrence with a4 in that relation.	For every pair of nouns, where each noun had a total frequency in the triple data of 10 or more, we computed their distributional similarity using the measure given by <TREF>Lin 1998</TREF>.	If a98a56a30a31a4 a33 is the set of co-occurrence types a30 a55 a14a16a95 a33 such that a99a100a30a42a4a43a14 a55 a14a32a95 a33 is positive then the similarity between two nouns, a4 and a10, can be computed as: a26a41a28a15a28a27a30a42a4a43a14a16a10 a33 a8 a75 a83a102a101a42a103a50 a85 a70a11a104 a83a102a6a13a85a106a105 a104 a83 a67 a85 a30a78a99a100a30a31a4a7a14 a55 a14a16a95 a33a41a107 a99a108a30a31a10a109a14 a55 a14a16a95 a33a86a33 a75 a83a102a101a42a103a50 a85 a70a11a104 a83a102a6a13a85 a99a108a30a31a4a7a14 a55 a14a32a95 a33a45a107 a75 a83a84a101a42a103a50 a85 a70a11a104 a83 a67 a85 a99a108a30a31a10a109a14 a55 a14a16a95 a33 where: a99a108a30a31a4a7a14 a55 a14a32a95 a33 a8a111a110a21a112a114a113 a54 a30a31a95a73a115a116a4a118a117 a55 a33 a54 a30a42a95a73a115 a55 a33 A thesaurus entry of size a3 for a target noun a4 is then defined as the a3 most similar nouns to a4  22 The WordNet Similarity Package We use the WordNet Similarity Package 005 and WordNet version 16.	2 The WordNet Similarity package supports a range of WordNet similarity scores.	2
We describe some related work in section 6 and conclude in section 7.	In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of <TREF>Lin 1998</TREF>.	This provides the a3 nearest neighbours to each target word, along with the distributional similarity score between the target word and its neighbour.	We then use the WordNet similarity package <REF>Patwardhan and Pedersen, 2003</REF> to give us a semantic similarity measure hereafter referred to as the WordNet similarity measure to weight the contribution that each neighbour makes to the various senses of the target word.	2
Let a28a15a34a20a10a18a28a20a34a15a28a25a30a31a4 a33 be the set of senses of a4  For each sense of a4 a4a35a28a37a36a39a38a40a28a20a34a15a10a13a28a15a34a20a28a27a30a31a4 a33  we obtain a ranking score by summing over the a26a41a28a15a28a27a30a42a4a43a14a16a10a45a44 a33 of each neighbour a10a46a44a47a38a48a5 a6  multiplied by a weight.	This weight is the WordNet similarity score a4a49a10a13a28a15a28  between the target sense a4a35a28a37a36  and the sense of a10a45a44 a10a13a28a37a50a51a38a52a28a15a34a15a10a13a28a20a34a15a28a27a30a42a10a45a44 a33  that maximises this score, divided by the sum of all such WordNet similarity scores for a28a20a34a15a10a13a28a15a34a20a28a27a30a31a4 a33 and a10a46a44  Thus we rank each sense a4a49a28 a36 a38a53a28a15a34a20a10a18a28a20a34a15a28a25a30a31a4 a33 using: a54a56a55 a34a15a57a41a58a41a59a60a34a15a10a13a61a37a34a63a62a64a61a37a65 a55 a34a27a30a31a4a35a28a37a36 a33 a8 a66 a67a69a68a32a70a27a71a73a72 a26a29a28a20a28a27a30a31a4a7a14a32a10 a44 a33a15a74 a4a49a10a13a28a20a28a27a30a31a4a35a28a37a36a42a14a32a10a46a44 a33 a75 a6a18a76a78a77a80a79 a70 a76a82a81 a67 a76a78a81a82a76a42a83a84a6a18a85 a4a35a10a13a28a15a28a25a30a31a4a49a28 a36 a79 a14a16a10a45a44 a33 1 where: a4a49a10a13a28a20a28a27a30a31a4a35a28a37a36a86a14a16a10a45a44 a33 a8 a87a89a88a20a90 a67 a76a92a91 a70 a76a78a81 a67 a76a78a81a82a76a93a83 a67a69a68 a85 a30a42a4a49a10a13a28a15a28a25a30a31a4a35a28a37a36a42a14a32a10a13a28a37a50 a33a86a33 21 Acquiring the Automatic Thesaurus The thesaurus was acquired using the method described by <TREF>Lin 1998</TREF>.	For input we used grammatical relation data extracted using an automatic parser <REF>Briscoe and Carroll, 2002</REF>.	For the experiments in sections 3 and 4 we used the 90 million words of written English from the BNC.	2
by comparing its strength of association measured by PMI with those of its lexical variants.	<REF>Like Lin 1999</REF>, we generate lexical variants of the target automatically by replacing either the verb or the noun constituent by a semantically similar word from the automatically-built thesaurus of <TREF>Lin 1998</TREF>.	We then use a standard statistic, the z-score, to calculate Fixednesslex: Fixednesslexv, n  PMIv, nPMIstd 2 where PMI is the mean and std the standard deviation over the PMI of the target and all its variants.	Fixednesssyn quantifies the degree of syntactic fixedness of the target combination, by comparing its behaviour in text with the behaviour of a typical verbobject, both defined as probability distributions over a predefined set of patterns.	2
We seek to overcome these difficulties by generating TOEFL-like tests automatically from WordNet <REF>Fellbaum, 1998</REF>.	While WordNet has been used before to evaluate corpus-analytic approaches to lexical similarity <TREF>Lin, 1998</TREF>, the metric proposed in that study, while useful for comparative purposes, lacks an intuitive interpretation.	In contrast, we emulate the TOEFL using WordNet and inherit the TOEFLs easy interpretability.	Given a corpus, we first derive a list of words occurring with sufficient marginal frequency to support a distributional comparison.	3
et al.	2004 we use a3a5a4a7a6a9a8 and obtain our thesaurus using the distributional similarity metric described by <TREF>Lin 1998</TREF>.	We use WordNet WN as our sense inventory.	The senses of a worda2 are each assigned a ranking score which sums over the distributional similarity scores of the neighbours and weights each neighbours score by a WN Similarity score <REF>Patwardhan and Pedersen, 2003</REF> between the sense of a2 and the sense of the neighbour that maximises the WN Similarity score.	2
Section 5 reports on the trade-off between the minimum cutoff and execution time.	Early experiments in thesaurus extraction <REF>Grefenstette, 1994</REF> suffered from the limited size of available corpora, but more recent experiments have used much larger corpora with greater success <TREF>Lin, 1998a</TREF>.	For these experiments we ran our relation extractor over the British National Corpus BNC consisting of 114 million words in 62 million sentences.	The POS tagging and chunking took 159 minutes, and the relation extraction took an addiSETCOSINE jwm; ; wn; ; jpjw m; ; j jwn; ; j COSINE P r;w0 wgtwm; r; w0 wgtwn; r; w0pP wgtwm; ; 2 Pwgtwn; ; 2 SETDICE 2jwm; ; wn; ; jjwm; ; jjwn; ; j DICE P r;w0 wgtwm; r; w0 wgtwn; r; w0P r;w0 wgtwm; r; w0wgtwn; r; w0 DICEy 2 P r;w0 minwgtwm; r; w0;wgtwn; r; w0P r;w0 wgtwm; r; w0wgtwn; r; w0 SETJACCARD jwm; ; wn; ; jjwm; ; wn; ; j JACCARD P r;w0 minwgtwm; r; w0;wgtwn; r; w0P r;w0 maxwgtwm; r; w0;wgtwn; r; w0 JACCARDy P r;w0 wgtwm; r; w0 wgtwn; r; w0P r;w0 wgtwm; r; w0wgtwn; r; w0 LIN P r;w0 wgtwm; r; w0wgtwn; r; w0P wgtwm; ; Pwgtwn; ;  Table 1: Measure functions evaluated tional 7:5 minutes.	2
The list of measure and weight functions we compared against is not complete, and we hope to add other functions to provide a general framework for thesaurus extraction experimentation.	We would also like to expand our evaluation to include direct methods used by others <TREF>Lin, 1998a</TREF> and using the extracted thesaurus in NLP tasks.	We have also investigated the speed/performance trade-off using frequency cutoffs.	This has lead to the proposal of a new approximate comparison algorithm based on canonical attributes and a process of coarseand ne-grained comparisons.	2
These experiments also cover a range of weight functions as de ned in Table 2.	The weight functions LIN98A, LIN98B, and GREF94 are taken from existing systems <TREF>Lin, 1998a</TREF>; <TREF>Lin, 1998b</TREF>; <REF>Grefenstette, 1994</REF>.	Our proposed weight functions are motivated by our intuition that highly predictive attributes are strong collocations with their terms.	Thus, we have implemented many of the statistics described in the Collocations chapter of Manning and Schcurrency1utze 1999, including the T-Test, 2-Test, Likelihood Ratio, and Mutual Information.	2
The resultant representation contained a total of 28 million relation occurrences over 10 million different relations.	We describe the functions evaluated in these experiments using an extension of the asterisk notation used by <TREF>Lin 1998a</TREF>, where an asterisk indicates a set ranging over all existing values of that variable.	For example, the set of attributes of the term w is: w; ;  fr;w0j9w;r;w0g For convenience, we further extend the notation for weighted attribute vectors.	A subscripted asterisk indicates that the variables are bound together: X r;w0 wgtwm; r; w0 wgtwn; r; w0 which is a notational abbreviation of: X r;w02wm; ; wn; ;  wgtwm;r;w0 wgtwn;r;w0 For weight functions we use similar notation: f w; ;  X r;w02w; ;  f w;r;w0 nw; ;  jw; ; j Nw jfwj9w; ; ,;gj Table 1 de nes the measure functions evaluated in these experiments.	2
431 Corpus-based Lexical Similarity Lexical similarity was computed using the Word Sketch Engine WSE <REF>Killgarrif et al , 2004</REF> similarity metric applied over British National Corpus.	The WSE similarity metric implements the word similarity measure based on grammatical relations as defined in <TREF>Lin, 1998</TREF> with minor modifications.	432 The Brandeis Semantic Ontology As a second source of lexical coherence, we used the Brandeis Semantic Ontology or BSO <REF>Pustejovsky et al , 2006</REF>.	The BSO is a lexicallybased ontology in the Generative Lexicon tradition <REF>Pustejovsky, 2001</REF>; <REF>Pustejovsky, 1995</REF>.	2
<REF>Mihalcea and Moldovan, 2001</REF> implements six semantic rules, using twin and autohyponym features, in addition to other WordNet-structure-based rules such as whether two synsets share a pertainym, antonym, or are clustered together in the same verb group.	A large body of work has attempted to capture corpus-based estimates of word similarity <REF>Pereira et al , 1993</REF>; <TREF>Lin, 1998</TREF>; however, the lack of large sense-tagged corpora prevent most such techniques from being used effectively to compare different senses of the same word.	Some corpus-based attempts that are capable of estimating similarity between word senses include the topic signatures method; here, <REF>Agirre and Lopez, 2003</REF> collect contexts for a polysemous word based either on sensetagged corpora or by using a weighted agglomeration of contexts of a polysemous words monosemous relatives ie , single-sense synsets related by hypernym, hyponym, or other relations from some large untagged corpus.	Other corpus-based techniques developed specifically for sense clustering include <REF>McCarthy, 2006</REF>, which uses a combination of word-to-word distributional similarity combined with the JCN WordNet-based similarity measure, and work by <REF>Chugur et al , 2002</REF> in finding co-occurrences of senses within documents in sense-tagged corpora.	3
3 Learning to merge word senses 31 WordNet-based features Here we describe the feature space we construct for classifying whether or not a pair of synsets should be merged; first, we employ a wide variety of linguistic features based on information derived from WordNet.	We use eight similarity measures implemented within the WordNet::Similarity package5, described in <REF>Pedersen et al , 2004</REF>; these include three measures derived from the paths between the synsets in WordNet: HSO Hirst and St-<REF>Onge, 1998</REF>, LCH <REF>Leacock and Chodorow, 1998</REF>, and WUP <REF>Wu and Palmer, 1994</REF>; three measures based on information content: RES <REF>Resnik, 1995</REF>, LIN <TREF>Lin, 1998</TREF>, and JCN <REF>Jiang and Conrath, 1997</REF>; the gloss-based Extended Lesk Measure LESK, <REF>Banerjee and Pedersen, 2003</REF>, and finally the gloss vector similarity measure VECTOR <REF>Patwardan, 2003</REF>.	We implement the TWIN feature <REF>Peters et al , 1998</REF>, which counts the number of shared synonyms between the two synsets.	Additionally we produce pairwise features indicating whether two senses share an ANTONYM, PERTAINYM, or derivationally-related forms DERIV.	2
Our thesaurus brings up only alternatives that have the same part-of-speech with the target word.	The choices could come from various inventories of near-synonyms or similar words, for example the Roget thesaurus Roget, 1852, dictionaries of synonyms <REF>Hayakawa, 1994</REF>, or clusters acquired from corpora <TREF>Lin, 1998</TREF>.	In this paper we focus on the task of automatically selecting the best near-synonym that should be used in a particular context.	The natural way to validate an algorithm for this task would be to ask human readers to evaluate the quality of the algorithms output, but this kind of evaluation would be very laborious.	2
For more details on hypernym collocations, see OHara, forthcoming.	Word-similarity classes <TREF>Lin, 1998</TREF> derived from clustering are also used to expand the pool of potential collocations; this type of semantic relatedness among words is expressed in the SimilarColl feature.	For the DictColl features, definition analysis OHara, forthcoming is used to determine the semantic relatedness of the defining words.	Dierences between these two sources of word relations are illustrated by looking at the information they provide for ballerina: word-clusters: dancer:0115 baryshnikov:0072 pianist:0056 choreographer:0049  18 other words nicole:0041 wrestler:0040 tibetans:0040 clown:0040 definition words: dancer:00013 female:00013 ballet:00004 This shows that word clusters capture a wider range of relatedness than the dictionary definitions at the expense of incidental associations eg , nicole.	2
Again, because context words are not disambiguated, the relations for all senses of a context word are conflated.	For details on the extraction of word clusters, see <TREF>Lin, 1998</TREF>; and, for details on the definition analysis, see OHara, forthcoming.	When formulating the features SimilarColl and DictColl, the words related to each context word are considered as potential collocations <REF>Wiebe et al , 1998</REF>.	Co-occurrence freSense Distinctions Precision Recall Fine-grained 566 565 Course-grained 660 658 Table 1: Results for Senseval-3 test data.	2
A set of seed words begins the process.	For each seed s i, the precision of the set s i C i,n in the training data is calculated, where C i,n is the set of n words most similar to s i, according to <TREF>Lins 1998</TREF> method.	If the precision of s i C i,n is greater than a threshold T, then the words in this set are retained as PSEs.	If it is not, neither s i nor the words in C i,n are retained.	2
Many variants of distributional similarity have been used in NLP <REF>Lee 1999</REF>; <REF>Lee and Pereira 1999</REF>.	<TREF>Dekang Lins 1998</TREF> method is used here.	In contrast to many implementations, which focus exclusively on verb-noun relationships, Lins method incorporates a variety of syntactic relations.	This is important for subjectivity recognition, because PSEs are not limited to verb-noun relationships.	2
The method is then used to identify an unusual form of collocation: One or more positions in the collocation may be filled by any word of an appropriate part of speech that is unique in the test data.	The third type of subjectivity clue we examine here are adjective and verb features identified using the results of a method for clustering words according to distributional similarity <TREF>Lin 1998</TREF> Section 34.	We hypothesized that two words may be distributionally similar because they are both potentially subjective eg , tragic, sad, and poignant are identified from bizarre.	In addition, we use distributional similarity to improve estimates of unseen events: A word is selected or discarded based on the precision of it together with its n most similar neighbors.	2
We also presented a procedure for automatically identifying potentially subjective collocations, including fixed collocations and collocations with placeholders for unique words.	In addition, we used the results of a method for clustering words according to distributional similarity <TREF>Lin 1998</TREF> to identify adjectival and verbal clues of subjectivity.	Table 9 summarizes the results of testing all of the above types of PSEs.	All show increased precision in the evaluations.	2
The verbs were clustered into 64 classes using the probabilistic co-occurrence model of <REF>Hofmann and Puzicha 1998</REF>.	The clustering algorithm uses a database of verb-direct-object relations extracted by <TREF>Lin 1998</TREF>.	We then use the verb class of the current predicate as a feature.	4.	2
The experiment was conducted using an 18 million tokens subset of the Reuters RCV1 corpus,2 parsed by Lins Minipar dependency parser <REF>Lin, 1993</REF>.	We considered first an evaluation based on WordNet data as a gold standard, as in <TREF>Lin, 1998</TREF>; <REF>Weeds and Weir, 2003</REF>.	However, we found that many word pairs from the Reuters Corpus that are clearly substitutable are not linked appropriately in WordNet.	We therefore conducted a manual evaluation based on the judgments of two human subjects.	3
Typical feature weighting functions include the logarithm of the frequency of word-feature cooccurrence <REF>Ruge, 1992</REF>, and the conditional probability of the feature given the word within probabilistic-based measures <REF>Pereira et al , 1993</REF>, <REF>Lee, 1997</REF>, <REF>Dagan et al , 1999</REF>.	Probably the most widely used association weight function is point-wise Mutual Information MI <REF>Church et al , 1990</REF>, <REF>Hindle, 1990</REF>, <TREF>Lin, 1998</TREF>, <REF>Dagan, 2000</REF>, defined by:  ,log, 2 fPwP fwPfwMI  A known weakness of MI is its tendency to assign high weights for rare features.	Yet, similarity measures that utilize MI showed good performance.	In particular, a common practice is to filter out features by minimal frequency and weight thresholds.	3
We picked the widely cited and competitive eg.	<REF>Weeds and Weir, 2003</REF> measure of <TREF>Lin 1998</TREF> as a representative case, and utilized it for our analysis and as a starting point for improvement.	21 Lins 98 Similarity Measure Lins similarity measure between two words, w and v, is defined as follows:,,, ,, ,           fvweightfwweight fvweightfwweight vwsim vFfwFf vFwFf where Fw and Fv are the active features of the two words and the weight function is defined as MI.	A feature is defined as a pair <term, syntacCountryState Ranks CountryEconomy Ranks Broadcast Goods Civilservant Bloc Nonaligned Neighboring Statistic Border Northwest 24 140 64 30 55 15 165 10 41 50 16 54 77 60 165 43 247 174 Devastate Developed Dependent Industrialized Shattered Club Black Million Electricity 81 36 101 49 16 155 122 31 130 8 78 26 85 141 38 109 245 154 Table 3: The top-10 common features for the word pairs country-state and country-economy, along with their corresponding ranks in the sorted feature lists of the two words.	2
2004 automatically determine domainspecific predominant senses of words, where the domain may be specified in the form of an untagged target text or simply by name for example, financial domain.	The system Figure 1 automatically generates a thesaurus <TREF>Lin, 1998</TREF> using a measure of distributional similarity and an untagged corpus.	The target text is used for this purpose, provided it is large enough to learn a thesaurus from.	Otherwise a large corpus with sense distribution similar to the target text text pertaining to the specified domain must be used.	2
This requires large amounts of partof-speech-tagged and chunked data from that domain.	Further, the target text must be large enough to learn a thesaurus from <TREF>Lin 1998</TREF> used a 64million-word corpus, or a large auxiliary text with a sense distribution similar to the target text must be provided McCarthy et al.	2004 separately used 90-, 325-, and 91-million-word corpora.	By contrast, in this paper we present a method that accurately determines sense dominance even in relatively small amounts of target text a few hundred sentences; although it does use a corpus, it does not require a similarly-sense-distributed corpus.	3
2003.	Typical relation-constrained DPs are those of <TREF>Lin 1998</TREF> and <REF>Lee 2001</REF>.	Below are contrived, but plausible, examples of each for the word pulse; the numbers are conditional probabilities.	relation-free DP pulse: beat 28, racing 2, grow 13, beans 09, heart 04,   .	2
The distance between two words, given their DPs, is calculated using a measure of DP distance, such as cosine.	While any of the measures of DP distance may be used with any of the measures of strength of association see Table 1, in practice -skew divergence ASD, cosine, and JensenShannon divergence JSD are used with conditional probability CP, whereas Lin is used with PMI, resulting in the distributional measures ASD cp <REF>Lee, 2001</REF>, Cos cp <REF>Schutze and Pedersen, 1997</REF>, JSD cp,andLin pmi <TREF>Lin, 1998</TREF>, respectively.	ASD cp is a modification of Kullback-Leibler divergence that overcomes the latters problem of division by zero, which can be caused by data sparseness.	JSD cp is another relative entropybased measure like ASD cp  but it is symmetric.	2
We then generated ranked sets of nearest neighbours of size k  200 and where a word is excluded from being a neighbour of itself for each word and each measure.	For a given word, we compute the overlap between neighbour sets using a comparison technique adapted from <TREF>Lin 1998</TREF>.	Given a word w, each word w0 in WScomp is assigned a rank score of k rank if it is one of the k nearest neighbours of w using measure m and zero otherwise.	If NSw;m is the vector of such scores for word w and measure m, then the overlap, CNSw;m1;NSw;m2, of two neighbour sets is the cosine between the two vectors: CNSw;m1;NSw;m2  P w0rm1w0;w rm2w0;wP k i1i2 The overlap score indicates the extent to which sets share members and the extent to which they are in the same order.	2
In the simplest case, the features of a word are de ned as the contexts in which it has been seen to occur.	simjami is a variant <TREF>Lin, 1998</TREF> in which the features of a word are those contexts for which the pointwise mutual information MI between the word and the context is positive, where MI can be calculated using Ic;w  log PcjwPc.	The related Dice Coe cient Frakes and Baeza-<REF>Yates, 1992</REF> is omitted here since it has been shown van <REF>Rijsbergen, 1979</REF> that Dice and Jaccards Coe cients are monotonic in each other.	Lins Measure <TREF>Lin, 1998</TREF> is based on his information-theoretic similarity theorem, which states, the similarity between A and B is measured by the ratio between the amount of information needed to state the commonality of A and B and the information needed to fully describe what A and B are.	2
Other potential applications apply the hypothesised relationship <REF>Harris, 1968</REF> between distributional similarity and semantic similarity; ie, similarity in the meaning of words can be predicted from their distributional similarity.	One advantage of automatically generated thesauruses <REF>Grefenstette, 1994</REF>; <TREF>Lin, 1998</TREF>; <REF>Curran and Moens, 2002</REF> over large-scale manually created thesauruses such as WordNet <REF>Fellbaum, 1998</REF> is that they might be tailored to a particular genre or domain.	However, due to the lack of a tight de nition for the concept of distributional similarity and the broad range of potential applications, a large number of measures of distributional similarity have been proposed or adopted see Section 2.	Previous work on the evaluation of distributional similarity methods tends to either compare sets of distributionally similar words to a manually created semantic resource <TREF>Lin, 1998</TREF>; <REF>Curran and Moens, 2002</REF> or be oriented towards a particular task such as language modelling <REF>Dagan et al , 1999</REF>; <REF>Lee, 1999</REF>.	3
However, due to the lack of a tight de nition for the concept of distributional similarity and the broad range of potential applications, a large number of measures of distributional similarity have been proposed or adopted see Section 2.	Previous work on the evaluation of distributional similarity methods tends to either compare sets of distributionally similar words to a manually created semantic resource <TREF>Lin, 1998</TREF>; <REF>Curran and Moens, 2002</REF> or be oriented towards a particular task such as language modelling <REF>Dagan et al , 1999</REF>; <REF>Lee, 1999</REF>.	The rst approach is not ideal since it assumes that the goal of distributional similarity methods is to predict semantic similarity and that the semantic resource used is a valid gold standard.	Further, the second approach is clearly advantageous when one wishes to apply distributional similarity methods in a particular application area.	3
Let Seenrp be the set of seen headwords for an argument rp of a predicate p Then we model the selectional preference S of rp for a possible headword w0 as a weighted sum of the similarities between w0 and the seen headwords: Srpw0  summationdisplay wSeenrp simw0,wwtrpw simw0,w is the similarity between the seen and the potential headword, and wtrpw is the weight of seen headword w Similarity simw0,w will be computed on the generalization corpus, again on the basis of extracted tuples p,rp,w.	We will be using the similarity metrics shown in Table 1: Cosine, the Dice and Jaccard coefficients, and <REF>Hindles 1990</REF> and <TREF>Lins 1998</TREF> mutual information-based metrics.	We write f for frequency, I for mutual information, and Rw for the set of arguments rp for which w occurs as a headword.	In this paper we only study corpus-based metrics.	2
Some approaches have used WordNet for the generalization step <REF>Resnik, 1996</REF>; <REF>Clark and Weir, 2001</REF>; <REF>Abe and Li, 1993</REF>, others EM-based clustering <REF>Rooth et al , 1999</REF>.	In this paper we propose a new, simple model for selectional preference induction that uses corpus-based semantic similarity metrics, such as Cosine or <TREF>Lins 1998</TREF> mutual informationbased metric, for the generalization step.	This model does not require any manually created lexical resources.	In addition, the corpus for computing the similarity metrics can be freely chosen, allowing greater variation in the domain of generalization than a fixed lexical resource.	2
The use of synonyms is another way of increasing the coverage of question terminology;; while semantic features try to achieve it by generalization, synonyms do it by lexical expansion.	Our plan is to use the synonyms obtained from very large corpora reported in <TREF>Lin, 1998</TREF>.	We are also planning to compare the lexical and semantic features we derived automatically in this work with manually selected features.	In our previous work, manually selected lexical featuresshowedslightlybetterperformanceforthe training data but no signi cant dierence for the test data.	2
Other approaches usually consider either given sets of synonyms among which one is to be chosen for a translation for instance <REF>Edmonds and Hirst, 2002</REF> or must choose a synonym word against unrelated terms in the context of a synonymy test <REF>Freitag et al , 2005</REF>, a seemingly easier task than actually proposing synonyms.	<TREF>Lin, 1998</TREF> proposes a different methodology for evaluation of candidate synonyms, by comparing similarity measures of the terms he provides with the similarity measures between them in Wordnet, using various semantic distances.	This makes for very complex evaluation procedures without an intuitive interpretation, and there is no assessment of the quality of the automated thesaurus.	6 Conclusion We have developed a general method to extract nearsynonyms from a dictionary, improving on the two baselines.	3
<REF>Mihalcea, 2003</REF>; <REF>Riloff and Wiebe, 2003</REF>.	Severalapproachesmakeuseofdependency triples <TREF>Lin, 1998</TREF>; <REF>Gorman and Curran, 2005</REF>.	Our vector representation of the behavior of a word type across all its instances in a corpus is based on <TREF>Lin 1998</TREF>s DESCRIPTION OF A WORD.	<REF>Yarowsky 1995</REF> uses a conceptually similar technique for WSD that learns from a small set of seed examples and then increases recall by bootstrapping, evaluated on 12 idiosyncratically polysemous words.	2
Severalapproachesmakeuseofdependency triples <TREF>Lin, 1998</TREF>; <REF>Gorman and Curran, 2005</REF>.	Our vector representation of the behavior of a word type across all its instances in a corpus is based on <TREF>Lin 1998</TREF>s DESCRIPTION OF A WORD.	<REF>Yarowsky 1995</REF> uses a conceptually similar technique for WSD that learns from a small set of seed examples and then increases recall by bootstrapping, evaluated on 12 idiosyncratically polysemous words.	In that task, often a single disambiguating feature can be found in the context of a polysemous word instance, motivating his use of the decision list algorithm.	2
It allows us to identify triple instances.	Each triple have the form w1Rw2 where w1 and w2 are lexical units and R is a syntactic relation <TREF>Lin, 1998</TREF>; Kilgarriff  al 2004.	Our approach can be distinguished from classical distributional approach by different points.	First, we use triple occurrences to build a distributional space one triple implies two contexts and two lexical units, but we use the transpose of the classical space: each point x i of this space is a syntactical context with the form Rw, each dimension j is a lexical units, and each value x i j is the frequency of corresponding triple occurrences.	2
The model is highly general and can be optimised for different tasks.	It extends prior work on syntax-based models <REF>Grefenstette, 1994</REF>; <TREF>Lin, 1998</TREF>, by providing a general framework for defining context so that a large number of syntactic relations can be used in the construction of the semantic space.	Our approach differs from <TREF>Lin 1998</TREF> in three important ways: a by introducing dependency paths we can capture non-immediate relationships between words ie , between subjects and objects, whereas Lin considers only local context dependency edges in our terminology; the semantic space is therefore constructed solely from isolated head/modifier pairs and their inter-dependencies are not taken into account; b Lin creates the semantic space from the set of dependency edges that are relevant for a given word; by introducing dependency labels and the path value function we can selectively weight the importance of different labels eg , subject, object, modifier and parametrize the space accordingly for different tasks; c considerable flexibility is allowed in our formulation for selecting the dimensions of the semantic space; the latter can be words see the leaves in Figure 1, parts of speech or dependency edges; in Lins approach, it is only dependency edges features in his terminology that form the dimensions of the semantic space.	Experiment 1 revealed that the dependency-based model adequately simulates semantic priming.	2
Write A for the lexical association function which computes the value of a cell of the matrix from a co-occurrence frequency: Ki j  A f bi;t j 3 Evaluation 31 Parameter Settings All our experiments were conducted on the British National Corpus BNC, a 100 million word collection of samples of written and spoken language <REF>Burnard, 1995</REF>.	We used <TREF>Lins 1998</TREF> broad coverage dependency parser MINIPAR to obtain a parsed version of the corpus.	MINIPAR employs a manually constructed grammar and a lexicon derived from WordNet with the addition of proper names 130,000 entries in total.	Lexicon entries contain part-of-speech and subcategorization information.	2
It extends prior work on syntax-based models <REF>Grefenstette, 1994</REF>; <TREF>Lin, 1998</TREF>, by providing a general framework for defining context so that a large number of syntactic relations can be used in the construction of the semantic space.	Our approach differs from <TREF>Lin 1998</TREF> in three important ways: a by introducing dependency paths we can capture non-immediate relationships between words ie , between subjects and objects, whereas Lin considers only local context dependency edges in our terminology; the semantic space is therefore constructed solely from isolated head/modifier pairs and their inter-dependencies are not taken into account; b Lin creates the semantic space from the set of dependency edges that are relevant for a given word; by introducing dependency labels and the path value function we can selectively weight the importance of different labels eg , subject, object, modifier and parametrize the space accordingly for different tasks; c considerable flexibility is allowed in our formulation for selecting the dimensions of the semantic space; the latter can be words see the leaves in Figure 1, parts of speech or dependency edges; in Lins approach, it is only dependency edges features in his terminology that form the dimensions of the semantic space.	Experiment 1 revealed that the dependency-based model adequately simulates semantic priming.	Experiment 2 showed that a model that relies on rich context specifications can reliably distinguish between different types of lexical relations.	2
However, at structural level, the concept-based seeds share the same or similar linguistic patterns eg Subject-Verb-Object patterns with the corresponding types of proper names.	The rationale behind using concept-based seeds in NE bootstrapping is similar to that for parsingbased word clustering <TREF>Lin 1998</TREF>: conceptually similar words occur in structurally similar context.	In fact, the anaphoric function of pronouns and common nouns to represent antecedent NEs indicates the substitutability of proper names by the corresponding common nouns or pronouns.	For example, this man can be substituted for the proper name John Smith in almost all structural patterns.	2
Distributional Similarity Score: The GD04 similarity score of the pair was used as a feature.	We 583 also attempted adding Lins 1998 similarity scores but they appeared to be redundant.	Intersection Feature: A binary feature indicating candidate pairs acquired by both methods, which was found to indicate higher entailment likelihood.	In summary, the above feature types utilize mutually complementary pattern-based and distributional information.	3
For the distributional similarity component we employ the similarity scheme of <REF>Geffet and Dagan, 2004</REF>, which was shown to yield improved predictions of non-directional lexical entailment pairs.	This scheme utilizes the symmetric similarity measure of <TREF>Lin, 1998</TREF> to induce improved feature weights via bootstrapping.	These weights identify the most characteristic features of each word, yielding cleaner feature vector representations and better similarity assessments.	22 Pattern-based <REF>Approaches Hearst 1992</REF> pioneered the use of lexicalsyntactic patterns for automatic extraction of lexical semantic relationships.	2
The degree of similarity between two target words is then determined by a vector comparison function.	Amongst the many proposals for distributional similarity measures, <TREF>Lin, 1998</TREF> is maybe the most widely used one, while <REF>Weeds et al , 2004</REF> provides a typical example for recent research.	Distributional similarity measures are typically computed through exhaustive processing of a corpus, and are therefore applicable to corpora of bounded size.	It was noted recently by Geffet and Dagan 2004, 2005 that distributional similarity captures a quite loose notion of semantic similarity, as exemplified by the pair country  party identified by Lins similarity measure.	2
The method uses a thesaurus obtained from the text by parsing, extracting grammatical relations and then listing each word w with its top k nearest neighbours, where k is a constant.	Like <REF>McCarthy et al , 2004</REF> we use k  50 and obtain our thesaurus using the distributional similarity metric described by <TREF>Lin, 1998</TREF> and we use WordNet WN as our sense inventory.	The senses of a word w are each assigned a ranking score which sums over the distributional similarity scores of the neighbours and weights each neighbours score by a WN Similarity score <REF>Patwardhan and Pedersen, 2003</REF> between the sense of w and the sense of the neighbour that maximises the WN Similarity score.	This weight is normalised by the sum of such WN similarity scores between all senses of w and the senses of the neighbour that maximises this score.	2
Therefore, we keep at most 20 paraphrases for a phrase when extracting phrasal paraphrases using each resource.	1 Thesaurus: The thesaurus4 used in this work was automatically constructed by <TREF>Lin 1998</TREF>.	The similarity of two words e1 and e2 was calculated through the surrounding context words that have dependency relations with the investigated words: Sime1,e2  P r,eTre1Tre2Ie1,r,eIe2,r,eP r,eTre1 Ie1,r,e P r,eTre2 Ie2,r,e 5 where Trei denotes the set of words that have dependency relation r with word ei.	Iei,r,e is the mutual information between ei, r and e For each word, we keep 20 most similar words as paraphrases.	2
Those words that obtain the best values are considered to be most similar.	Practical implementations of algorithms based on this principle have led to excellent results as documented in papers by <REF>Ruge 1992</REF>, <REF>Grefenstette 1994</REF>, <REF>Agarwal 1995</REF>, <REF>Landauer  Dumais 1997</REF>, <REF>Schtze 1997</REF>, and <TREF>Lin 1998</TREF>.	21 Human Data In this section we relate the results of our version of such an algorithm to similarity estimates obtained by human subjects.	Fortunately, we did not need to conduct our own experiment to obtain the humans similarity estimates.	2
Given the great deal of similar work in information extraction and ontology learning, we focus here only on techniques for weakly supervised or unsupervised semantic class ie, supertype-based learning, since that is most related to the work in this paper.	Fully unsupervised semantic clustering eg, <TREF>Lin, 1998</TREF>; <REF>Lin and Pantel, 2002</REF>; <REF>Davidov and Rappoport, 2006</REF> has the disadvantage that it may or may not produce the types and granularities of semantic classes desired by a user.	Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes eg, <REF>Caraballo, 1999</REF>; <REF>Cimiano and Volker, 2005</REF>; <REF>Mann, 2002</REF>, and learning semantic relations such as meronymy <REF>Berland and Charniak, 1999</REF>; <REF>Girju et al, 2003</REF>.	Our research focuses on semantic lexicon induction, which aims to generate lists of words that belong to a given semantic class eg, lists of FISH or VEHICLE words.	3
Five part-of-speech features, two lexical features, and a paragraph feature were used.	Toidentify richer features, <REF>Wiebe, 2000</REF> used Lins 1998 method for clustering words according to distributional similarity,seededby a small amount of detailed manual annotation, to automatically identify adjective PSEs.	There are two parameters of this process, neither of whichwas varied in <REF>Wiebe, 2000</REF>: C, the cluster size considered, andFT, a lteringthreshold, such that, if the seed word and the words in its cluster have, as a set, lower precision than the ltering threshold on the training data, the entire cluster, including the seed word, is ltered out.	This process is adapted for use in the current paper, as described in section 7.	2
,, iwcount : frequency of the triples including word iw  N: number of triples in the corpus.	We use it instead of point-wise mutual information in <TREF>Lin 1998</TREF> because the latter tends to overestimate the association between two parts with low frequencies.	Weighted mutual information meliorates this effect by adding , ji attwp  24 Combining the Three Extractors In terms of combining the outputs of the different methods, the ensemble method is a good candidate.	Originally, the ensemble method is a machine learning technique of combining the outputs of several classifiers to improve the classification performance <REF>Dietterich, 2000</REF>.	3
The similarity is the sum of the WordNet similarities between all attribute keywords in the two exhibits K1, K2, normalised over the length of both keyword sets: summationtext k1K1 summationtext k2K2 WNsimk1,k2 K1K2 For the purposes of this experiment we have chosen to use three WordNet similarity/relatedness measures to simulate the conceptual connections that visitors make between exhibits.	The Lin <TREF>Lin, 1998</TREF> and Leacock-Chodorow <REF>Leacock et al , 1998</REF> similarity measures and the BanerjeePedersen <REF>Patwardhan and Pedersen, 2003</REF> relatedness measures were used.	The similarities were normalised and transformed into probability matrices such that summationtextj PWNsimecj  1 for each next exhibit ci.	The use of WordNet measures is intended to simulate the mental connections that visitors make between exhibit content, given that each visit can interpret content in a number of different ways.	2
Using Language Weavers 1 English-to-Spanish machine translation system, English marginal notes can be translated into Spanish.	22 Vocabulary Support Synonyms for lower frequency more difficult words are output using a statistically-generated word similarity matrix <TREF>Lin, 1998</TREF>.	ATA v10 generates antonyms for vocabulary in the text using WordNet .	2 Cognates are words which have the same spelling and meaning in two languages eg , animal in English and Spanish.	2
The parameters K and T are usually considered to be small numbers.	3 Word Similarity Following <TREF>Lin 1998</TREF>, we represent each word by a feature vector.	Each feature corresponds to a context in which the word occurs.	For example, threaten with  is a context.	2
Table 1 shows the number of feature types and tokens extracted for each phrase.	This shows that we have extracted a reasonable number of features for each phrase, since distributional similarity techniques have been shown to work well for words which occur more than 100 times in a given corpus <TREF>Lin, 1998</TREF>; <REF>Weeds and Weir, 2003</REF>.	We then computed the distributional similarity between each co-occurrence vector using the -skew divergence measure <REF>Lee, 1999</REF>.	The -skew divergence measure is an approximation to the KullbackLeibler KL divergence meassure between two distributions p and q: Dpq  summationdisplay x pxlogpxqx 5We currently retain all of the distinctions between grammatical relations output by RASP.	2
In their work on QA, Lin and Pantel restrict the grammatical relations considered to two slots at either end of the path where the word occupying the slot is a noun.	Co-occurrence vectors for paths are then built up using evidence from multiple occurrences of the paths in corpus data, for which similarity can then be calculated using a standard metric eg , <TREF>Lin 1998</TREF>.	In our work, we extend the notion of distributional similarity from linear paths to trees.	This allows us to compute distributional similarity for any part of an expression, of arbitrary length and complexity although, in practice, we are still limited by data sparseness.	2
W ORD S IMILAR W ORDS  WITH SIMILARITY SCORE  EAT cook 0127, drink 0108, consume 0101, feed 0094, taste 0093, like 0092, serve 0089, bake 0087, sleep 0086, pick 0085, fry 0084, freeze 0081, enjoy 0079, smoke 0078, harvest 0076, love 0076, chop 0074, sprinkle 0072, Toss 0072, chew 0072 SALAD soup 0172, sandwich 0169, sauce 0152, pasta 0149, dish 0135, vegetable 0135, cheese 0132, dessert 013, entree 0121, bread 0116, meat 0116, chicken 0115, pizza 0114, rice 0112, seafood 011, dressing 0109, cake 0107, steak 0105, noodle 0105, bean 0102 the collocation database for the words eat and salad  The database contains a total of 11 million unique dependency relationships.	22 Corpus-based thesaurus Using the collocation database, <TREF>Lin 1998b</TREF> used an unsupervised method to construct a corpusbased thesaurus consisting of 11839 nouns, 3639 verbs and 5658 adjectives/adverbs.	Given a word w, the thesaurus returns a set of similar words of w along with their similarity to w  For example, the 20 most similar words of eat and salad are shown in Table 1.	3 Training Data Extraction We parsed a 125-million word newspaper corpus with Minipar 3, a descendent of Principar <REF>Lin, 1994</REF>.	2
Below, we briefly describe these resources.	21 Collocation database Given a word w in a dependency relationship such as subject or object , the collocation database is used to retrieve the words that occurred in that relationship with w, in a large corpus, along with their frequencies <TREF>Lin, 1998a</TREF>.	Figure 1 shows excerpts of the entries in 2 Available at wwwcsualbertaca/lindek/demoshtm.	eat : object: almond 1, a pple 25, bean 5, beam 1, binge 1, bread 13, cake 17, cheese 8, dish 14, disorder 20, egg 31, grape 12, grub 2, hay 3, junk 1, meat 70, poultry 3, rabbit 4, soup 5, sandwich 18, pasta 7, vegetable 35,  subject: adult 3, animal 8, beetle 1, cat 3, child 41, decrease 1, dog 24, family 29, guest 7, kid 22, patient 7, refugee 2, rider 1, Russian 1, shark 2, something 19, We 239, wolf 5,  salad : adj-modifier: assorted 1, crisp 4, fresh 13, good 3, grilled 5, leftover 3, mixed 4, olive 3, prepared 3, side 4, small 6, special 5, vegetable 3,  object-of: add 3, consume 1, dress 1, grow 1, harvest 2, have 20, like 5, love 1, mix 1, pick 1, place 3, prepare 4, return 3, rinse 1, season 1, serve 8, sprinkle 1, taste 1, test 1, Toss 8, try 3,  Figure 1.	2
eat : object: almond 1, a pple 25, bean 5, beam 1, binge 1, bread 13, cake 17, cheese 8, dish 14, disorder 20, egg 31, grape 12, grub 2, hay 3, junk 1, meat 70, poultry 3, rabbit 4, soup 5, sandwich 18, pasta 7, vegetable 35,  subject: adult 3, animal 8, beetle 1, cat 3, child 41, decrease 1, dog 24, family 29, guest 7, kid 22, patient 7, refugee 2, rider 1, Russian 1, shark 2, something 19, We 239, wolf 5,  salad : adj-modifier: assorted 1, crisp 4, fresh 13, good 3, grilled 5, leftover 3, mixed 4, olive 3, prepared 3, side 4, small 6, special 5, vegetable 3,  object-of: add 3, consume 1, dress 1, grow 1, harvest 2, have 20, like 5, love 1, mix 1, pick 1, place 3, prepare 4, return 3, rinse 1, season 1, serve 8, sprinkle 1, taste 1, test 1, Toss 8, try 3,  Figure 1.	Excepts of entries in the collocation database for eat and salad  Table 1  The top 20 most similar words of eat and salad as given by <TREF>Lin, 1998b</TREF>.	W ORD S IMILAR W ORDS  WITH SIMILARITY SCORE  EAT cook 0127, drink 0108, consume 0101, feed 0094, taste 0093, like 0092, serve 0089, bake 0087, sleep 0086, pick 0085, fry 0084, freeze 0081, enjoy 0079, smoke 0078, harvest 0076, love 0076, chop 0074, sprinkle 0072, Toss 0072, chew 0072 SALAD soup 0172, sandwich 0169, sauce 0152, pasta 0149, dish 0135, vegetable 0135, cheese 0132, dessert 013, entree 0121, bread 0116, meat 0116, chicken 0115, pizza 0114, rice 0112, seafood 011, dressing 0109, cake 0107, steak 0105, noodle 0105, bean 0102 the collocation database for the words eat and salad  The database contains a total of 11 million unique dependency relationships.	22 Corpus-based thesaurus Using the collocation database, <TREF>Lin 1998b</TREF> used an unsupervised method to construct a corpusbased thesaurus consisting of 11839 nouns, 3639 verbs and 5658 adjectives/adverbs.	2
Three kinds of approaches are prevalent in the literature.	The first kind <REF>Wei 1993</REF>; <TREF>Lin 1998b</TREF> is a chiefly theoretical examination of a proposed measure for those mathematical properties thought desirable, such as whether it is a metric or the inverse of a metric, whether it has singularities, whether its parameter-projections are smooth functions, and so on.	In our opinion, such analyses act at best as a coarse filter in the comparison of a set of measures and an even coarser one in the assessment of a single measure.	The second kind of evaluation is comparison with human judgments.	3
Each cluster corresponds to a sense of the headword.	2 Feature Representation Following <TREF>Lin 1998</TREF>, we represent each word by a feature vector.	Each feature corresponds to a context in which the word occurs.	For example, sip  is a verbobject context.	2
All words in the vocabulary sharing the same root form are grouped together.	Then we do corpus analysis to filter out the words which are clustered incorrectly, according to word distributional similarity, following <REF>Xu and Croft, 1998</REF>; <TREF>Lin 1998</TREF>.	The rationale behind this is that words sharing the same meaning tend to occur in the same contexts.	The context of each word in the vocabulary is represented by a vector containing the frequencies of the context words which co-occur with the word within a predefined window in a training corpus.	2
Thus, correct match of an argument corresponds to correct role identification.	The templates were represented as Minipar <TREF>Lin, 1998b</TREF> dependency parse-trees.	The Contextual Preferences for h were constructed manually: the named-entity types for cpv:nh were set by adapting the entity types given in the guidelines to the types supported by the Lingpipe NER described in Section 32.	cpgh was generated from a short list of nouns and verbs that were extracted from the verbal event definition in the ACE guidelines.	2
As a more natural ranking method, we also utilize SCBC directly, denoted rankedCBC, having mv:er,t  SCBCr,t.	In addition, we tried a simpler method that directly compares the terms in two cpv:e lists, utilizing the commonly-used term similarity metric of <TREF>Lin, 1998a</TREF>.	This method, denoted LIN, uses the same raw distributional data as CBC but computes only pair-wise similarities, without any clustering phase.	We calculated the scores of the 1000 most similar terms for every term in the Reuters RVC1 corpus3.	2
First of all, it allows us to provide both positive and negative examples, avoiding the use of one-class classification algorithms that in practice perform poorly <REF>Dagan et al , 2006</REF>.	Second, the large availability of manually constructed substitution lexica, such as WordNet <REF>Fellbaum, 1998</REF>, or the use of repositories based on statistical word similarities, such as the database constructed by <TREF>Lin 1998</TREF>, allows us to find an adequate substitution lexicon for each target word in most of the cases.	For example, as shown in Table 1, the word job has different senses depending on its context, some of them entailing its direct hyponym position eg , looking for permanent job, others entailing the word task eg , the job of repairing.	The problem of deciding whether a particular instance of job can be replaced by position, and not by the word place, can be solved by looking for the most similar contexts where either position or place occur in the training data, and then selecting the class ie , the entailed word characterized by the most similar ones, in an instance based style.	2
<REF>Cimiano and Volker 2005</REF> assign a particular entity to the fine-grained class suchthatthecontextualsimilarityismaximalamong the set of fine-grained subclasses of a coarse-grained category.	Contextual similarity has been measured by adopting lexico-syntactic features provided by a dependency parser, as proposed in <TREF>Lin, 1998</TREF>.	3 Instance Based Lexical Entailment Dagan et al.	2006 adapted the classical supervised WSD setting to approach the sense matching problem ie , the binary lexical entailment problem of deciding whether a word, such as position, entails a different word, such as job, in a given context by defining a one-class learning algorithm based on support vector machines SVM.	2
It is worthwhile to remark that, due to the ambiguity of the entailed words eg , position could also entail either perspective or place, not every occurrence of them should be taken into account, in order to avoid misleading predictions caused by the irrelevant senses.	Therefore, approaches based on a more classical contextual similarity technique <TREF>Lin, 1998</TREF>; <REF>Dagan, 2000</REF>, where words are described globally by context vectors, are doomed to fail.	We will provide empirical evidence of this in the evaluation section.	Choosing an appropriate similarity function for the contexts of the words to be substituted is a primary issue.	3
There are a number of ways to measure the distance between two nouns in the WordNet noun hierarchy see Budanitsky 1999 for a review.	In previous work <REF>Weeds and Weir 2003b</REF>, we used the WordNet-based similarity measure first proposed in <REF>Lin 1997</REF> and used in <TREF>Lin 1998a</TREF>: wn sim lin w 1, w 2   max c 1 Sw 1 c 2 Sw 2  parenleftbigg max csupc 1 supc 2  2logPc log Pc 1   log Pc 2  parenrightbigg 49 where Sw is the set of senses of the word w in WordNet, supc is the set of possibly indirect super-classes of concept c in WordNet, and Pc is the probability that a randomly selected word refers to an instance of concept c estimated over some corpus such as SemCor <REF>Miller et al 1994</REF>.	However, in other research <REF>Budanitsky and Hirst 2001</REF>; <REF>Patwardhan, Banerjee, and Pedersen 2003</REF>; <REF>McCarthy, Koeling, and Weeds 2004</REF>, it has been shown that the distance measure of <REF>Jiang and Conrath 1997</REF> referred to herein as the JC measure is a superior WordNet-based semantic similarity measure: wn dist JC w 1, w 2   max c 1 Sw 1 c 2 Sw 2  parenleftbigg max csupc 1 supc 2  2logc  log Pc 1   log Pc 2  parenrightbigg 50 In our work, we make an empirical comparison of neighbors derived using a WordNet-based measure and each of the distributional similarity measures using the technique discussed in Section 3.	We have carried out the same experiments using both the Lin measure and the JC measure.	2
45 Hindles <REF>Measure Hindle 1990</REF> proposed an MI-based measure, which he used to show that nouns could be reliably clustered based on their verb co-occurrences.	We consider the variant of 458 Weeds and Weir Co-occurrence Retrieval Figure 1 Variation with parameters  and  in development set mean similarity between neighbor sets of the additive t-test based CRM and of dist   Hindles Measure proposed by <TREF>Lin 1998a</TREF>, which overcomes the problem associated with calculating MI for word-feature combinations that do not occur: sim hind w 1, w 2   summationdisplay Tw 1 Tw 2  minIc, w 1 , Ic, w 2  38 where Tw 1  c : Ic, n > 0.	This expression is the same as the numerator in the expressions for precision and recall in the difference-weighted MI-based CRM: P dw mi w 1, w 2   summationtext TP Iw 1, c  minIw 1, c,Iw 2, c Iw 1, c summationtext Fw 1  Iw 1, c  summationtext TP minIw 1, c, Iw 2, c summationtext Fw 1  Iw 1, c 39 R dw mi w 1, w 2   summationtext TP Iw 2, c  minIw 2, c,Iw 1, c Iw 2, c summationtext Fw 2  Iw 2, c  summationtext TP minIw 2, c, Iw 1, c summationtext Fw 2  Iw 2, c 40 since TP  Tw 1   Tw 2 .	However, we also note that the denominator in the expression for recall depends only on w 2, and therefore, for a given w 2, is a constant.	2
Further, the noun hyponymy hierarchy in WordNet, which will be used as a pseudo-gold standard for comparison, is widely recognized in this area of research.	Some previous work on distributional similarity between nouns has used only a single grammatical relation eg , <REF>Lee 1999</REF>, whereas other work has considered multiple grammatical relations eg , <TREF>Lin 1998a</TREF>.	We consider only a single grammatical relation because we believe that it is important to evaluate the usefulness of each grammatical relation in calculating similarity before deciding how to combine information from 5 This results in a single 80:20 split of the complete data set, in which we are guaranteed that the original relative frequencies of the target nouns are maintained.	6 The use of grammatical relations to model context precludes finding similarities between words of different parts of speech.	2
Second, as we noted in Section 22, standard dictionary definitions are not usually fine-grained enough they define the core meaning but not all the nuances of a word and can even be circular, defining each of several nearsynonyms in terms of the other near-synonyms.	And third, although corpus-based methods eg , Lins 1998 do compute different similarity values for different pairs of near-synonyms of the same cluster, Church et al.	1994 and <REF>Edmonds 1997</REF> show that such methods are not yet capable of uncovering the more subtle differences in the use of near-synonyms for lexical choice.	But one benefit of the clustered model of lexical knowledge is that it naturally lends itself to the computation of explicit differences or degrees of similarity between near-synonyms.	3
Recent research in computational linguistics has focused more on developing methods to compute the degree of semantic similarity between any two words, or, more precisely, between the simple or primitive concepts 15 denoted by any two words.	There are many different similarity measures, which variously use taxonomic lexical hierarchies or lexical-semantic networks, large text corpora, word definitions in machine-readable dictionaries or other semantic formalisms, or a combination of these <REF>Dagan, Marcus, and Markovitch 1993</REF>; <REF>Kozima and Furugori 1993</REF>; <REF>Pereira, Tishby, and Lee 1993</REF>; <REF>Church et al 1994</REF>; <REF>Grefenstette 1994</REF>; <REF>Resnik 1995</REF>; <REF>McMahon and Smith 1996</REF>; <REF>Jiang and Conrath 1997</REF>; Sch utze 1998; <TREF>Lin 1998</TREF>; <REF>Resnik and Diab 2000</REF>; <REF>Budanitsky 1999</REF>; <REF>Budanitsky and Hirst 2001, 2002</REF>.	Unfortunately, these methods are generally unhelpful in computing the similarity of near-synonyms because the measures lack the required precision.	First, taxonomic hierarchies and semantic networks inherently treat near-synonyms as absolute synonyms in grouping near-synonyms into single nodes eg , in WordNet.	3
33 Evaluation of Class Attributes Extraction Parameters: Given a target class specified as a set of instances and a set of five seed attributes for a class eg, quality, speed, number of users, market share, reliability for SearchEngine, the method described in Section 22 extracts ranked lists of class attributes from the input query logs.	Internally, the ranking uses Jensen-Shannon <TREF>Lee, 1999</TREF> to compute similarity scores between internal representations of seed attributes, on one hand, and each of the candidate attributes, on the other hand.	Evaluation Procedure: To remove any possible bias towards higher-ranked attributes during the assessment of class attributes, the ranked lists of attributes to be evaluated are sorted alphabetically into a merged list.	Each attribute of the merged list is  0  02  04  06  08  1  0  10  20  30  40  50 Precision Rank Class: Holiday manually assembled instances automatically extracted instances  0  02  04  06  08  1  0  10  20  30  40  50 Precision Rank Class: Average-Class manually assembled instances automatically extracted instances  0  02  04  06  08  1  0  10  20  30  40  50 Precision Rank Class: Mountain manually assembled instances automatically extracted instances  0  02  04  06  08  1  0  10  20  30  40  50 Precision Rank Class: Average-Class manually assembled instances automatically extracted instances Figure 3: Accuracy of attributes extracted based on manually assembled, gold standard M vs automatically extracted E instance sets, for a few target classes leftmost graphs and as an average over all 37 target classes rightmost graphs.	2
In class-based smoothing, classes are used as the basis according to which the co-occurrence probability of unseen word combinations is estimated.	Classes can be induced directly from the corpus using distributional clustering <REF>Pereira, Tishby, and Lee 1993</REF>; <REF>Brown et al 1992</REF>; <REF>Lee and Pereira 1999</REF> or taken from a manually crafted taxonomy <REF>Resnik 1993</REF>.	In the latter case the taxonomy is used to provide a mapping from words to conceptual classes.	Distance-weighted averaging differs from distributional clustering in that it does not explicitly cluster words.	2
We used two measures, the Jensen-Shannon divergence and the confusion probability.	The choice of these two measures was motivated by work described in <REF>Dagan, Lee, and Pereira 1999</REF>, in which the JensenShannon divergence outperforms related similarity measures such as the confusion probability or the L 1 norm on a pseudodisambiguation task that uses verb-object pairs.	The confusion probability has been used by several authors to smooth word co367 Lapata The Disambiguation of Nominalizations occurrence probabilities <REF>Essen and Steinbiss 1992</REF>; <REF>Grishman and Sterling 1994</REF> and shown to give promising performance.	<REF>Grishman and Sterling 1994</REF> in particular employ the confusion probability to re-create the frequencies of verb-noun co-occurrences in which the noun is the object or the subject of the verb in question.	2
<REF>Briefly, Clark and Weir 2002</REF> populate the WordNet hierarchy based on corpus frequencies of all nouns for a verb/slot pair, and then determine the appropriate probability estimate at each node in the hierarchy by using a24 a102 to determine whether to generalize an estimate to a parent node in the hierarchy.	We compare SPD to other measures applied directly to the unpropagated probability profiles given by the Clark-Weir method: the probability distribution distance given by skew divergence skew <TREF>Lee, 1999</TREF>, as well as the general vector distance given by cosine cos.	These are the measures aside from SPD that performed best in our pilot experiments.	It is worth noting that the method of <REF>Clark and Weir 2002</REF> does not yield a tree cut, but instead generally populates the WordNet hierarchy with non-zero probabilities.	2
Due to the original KL distance is asymmetric and is not defined when zero frequency occurs.	Some enhanced KL models were developed to prevent these problems such as Jensen-Shannon <REF>Jianhua, 1991</REF>, which introducing a probabilistic variable m, or  -Skew Divergence <TREF>Lee, 1999</TREF>, by adopting adjustable variable .	Research shows that Skew Divergence achieves better performance than other measures.	<REF>Lee, 2001</REF> 1yxS rgenceDSkewDive yxxKL aaa  2/,2/yx,JS Shannon-DJensen yxm myKLmxKL   To convert distance to similarity value, we adopt the formula inspired by <REF>Mochihashi, and Matsumoto 2002</REF>.	2
Figure 2 exemplifies this process for two TOMs TCM1 and TCM2 in an imaginary hierarchy.	The UBC is at the classes B, c and D To quantify the similarity between the probability distributions for the target slots we use the a-skew divergence aSD proposed by <TREF>Lee 1999</TREF>.	1 This measure, defined in equation 2, is a smoothed version of the Kulback-Liebler divergence, plx and p2x are the two probability distributions which are being compared.	The  constant is a value between 0 and 1 We also experimented with euclidian distance, the L1 norm, and cosine measures.	2
The distributional similarity was measured by means of three different similarity measures: the Jaccards coefficient, L1 distance, and the skew divergence.	This choice of similarity measures was motivated by results of studies by <REF>Levy et al 1998</REF> and <TREF>Lee 1999</TREF> which compared several well known measures on similar tasks and found these three to be superior to many others.	Another reason for this choice is that there are different ideas underlying these measures: while the Jaccards coefficient is a binary measure, L1 and the skew divergence are probabilistic, the former being geometrically motivated and the latter being a version of the information theoretic Kullback Leibler divergence cf.	, <TREF>Lee 1999</TREF>.	2
Comparing the different divergence measures for LDA, we found that KL and JS perform significantly better than symmetrised KL divergence.	Interestingly, the performance of the asymmetric KL divergence and the symmetric JS divergence is very close, which makes it difficult to conclude whether the relation discovery domain is a symmetric domain or an asymmetric domain like <TREF>Lees 1999</TREF> task of improving probability estimates for unseen word co-occurrences.	A shortcoming of all the models we will describe here is that they are derived from the basic bag-of-words models and as such do not account for word order or other notions of syntax.	Related work on relation discovery by Zhang et al.	3
The optimal configuration varies by the divergence measure with D  50 and C  14 for KL divergence, D  200 and C  4 for symmetrised KL, and D  150 and C  2 for JS divergence.	For all divergence measures, <TREF>Lees 1999</TREF> method outperformed Dagan et als 1997 method.	Also for all divergence measures, the model hyper-parameter  was found to be optimal at 00001.	The  hyper-parameter was always set to 50/T following <REF>Griffiths and Steyvers 2004</REF>.	2
For a48 a49 a1a51a50a23a52a54a53a19a5 and word-conditional context distributions a55 and a56, we have the so-called a48 -divergences <REF>Zhu and Rohwer, 1998</REF>: a57 a58 a1a59a55a60a52a61a56a4a5a63a62a59a64 a53a65a14 a7 a55 a58 a56 a11a38a66 a58 a48a67a1a45a53a18a14a16a48a42a5 1 Divergences a57 a68 and a57 a11 are defined as limits as a48a6a69 a50 and a48a6a69a70a53 :a57 a11 a1a59a55a60a52a61a56a4a5a71a64 a57 a68 a1a51a56a67a52a51a55a72a5a71a64a74a73 a55a76a75a78a77a47a79 a55 a56 In other words, a57 a11a19a1a59a55a60a52a61a56a4a5 is the KL-divergence of a55 from a56  Members of this divergence family are in some sense preferred by theory to alternative measures.	It can be shown that the a48 -divergences or divergences defined by combinations of them, such as the Jensen-Shannon or skew divergences <TREF>Lee, 1999</TREF> are the only ones that are robust to redundant contexts ie , only divergences in this family are invariant <REF>Csiszar, 1975</REF>.	Several notions of lexical similarity have been based on the KL-divergence.	Note that if any a56 a8 a64a80a50, then a57 a11a81a1a59a55a60a52a61a56a4a5 is infinite; in general, the KLdivergence is very sensitive to small probabilities, and careful attention must be paid to smoothing if it is to be used with text co-occurrence data.	2
We do not know whether or to what extent this particular parameter setting is universally best, best only for English, best for newswire English, or best only for the specific test we have devised.	We have restricted our attention to a relatively small space of similarity measures, excluding many previously proposed measures of lexical affinity but see Weeds, et al 2004, and <TREF>Lee 1999</TREF> for some empirical comparisons.	Lee observed that measures from the space of invariant divergences particularly the JS and skew divergences perform at least as well as any of a wide variety of alternatives.	As noted, we experimented with the JS divergence and observed accuracies that tracked those of the Hellinger closely.	2
We draw a distinction between the task of identifying terms which are topically related and identifying the specific semantic class.	For example, the terms dog, puppy, canine, schnauzer, cat and pet are highly related terms, which can be identified using techniques that include distributional similarity <TREF>Lee, 1999</TREF> and withindocument cooccurrence measures such as pointwise mutual information <REF>Turney et al , 2003</REF>.	These techniques, however, do not allow us to distinguish the more specific relationships:  hypernymdog,puppy This work was carried out while these authors were at Yahoo.	Research.	3
One is Jensen-Shannon divergence <REF>Lin, 1991</REF>, a symmetric measure based on KL-divergence defined as the average of the KL divergences of each distribution to their average distribution.	Jensen-Shannon is well defined for all distributions becausetheaverageofpi andqi isnon-zerowhenevereither number is These measures and others are surveyed in <REF>Lee, 2001</REF>, who finds that Jensen-Shannon is outperformed by the Skew divergence measure introduced by Lee in 1999.	The skew divergence2 accounts for zeros in q by mixing in a small amount of p sp,q  Dp bardbl q  1p  summationtexti pi log piqi1pi Lee found that as   1, the performance of skew divergence on natural language tasks improves.	In particular, it outperforms most other models and even beats pure KL divergence modified to avoid zeros with sophisticated smoothing models.	2
Opinion-piece data are used for training, and a different set of opinion-piece data and the subjective-element data are used for testing.	With distributional similarity, words are judged to be more or less similar based on their distributional patterning in text <TREF>Lee 1999</TREF>; <REF>Lee and Pereira 1999</REF>.	Our Table 5 Random sample of fixed-3-gram collocations in OP1.	one-noun of-prep his-det worst-adj of-prep all-det quality-noun of-prep the-det to-prep do-verb so-adverb in-prep the-det company-noun you-pronoun and-conj your-pronoun have-verb taken-verb the-det rest-noun of-prep us-pronoun are-verb at-prep least-adj but-conj if-prep you-pronoun as-prep a-det weapon-noun continue-verb to-to do-verb purpose-noun of-prep the-det could-modal have-verb be-verb it-pronoun seem-verb to-prep to-pronoun continue-verb to-prep have-verb be-verb the-det do-verb something-noun about-prep cause-verb you-pronoun to-to evidence-noun to-to back-adverb that-prep you-pronoun are-verb i-pronoun be-verb not-adverb of-prep the-det century-noun of-prep money-noun be-prep 291 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language Table 6 Random sample of unique generalized collocations in OP1.	2
However, due to the lack of a tight de nition for the concept of distributional similarity and the broad range of potential applications, a large number of measures of distributional similarity have been proposed or adopted see Section 2.	Previous work on the evaluation of distributional similarity methods tends to either compare sets of distributionally similar words to a manually created semantic resource <REF>Lin, 1998</REF>; <REF>Curran and Moens, 2002</REF> or be oriented towards a particular task such as language modelling <REF>Dagan et al , 1999</REF>; <TREF>Lee, 1999</TREF>.	The rst approach is not ideal since it assumes that the goal of distributional similarity methods is to predict semantic similarity and that the semantic resource used is a valid gold standard.	Further, the second approach is clearly advantageous when one wishes to apply distributional similarity methods in a particular application area.	2
It seems likely that distance measures that are known to work well for comparing co-occurrence distributions will also give us suitable psd similarity measures.	Negative semi-definite kernels are bydefinitionsymmetric, whichrulestheKullbackLeibler divergence and <TREF>Lees 1999</TREF>-skew divergence out of consideration.	The nsd condition 2 ismetifthedistancefunctionisasquaredmetricin a Hilbert space.	In this paper we use a parametric familyofsquaredHilbertianmetricsonprobability distributions that has been discussed by <REF>Hein and Bousquet 2005</REF>.	3
23 Distributional Kernels Given the effectiveness of distributional similarity measures for numerous tasks in NLP and the interpretation of kernels as similarity functions, it seems natural to consider the use of kernels tailored for co-occurrence distributions when performing semantic classification.	As shown in Section 22 the standardly used linear and Gaussian kernelsderivefromtheL2 distance, yet<TREF>Lee1999</TREF> has shown that this distance measure is relatively poor at comparing co-occurrence distributions.	Information theory provides a number of alternative distance functions on probability measures, of which the L1 distance also called variational distance, Kullback-Leibler divergence and JensenShannon divergence are well-known in NLP and 1Negated nsd functions are sometimes called conditionally psd; they constitute a superset of the psd functions.	650 Distance Definition Derived linear kernel L2 distance2 summationtextcPcw1Pcw22 summationtextc Pcw1Pcw2 L1 distance summationtextcPcw1Pcw2 summationtextc minPcw1,Pcw2 Jensen-Shannon summationtextc Pcw1log2 2Pcw1Pcw1Pcw2  summationtextc Pcw1log2 Pcw1Pcw1Pcw2  divergence Pcw2log2 2Pcw2Pcw1Pcw2 Pcw2log2 Pcw2Pcw1Pcw2 Hellinger distance summationtextcradicalbigPcw1radicalbigPcw22 summationtextcradicalbigPcw1Pcw2 Table 1: Squared metric distances on co-occurrence distributions and corresponding linear kernels were shown by Lee to give better similarity estimates than the L2 distance.	2
A key feature of this type of smoothing is the function which measures distributional similarity from cooccurrence frequencies.	Several measures of distributional similarity have been proposed in the literature <REF>Dagan et al , 1999</REF>; <TREF>Lee, 1999</TREF>.	We used two measures, the Jensen-Shannon divergence and the confusion probability.	Those two measures have been previously shown to give promising performance for the task of estimating the frequencies of unseen verb-argument pairs <REF>Dagan et al , 1999</REF>; <REF>Grishman and Sterling, 1994</REF>; <REF>Lapata, 2000</REF>; <TREF>Lee, 1999</TREF>.	2
We used two measures, the Jensen-Shannon divergence and the confusion probability.	Those two measures have been previously shown to give promising performance for the task of estimating the frequencies of unseen verb-argument pairs <REF>Dagan et al , 1999</REF>; <REF>Grishman and Sterling, 1994</REF>; <REF>Lapata, 2000</REF>; <TREF>Lee, 1999</TREF>.	In the following we describe these two similarity measures and show how they can be used to recreate the frequencies for unseen adjective-noun pairs.	Jensen-Shannon Divergence.	2
The two measures are shown in Figure 2.	The Skew divergence represents a generalisation of the Kullback-Leibler divergence and was proposed by <TREF>Lee 1999</TREF> as a linguistically motivated distance measure.	We use a value of   :99.	We explored in detail the influence of different types and sizes of context by varying the context specification and path value functions.	2
There are a number of studies that, starting from this hypothesis, have built automatic or semi-automatic procedures for clustering words <REF>Brill and Marcus, 1992</REF>; <REF>Pereira et al , 1993</REF>; <REF>Martin et al , 1998</REF>, especially in the field of cognitive sciences <REF>Redington et al , 1998</REF>; <REF>Gobet and Pine, 1997</REF>; <REF>Clark, 2000</REF>.	They examine the distributional behaviour of some target words, comparing the lexical distribution of their respective collocates using quantitative measures of distributional similarity <TREF>Lee, 1999</TREF>.	In <REF>Brill and Marcus, 1992</REF> it is given a semiautomatic procedure that, starting from lexical statistical data collected from a large corpus, aims to arrange target words in a tree more precisely a dendrogram, instead of clustering them automatically.	This procedure requires a linguistic examination of the resulting tree, in order to identify the word classes that are most appropriate to describe the phenomenon under investigation.	2
The formula is symmetric but does not satisfy the triangle inequality.	For speed the estimate may be calculated from the shared features alone <TREF>Lee, 1999</TREF>.	After calculating all the pairwise estimates, we retained lists of the 100 most similar nouns for each of the nouns in the corpus data.	No other data is used in the similarity calculations.	2
1993 and <TREF>Lee 1999</TREF>, among others.	We use the cosine similarity measure for windowbased contexts and the following commonly used similarity measures for the syntactic vector space: <REF>Hindles 1990</REF> measure, the weighted Lin measure <REF>Wu and Zhou, 2003</REF>, the -Skew divergence measure <TREF>Lee, 1999</TREF>, the Jensen-Shannon JS divergence measure <REF>Lin, 1991</REF>, Jaccards coef cient van <REF>Rijsbergen, 1979</REF> and the Confusion probability <REF>Essen and Steinbiss, 1992</REF>.	The Jensen-Shannon measure JS x1, x2  summationtext yY summationtext xx1,x2 parenleftbigg P yx log parenleftbigg Pyx 1 2 Pyx1Pyx2 parenrightbiggparenrightbigg subsequently performed best for our task.	We compare the different ranking methodologies and data sets with respect to a manually-de ned gold standard list of 20 goal-type verbs and 20 nouns.	3
We use verb-object relations in both active and passive voice constructions as did Pereira et al.	1993 and <TREF>Lee 1999</TREF>, among others.	We use the cosine similarity measure for windowbased contexts and the following commonly used similarity measures for the syntactic vector space: <REF>Hindles 1990</REF> measure, the weighted Lin measure <REF>Wu and Zhou, 2003</REF>, the -Skew divergence measure <TREF>Lee, 1999</TREF>, the Jensen-Shannon JS divergence measure <REF>Lin, 1991</REF>, Jaccards coef cient van <REF>Rijsbergen, 1979</REF> and the Confusion probability <REF>Essen and Steinbiss, 1992</REF>.	The Jensen-Shannon measure JS x1, x2  summationtext yY summationtext xx1,x2 parenleftbigg P yx log parenleftbigg Pyx 1 2 Pyx1Pyx2 parenrightbiggparenrightbigg subsequently performed best for our task.	3
Although -Skew outperforms the simpler measures in ranking nouns, its performance on verbs is worse than the performance of Weighted Lin.	<REF>While Lee 1999</REF> argues that -Skews asymmetry can be advantageous for nouns, this probably does not hold for verbs: verb hierarchies have much shallower structure than noun hierarchies with most verbs concentrated on one level <REF>Miller et al , 1990</REF>.	This would explain why JS, which is symmetric compared to the -Skew metric, performed better in our experiments.	In the evaluation presented here we therefore use Google Scholar data and the JS measure.	3
We are mainly interested in the symmetric measures dxi, yj  dyj, xi because of a symmetric positive semi-de nite matrix required by kernel methods.	Consequently, such measures as the skew divergence were excluded from the consideration <TREF>Lee, 1999</TREF>.	The Euclidean measure as de ned in Table 1 does not necessarily vary from 0 to 1.	It was therefore normalized by dividing an l2 score in Table 1 by a maximum score and retracting it from 1.	3
This shows that we have extracted a reasonable number of features for each phrase, since distributional similarity techniques have been shown to work well for words which occur more than 100 times in a given corpus <REF>Lin, 1998</REF>; <REF>Weeds and Weir, 2003</REF>.	We then computed the distributional similarity between each co-occurrence vector using the -skew divergence measure <TREF>Lee, 1999</TREF>.	The -skew divergence measure is an approximation to the KullbackLeibler KL divergence meassure between two distributions p and q: Dpq  summationdisplay x pxlogpxqx 5We currently retain all of the distinctions between grammatical relations output by RASP.	10 The -skew divergence measure is designed to be used when unreliable maximum likelihood estimates MLE of probabilities would result in the KL divergence being equal to .	2
Second, whereas semantic relatedness is symmetric, distributional similarity is a potentially asymmetrical relationship.	If distributional similarity is conceived of as substitutability, as <REF>Weeds and Weir 2005</REF> and <TREF>Lee 1999</TREF> emphasize, then asymmetries arise when one word appears in a subset of the contexts in which the other appears; for example, the adjectives that typically modify apple are a subset of those that modify fruit,sofruit substitutes for apple better than apple substitutes for fruit.	While some distributional similarity measures, such as cosine, are symmetric, many, such as -skew divergence and the co-occurrence retrieval models developed by Weeds and Weir, are not.	But this is simply not an adequate model of semantic relatedness, for which substitutability is far too strict a requirement: window and house are semantically related, but they are not plausibly substitutable in most contexts.	3
Baseline1 and Baseline2 in our system use different back-off schema.	The following formula is introduced in <TREF>Lee 1999</TREF> for word similarity-based smoothing: 4 , ,    1         tt tt wSw tt wSw tttt tt wwsim wtagPwwsim wtagP where Sw is a set of candidate similar words and simw,w is the similarity between word w and w.	Word similarity-based smoothing approach is used in our system to make advantage of the huge unlabeled corpus.	In order to plug the word similarity-based smoothing into our HMM model, we made several extensions to formula 4.	2
This is advantageous in the computation of similarity, since computing the sums over all co-occurrence types rather than just those co-occurring with at least one of the words is 1 very computationally expensive and 2 due to their vast number, the effect of these zero frequency co-occurrence types tends to outweigh the effect of those co-occurrence types that have actually occurred.	Giving such weight to these shared non-occurrences seems unintuitive and has been shown by <TREF>Lee 1999</TREF> to be undesirable in the calculation of distributional similarity.	Hence, when using the 448 Weeds and Weir Co-occurrence Retrieval ALLR as the weight function, we use the additional restriction that Pc, w > 0 when selecting features.	24 Difference-Weighted Models In additive models, no distinction is made between features that have occurred to the same extent with each word and features that have occurred to different extents with each word.	2
Further, the noun hyponymy hierarchy in WordNet, which will be used as a pseudo-gold standard for comparison, is widely recognized in this area of research.	Some previous work on distributional similarity between nouns has used only a single grammatical relation eg , <TREF>Lee 1999</TREF>, whereas other work has considered multiple grammatical relations eg , <REF>Lin 1998a</REF>.	We consider only a single grammatical relation because we believe that it is important to evaluate the usefulness of each grammatical relation in calculating similarity before deciding how to combine information from 5 This results in a single 80:20 split of the complete data set, in which we are guaranteed that the original relative frequencies of the target nouns are maintained.	6 The use of grammatical relations to model context precludes finding similarities between words of different parts of speech.	2
A statistical technique using a language model that assigns a zero probability to these previously unseen events will rule the correct parse or interpretation of the utterance impossible.	Similarity-based smoothing <REF>Hindle 1990</REF>; <REF>Brown et al 1992</REF>; <REF>Dagan, Marcus, and Markovitch 1993</REF>; <REF>Pereira, Tishby, and Lee 1993</REF>; <REF>Dagan, Lee, and Pereira 1999</REF> provides an intuitively appealing approach to language modeling.	In order to estimate the probability of an unseen co-occurrence of events, estimates based on seen occurrences of similar events can be combined.	For example, in a speech recognition task, we might predict that cat is a more likely subject of growl than the word cap, even though neither co-occurrence has been seen before, based on the fact that cat is similar to words that do occur as the subject of growl eg , dog and tiger, whereas cap is not.	2
They demonstrate that the counts re-created using this smoothing technique correlate significantly with plausibility judgments for adjective-noun bigrams.	They also show that this class-based approach outperforms distance-weighted averaging <REF>Dagan, Lee, and Pereira 1999</REF>, a smoothing method that re-creates unseen word co-occurrences on the basis of distributional similarity without relying on a predefined taxonomy, in predicting plausibility.	In the current study, we used the smoothing technique of <REF>Lapata, Keller, and McDonald 2001</REF> to re-create not only adjective-noun bigrams, but also noun-noun 475 Keller and Lapata Web Frequencies for Unseen Bigrams Table 12 Correlation of counts re-created using class-based smoothing with Web counts.	Adjective-Noun Noun-Noun Verb-Object Seen Bigrams AltaVista 344 362 361 Google 330 343 349 Unseen Bigrams AltaVista 439 386 412 Google 444 421 397 p <05 one-tailed.	3
By using Japanese HTML documents, we empirically show that our proposed method can obtain a significant number of hyponymy relations which would otherwise be missed by alternative methods.	Hyponymy relations can play a crucial role in various NLP systems, and there have been many attempts to develop automatic methods to acquire hyponymy relations from text corpora <REF>Hearst, 1992</REF>; <TREF>Caraballo, 1999</TREF>; <REF>Imasumi, 2001</REF>; <REF>Fleischman et al , 2003</REF>; <REF>Morin and Jacquemin, 2003</REF>; <REF>Ando et al , 2003</REF>.	Most of these techniques have relied on particular linguistic patterns, such as NP such as NP The frequencies of use for such linguistic patterns are relatively low, though, and there can be many expressions that do not appear in such patterns even if we look at large corpora.	The effort of searching for other clues indicating hyponymy relations is thus significant.	2
Previous work on automatic methods for building semantic lexicons could be divided into two main groups.	One is automatic thesaurus acquisition, that is, to identify synonyms or topically related words from corpora based on various measures of similarity eg <REF>Riloff and Shepherd, 1997</REF>; <REF>Lin, 1998</REF>; <TREF>Caraballo, 1999</TREF>; <REF>Thelen and Riloff, 2002</REF>; <REF>You and Chen, 2006</REF>.	Another line of research, which is more closely related to the current study, is to extend existing thesauri by classifying new words with respect to their given structures eg <REF>Tokunaga et al, 1997</REF>; <REF>Pekar, 2004</REF>.	An early effort along this line is <REF>Hearst 1992</REF>, who attempted to identify hyponyms from large text corpora, based on a set of lexico-syntactic patterns, to augment and critique the content of WordNet.	2
In contrast, in this paper we focus on the problem of determining the categories of interest.	Another thread of work is on finding synonymous terms and word associations, as well as automatic acquisition of IS-A or genus-head relations from dictionary definitions and free text <REF>Hearst, 1992</REF>; <TREF>Caraballo, 1999</TREF>.	That work focuses on finding the right position for a word within a lexicon, rather than building up comprehensible and coherent faceted hierarchies.	A major class of solutions for creating subject hierarchies uses data clustering.	2
One way to overcome this problem might be to give judges information about a sequence of higher ancestors, in order to make the judgement easier.	It is difficult to compare these results with results from other studies such as that of <TREF>Caraballo 1999</TREF>, as the data used is not the same.	However, it seems that our figures are in the same range as those reported in previous studies.	<REF>Charniak  Roark 1998</REF>, evaluating the semantic lexicon against gold standard resources the MUC-4 and the WSJ corpus, reports that the ratio of valid to total entries for their system lies between 20 and 40.	2
However, it is well known that any knowledge-based system suffers from the so-called knowledge acquisition bottleneck, ie the difficulty to actually model the domain in question.	As stated in <TREF>Caraballo, 1999</TREF>, WordNet has been an important lexical knowledge base, but it is insufficient for domain specific texts.	So, many attempts have been made to automatically produce taxonomies <REF>Grefenstette, 1994</REF>, but <TREF>Caraballo, 1999</TREF> is certainly the first work which proposes a complete overview of the problem by 1 automatically building a hierarchical structure of nouns based on bottom-up clustering methods and 2 labeling the internal nodes of the resulting tree with hypernyms from the nouns clustered underneath by using patterns such as B is a kind of A.	2008.	3
As stated in <TREF>Caraballo, 1999</TREF>, WordNet has been an important lexical knowledge base, but it is insufficient for domain specific texts.	So, many attempts have been made to automatically produce taxonomies <REF>Grefenstette, 1994</REF>, but <TREF>Caraballo, 1999</TREF> is certainly the first work which proposes a complete overview of the problem by 1 automatically building a hierarchical structure of nouns based on bottom-up clustering methods and 2 labeling the internal nodes of the resulting tree with hypernyms from the nouns clustered underneath by using patterns such as B is a kind of A.	2008.	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 30 Unported license http://creativecommonsorg/licenses/by-ncsa/30/.	3
For example, the phrase France, Germany, Italy, and other European countries suggests that France, Germany and Italy are part of the class of European countries.	Such hierarchical examples are quite sparse, and greater coverage was later attained by <REF>Riloff and Shepherd 1997</REF> and <REF>Roark and Charniak 1998</REF> in extracting relations not of hierarchy but of similarity, by finding conjunctions or co-ordinations such as cloves, cinammon, and nutmeg and cars and trucks This work was extended by <TREF>Caraballo 1999</TREF>, who built classes of related words in this fashion and then reasoned that if a hierarchical relationship could be extracted for any member of this class, it could be applied to all members of the class.	This technique can often mistakenly reason across an ambiguous middle-term, a situation that was improved upon by <REF>Cederberg and Widdows 2003</REF>, by combining pattern-based extraction with contextual filtering using latent semantic analysis.	Prior work in discovering non-compositional phrases has been carried out by <REF>Lin 1999</REF> and Baldwin et al.	2
The extraction patterns used in our research are linguistically richer patterns, requiring shallow parsing and syntactic role assignment.	In recent years several techniques have been developed for semantic lexicon creation eg , <REF>Hearst, 1992</REF>; <REF>Riloff and Shepherd, 1997</REF>; <REF>Roark and Charniak, 1998</REF>; <TREF>Caraballo, 1999</TREF>.	Semantic word learning is different from subjective word learning, but we have shown that MetaBootstrapping and Basilisk could be successfully applied to subjectivity learning.	Perhaps some of these other methods could also be used to learn subjective words.	2
However, such clustering algorithms fail to name their classes.	<TREF>Caraballo 1999</TREF> was the first to use clustering for labeling is-a relations using conjunction and apposition features to build noun clusters.	<REF>Recently, Pantel and Ravichandran 2004</REF> extended this approach by making use of all syntactic dependency features for each noun.	3 Syntactical co-occurrence approach Much of the research discussed above takes a similar approach of searching text for simple surface or lexico-syntactic patterns in a bottom-up approach.	3
This includes the extraction of hyponymy and synonymy relations <REF>Hearst 1992</REF>; <TREF>Caraballo 1999</TREF>, among others as well as meronymy <REF>Berland and Charniak 1999</REF>; <REF>Meyer 2001</REF>.	10 One approach to the extraction of instances of a particular lexical relation is the use of patterns that express lexical relations structurally explicitly in a corpus <REF>Hearst 1992</REF>; <REF>Berland and Charniak 1999</REF>; <TREF>Caraballo 1999</TREF>; <REF>Meyer 2001</REF>, and this is the approach we focus on here.	As an example, the pattern NP 1 and other NP 2 usually expresses a hyponymy/similarity relation between the hyponym NP 1 and its hypernym NP 2 <REF>Hearst 1992</REF>, and it can therefore be postulated that two noun phrases that occur in such a pattern in a corpus should be linked in an ontology via a hyponymy link.	Applications of the extracted relations to anaphora resolution are less frequent.	2
372 Markert and Nissim Knowledge Sources for Anaphora Resolution consuming hand-modeling.	This includes the extraction of hyponymy and synonymy relations <REF>Hearst 1992</REF>; <TREF>Caraballo 1999</TREF>, among others as well as meronymy <REF>Berland and Charniak 1999</REF>; <REF>Meyer 2001</REF>.	10 One approach to the extraction of instances of a particular lexical relation is the use of patterns that express lexical relations structurally explicitly in a corpus <REF>Hearst 1992</REF>; <REF>Berland and Charniak 1999</REF>; <TREF>Caraballo 1999</TREF>; <REF>Meyer 2001</REF>, and this is the approach we focus on here.	As an example, the pattern NP 1 and other NP 2 usually expresses a hyponymy/similarity relation between the hyponym NP 1 and its hypernym NP 2 <REF>Hearst 1992</REF>, and it can therefore be postulated that two noun phrases that occur in such a pattern in a corpus should be linked in an ontology via a hyponymy link.	2
The literature on automated text categorization is enormous, but assumes that a set of categories has already been created, whereas the problem here is to determine the categories of interest.	There has also been extensive work on finding synonymous terms and word associations, as well as automatic acquisition of IS-A or genus-head relations from dictionary definitions and glosses <REF>Klavans and Whitman, 2001</REF> and from free text <REF>Hearst, 1992</REF>; <TREF>Caraballo, 1999</TREF>.	<REF>Sanderson and Croft 1999</REF> propose a method called subsumption for building a hierarchy for a set of documents retrieved for a query.	For two terms x and y, x is said to subsume y if the following conditions hold: a2a4a3a6a5a8a7a9a11a10a13a12a15a14a17a16a18a20a19a21a2a4a3a6a9a22a7a5a23a10a25a24a27a26.	2
Other kinds of models that have been studied in the context of lexical acquisition are those based on lexico-syntactic patterns of the kind X, Y and other Zs, as in the phrase bluejays, robins and other birds.	These types of models have been used for hyponym discovery <REF>Hearst, 1992</REF>; <REF>Roark and Charniak, 1998</REF>, meronym discovery <REF>Berland and Charniak, 1999</REF>, and hierarchy building <TREF>Caraballo, 1999</TREF>.	These methods are very interesting but of limited applicability, because nouns that do not appear in known lexico-syntactic patterns cannot be learned.	7 Conclusion All the approaches cited above focus on some aspect of the problem of lexical acquisition.	3
Alternatively, some systems are based on the observation that related terms appear together in particular contexts.	These systems extract related terms directly by recognising linguistic patterns eg X, Y and other Zs which link synonyms and hyponyms <REF>Hearst, 1992</REF>; <TREF>Caraballo, 1999</TREF>.	Our previous work <REF>Curran and Moens, 2002</REF> has evaluated thesaurus extraction performance and ef ciency using several different context models.	In this paper, we evaluate some existing similarity metrics and propose and motivate a new metric which outperforms the existing metrics.	2
These methods use clustering algorithms to group words according to their meanings in text, label the clusters using its members lexical or syntactic dependencies, and then extract an is-a relation between each cluster member and the cluster label.	<TREF>Caraballo 1999</TREF> proposed the first attempt, which used conjunction and apposition features to build noun clusters.	<REF>Recently, Pantel and Ravichandran 2004</REF> extended this approach by making use of all syntactic dependency features for each noun.	The advantage of clustering approaches is that they permit algorithms to identify is-a relations that do not explicitly appear in text, however they generally fail to produce coherent clusters from fewer than 100 million words; hence they are unreliable for small corpora.	2
In Section 3, we show how latent semantic analysis can be used to filter potential relationships according to their semantic plausibility.	In Section 4, we show how correctly extracted relationships can be used as seed-cases to extract several more relationships, thus improving recall; this work shares some similarities with that of <TREF>Caraballo 1999</TREF>.	In Section 5 we show that combining the techniques of Section 3 and Section 4 improves both precision and recall.	Section 6 demonstrates that 1Another possible view is that hyponymy should only refer to core relationships, not contingent ones so pheasant a60 bird might be accepted but pheasant a60 food might not be, because it depends on context and culture.	2
4 Improving Recall Using Coordination Information One of the main challenges facing hyponymy extraction is that comparatively few of the correct relations that might be found in text are expressed overtly by the simple lexicosyntactic patterns used in Section 2, as was apparent in the results presented in that section.	This problem has been addressed by <TREF>Caraballo 1999</TREF>, who describes a system that first builds an unlabelled hierarchy of noun clusters using agglomerative bottom-up clustering of vectors of noun coordination information.	The leaves of this hierarchy corresponding to nouns are assigned hypernyms using Hearst-style lexicosyntactic patterns.	Internal nodes in the hierarchy are then labelled with hypernyms of the leaves they subsume according to a vote of these subsumed leaves.	2
This paper suggests many possibilities for future work.	First of all, it would be interesting to apply LSA to a system for building an entire hypernym-labelled ontology in roughly the way described in <TREF>Caraballo, 1999</TREF>, perhaps by using an LSA-weighted voting method to determine which hypernym would be used to label each node.	We are considering how to extend our techniques to such a task.	Also, systematic comparison of the lexicosyntactic patterns used for extraction to determine the relative productiveness and accuracy of each pattern might prove illuminating, as would comparison across different corpora to determine the impact of the topic area and medium/format of documents on the effectiveness of hyponymy extraction.	2
The sparseness of these patterns prevents this from being an effective approach to the problem we address here.	<REF>In Caraballo 1999</REF>, we construct a hierarchy of nouns, including hypernym relations.	However, there are several areas where that work could benefit from the research presented here.	The hypernyms used to label the internal nodes of that hierarchy are chosen in a simple fashion; pattern-matching as in <REF>Hearst 1992</REF> is used to identify candidate hypernyms of the words dominated by a particular node, and a simple voting scheme selects the hypernyms to be used.	3
This project is meant to provide a tool to support other methods.	<REF>See Caraballo 1999</REF> for a detailed description of a method to construct such a hierarchy.	2 Previous work To the best of our knowledge, this is the first attempt to automatically rank nouns based on specificity.	<REF>Hearst 1992</REF> found individual pairs of hypernyms and hyponyms from text using pattern-matching techniques.	2
our disposal, WordNet <REF>Fellbaum, 1998</REF> contains very little information that would be considered as being about attributesonly information about parts, not about qualities such as height, or even to the values of such attributes in the adjective networkand this information is still very sparse.	On the other hand, the only work on the extraction of lexical semantic relations we are aware of has concentrated on the type of relations found in WordNet: hyponymy <REF>Hearst, 1998</REF>; <TREF>Caraballo, 1999</TREF> and meronymy <REF>Berland and Charniak, 1999</REF>; <REF>Poesio et al, 2002</REF>.	2 The work discussed here could be perhaps best described as an example of empirical ontology: using linguistics and philosophical ideas to improve the results of empirical work on lexical / ontology acquisition, and vice versa, using findings from empirical analysis to question some of the assumptions of theoretical work on ontology and the lexicon.	Specifically, we discuss work on the acquisition of nominal concept attributes whose goal is twofold: on the one hand, to clarify the notion of attribute and its role in lexical semantics, if any; on the other, to develop methods to acquire such information automatically eg , to supplement WordNet.	2
My analysis of the Sinica Corpus shows that contrary to expectation, most of unknown words in Chinese are common nouns, adjectives, and verbs rather than proper nouns.	Other previous research has focused on features related to unknown word contexts <TREF>Caraballo 1999</TREF>; <REF>Roark and Charniak 1998</REF>.	While context is clearly an important feature, this paper focuses on non-contextual features, which may play a key role for unknown words that occur only once and hence have limited context.	The feature I focus on, following <REF>Ciaramita 2002</REF>, is morphological similarity to words whose semantic category is known.	2
<REF>CompuTerm 2004</REF> 3rd International Workshop on Computational Terminology 49 234 Explicit Patterns Relations This knowledge source infers specific relations between terms based on characteristic cue-phrases which relate them.	For example, the cue-phrase such as <REF>Hearst 1992</REF> <TREF>Caraballo 1999</TREF> suggest a kind-of relation, eg, a ligand such as triethylphosphine tells us that triethylphosphene is a kind of ligand.	Likewise, in the TREC domain, air toxics such as benzene can suggest that benzene is a kind of air toxic.	However, since such cue-phrase patterns tend to be sparse in occurrence, we do not use them in the evaluations described below.	3
Fully unsupervised semantic clustering eg, <REF>Lin, 1998</REF>; <REF>Lin and Pantel, 2002</REF>; <REF>Davidov and Rappoport, 2006</REF> has the disadvantage that it may or may not produce the types and granularities of semantic classes desired by a user.	Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes eg, <TREF>Caraballo, 1999</TREF>; <REF>Cimiano and Volker, 2005</REF>; <REF>Mann, 2002</REF>, and learning semantic relations such as meronymy <REF>Berland and Charniak, 1999</REF>; <REF>Girju et al, 2003</REF>.	Our research focuses on semantic lexicon induction, which aims to generate lists of words that belong to a given semantic class eg, lists of FISH or VEHICLE words.	Weakly supervised learning methods for semantic lexicon generation have utilized co-occurrence statistics <REF>Riloff and Shepherd, 1997</REF>; <REF>Roark and Charniak, 1998</REF>, syntactic information <REF>Tanev and Magnini, 2006</REF>; <REF>Pantel and Ravichandran, 2004</REF>; <REF>Phillips and Riloff, 2002</REF>, lexico-syntactic contextual patterns eg, resides in <location> or moved to <location> <REF>Riloff and Jones, 1999</REF>; <REF>Thelen and Riloff, 2002</REF>, and local and global contexts <REF>Fleischman and Hovy, 2002</REF>.	2
Roark and Charniak <REF>Roark and Charniak, 1998</REF> followed up on this work by using a parser to explicitly capture these structures.	Caraballo <TREF>Caraballo, 1999</TREF> also exploited these syntactic structures and applied a cosine vector model to produce semantic groupings.	In our view, these previous systems used weak syntactic models because the syntactic structures sometimes identified desirable semantic associations and sometimes did not.	To compensate, statistical models were used to separate the meaningful semantic associations from the spurious ones.	3
There are inherent problems in evaluating automatic thesaurus extraction techniques, and much research assumes a gold standard that does not exist see Kilgarriff 2003 and Weeds 2003 for more discussion of this.	A further problem for distributional similarity methods for automatic thesaurus generation is that they do not offer any obvious way to distinguish between linguistic relations such as synonymy, antonymy, and hyponymy see Caraballo 1999 and Lin et al 2003 for work on this.	Thus, one may question 1 You shall know a word by the company it keeps<REF>Firth 1957</REF> 440 Weeds and Weir Co-occurrence Retrieval the benefit of automatically generating a thesaurus if one has access to large-scale manually constructed thesauri eg , WordNet <REF>Fellbaum 1998</REF>, Rogets <REF>Roget 1911</REF>, the Macquarie <REF>Bernard 1990</REF> and Moby 2 .	Automatic techniques give us the opportunity to model language change over time or across domains and genres.	3
The number of dependency types may be reduced in future work.	3 The Probability Model The DAG-like nature of the dependency structures makes it difficult to apply generative modelling techniques <REF>Abney, 1997</REF>; <TREF>Johnson et al , 1999</TREF>, so we have defined a conditional model, similar to the model of <REF>Collins 1996</REF> see also the conditional model in <REF>Eisner 1996b</REF>.	While the model of <REF>Collins 1996</REF> is technically unsound <REF>Collins, 1999</REF>, our aim at this stage is to demonstrate that accurate, efficient wide-coverage parsing is possible with CCG, even with an over-simplified statistical model.	Future work will look at alternative models4 4The reentrancies creating the DAG-like structures are fairly limited, and moreover determined by the lexical categories.	3
The features are shown with hidden variables corresponding to wordspecific hidden values, such as shares1 or bought3.	In our experiments, we made use of features such as those in Figure 2 in combination with the following four definitions of the hiddenvalue 3We also performed some experiments using the conjugate gradient descent algorithm <TREF>Johnson et al , 1999</TREF>.	However, we did not find a significant difference between the performance of either method.	Since stochastic gradient descent was faster and required less memory, our final experiments used the stochastic gradient method.	2
Mainstream approaches in statistical parsing are based on nondeterministic parsing techniques, usually employing some kind of dynamic programming, in combination with generative probabilistic models that provide an n-best ranking of the set of candidate analyses derived by the parser <REF>Collins, 1997</REF>; <REF>Collins, 1999</REF>; <REF>Charniak, 2000</REF>.	These parsers can be enhanced by using a discriminative model, which reranks the analyses output by the parser <TREF>Johnson et al , 1999</TREF>; <REF>Collins and Duffy, 2005</REF>; <REF>Charniak and Johnson, 2005</REF>.	Alternatively, discriminative models can be used to search the complete space of possible parses <REF>Taskar et al , 2004</REF>; <REF>McDonald et al , 2005</REF>.	A radically different approach is to perform disambiguation deterministically, using a greedy parsing algorithm that approximates a globally optimal solution by making a sequence of locally optimal choices, guided by a classifier trained on gold standard derivations from a treebank.	2
If a complete structure is represented with a feature forest of a tractable size, the parameters can be efficiently estimated by dynamic programming.	A series of studies on parsing with wide-coverage LFG <TREF>Johnson et al , 1999</TREF>; <REF>Riezler et al , 2000</REF>; <REF>Riezler et al , 2002</REF> have had a similar motivation to ours.	Their models have also been based on a discriminative model to select a parsing result from all candidates given by the grammar.	A significant difference is that we apply maximum entropy estimation for feature forests to avoid the inherent problem with estimation: the exponential explosion of parsing results given by the grammar.	3
Wellknown computational linguistic models such as MLE MCLE Y  yi; X  xi X  xi Y  yi; X  xi Figure 1: The MLE makes the training data yi; xi as likely as possible relative to , while the MCLE makes yi; xi as likely as possible relative to other pairs y0; xi.	Maximum-Entropy Markov Models <REF>McCallum et al , 2000</REF> and Stochastic Unification-based Grammars <TREF>Johnson et al , 1999</TREF> are standardly estimated with conditional estimators, and it would be interesting to know whether conditional estimation affects the quality of the estimated model.	It should be noted that in practice, the MCLE of a model with a large number of features with complex dependencies may yield far better performance than the MLE of the much smaller model that could be estimated with the same computational effort.	Nevertheless, as this paper shows, conditional estimators can be used with other kinds of models besides MaxEnt models, and in any event it is interesting to ask whether the MLE differs from the MCLE in actual applications, and if so, how.	2
However, because these constraints can be non-local or context-sensitive, developing stochastic versions of UBGs and associated estimation procedures is not as straight-forward as it is for, eg, PCFGs.	Recent work has shown how to define probability distributions over the parses of UBGs <REF>Abney, 1997</REF> and efficiently estimate and use conditional probabilities for parsing <TREF>Johnson et al , 1999</TREF>.	Like most other practical stochastic grammar estimation procedures, this latter estimation procedure requires a parsed training corpus.	Unfortunately, large parsed UBG corpora are not yet available.	3
Therefore there are a large number of features available that could be used by stochastic models for disambiguation.	Other researchers have worked on extracting features useful for disambiguation from unification grammar analyses and have built log linear models aka Stochastic Unification Based Grammars <TREF>Johnson et al , 1999</TREF>; <REF>Riezler et al , 2000</REF>.	Here we also use log linear models to estimate conditional probabilities of sentence analyses.	Since feature selection is almost prohibitive for these models, because of high computational costs, we use PCFG models to select features for log linear models.	2
One is for a simple model with a relatively small number of features, and the other is for a model with a large number of features.	The usefulness of priors in maximum entropy models is not new to this work: Gaussian prior smoothing is advocated in <REF>Chen and Rosenfeld 2000</REF>, and used in all the stochastic LFG work <TREF>Johnson et al , 1999</TREF>.	However, until recently, its role and importance have not been widely understood.	For example, <REF>Zhang and Oles 2001</REF> attribute the perceived limited success of logistic regression for text categorization to a lack of use of regularization.	2
After filtering by the generator, the remaining fstructures were weighted by the stochastic disambiguation component.	Similar to stochastic disambiguation for constraint-based parsing <TREF>Johnson et al , 1999</TREF>; <REF>Riezler et al , 2002</REF>, an exponential aka log-linear or maximumentropy probability model on transferred structures is estimated from a set of training data.	The data for estimation consists of pairs of original sentences y and goldstandard summarized f-structures s which were manually selected from the transfer output for each sentence.	For training data sj,yjmj1 and a set of possible summarized structures Sy for each sentence y, the objective was to maximize a discriminative criterion, namely the conditional likelihood L of a summarized f-structure given the sentence.	2
Experiments on the Penn WSJ treebank show that the model achieves state-of-the-art performance, for both constituent and dependency accuracy.	In global linear models GLMs for structured prediction, eg, <TREF>Johnson et al, 1999</TREF>; <REF>Lafferty et al, 2001</REF>; <REF>Collins, 2002</REF>; <REF>Altun et al, 2003</REF>; <REF>Taskar et al, 2004</REF>, the optimal label y for an input x is y  arg max yYx w fx,y 1 where Yx is the set of possible labels for the input x; fx,y  Rd is a feature vector that represents the pair x,y; and w is a parameter vector.	This paper describes a GLM for natural language parsing, trained using the averaged perceptron.	The parser we describe recovers full syntactic representations, similar to those derived by a probabilistic context-free grammar PCFG.	2
This section describes the relationship between our work and this previous work.	In reranking approaches, a first-pass parser is used to enumerate a small set of candidate parses for an input sentence; the reranking model, which is a GLM, is used to select between these parses eg, <REF>Ratnaparkhi et al, 1994</REF>; <TREF>Johnson et al, 1999</TREF>; <REF>Collins, 2000</REF>; <REF>Charniak and Johnson, 2005</REF>.	A crucial advantage of our approach is that it considers a very large set of alternatives in Yx, and can thereby avoid search errors that may be made in the first-pass parser1 Another approach that allows efficient training of GLMs is to use simpler syntactic representations, in particular dependency structures McDon1Some features used within reranking approaches may be difficult to incorporate within dynamic programming, but it is nevertheless useful to make use of GLMs in the dynamicprogramming stage of parsing.	Our parser could, of course, be used as the first-stage parser in a reranking approach.	2
Moreover, property design can be carried out in a targeted way, ie properties can be designed in order to improve the disambiguation of grammatical relations that, so far, are disambiguated particularly poorly or that are of special interest for the task that the systems output is used for.	By demonstrating that property design is the key to good log-linear models for deepsyntactic disambiguation, our work confirms that specifying the features of a SUBG stochastic unification-based grammar is as much an empirical matter as specifying the grammar itself<TREF>Johnson et al , 1999</TREF>.	Acknowledgements The work described in this paper has been carried out in the DLFG project, which was funded by the German Research Foundation DFG.	Furthermore, I thank the audiences at several ParGram meetings, at the Research Workshop of the Israel Science Foundation on Large-scale Grammar Development and Grammar Engineering at the University of Haifa and at the SFB 732 Opening Colloquium in Stuttgart for their important feedback on earlier versions of this work.	2
Firstly, the rudimentary character of functional annotations in standard treebanks has hindered the direct use of such data for statistical estimation of linguistically fine-grained statistical parsing systems.	Rather, parameter estimation for such models had to resort to unsupervised techniques <REF>Bouma et al , 2000</REF>; <REF>Riezler et al , 2000</REF>, or training corpora tailored to the specific grammars had to be created by parsing and manual disambiguation, resulting in relatively small training sets of around 1,000 sentences <TREF>Johnson et al , 1999</TREF>.	Furthermore, the effort involved in coding broadcoverage grammars by hand has often led to the specialization of grammars to relatively small domains, thus sacrificing grammar coverage ie the percentage of sentences for which at least one analysis is found on free text.	The approach presented in this paper is a first attempt to scale up stochastic parsing systems based on linguistically fine-grained handcoded grammars to the UPenn Wall Street Journal henceforth WSJ treebank <REF>Marcus et al , 1994</REF>.	2
Recent work in statistical approaches to parsing and tagging has begun to consider methods which incorporate global features of candidate structures.	Examples of such techniques are Markov Random Fields <REF>Abney 1997</REF>; Della <REF>Pietra et al 1997</REF>; <TREF>Johnson et al 1999</TREF>, and boosting algorithms <REF>Freund et al 1998</REF>; <REF>Collins 2000</REF>; <REF>Walker et al 2001</REF>.	One appeal of these methods is their flexibility in incorporating features into a model: essentially any features which might be useful in discriminating good from bad structures can be included.	A second appeal of these methods is that their training criterion is often discriminative, attempting to explicitly push the score or probability of the correct structure for each training sentence above the score of competing structures.	2
Following Abney, we propose a loglinear framework which incorporates long-range dependencies as features without loss of consistency.	Log-linear models have previously been applied to statistical parsing <TREF>Johnson et al , 1999</TREF>; <REF>Toutanova et al , 2002</REF>; <REF>Riezler et al , 2002</REF>; <REF>Osborne, 2000</REF>.	Typically, these approaches have enumerated all possible parses for model estimation and finding the most probable parse.	For grammars extracted from the Penn Treebank in our case CCGbank <REF>Hockenmaier, 2003</REF>, enumerating all parses is infeasible.	2
As expected, we observed that the regularization term increases the accuracy, especially when the training data is small; but we did not observe much difference when we used different regularization terms.	The results we report are with the Gaussian prior regularization term described in <TREF>Johnson et al , 1999</TREF>.	Our goal in this paper is not to build the best tagger or recognizer, but to compare different loss functions and optimization methods.	Since we did not spend much effort on designing the most useful features, our results are slightly worse than, but comparable to the best performing models.	2
This method generates 50-best lists that are of substantially higher quality than previously obtainable.	We used these parses as the input to a MaxEnt reranker <TREF>Johnson et al , 1999</TREF>; <REF>Riezler et al , 2002</REF> that selects the best parse from the set of parses for each sentence, obtaining an f-score of 910 on sentences of length 100 or less.	We describe a reranking parser which uses a regularized MaxEnt reranker to select the best parse from the 50-best parses returned by a generative parsing model.	The 50-best parser is a probabilistic parser that on its own produces high quality parses; the maximum probability parse trees according to the parsers model have an f-score of 0897 on section 23 of the Penn Treebank <REF>Charniak, 2000</REF>, which is still state-of-the-art.	2
