The second instantiation finds the borders of phrases beginning and end and then pairs them in an optimal way into different phrases~~~These problems formulations are similar to those studied in <REF>Ramshaw and Marcus, 1995</REF> and <TREF>Church, 1988</TREF>; <REF>Argamon et al , 1998</REF>, respectively~~~The experimental results presented using the SNoW based approach compare favorably with previously published results, both for NPs and SV phrases~~~A s important, we present a few experiments that shed light on some of the issues involved in using learned predictors that interact to produce the desired inference.
Our earlier example would be marked for base NPs as: I wont to California last May~~~This approach has been studied in <TREF>Church, 1988</TREF>; <REF>Argamon et al , 1998</REF>~~~331 Architecture The architecture used for the Open/Close predictors is shown in Figure 2~~~Two SNoW predictors are used, one to predict if the word currently in consideration is the first in the phrase an open bracket, and the other to predict if it is the last a close bracket.
A lot of the work on shallow parsing over the past years has concentrated on manual construction of rules~~~The observation that shallow syntactic information can be extracted using local information by examining the pattern itself, its nearby context and the local part-of-speech information has motivated the use of learning methods to recognize these patterns <TREF>Church, 1988</TREF>; <REF>Ramshaw and Marcus, 1995</REF>; <REF>Argamon et al , 1998</REF>; <REF>Cardie and Pierce, 1998</REF>~~~ Research supported by NSF grants IIS-9801638 and SBR-9873450~~~t Research supported by NSF grant CCR-9502540.
HMMs have long been central in speech recognition <REF>Rabiner, 1989</REF>~~~Their application to partof-speech tagging <TREF>Church, 1988</TREF>; <REF>DeRose, 1988</REF> kicked off the era of statistical NLP, and they have found additional NLP applications to phrase chunking, text segmentation, word-sense disambiguation, and information extraction~~~The algorithm is also important to teach for pedagogical reasons, as the entry point to a family of EM algorithms for unsupervised parameter estimation~~~Indeed, it is an instructive special case of 1 the inside-outside algorithm for estimation of probabilistic context-free grammars; 2 belief propagation for training singly-connected Bayesian networks and junction trees <REF>Pearl, 1988</REF>; <REF>Lauritzen, 1995</REF>; 3 algorithms for learning alignment models such as weighted edit distance; 4 general finitestate parameter estimation <REF>Eisner, 2002</REF>.
Subsequent analysis suggested that half the errors could be removed with only a little additional work, suggesting that over 90 performance is achievable~~~In a related test, we explored the bracketings produced by Churchs PARTS program <TREF>Church, 1988</TREF>~~~We extracted 200 sentences of WSJ text by taking every tenth sentence from a collection of manually corrected parse trees data from the TREEBANK Project at the University of Pennsylvania~~~We evaluated the NP bracketings in these 200 sentences by hand, and tried to classify the errors.
Given that each token has on the average more than 2 possible tags, the procedural description above is very inefficient for all but very short sentences~~~However, the observation that our constraints are localized to a window of a small number of tokens say at most 5 tokens in a sequence, suggests a more efficient scheme originally used by <TREF>Church 1988</TREF>~~~Assume our constraint windows are allowed to look at a window of at most size k sequential parses~~~Let us take the first k tokens of a sentence and generate all possible paths of k arcs spanning k  1 nodes, and apply all constraints to these short paths.
Part-of-speech tagging is one of the preliminary steps in many natural language processing systems in which the proper part-of-speech tag of the tokens comprising the sentences are disambiguated using either statistical or symbolic local contextual information~~~Tagging systems have used either a statistical approach where a large corpora is employed to train a probabilistic model which then is used to tag unseen text, eg , <TREF>Church 1988</TREF>, Cutting et al~~~1992, <REF>DeRose 1988</REF>, or a constraint-based approach which employs a large number of hand-crafted linguistic constraints that are used to eliminate impossible sequences or morphological parses for a given word in a given context, recently most prominently exemplified by the Constraint Grammar work <REF>Karlsson et al , 1995</REF>; <REF>Voutilainen, 1995b</REF>; <REF>Voutilainen et al , 1992</REF>; <REF>Voutilainen and Tapanainen, 1993</REF>~~~BriU 1992; 1994; 1995 has presented a transformationbased learning approach.
 Right close double quote 231 Automated Stage~~~During the early stages of the Penn Treebank project, the initial automatic POS assignment was provided by PARTS <TREF>Church 1988</TREF>, a stochastic algorithm developed at ATT Bell Labs~~~PARTS uses a modified version of the Brown Corpus tagset close to our own and assigns POS tags with an error rate of 3-5~~~The output of PARTS was automatically tokenized 8 and the tags assigned by PARTS were automatically mapped onto the Penn Treebank tagset.
Various methods for POS tagging have been proposed in recent years~~~For simplicity, we adapted the method proposed by <REF>Churchl1988</REF> to tag the definition sentence~~~In the second stage, we select the label which is associated with word lists most similar to the definition as the result~~~We sum up the above descriptions and outline the procedure for labeling a dictionary sense.
The experimental results show the proposed method significantly outperforms both hand-crafted and conventional statistical methods~~~The last few years have seen the great success of stochastic part-of-speech POS taggers <TREF>Church, 1988</TREF>: <REF>Kupiec, 1992</REF>; Charniak et M , 1993; <REF>Brill, 1992</REF>; <REF>Nagata, 1994</REF>~~~The stochastic approach generally attains 94 to 96 accuracy and replaces the labor-intensive compilation of linguistics rules by using an automated learning algorithm~~~However, 1NTT is an abbreviation of Nippon Telegraph and Telephone Corporation.
1~~~A recent trend in natural language processing has been toward a greater emphasis on statistical approaches, beginning with the success of statistical part-of-speech tagging programs <TREF>Church 1988</TREF>, and continuing with other work using statistical part-of-speech tagging programs, such as BBN PLUM <REF>Weischedel et al 1993</REF> and NYU Proteus <REF>Grishman and Sterling 1993</REF>~~~More recently, statistical methods have been applied to domain-specific semantic parsing <REF>Miller et al 1994</REF>, and to the more difficult problem of wide-coverage syntactic parsing <REF>Magerman 1995</REF>~~~Nevertheless, most natural language systems remain primarily rule based, and even systems that do use statistical techniques, such as ATT Chronus <REF>Levin and Pieraccini 1995</REF>, continue to require a significant rule based component.
The training is performed on ambiguity classes and not on individual word tokens~~~<REF>Kallgren 1996</REF> gives a more covering description of how XPOST is used on the Swedish material and also sketches the major differences between this algorithm and some others used for tagging, such as PARTS <TREF>Church 1988</TREF> and VOLSUNGA <REF>DeRose 1988</REF>~~~A characteristic tbature of the SUC is its high number of different tags~~~The number of part-ofspeech tags used in the SUC is 21.
Disambiguation of capitalized words is usually handled by POS taggers, which treat capitalized words in the same way as other categories, that is, by accounting for the immediate syntactic context and using estimates collected from a training corpus~~~<REF>As Church 1988</REF> rightly pointed out, however, Proper nouns and capitalized words are particularly problematic: some capitalized words are proper nouns and some are not~~~Estimates from the Brown Corpus can be misleading~~~For example, the capitalized word Acts is found twice in the Brown Corpus, both times as a proper noun in a title.
The morphological ambiguity will differ depending on the level of tagging used in each case, as shown in table 2~~~There are two kinds of methods for morphological disambiguation: on one hand, statistical methods need little effort and obtain very good results <TREF>Church, 1988</TREF>; Cutting el al, 1992, at least when applied to English, but when we try to apply them to Basque we encounter additional problems; on the other hand, some rule-based systems <REF>Brill, 1992</REF>; <REF>Voutilainen et al, 1992</REF> are at least as good as statistical systems and are better adapted to free-order languages and agglutinative languages~~~So, we 381 have selected one of each group: Constraint Grammar formalism <REF>Karlsson et al, 1995</REF> and the HMM based TATOO tagger <REF>Armstrong et al, 1995</REF>, which has been designed to be applied it to the output of a morphological analyser and the tagset can be switched easily without changing the input text~~~second  third 70 ks I M M MCG MCG Figure 1-Initial ambiguity3.
We are tagging this material with a much simpler tagset than used by previous projects, as discussed at the Oct 1989 DARPA Workshop~~~The material is first processed using Ken Churchs tagger <TREF>Church 1988</TREF>, which labels it as if it were Brown Corpus material, and then is mapped to our tagset by a SEDscript~~~Because of fundamental differences in tagging strategy between the Penn Treebank Project and the Brown project, the resulting mapping is about 9 inaccurate, given the tagging guidelines of the Penn Treebank project as given in 40 pages of explicit tagging guidelines~~~This material is then hand-corrected by our annotators; the result is consistent within annotators to about 3 cf.
H90-1055:17~~~Deducing Linguistic Structure from the Statistics of Large Corpora Eric Brill David Magerman Mitchell Marcus Beatrice Santorini Department of Computer and Information Science University of Pennsylvania Philadelphia, PA 19104 1 Introduction Within the last two years, approaches using both stochastic and symbolic techniques have proved adequate to deduce lexical ambiguity resolution rules with less than 3-4 error rate, when trained on moderate sized 500K word corpora of English text eg <TREF>Church, 1988</TREF>; <REF>Hindle, 1989</REF>~~~The success of these techniques suggests that much of the grammatical structure of language may be derived automatically through distributional analysis, an approach attempted and abandoned in the 1950s~~~We describe here two experiments to see how far purely distributional techniques can be pushed to automatically provide both a set of part of speech tags for English, and a grammatical analysis of free English text.
This line of research was motivated by a series of successful applications of mutual information statistics to other problems in natural language processing~~~In the last decade, research in speech recognition <REF>Jelinek 1985</REF>, noun classification <REF>Hindle 1988</REF>, predicate argument relations <REF>Church  Hanks 1989</REF>, and other areas have shown that mutual information statistics provide a wealth of information for solving these problems~~~22 Mutual Information Statistics The mutual information statistic <REF>Fano 1961</REF> is a measure of the interdependence of two signals in a message~~~It is a function of the probabilities of the two events: Mz, u  log u xzPvy In this paper, the events x and y will be part-of-speech n-grams instead of single parts-of-speech, as in some earlier work.
Each of these three steps will be described below~~~3 The preprocessing stage The noun phrase parser identifies simple non-recursive noun phrases such as DetAdjN or NN The method used for this process involves an algorithm of the type described in <TREF>Church 1988</TREF> which was trained on a manually marked part of our corpus~~~The module is thus geared to the particular type of second language text the checker needs to deal with~~~The resulting information is passed on to a preprocessing module consisting of a number of automata groups.
Measures/NNS of/IN manufacturing/VBG activity/NN fell/VBD more/RBR than/IN the/DT overall/JJ measures/NNS/~~~Figure 1: An example sentence with baseNP brackets A number of researchers have dealt with the problem of baseNP identification <TREF>Church 1988</TREF>; <REF>Bourigault 1992</REF>; <REF>Voutilainen 1993</REF>; <REF>Justeson  Katz 1995</REF>~~~Recently some researchers have made experiments with the same test corpus extracted from the 20 th section of the Penn Treebank Wall Street Journal Penn Treebank~~~<REF>Ramshaw  Markus 1998</REF> applied transformbased error-driven algorithm <REF>Brill 1995</REF> to learn a set of transformation rules, and using those rules to locally updates the bracket positions.
4 Concluding remarks Though there can be little doubt that the ruling system of bakeoffs actively encourages a degree of oneupmanship, our paper and our software are not offered in a competitive spirit~~~As we said at the out211 set, we dont necessarily believe HunPos to be in any way better than TnT, and certainly the main ideas have been pioneered by <REF>DeRose 1988</REF>, <TREF>Church 1988</TREF>, and others long before this generation of HMM work~~~But to improve the results beyond what a basic HMM can achieve one needs to tune the system, and progress can only be made if the experiments are end to end replicable~~~There is no doubt many other systems could be tweaked further and improve on our results what matters is that anybody could now also tweak HunPos without any restriction to improve the state of the art.
In this paper we describe experiments we performed to ascertain how well humans, given an annotated training set, can generate rules for base noun phrase chunking~~~Much previous work has been done on this problem and many different methods have been used: Churchs PARTS 1988 program uses a Markov model; <REF>Bourigault 1992</REF> uses heuristics along with a grammar; Voutilainens NP<REF>Tool 1993</REF> uses a lexicon combined with a constraint grammar; <REF>Juteson and Katz 1995</REF> use repeated phrases; <REF>Veenstra 1998</REF>, Argamon, Dagan  <REF>Krymolowski1998</REF> and Daelemaus, van den <REF>Bosch  Zavrel 1999</REF> use memory-based systems; Ramshaw  Marcus In Press and <REF>Cardie  Pierce 1998</REF> use rule-based systems~~~2 Learning Base Noun Phrases by Machine We used the base noun phrase system of Ramshaw and Marcus RM as the machine learning system with which to compare the human learners~~~It is difficult to compare different machine learning approaches to base NP annotation, since different definitions of base NP are used in many of the papers, but the RM system is the best of those that have been tested on the Penn Treebank.
The suggestion which we want to explore is that the association revealed by textual distribution whether its source is a complementation relation, a modification relation, or something else gives us information needed to resolve the prepositional attachment~~~Discovering Lexical Association in Text A 13 million word sample of Associated Press new stories from 1989 were automatically parsed by the Fidditch parser <REF>Hindle 1983</REF>, using Churchs part of speech analyzer as a preprocessor <TREF>Church 1988</TREF>~~~From the syntactic analysis provided by the parser for each sentence, we extracted a table containing all the heads of all noun phrases~~~For each noun phrase head, we recorded the following preposition if any occurred ignoring whether or not the parser attached the preposition to the noun phrase, and the preceding verb if the noun phrase was the object of that verb.
We also looked at whether the token constituted an entire intermediate or intonational phrase--possibly with other cue phrases--or not, and what each tokens position within its intermediate phrase and larger intonational phrase was--first-inphrase again, including tokens preceded only by other cue phrases as well as tokens that were absolutely first in intermediate phrase, last, or other~~~We also examined each items part of speech, using Churchs 1988 part-of-speech tagger~~~Finally, we investigated orthographic features of the transcript that might be associated with a discourse/sentential distinction, such as immediately preceding and succeeding punctuation and paragraph boundaries~~~In both the syntactic and orthographic analyses we were particularly interested in discovering how successful nonprosodic features that might be obtained automatically from a text would be in differentiating discourse from sentential uses.
While the use of orthographic and part-of-speech data represents only a fractional improvement over orthographic information alone, it is possible that, since the latter is not subject to transcriber idiosyncracy, such an approach may prove more reliable than orthography alone in the general case~~~And, for text-to-speech applications, it 7 The parbof-speech tagger employed in this analysis <TREF>Church 1988</TREF> uses a subset of the part-of-speech tags used in Francis and Kuera 1982~~~We have translated these for Table 12~~~Note that intensifier corresponds to QU in Francis and Kuera 1982.
Research on corpus-based natural language learning and processing is rapidly accelerating following the introduction of large on-line corpora, faster computers, and cheap storage devices~~~Recent work involves novel ways to employ annotated corpus in part of speech tagging <TREF>Church 1988</TREF> <REF>Derose 1988</REF> and the application of mutual information statistics on the corpora to uncover lexical information <REF>Church 1989</REF>~~~The goal of the research is the construction of robust and portable natural language processing systems~~~The wide range of topics available on the Internet calls for an easily adaptable information extraction system for different domains.
Part-of-speech tagging is to assign the correct tag to each word in the context of the sentence~~~here are three main approaches in tagging problem: rule-based approach Klein and Simmons 13; <REF>Brodda 1982</REF>; <REF>Paulussen and Martin 1992</REF>; <REF>Brill et al 1990</REF>, statistical approach Church :1988; <REF>Merialdo 1994</REF>; <REF>Foster 1991</REF>; <REF>Weischedel et al 1993</REF>; <REF>Kupiec 1992</REF> and connectionist approach <REF>Benello et al 1989</REF>; <REF>Nakanmra et al 1989</REF>~~~In these approaches, statistical approach has the following advantages :  a theoretical framework is provided  automatic learning facility is provided  the probabilities provide a straightforward way to disambiguate Many information sources must be combined to solve tagging problem with statistical approach~~~It is a significant assumption that tire correct tag can generally be chosen from Ihe local context.
In this study, we measure performance solely through the cross-entropy of test data; it would be interesting to see how these cross-entropy differences correlate with performance in end applications such as speech recognition~~~In addition, it would be interesting to see whether these results extend to fields other than language modeling where smoothing is used, such as prepositional phrase attachment <REF>Collins and Brooks, 1995</REF>, part-of-speech tagging <TREF>Church, 1988</TREF>, and stochastic parsing <REF>Magerman, 1994</REF>~~~317 Acknowledgements The authors would like to thank Stuart Shieber and the anonymous reviewers for their comments on previous versions of this paper~~~We would also like to thank William Gale and Geoffrey Sampson for supplying us with code for Good-Turing frequency estimation without tears.
In addition, we introduce two novel smoothing techniques, one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique, both of which outperform existing methods~~~Smoothing is a technique essential in the construction of n-gram language models, a staple in speech recognition <REF>Bahl, Jelinek, and Mercer, 1983</REF> as well as many other domains <TREF>Church, 1988</TREF>; <REF>Brown et al , 1990</REF>; <REF>Kernighan, Church, and Gale, 1990</REF>~~~A language model is a probability distribution over strings Ps that attempts to reflect the frequency with which each string s occurs as a sentence in natural text~~~Language models are used in speech recognition to resolve acoustically ambiguous utterances.
In practice, computational limitations do not allow the enumeration of all possible assignments for long sentences, and smoothing is required for infrequent events~~~This is described in more detail in the original publication <TREF>Church, 1988</TREF>~~~Although more sophisticated algorithms for unsupervised learning which can be trained on plain text instead on manually tagged corpora are well established see eg <REF>Merialdo, 1994</REF>, we decided not to use them~~~The main reason is that with large tag sets, the sparse-data-problem can become so severe that unsupervised training easily ends up in local minima, which can lead to poor results without any indication to the user.
<REF>Lezius, Rapp  Wettler 1996</REF> give an overview on some German tagging projects~~~Although we considered a number of algorithms, we decided to use the trigram algorithm described by <TREF>Church 1988</TREF> for tagging~~~It is simple, fast, robust, and among the statistical taggers still more or less unsurpassed in terms of accuracy~~~Conceptually, the Church-algorithm works as follows: For each sentence of a text, it generates all possible assignments of part-of-speech tags to words.
A more detailed discussion of LTAGs with an example and some of the key properties of elementary trees is presented in Appendix A 4~~~Supertags Part-of-speech disambiguation techniques POS taggers <TREF>Church 1988</TREF>; <REF>Weischedel et al 1993</REF>; <REF>Brill 1993</REF> are often used prior to parsing to eliminate or substantially reduce the part-of-speech ambiguity~~~The POS taggers are all local in the sense that they use information from a limited context in deciding which tags to choose for each word~~~As is well known, these taggers are quite successful.
We tested the performance of the unigram model on the previously discussed two sets of data~~~The words are first assigned standard parts of speech using a conventional tagger <TREF>Church 1988</TREF> and then are assigned supertags according to the unigram model~~~A word in a sentence is considered correctly supertagged if it is assigned the same supertag as it is associated with in the correct parse of the sentence~~~The results of these experiments are tabulated in Table 4.
Some external mechanism is assumed to consistently or stochastically annotate substrings as phrases 2~~~Our goal is to come up with a mechanism that, given an input string, identifies the phrases in this string, this is a fundamental task with applications in natural language <TREF>Church, 1988</TREF>; <REF>Ramshaw and Marcus, 1995</REF>; <REF>Mufioz et al , 1999</REF>; <REF>Cardie and Pierce, 1998</REF>~~~The identification mechanism works by using classifiers that process the input string and recognize in the input string local signals which  This research is supported by NSF grants IIS-9801638, SBR-9873450 and IIS-9984168~~~1Full version is in <REF>Punyakanok and Roth, 2000</REF>.
Much research has been donc Oll knowledge acquisition fiom large-scalc annotated corpora as a rich source of linguistic knowledge~~~Mtior works done to create English POS taggers henceforth, taggers, for example, include <TREF>Church 1988</TREF>, <REF>Kupicc 1992</REF>, <REF>Brill 1992</REF>and <REF>Voutilaincn et al 1992</REF>~~~The problem with this framework, however, is that such reliable corpora are hardly awdlable duc to a huge amount of the labor-intensive work required~~~In case of the acquisition of non-core knowledge, such as specific, lexically or dolnain dependent knowledge, preparation of annotated corpora becomes more serious problem.
2~~~PART:OF-SPEECH TAG SEQUENCE GRAMMAR We utilised the ANLT metagrammatical formalism to develop a feature-based, declarative description of part-of-speech PoS label sequences see eg <TREF>Church, 1988</TREF> for English~~~This grammar compiles into a DCG-like grammar of approximately 400 rules~~~It has been designed to enumerate possible valencies for predicates verbs, adjectives and nouns by including separate rules for each pattern of possible complementation in English.
But dictionaries of technical terminology have many one-word terms~~~Simplex or complex NPs eg , <TREF>Church 1988</TREF>; <REF>Hindle and Rooth 1991</REF>; <REF>Wacholder 1998</REF> identify simplex or base NPs  NPs which do not have any component NPs -at least in part because this bypasses the need to solve the quite difficult attachment problem, ie, to determine which simpler NPs should be combined to output a more complex NP~~~But if people find complex NPs more useful than simpler ones, it is important to focus on improvement of techniques to reliably identify more complex terms~~~Semantic and syntactic terms variants.
As described in Section 3, each indicator has a unique value for each verb, which corresponds to the frequency of the aspectual marker with the verb except verb frequency, which is an absolute measure over the corpus~~~6 Similar baselines for comparison have been used for many classification problems <REF>Duda and Hart 1973</REF>, eg, part-of-speech tagging <TREF>Church 1988</TREF>; <REF>Allen 1995</REF>~~~611 Computational Linguistics Volume 26, Number 4 The second and third columns of Table 9 show the average value for each indicator over stative and event clauses, as measured over the training examples which exclude be and have~~~These values are computed solely over the 739 training cases in order to avoid biasing the classification experiments in the sections below, which are evaluated over the unseen test cases.
To assign capitalized unknown words the category proper noun seems a good heuristic, but may not always work~~~As argued in <TREF>Church 1988</TREF>, who proposes a more elaborated heuristic, <REF>Dermatas and Kokkinakis 1995</REF> proposed a simple probabilistic approach to unknown-word guessing: HCRC, Language Technology Group, University of Edinburgh, 2 Buccleuch Place, Edinburgh EH8 9LW, Scotland, UK~~~Q 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 3 Table 1 The most frequent open-class tags from the Penn tag set~~~Tag Meaning Example Tag Meaning Example NN common noun table NNS noun plural tables NNP proper noun John NNPS plural proper noun Vikings JJ adjective green RB adverb naturally VB verb base form take VBD verb past took VBG gerund taking VBN past participle taken VBZ verb present, 3d person takes VBP verb, present, non-3d take the probability that an unknown word has a particular Pos-tag is estimated from the probability distribution of hapax words words that occur only once in the previously seen texts.
Although our primary goal was not to compare the taggers themselves but rather their performance with the guessing components, we attribute the difference in their performance to the fact that Brills tagger uses the information about the most likely tag for a word whereas the HMM tagger did not have this information and instead used the priors for a set of POS-tags ambiguity class~~~When we removed from the lexicon all the hapax words and, following the recommendation of <TREF>Church 1988</TREF>, all the capitalized words with frequency less than 20, we obtained some 51,522 unknown word-tokens 25,359 wordtypes out of more than a million word-tokens in the Brown Corpus~~~We tagged the fifteen subcorpora of the Brown Corpus by the four combinations of the taggers and the guessers using the lexicon of 22,260 word-types~~~42 Results of the Experiment Table 4 displays the tagging results on the unknown words obtained by the four different combinations of taggers and guessers.
In the 329 rule-based approach a large number of hand crafted rules are used to select the correct morphological parse or POS tag of a given word in a given context <REF>Karlsson et al , 1995</REF>; Oflazer and Tcurrency1ur, 1997~~~In the statistical approach a hand tagged corpus is used to train a probabilistic model which is then used to select the best tags in unseen text <TREF>Church, 1988</TREF>; Hakkani-Tcurrency1ur et al , 2002~~~Examples of statistical and machine learning approaches that have been used for tagging include transformation based learning <REF>Brill, 1995</REF>, memory based learning <REF>Daelemans et al , 1996</REF>, and maximum entropy models <REF>Ratnaparkhi, 1996</REF>~~~It is also possible to train statistical models using unlabeled data with the expectation maximization algorithm <REF>Cutting et al , 1992</REF>.
After presenting our results and evaluation, we discuss simulation experiments that show how our method performs under different conditions of sparseness of data~~~3 Data Collection For our experiments, we use the 21 million word 1987 Wall Street Journal corpus 4, automatically annotated with part-of-speech tags using the PARTS tagger <TREF>Church, 1988</TREF>~~~In order to verify our hypothesis about the orientations of conjoined adjectives, and also to train and evaluate our subsequent algorithms, we need a 3Certain words inflected with negative affixes such as inor un- tend to be mostly negative, but this rule applies only to a fraction of the negative words~~~Furthermore, there are words so inflected which have positive orientation, eg, independent and unbiased.
For example, ve O, dead can be used tkr emphasis, and relet am relet as in her lhce became redder and redder can be used to indicate a progression of coloring, qb distinguish between truly gradablc adjectives and non-gradable adjectives in these exceptional contexts, we have developed a trainable log-linear statistical model that lakes into account tile number of times an adiective has been observed in a form or context indicating gradability relative to the number of limes it has been seen in non-gradable contexts~~~We use a shallow parser to retrieve from a large corpus tagged for part-of-speech with Churchs PARTS tagger <TREF>Church, 1988</TREF> all adjectives and their modifiers~~~Although the most common use of an adverb modifying an adjective is to function as an intensilier or diminisher <REF>Quirk et al , 1985</REF>, p 445, adverbs can also add to tile semantic content of the adjectival phrase instead of providing a grading effect eg , immediately available, politically vuhmrable, or function as cmphasizers, adding to the force o1 tile base adjective and not lo its degree eg , virtually impossible; compare re O, impossible~~~Therefore, we compiled by hand a list of 73 adverbs and noun phrases such as a little, exceedingly, somewhat, and veo that are fiequently used as grading moditicrs.
More precisely, assume that the word wh occurs in a sentence W  wlWkwn, and that w is a word we are considering substituting for it, yielding sentence W I Word w is then preferred over wk iff PW > PW, where PW and PW are the probabilities of sentences W and W f respectively~~~1 We calculate PW using the tag sequence of W as an intermediate quantity, and summing, over all possible tag sequences, the probability of the sentence with that tagging; that is: PW   PW, T T where T is a tag sequence for sentence W The above probabilities are estimated as is traditionally done in trigram-based part-of-speech tagging <TREF>Church, 1988</TREF>; <REF>DeRose, 1988</REF>: PW,T  PWITPT  1  HPwiti HPt, lt,2t,l2 i i where T  tltn, and Ptitl-2ti-1 is the prob ability of seeing a part-of-speech tag tl given the two preceding part-of-speech tags ti-2 and ti-1~~~Equations 1 and 2 will also be used to tag sentences W and W  with their most likely part-of-speech sequences~~~This will allow us to determine the tag that 1To enable fair comparisons between sequences of different length as when considering maybe and may be, we actually compare the per-word geometric mean of the sentence probabilities.
Its recall is very high 997 of all words receive the correct morphological analysis, but this system leaves 3-7 of all words ambiguous, trading precision for recall~~~157 ena or the linguists abstraction capabilities eg knowledge about what is relevant in the context, they tend to reach a 95-97 accuracy in the analysis of several languages, in particular English <REF>Marshall 1983</REF>; Black et aL 1992; <TREF>Church 1988</TREF>; <REF>Cutting et al 1992</REF>; de <REF>Marcken 1990</REF>; <REF>DeRose 1988</REF>; <REF>Hindle 1989</REF>; <REF>Merialdo 1994</REF>; <REF>Weischedel et al 1993</REF>; <REF>Brill 1992</REF>; <REF>Samuelsson 1994</REF>; Eineborg and Gambick 1994, etc~~~Interestingly, no significant improvement beyond the 97 barrier by means of purely data-driven systems has been reported so far~~~In terms of the accuracy of known systems, the data-driven approach seems then to provide the best model of part-of-speech distribution.
Figure 1: Base NP Examples base noun phrases with initial determiners and modifiers removed: <REF>Justeson  Katz 1995</REF> look for repeated phrases; <REF>Bourigault 1992</REF> uses a handcrafted noun phrase grammar in conjunction with heuristics for finding maximal length noun phrases; Voutilainens NP<REF>Tool 1993</REF> uses a handcrafted lexicon and constraint grammar to find terminological noun phrases that include phrase-final prepositional phrases~~~Churchs PARTS program 1988, on the other hand, uses a probabilistic model automatically trained on the Brown corpus to locate core noun phrases as well as to assign parts of speech~~~More recently, Ramshaw  Marcus In press apply transformation-based learning <REF>Brill, 1995</REF> to the problem~~~Unfortunately, it is difficult to directly compare approaches.
1993 call the core noun phrase, that is a noun phrase with no modification to the right of the head~~~Several approaches provide similar output based on statistics <TREF>Church 1988</TREF>, <REF>Zhai 1997</REF>, for example, a finite-state machine <REF>AitMokhtar and Chanod 1997</REF>, or a hybrid approach combining statistics and linguistic rules <REF>Voutilainen and Padro 1997</REF>~~~The SPECIALIST parser is based on the notion of barrier words <REF>Tersmette et al 1988</REF>, which indicate boundaries between phrases~~~After lexical look-up and resolution of category label ambiguity by the Xerox tagger, complementizers, conjunctions, modals, prepositions, and verbs are marked as boundaries.
Finally, memory-based learning is adopted to further improve the performance of the chunk tagger~~~The idea of using statistics for chunking goes back to <TREF>Church1988</TREF>, who used corpus frequencies to determine the boundaries of simple nonrecursive noun phrases~~~Skut and <REF>Brants1998</REF> modified Churchs approach in a way permitting efficient and reliable recognition of structures of limited depth and encoded the structure in such a way that it can be recognised by a Viterbi tagger~~~Our approach follows Skut and Brants way by employing HMM-based tagging method to model the chunking process.
Thus, Examples 3-5 illustrate how the syntactic context of a word can help determine its meaning~~~22 Motivation from previous work 221 Parsing In recent years, the success of statistical parsing techniques can be attributed to several factors, such as the increasing size of computing machinery to accommodate larger models, the availability of resources such as the Penn Treebank <REF>Marcus et al , 1993</REF> and the success of machine learning techniques for lowerlevel NLP problems, such as part-of-speech tagging <TREF>Church, 1988</TREF>; <REF>Brill, 1995</REF>, and PPattachment <REF>Brill and Resnik, 1994</REF>; <REF>Collins and Brooks, 1995</REF>~~~However, perhaps even more significant has been the lexicalization of the grammar formalisms being probabilistically modeled: crucially, all the recent, successful statistical parsers have in some way made use of bilexical dependencies~~~This includes both the parsers that attach probabilities to parser moves <REF>Magerman, 1995</REF>; <REF>Ratnaparkhi, 1997</REF>, but also those of the lexicalized PCFG variety <REF>Collins, 1997</REF>; <REF>Charniak, 1997</REF>.
to  NP only  18 billion  PP in  NP September  While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information  by examining the pattern itself, its nearby context and the local part-of-speech information~~~Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers <REF>Collins, 1997</REF>; <REF>Charniak, 1997a</REF>; <REF>Charniak, 1997b</REF>; <REF>Ratnaparkhi, 1997</REF>, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns  syntactic phrases or words that participate in a syntactic relationship <TREF>Church, 1988</TREF>; <REF>Ramshaw and Marcus, 1995</REF>; <REF>Argamon et al , 1998</REF>; <REF>Cardie and Pierce, 1998</REF>; <REF>Munoz et al , 1999</REF>; <REF>Punyakanok and Roth, 2001</REF>; <REF>Buchholz et al , 1999</REF>; Tjong <REF>Kim Sang and Buchholz, 2000</REF>~~~Research on shallow parsing was inspired by psycholinguistics arguments <REF>Gee and Grosjean, 1983</REF> that suggest that in many scenarios eg , conversational full parsing is not a realistic strategy for sentence processing and analysis, and was further motivated by several arguments from a natural language engineering viewpoint~~~First, it has been noted that in many natural language applications it is sufficient to use shallow parsing information; information such as noun phrases NPs and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization <REF>Grishman, 1995</REF>; <REF>Appelt et al , 1993</REF>.
33 Dialogue Act Decoding The HMM representation allows us to use efficient dynamic programming algorithms to compute relevant aspects of the model, such as  the most probable DA sequence the Viterbi algorithm  the posterior probability of various DAs for a given utterance, after considering all the evidence the forward-backward algorithm The Viterbi algorithm for HMMs <REF>Viterbi 1967</REF> finds the globally most probable state sequence~~~When applied to a discourse model with locally decomposable likelihoods and Markovian discourse grammar, it will therefore find precisely the DA 348 Stolcke et al Dialogue Act Modeling sequence with the highest posterior probability: U  argmaxPUIE  4 u The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition <REF>Bahl, Jelinek, and Mercer 1983</REF> and tagging <TREF>Church 1988</TREF>~~~It maximizes the probability of getting the entire DA sequence correct, but it does not necessarily find the DA sequence that has the most DA labels correct <REF>Dermatas and Kokkinakis 1995</REF>~~~To minimize the total number of utterance labeling errors, we need to maximize the probability of getting each DA label correct individually, ie, we need to maximize PUilE for each i  1  n We can compute the per-utterance posterior DA probabilities by summing: PuE  E PUIE 5 U: Uiu where the summation is over all sequences U whose ith element matches the label in question.
Various methods for POS tagging have been proposed in recent years~~~For simplicity, we adopted the method proposed by <TREF>Church 1988</TREF> to tag definition sentences~~~Experiments indicated an average error rate for tagging of less than 10~~~Tagging errors have limited negative impact, because words in the LLOCE are organized primarily according to topic, not part of speech.
I think we I need l to uh I I I need l r I m I et I r m I Algorithm Our algorithm for labeling potential repair patterns encodes the assumption that speech repairs can be processed one at a time~~~The algorithm runs in lockstep with a part-of-speech tagger <TREF>Church, 1988</TREF>, which is used for deciding possible word replacements~~~Words are fed in one at a time~~~The detection clues are checked first.
7About half of the difference between the detection recall rate and the correction recall rate is due to abridged repairs being misclassified as modification repairs~~~299 Part-of-speech tagging is the process of assigning to a word the category that is most probable given the sentential context <TREF>Church, 1988</TREF>~~~The sentential context is typically approximated by only a set number of previous categories, usually one or two~~~Good part-of-speech results can be obtained using only the preceding category <REF>Weischedel et al , 1993</REF>, which is what we will be using.
Recent research advances may lead to the development of viable book indexing methods for Chinese books~~~These include the availability of efficient and high precision word segmentation methods for Chinese text <REF>Chang et al , 1991</REF>; <REF>Sproat and Shih, 1990</REF>; <REF>Wang et al , 1990</REF>, the availability of statistical analysis of a Chinese corpus <REF>Liu et al , 1975</REF> and large-scale electronic Chinese dictionaries with partof-speech information <REF>Chang et al , 1988</REF>; BDC, 1992, the corpus-based statistical part-of-speech tagger <TREF>Church, 1988</TREF>; <REF>DeRose, 1988</REF>; <REF>Beale, 1988</REF>, as well as phrasal and clausal analyzers <TREF>Church 1988</TREF>; <REF>Ejerhed 1990</REF> 2~~~Problem description As being pointed out in <REF>Salton, 1988</REF>, back-of-book indexes may consist of more than one word that are derived from a noun phrase~~~Given the text of a book, an indexing system, must perform some kind of phrasal and statistical analysis in order to produce a list of candidate indexes and their occurrence statistics in order to generate indexes as shown in Figure 1 which is an excerpt from the reconstruction of indexes of a book on transformational grammar for Mandarin Chinese <REF>Tang, 1977</REF>.
NPtool parse Apparent correct parse less time less time the other hand the other hand many advantages many advantages bnary addressing binary addressing and and instruction formats instruction formats a purely binary computer a purely binary computer Table 1: Apparent errors made by Voutilainens N<REF>Ptool Kupiec 1993</REF> also briefly mentions the use of finite state NP recognizers for both English and French to prepare the input for a program that identified the correspondences between NPs in bilingual corpora, but he does not directly discuss their performance~~~Using statistical methods, Churchs Parts program 1988, in addition to identifying parts of speech, also inserted brackets identifying core NPs~~~These brackets were placed using a statistical model trained on Brown corpus material in which NP brackets had been inserted semi-automatically~~~In the small test sample shown, this system achieved 98 recall for correct brackets.
In the small test sample shown, this system achieved 98 recall for correct brackets~~~At about the same time, <REF>Ejerhed 1988</REF>, working with Church, performed comparisons between finite state methods and Churchs stochastic models for identifying both non-recursive clauses and non-recursive NPs in English text~~~In those comparisons, the stochastic methods outperformed the hand built finite-state models, with claimed accuracies of 935 clauses and 986 NPs for the statistical models compared to to 87 clauses and 978 NPs for the finite-state methods~~~Running Churchs program on test material, however, reveals that the definition of NP embodied in Churchs program is quite simplified in that it does not include, for example, structures or words conjoined within NP by either explicit conjunctions like and and or, or implicitly by commas.
They consider a range of input variables, including textderived information such as detailed POS labels and syntactic constituent structure, and in some experiments, acoustic information~~~POS labels were given by Churchs tagger <TREF>Church 1988</TREF> and syntactic constituents by Hindles parser <REF>Hindle 1987</REF>~~~The acoustic information previous boundary location, pitch accent location, and phrase duration, which was based on hand-labeled prosodic markers, did not improve performance but resulted in a much smaller tree for prediction~~~All of these approaches have influenced the model proposed here.
These tools can then be used by other systems to address more complex tasks~~~For example, previous work has addressed low-level tasks such as tagging a free-style corpus with part-of-speech information <TREF>Church 1988</TREF>, aligning a bilingual corpus <REF>Gale and Church 1991b</REF>; <REF>Brown, Lai, and Mercer 1991</REF>, and producing a list of collocations <REF>Smadja 1993</REF>~~~While each of these tools is based on simple statistics and tackles elementary tasks, we have demonstrated with our work on Champollion that by combining them, one can reach new levels of complexity in the automatic treatment of natural languages~~~Acknowledgments This work was supported jointly by the Advanced Research Projects Agency and the Office of Naval Research under grant N00014-89-J-1782, by the Office of Naval Research under grant N00014-95-1-0745, by the National Science Foundation under grant GER-90-24069, and by the New York State Science and Technology Foundation under grants NYSSTF-CAT91-053 and NYSSTF-CAT94-013.
The parser will eventually disambiguate all the descriptions and pick one per object, for a given reading of the sentence~~~This is what the parser is expected to do for disambiguating the standard POS, unless a separate POS disambiguation module is used <TREF>Church, 1988</TREF>~~~Many parsers, including XTAG, use such a module alhd a POS tagger~~~LTAGs present a novel opportunity to reduce the amount of disambiguation done by the parser.
Much recent research in the field of natural language processing NLP has focused on an empirical, corpus-based approach <REF>Church and Mercer, 1993</REF>~~~The high accuracy achieved by a corpus-based approach to part-of-speech tagging and noun phrase parsing, as demonstrated by <TREF>Church, 1988</TREF>, has inspired similar approaches to other problems in natural language processing, including syntactic parsing and word sense disambiguation WSD~~~The availability of large quantities of part-ofspeech tagged and syntactically parsed sentences like the Penn Treebank corpus <REF>Marcus, Santorini, and Marcinkiewicz, 1993</REF> has contributed greatly to the development of robust, broad coverage partof-speech taggers and syntactic parsers~~~The Penn Treebank corpus contains a sufficient number of partof-speech tagged and syntactically parsed sentences to serve as adequate training material for building broad coverage part-of-speech taggers and parsers.
One method of handling large vocabularies is simply increasing the size of the lexicon~~~Research efforts at IBM Chodorow, et al 1988; Neff, et al 1989, Bell Labs Church, et al 1989, New Mexico State University <REF>Wilks 1987</REF>, and elsewhere have used mechanical processing of on-line dictionaries to infer at least minimal syntactic and semantic information from dictionary definitions~~~However, even assuming a very large lexicon already exists, it can never be complete~~~Systems aiming for coverage of unrestricted language in broad domains must continually deal with new words and novel word senses.
In order to make these improvements, we need access to word-class information Pos information <REF>Johansson et al 1986</REF>; <REF>Black, Garside, and Leech 1993</REF> or semantic information <REF>Beckwith et al 1991</REF>, which is usually obtained in three main ways: Firstly, we can use corpora that have been manually tagged by linguistically informed experts <REF>Derouault and Merialdo 1986</REF>~~~Secondly, we can construct automatic part-ofspeech taggers and process untagged corpora <REF>Kupiec 1992</REF>; <REF>Black, Garside, and Leech 1993</REF>; this method boasts a high degree of accuracy, although often the construction of the automatic tagger involves a bootstrapping process based on a core corpus which has been manually tagged <TREF>Church 1988</TREF>~~~The third option is to derive a fully automatic word-classification system from untagged corpora~~~Some advantages of this last approach include its applicability to any natural language for which some corpus exists, independent of the degree of development of its grammar, and its parsimonious commitment to the machinery of modern linguistics.
If an external resource is used in the form of a morphological analyzer MA, this will almost always overgenerate, yielding false ambiguity~~~But even if the MA is tight, a considerable proportion of ambiguous tokens will come from legitimate but rare analyses of frequent types <TREF>Church, 1988</TREF>~~~For example the word nem, can mean both not and gender, so both ADV and NOUN are valid analyses, but the adverbial reading is about five orders of magnitude more frequent than the noun reading, 12596 vs 4 tokens in the 1 m word manually annotated Szeged Korpusz <REF>Csendes et al , 2004</REF>~~~Thus the difficulty of the task is better measured by the average information required for disambiguating a token.
Almost all approaches to sequence problems such as partof-speech tagging take a unidirectional approach to conditioning inference along the sequence~~~Regardless of whether one is using HMMs, maximum entropy conditional sequence models, or other techniques like decision trees, most systems work in one direction through the sequence normally left to right, but occasionally right to left, eg, <TREF>Church 1988</TREF>~~~There are a few exceptions, such as Brills transformation-based learning <REF>Brill, 1995</REF>, but most of the best known and most successful approaches of recent years have been unidirectional~~~Most sequence models can be seen as chaining together the scores or decisions from successive local models to form a global model for an entire sequence.
Part-of-speech tagging is required to detect new terms formed through conversion~~~This is quite feasible using statistical taggers like those of <REF>Garside 1987</REF>, <TREF>Church 1988</TREF> or <REF>Foster 1991</REF> which achieve performance upwards of 97 on unrestricted text~~~Terms formed through semantic drift are the wolves in sheeps clothing stealing through terminological pastures~~~They are well enough conceMcd to allude at times even the human reader and no automatic term-recognition system has attempted to distinguish such terms, despite the prevalence ofpolysemy in such fields as the social sciences Riggs, 1993 and the importance for purposes of terminological standardization that deviant usage be tracked.
4~~~Partof-Speech Tagging Part-of-speech tagging is the process of assigning to a word the category that is most probable given the sentential context <TREF>Church, 1988</TREF>~~~The sentential context is typically approximated by only a set number of previous categories, usually one or two~~~Since the context is limited, we are making the Markov assumption, that the next transition depends only on the input, which is the word that we the following changes: 1 we -parated Weposifiom from subordinating conjunctions; 2 we separated uses of to as a preposition from in me as part of a to-infinilive; 3 rather than classify verbs by tense, we classified them into four groups, conjugations of be, conjugations of have, verbs that are followed by a to-infinitive, and verbs that are followed immediately by another verb.
On sentences with <40 words, the former model performs at 69 precision, 75 recall, and the latter at 77 precision and 78 recall~~~Ever since the success of HMMs application to part-of-speech tagging in <TREF>Church, 1988</TREF>, machine learning approaches to natural language processing have steadily become more widespread~~~This increase has of course been due to their proven efficacy in many tasks, but also to their engineering effiCacy~~~Many machine learning approaches let the data speak for itself data ipsa loquuntur, as it were, allowing the modeler to focus on what features of the data are important, rather than on the complicated interaction of such features, as had often been the case with hand-crafted NLP systems.
In conclusion, we argue strongly that the use of an independent morphological dictionary is the preferred choice to more annotated data under such circumstances~~~1 Full Morphological Tagging English Part of Speech POS tagging has been widely described in the recent past, starting with the <TREF>Church, 1988</TREF> paper, followed by numerous others using various methods: neural networks <REF>Julian Benello and Anderson, 1989</REF>, HMM tagging <REF>Merialdo, 1992</REF>, decision trees <REF>Schmid, 1994</REF>, transformation-based error-driven learning <REF>Brill, 1995</REF>, and maximum entropy <REF>Ratnaparkhi, 1996</REF>, to select just a few~~~However different the methods were, English dominated in these tests~~~Unfortunately, English is a morphologically impoverished language: there are no complicated agreement relations, word order variation is minireal, and the morphological categories are either extremely simple -s for plural of nouns, for example, or almost nonexistent cases expressed by inflection, for example with not too many exceptions and irregularities.
The suggestion which we want to explore is that the association revealed by textual distribution whether its source is a complementation relation, a modification relation, or something else gives us information needed to resolve the prepositional attachment~~~Discovering Lexical Association in Text A 13 million word sample of Associated Press new stories from 1989 were automatically parsed by the Fidditch parser <REF>Hindle 1983</REF>, using Churchs part of speech analyzer as a preprocessor <TREF>Church 1988</TREF>~~~From the syntactic analysis provided by the parser for each sentence, we extracted a table containiffg all the heads of all noun phrases~~~For each noun phrase head, we recorded the following preposition if any occurred ignoring whether or not the parser attached the preposition to the noun phrase, and the preceding verb if the noun phrase was the object of that verb.
460 of this data has an impact on the tagging accuracy of both the HMM itself and the derived transducer~~~The training of the HMM can be done on either a tagged or untagged corpus, and is not a topic of this paper since it is exhaustively described in the literature <REF>Bahl and Mercer, 1976</REF>; <TREF>Church, 1988</TREF>~~~An HMM can be identically represented by a weighted FST in a straightforward way~~~We are, however, interested in non-weighted transducers.
In the early nineties, <REF>Abney 1991</REF> proposed to approach parsing by starting with finding related chunks of words~~~By then, <TREF>Church 1988</TREF> had already reported on recognition of base noun phrases with statistical methods~~~<REF>Ramshaw and Marcus 1995</REF> approached chunking by using a machine learning method~~~Their work has inspired many others to study the application of learning methods to noun phrase chunking 5.
Realizing the difficulties o1 complete parsing, many researches turned to explore the partial parsing techniques~~~<TREF>Church1988</TREF> proposed a silnple stochastic technique for lecognizing the non-recursive base noun phrases in English~~~;outilaimen1993 designed an English noun phrase recognition tool -- NPTbol~~~<REF>Abney1997</REF> applied both rule-based and statistics-based approaches for parsing chunks in English.
Several approaches have been proposed to construct automatic taggers~~~Most work on statistical methods has used n-gram models or Hidden Markov Model-based taggers eg <TREF>Church, 1988</TREF>; <REF>DeRose, 1988</REF>; <REF>Cutting et al 1992</REF>; <REF>Merialdo, 1994</REF>, etc~~~In 14 these approaches, a tag sequence is chosen for a sentence that maximizes the product of lexical and contextual probabilities as estimated from a tagged corpus~~~In rule-based approaches, words are assigned a tag based on a set of rules and a lexicon.
Categorical ambiguity, however, is of a different kind and is resolved in a different way~~~For the purposes of the present paper, it will be assumed that only content words are at issue, and that the syntactic category of all content words in the text that is under study can be determined automatically <TREF>Church, 1988</TREF>; <REF>DeRose, 1988</REF>~~~The problem is simply to decide which sense of a content word--noun, verb, adjective, or adverb---is appropriate in a given linguistic context~~~It will also be assumed that sense resolution for individual words can be accomplished on the basis of information about the irnrnediate linguistic context.
The program can be trained even with a relatively small amount of treebank data; then it can be J used for parsing unrestricted pre-tagged text~~~As far as coverage is concerned, our parser can handle recursive structures, which is an advantage compared to simpler techniques such as that described by <TREF>Church 1988</TREF>~~~On the other hand, the Markov assumption underlying our approach means that only strictly local dependencies are recognised~~~For full parsing, one would probably need non-local contextual information, such as the long-range trigrams in Link Grammar Della <REF>Pietra et al , 1994</REF>.
Regardless of whether or not abstractions such as phrases occur in the model, most of the relevant information is contained directly in the sequence of words and part-of-speech tags to be processed~~~An archetypal representative of this approach is the method described by <TREF>Church 1988</TREF>, who used corpus frequencies to determine the boundaries of simple non, recursive NPs~~~For each pair of part-of-speech tags ti, tj, the probability of an NP boundary   or  occurring between ti and tj is computed~~~On the basis of these context probabilities, the program inserts the symbols  and  into sequences of part-of-speech tags.
However, it does undeniably reduce confusion with respect to the proper noun category~~~Some well-known previous efforts <TREF>Church 1988</TREF>; de <REF>Marcken 1990</REF> have dealt with unknown words using various heuristics~~~For instance, Churchs program PARTS has a prepass prior to applying the tri-tag probability model that predicts proper nouns based on capitalization~~~The new aspects of our work are 1 incorporating the treatment of unknown words uniformly within the probability model, 2 approximating the component probabilities for unknowns directly from the training data, and 3 measuring the contribution of the tri-tag model, of the ending, and of capitalization.
Purely rule-based techniques seemed too brittle for dealing with the variety of constructions, the long sentences averaging 29 words per sentence, and the degree of unexpected input~~~Statistical models based on local information eg , <REF>DeRose 1988</REF>; <TREF>Church 1988</TREF> might operate effectively in spite of sentence length and unexpected input~~~To see whether our four hypotheses in italics above effectively addressed the four concerns above, we chose to test the hypotheses on two well-known problems: ambiguity both at the structural level and at the part-of-speech level and inferring syntactic and semantic information about unknown words~~~Guided by the past success of probabilistic models in speech processing, we have integrated probabilistic models into our language processing systems.
We report in Section 2 on our experiments on the assignment of part of speech to words in text~~~The effectiveness of such models is well known <REF>DeRose 1988</REF>; <TREF>Church 1988</TREF>; <REF>Kupiec 1989</REF>; <REF>Jelinek 1985</REF>, and they are currently in use in parsers eg de <REF>Marcken 1990</REF>~~~Our work is an incremental improvement on these models in three ways: 1 Much less training data than theoretically required proved adequate; 2 we integrated a probabilistic model of word features to handle unknown words uniformly within the probabilistic model and measured its contribution; and 3 we have applied the forward-backward algorithm to accurately compute the most likely tag set~~~In Section 3, we demonstrate that probability models can improve the performance of knowledge-based syntactic and semantic processing in dealing with structural ambiguity and with unknown words.
1 is a typical example of the ambiguities encountered in a running text: little POS ambiguity, but a lot of gender, number and case ambiguity columns 3 to 5~~~485 3 The Model Instead of employing the source-channel paradigm for tagging more or less explicitly present eg in <REF>Merialdo, 1992</REF>, <TREF>Church, 1988</TREF>, Hajji, Hladk, 1997 used in the past notwithstanding some exceptions, such as Maximum Entropy and rule-based taggers, we are using here a direct approach to modeling, for which we have chosen an exponential probabilistic model~~~Such model when predicting an event 5 y E Y in a context x has the general form PAC,e YIX  exp-in----1 Aifi y, x Zx 3 where fi Y, x is the set of size n of binary-valued yes/no features of the event value being predicted and its context, hi is a weigth in the exponential sense of the feature fi, and the normalization factor Zx is defined naturally as zx  exp z x 4 yEY i----1 ,Ve use a separate model for each ambiguity class AC which actually appeared in the training data of each of the 13 morphological categories 6~~~The final PAC Yix distribution is further smoothed using unigram distributions on subtags again, separately for each category.
This was expanded upon by <REF>Gale et al , 1992</REF>, and in a class-based variant by <REF>Yarowsky, 1992</REF>~~~Decision trees <REF>Brown, 1991</REF> have been usefully applied to word-sense ambiguities, and HMM part-of-speech taggers <REF>Jelinek 1985</REF>, <TREF>Church 1988</TREF>, <REF>Merialdo 1990</REF> have addressed the syntactic ambiguities presented here~~~<REF>Hearst 1991</REF> presented an effective approach to modeling local contextual evidence, while <REF>Resnik 1993</REF> gave a classic treatment of the use of word classes in selectional constraints~~~An algorithm for combining syntactic and semantic evidence in lexical ambiguity resolution has been realized in <REF>Chang et al , 1992</REF>.
Preprocessing the Corpus with a Part of Speech Tagger Phrasal verbs involving the preposition to raise an interesting problem because of the possible confusion with the infinitive marker to~~~We have found that if we first tag every word in the corpus with a part of speech using a method such as <TREF>Church 1988</TREF> or <REF>DeRose 1988</REF>, and then measure associations between tagged words, we can identify interesting contrasts between verbs associated with a following preposition toin and verbs associated with a following infinitive marker toto~~~Part of speech notation is borrowed from <REF>Francis and Kucera 1982</REF>; m  preposition; to  infinitive marker; vb  bare verb; vbg  verb  ing; vbd  verb  ed; vbz  verb  s; vbn  verb  en~~~The score identifies quite a number of verbs associated in an interesting way with to; restricting our attention to pairs with a score of 30 or more, there are 768 verbs associated with the preposition toin and 551 verbs with the infinitive marker toto.
In the partof-speech tagging field, the disambiguation of capitalized words is treated similarly to the disambiguation of common words~~~However, as <TREF>Church 1988</TREF> rightly pointed out Proper nouns and capitalized words are particularly problematic: some capitalized words are proper nouns and some are not~~~Estimates from the Brown Corpus can be misleading~~~For example, the capitalized word Acts is found twice in Brown Corpus, both times as a proper noun in a title.
Automatic morphological disambiguation is an important component in higher level analysis of natural language text corpora~~~There has been a large number of studies in tagging and morphological disambiguation using various techniques such as statistical techniques, eg, <TREF>Church, 1988</TREF>; <REF>Cutting et al , 1992</REF>; <REF>DeRose, 1988</REF>, constraint-based techniques <REF>Karlsson et al , 1995</REF>; <REF>Voutilainen, 1995b</REF>; Voutilainen, Heikkil/i, and <REF>Anttila, 1992</REF>; <REF>Voutilainen and Tapanainen, 1993</REF>; <REF>Oflazer and KuruSz, 1994</REF>; <REF>Oflazer and Till 1996</REF> and transformation-based techniques <REF>Brilt, 1992</REF>; <REF>Brill, 1994</REF>; <REF>Brill, 1995</REF>~~~This paper presents a novel approach to constraint based morphological disambiguation which relieves the rule developer from worrying about conflicting rule ordering requirements~~~The approach depends on assigning votes to constraints according to their complexity and specificity, and then letting constraints cast votes on matching parses of a given lexical item.
Given that each token has on the average more than 2 possible tags, the procedural description above is very inefficient for M1 but very short sentences~~~However, the observation that our constraints are localized to a window of a small number of tokens say at most 5 tokens in a sequence, suggests a more efficient scheme originally used by <TREF>Church 1988</TREF>~~~Assume our constraint windows are allowed to look at a window of at most size k sequential parses~~~Let us take the first k tokens of a sentence and generate all possible paths of k arcs spanning k  1 nodes, and apply all constraints to these short paths.
Part-of-speech tagging is one of the preliminary steps in many natural language processing systems in which the proper part-of-speech tag of the tokens comprising the sentences are disambiguated using either statistical or symbolic local contextual information~~~Tagging systems have used either a statistical approach where a large corpora is employed to train a probabilistic model which then is used to tag unseen text, eg, <TREF>Church 1988</TREF>, Cutting et al~~~1992, DeR,ose 1988, or a constraint-based approach which employs a large number of hand-crafted linguistic constraints that are used to eliminate impossible sequences or morphological parses for a given word in a given context, recently most prominently exemplified by the Constraint Grammar work <REF>Karlsson et al, 1995</REF>; <REF>Voutilainen, 1995b</REF>; <REF>Voutilainen et al, 1992</REF>; <REF>Voutilainen and Tapanainen, 1993</REF>~~~Brill 1992; 1994; 1995 has presented a transformationbased learning approach.
Models G and C For model G, I induced a simple grammar from the training corpus~~~I used Ken Churchs tagger <TREF>Church 1988</TREF> to 234 assign part-of-speech probabilities to words~~~The grammar contains a rule x ---> T for every Treebank chunk x t in the training corpus~~~x is the syntactic category of the chunk, and y is the part-of-speech sequence assigned to the words of the chunk.
The corpus used in our first experiment was derived from newswire text automatically parsed by 183 Hindles parser Fidditch <REF>Hindle, 1993</REF>~~~More recently, we have constructed similar tables with the help of a statistical part-of-speech tagger <TREF>Church, 1988</TREF> and of tools for regular expression pattern matching on tagged corpora <REF>Yarowsky, 1992</REF>~~~We have not yet compared the accuracy and coverage of the two methods, or what systematic biases they might introduce, although we took care to filter out certain systematic errors, for instance the misparsing of the subject of a complement clause as the direct object of a main verb for report verbs like say~~~We will consider here only the problem of classifying nouns according to their distribution as direct objects of verbs; the converse problem is formally similar.
1; they closely replicate Brills results 1993b, page 96, allowing for the fact that his tests used more templates, including templates like if one of the three previous tags is A~~~Brills results demonstrate that this approach can outperform the Hidden Markov Model approaches that are frequently used for part-of-speech tagging <REF>Jelinek, 1985</REF>; <TREF>Church, 1988</TREF>; <REF>DeRose, 1988</REF>; <REF>Cutting et al , 1992</REF>; <REF>Weischedel et al , 1993</REF>, as well as showing promise for other applications~~~The resulting model, encoded as a list of rules, is also typically more compact and for some purposes more easily interpretable than a table of HMM probabilities~~~An Incremental Algorithm It is worthwhile noting first that it is possible in some circumstances to significantly speed up the straightforward algorithm described above.
We show the method to be efficient and easily adaptable to different text genres, including single-case texts~~~Labeling of sentence boundaries is a necessary prerequisite for many natural language processing NLP tasks, including part-of-speech tagging <TREF>Church, 1988</TREF>, <REF>Cutting et al , 1991</REF>, and sentence alignment <REF>Gale and Church, 1993</REF>, Kay and R<REF>Sscheisen, 1993</REF>~~~End-of-sentence punctuation marks are ambiguous; for example, a period can denote an abbreviation, the end of a sentence, or both, as shown in the examples below: 1 The group included Dr JM Freeman and T Boone Pickens Jr~~~2 This issue crosses party lines and crosses philosophical lines.
21 Assignment of Descriptors The first stage of the process is lexical analysis, which breaks the input text a stream of characters into tokens~~~Our implementation uses a slightlymodified version of the tokenizer from the PARTS part-of-speech tagger <TREF>Church, 1988</TREF> for this task~~~A token can be a sequence of alphabetic characters, a sequence of digits numbers containing periods acting as decimal points are considered a single token, or a single non-alphanumeric character~~~A lookup module then uses a lexicon with part-of-speech tags for each token.
The list of candidate terms contains both multi-word noun phrases and single words~~~The multi-word terms match a small set of syntactic patterns defined by regular expressions and are found by searching a version of the document tagged with parts of speech <TREF>Church, 1988</TREF>~~~The set of syntactic patterns is considered as a parameter and can be adopted to a specific domain by the user~~~Currently our patterns match only sequences of nouns, which seem to yield the best hit rate in our environment.
This current practice is very laborious and runs the risk of missing many important terms~~~Termight uses a part of speech tagger <TREF>Church, 1988</TREF> to identify a list of candidate terms which is then filtered by a manual pass~~~We have found, however, that the manual pass dominates the cost of the monolingual task, and consequently, we have tried to design an interactive user interface see Figure 1 that minimizes the burden on the expert terminologist~~~The terminologist is presented with a list of candidate terms, and corrects the list with a minimum number of key strokes.
We rewrite this term as follows: PrW1,ND1,N N  I-IPrWiDilWl,ilDl,il i1 N  l-I PrWilWl,i-lDl,i PrDilWl,i-lDl,i-1 i1 7 Equation 7 involves two probability distributions that need to be estimated~~~These are the same distributions that are needed by previous POS-based language models Equation 5 and POS taggers <TREF>Church 1988</TREF>; <REF>Charniak et al 1993</REF>~~~However, these approaches simplify the context so that the lexical probability is just conditioned on the POS category of the word, and the POS probability is conditioned on just the preceding POS tags, which leads to the following two approximations~~~PrWiIWl,ilDl,i  PrWilDi 8 PrDiIWulDl,il  PrDiIDul 9 However, to successfully incorporate POS information, we need to account for the full richness of the probability distributions, as will be demonstrated in Section 344.
Figure 1: Base NP Examples base noun phrases with initial determiners and modifiers removed: <REF>Justeson  Katz 1995</REF> look for repeated phrases; <REF>Bourigault 1992</REF> uses a handcrafted noun phrase grammar in conjunction with heuristics for finding maximal length noun phrases; Voutilainens NP<REF>Tool 1993</REF> uses a handcrafted lexicon and constraint grammar to find terminological noun phrases that include phrase-final prepositional phrases~~~Churchs PARTS program 1988, on the other hand, uses a probabilistic model automatically trained on the Brown corpus to locate core noun phrases as well as to assign parts of speech~~~More recently, Ramshaw  Marcus In press apply transformation-based learning <REF>Brill, 1995</REF> to the problem~~~Unfortunately, it is difficult to directly compare approaches.
The recursive expansion of the tree stops if either the information gained by consulting further fv-pairs or the frequencies upon which the calculus is based are smaller than defined thresholds~~~4 TAGGING ALGORITHM Starting point for the implementation of a feature structure tagger was a second-0rdcr-IIMM tagger trigrams based on a modified version of the Viterbi algorithm <REF>Viterbi, 1967</REF>; <TREF>Church, 1988</TREF> which we had earlier implemented in C Kempe,1994~~~There we replaced the function which estimated the contextual probability of a tag state transition probability hy dividing a trigram frequency by a bigram frequency eq~~~3 with a flmction which accomplished this calculus either using PF1Ls in the above-described way eqs 6, 7 or by consulting a decision tree fig.
1992 circumvent this problem by training their taggers on untagged data using tile Itaum-Welch algorithm also know as the forward-backward algorithm~~~They report rates of correctly tagged words which are comparable to that presented by <TREF>Church 1988</TREF> and <REF>Kempe 1993</REF>~~~A third and rather new approach is tagging with artificial neural networks~~~In the area of speech recognition neural networks have been used for a decade r, ow.
For example, if we choose to create a pseudo-word out of the words make and take, we would change the test data like this: make plans  make, take plans take action  make, take action The method being tested must choose between the two words that make up the pseudo-word~~~32 Data We used a statistical part-of-speech tagger <TREF>Church, 1988</TREF> and pattern matching and concordancing tools due to David Yarowsky to identify transitive main verbs and head nouns of the corresponding direct objects in 44 million words of 1988 Associated Press newswire~~~We selected the noun-verb pairs for the 1000 most frequent nouns in the corpus~~~These pairs are undoubtedly somewhat noisy given the errors inherent in the part-of-speech tagging and pattern matching.
151 Computational Linguistics Volume 19, Number 1 <REF>Garside and Leech 1987</REF> have been shown to reach 95-99 performance on free-style text~~~We preprocessed the corpus with a stochastic part-of-speech tagger developed at Bell Laboratories by Ken Church <TREF>Church 1988</TREF>~~~9 In the rest of this section, we describe the algorithm used for the first stage of Xtract in some detail~~~We assume that the corpus is preprocessed by a part of speech tagger and we note wi a collocate of w if the two words appear in a common sentence within a distance of 5 words.
Such techniques have various applications~~~Speech recognition <REF>Bahl, Jelinek, and Mercer 1983</REF> and text compression eg , <REF>Bell, Witten, and Cleary 1989</REF>; <REF>Guazzo 1980</REF> have been of long-standing interest, and some new applications are currently being investigated, such as machine translation <REF>Brown et al 1988</REF>, spelling correction <REF>Mays, Damerau, and Mercer 1990</REF>; <REF>Church and Gale 1990</REF>, parsing <REF>Debili 1982</REF>; <REF>Hindle and Rooth 1990</REF>~~~As pointed out by <REF>Bell, Witten, and Cleary 1989</REF>, these applications fall under two research paradigms: statistical approaches and lexical approaches~~~In the statistical approach, language is modeled as a stochastic process and the corpus is used to estimate probabilities.
On one hand, according to the linguistic approach, experts encode handcrafted rules or constraints based on abstractions derived from language paradigms usually with the aid of corpora <REF>Green and Rubin, 1971</REF>; <REF>Voutilainen 1995</REF>~~~On the other hand, according to the data-driven approach, a frequency-based language model is acquired from corpora and has the forms of ngrams <TREF>Church, 1988</TREF>; <REF>Cutting et al , 1992</REF>, rules <REF>Hindle, 1989</REF>; <REF>Brill, 1995</REF>, decision trees <REF>Cardie, 1994</REF>; <REF>Daelemans et al , 1996</REF> or neural networks <REF>Schmid, 1994</REF>~~~In order to increase their robusmess, most POS taggers include a guesser, which tries to extract the POS of words not present in the lexicon~~~As a common strategy, POS guessers examine the endings of unknown words <REF>Cutting et al 1992</REF> along with their capitalization, or consider the distribution of unknown words over specific parts-of-speech Weischedel et aL, 1993.
In practice, computational limitations do not allow the enumeration of all possible assignments for long sentences, and smoothing is required for infrequent events~~~This is described in more detail in the original publication <TREF>Church, 1988</TREF>~~~Although more sophisticated algorithms for unsupervised learning which can be trained on plain text instead on manually tagged corpora are well established see eg <REF>Merialdo, 1994</REF>, we decided not to use them~~~The main reason is that with large tag sets, the sparse-data-problem can become so severe that unsupervised training easily ends up in local minima, which call lead to poor results without any indication to the user.
<REF>Lezius, Rapp  Wettler 1996</REF> give an overview on some German tagging projects~~~Although we considered a number of algorithms, we decided to use the trigram algorithm described by <TREF>Church 1988</TREF> for tagging~~~It is simple, fast, robust, and among the statistical taggers still more or less unsurpassed in terms of accuracy~~~Conceptually, the Church-algorithm works as follows: For each sentence of a text, it generates all possible assignments of part-of-speech tags to words.
2~~~Part-of-Speech Tagging The prototype source-channel application in natural language is part-of-speech tagging <TREF>Church 1988</TREF>~~~We review it here for purposes of comparison with machine translation~~~Source strings comprise sequences of part-of-speech tags like noun, verb, etc A simple source model assigns a probability to a tag sequence tl tm based on the probabilities of the tag pairs inside it.
Indeed, recent increased interest in the problem of disambiguating lexical category in English has led to significant progress in developing effective programs for assigning lexical category in unrestricted text~~~The most successful and comprehensive of these are based on probabilistic modeling of category sequence and word category <REF>Church 1987</REF>; <REF>Garside, Leech and Sampson 1987</REF>; <REF>DeRose 1988</REF>~~~These stochastic methods show impressive performance: Church reports a success rate of 95 to 99, and shows a sample text with an error rate of less than one percent~~~What may seem particularly surprising is that these methods succeed essentially without reference to syntactic structure; purely surface lexical patterns are involved.
Part-of-Speech Tagging Many of the very same methods are being applied to problems in natural language processing by many of the very same researchers~~~As a result, the empirical approach has been adopted by almost all contemporary part-of-speech programs: <REF>Bahl and Mercer 1976</REF>, <REF>Leech, Garside, and Atwell 1983</REF>, <REF>Jelinek 1985</REF>, <REF>Deroualt and Merialdo 1986</REF>, <REF>Garside, Leech, and Sampson 1987</REF>, <TREF>Church 1988</TREF>, <REF>DeRose 1988</REF>, <REF>Hindle 1989</REF>, Kupiec 1989, 1992, Ayuso et al~~~1990, de<REF>Marcken 1990</REF>, <REF>Karlsson 1990</REF>, <REF>Boggess, Agarwal, and Davis 1991</REF>, <REF>Merialdo 1991</REF>, and <REF>Voutilainen, Heikkila, and Anttila 1992</REF>~~~These programs input a sequence of words, eg, The chair will table the motion, and output a sequence of part-of-speech tags, eg, art noun modal verb art noun.
English parsing is divided into two tasks: shallow parsing and deep parsing~~~The shallow parser constructs Verb Groups VGs and basic Noun Phrases NPs, also called BaseNPs <TREF>Church 1988</TREF>~~~The deep parser utilizes syntactic subcategorization features and semantic features of a head eg , VG to decode both syntactic and logical dependency relationships such as Verb-Subject, Verb-Object, Head-Modifier, etc Part-of-Speech POS Tagging General Lexicon Lexical lookup Named Entity NE Taggig Shallow Parsing PV Identification Deep parsing General Lexicon PV Expert Lexicon Figure 1~~~System Architecture The general lexicon lookup component involves stemming that transforms regular or irregular inflected verbs into the base forms to facilitate the later phrasal verb matching.
We redistribute the probability mass of low count sequences to unseen sequences~~~Generalized Forward Backward Reestimation Generalization of the Forward and Viterbi Algorithm In English part of speech taggers, the maximization of Equation 1 to get the most likely tag sequence, is accomplished by the Viterbi algorithm <TREF>Church, 1988</TREF>, and the maximum likelihood estimates of the parameters of Equation 2 are obtained from untagged corpus by the ForwardBackward algorithm <REF>Cutting et al , 1992</REF>~~~However, it is impossible to apply the Viterbi algorithm and the Forward-Backward algorithm for word segmentation of those languages that have no delimiter between words, such as Japanese and Chinese, because word segmentation hypotheses overlap one another~~~Figure 3 shows an example of overlapping word hypotheses and possible word segmentations for the string Ntig-f all prefectures in the nation.
An event greater improvement over the baseline is illustrated by the increase in the number of event clauses correctly classified, ie event rrall~~~As shown in Table 7, an event recall of 677 was achieved by the classification rule, as compared to speech tagging <TREF>Church, 1988</TREF>; <REF>Alien, 1995</REF>~~~13 I I I I I I I I I I I I I I I I I I the 00 event recall achieved by the baseline, while suffering no loss in overall accuracy~~~This difference in recall is more dramatic than the accuracy improvement because of the dominance of stative clauses in the test set.
What can be done at the present stage is the recognition of relatively simple structures such as NPs and PPs~~~<TREF>Church, 1988</TREF> used a simple mechanism to mark the boundaries of NPs~~~He used part-of-speech tagging and added two flags to the part-of-speech tags to mark the beginning and the end of an NP~~~Our goal is more ambitious in that we mark not only the phrase boundaries of NPs but also the complete structure of a wider class of phrases, starting with APs, NPs and PPs.
For a larger dataset, such as the Canadian Hansards, it was not possible to check the results by hand~~~We used the same procedure which is used in <TREF>Church, 1988</TREF>~~~This procedure was developed by Kathryn Baker private communication~~~ratio.
In fact, whereas stochastic taggers have to store word-tag, bigram, and trigram probabilities, the rule-based tagger and therefore the finite-state one only have to encode a small number of rules between 200 and 300~~~We empirically compared our tagger with Eric Brills implementation of his tagger, and with our implementation of a trigram tagger adapted from the work of <TREF>Church 1988</TREF> that we previously implemented for another purpose~~~We ran the three programs on large files and piped their output into a file~~~In the times reported, we included the time spent reading the input and writing the output.
Although finite-state machines have been used for part-of-speech tagging <REF>Tapanainen and Voutilainen 1993</REF>; <REF>Silberztein 1993</REF>, none of these approaches has the same flexibility as stochastic techniques~~~Unlike stochastic approaches to part-of-speech tagging <TREF>Church 1988</TREF>; <REF>Kupiec 1992</REF>; <REF>Cutting et al 1992</REF>; <REF>Merialdo 1990</REF>; <REF>DeRose 1988</REF>; <REF>Weischedel et al 1993</REF>, up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired~~~<REF>Recently, Brill 1992</REF> described a rule-based tagger that performs as well as taggers based upon probabilistic models and overcomes the limitations common in rule-based approaches to language processing: it is robust and the rules are automatically ac Mitsubishi Electric Research Laboratories, 201 Broadway, Cambridge, MA 02139~~~E-mail: rocbe/schabesmerlcom.
A corpus is manually tagged with the categories and transition probabilities between two or three categories are estimated from their relative frequencies~~~This method is commonly used for part-of-speech tagging <TREF>Church, 1988</TREF>~~~The fourth method is a variation of the third method and is also used for part-of-speech tagging~~~This method does not need a pre-annotated corpus for parameter estimation.
2~~~Previous <REF>Works Church 1988</REF> proposes a part of speech tagger and a simple noun phrase extractor~~~His noun phrase extractor brackets the noun phrases of input tagged texts according to two probability matrices: one is starting noun phrase matrix; the other is ending noun phrase matrix~~~The methodology is a simple version of Garside and Leechs probabilistic parser 1985.
The testing scale is large enough about 150,000 words~~~In contrast, <TREF>Church 1988</TREF> tests a text and extracts the simple noun phrases only~~~Bourigaults work 1992 is evaluated manually, and dose not report the precision~~~Hence, the real performance is not known.
For more details, we refer the reader to <REF>Mgrquez and Rodrfguez, 1997</REF>~~~22 STT: A Statistical Tree-based Tagger The aim of statistical or probabilistic tagging <TREF>Church, 1988</TREF>; <REF>Cutting et al , 1992</REF> is to assign the most likely sequence of tags given the observed sequence of words~~~For doing so, two kinds of information are used: the lexical probabilities, ie, the probability of a particular tag conditional on the particular word, and the contextual probabilities, which describe the probability of a particular tag conditional on the surrounding tags~~~Contextual or transition probabilities are usually reduced to the conditioning of the preceding tag bigrams, or pair of tags trigrams, however, the general formulation allows a broader definition of context.
based approach implemented with finite-state machines <REF>Koskenniemi et al , 1992</REF>; <REF>Voutilainen and Tapanainen, 1993</REF>~~~A completely different approach to tagging uses statistical methods, eg , <TREF>Church, 1988</TREF>; <REF>Cutting et al , 1993</REF>~~~These systems essentially train a statistical model using a previously hand-tagged corpus and provide the capability of resolving ambiguity on the basis of most likely interpretation~~~The models that have been widely used assume that the part-ofspeech of a word depends on the categories of the two preceding words.
As far as coreference resolution is concerned, the goal of these NLP modules is to determine the boundary of the markables, and to provide the necessary information about each markable for subsequent generation of features in the training examples~~~Our part-of-speech tagger is a standard statistical tagger based on the Hidden Markov Model HMM <TREF>Church 1988</TREF>~~~Similarly, we built a statistical HMM-based noun phrase identification module that determines the noun phrase boundaries solely based on the part-of-speech tags assigned to the words in a sentence~~~We also implemented a module that recognizes MUC-style named entities, that is, organization, person, location, date, time, money, and percent.
There has recently been a great deal of work exploring methods for automatically training part of speech taggers, as an alternative to laboriously hand-crafting rules for tagging, as was done in the past <REF>Klein and Simmons, 1963</REF>; <REF>Harris, 1962</REF>~~~Almost all of the work in the area of automatically trained taggers has explored Markov-model based part of speech tagging <REF>Jelinek, 1985</REF>; <TREF>Church, 1988</TREF>; <REF>Derose, 1988</REF>; <REF>DeMarcken, 1990</REF>; <REF>Cutting et al , 1992</REF>; <REF>Kupiec, 1992</REF>; <REF>Charniak et al , 1993</REF>; <REF>Weischedel et al , 1993</REF>; <REF>Schutze and Singer, 1994</REF>; <REF>Lin et al , 1994</REF>; <REF>Elworthy, 1994</REF>; <REF>Merialdo, 1995</REF>~~~2 For a Markov-model based tagger, training consists of learning both lexical probabilities Pwordltag and contextual probabilities Ptagiltagil tagi-n~~~Once trained, a sentence can be tagged by searching for the tag sequence that maximizes the product of lexical and contextual probabilities.
In the examples ahove, tagging of presents as vbz in the first sentence cuts off a potentially long and cosily garden path with presents as a plural noun followed by a headless relative clause starting with that a proposal  In the second sentence, tagging resolves ambiguity of used vim vs vbd, and associates vbz vs nns~~~Perhaps more imlxmantly, elimination of word-level lexical ambiguity allows the parser to make projection about the input which is yet to be parsed, using a simple lookabead; in particular, phrase boundaries can be determined with a degree of confidence <TREF>Church, 1988</TREF>~~~This latter property is critical for implementing skip-and-fit recovery technique outlined in the previous section~~~Tagging of input also helps to reduce the number of parse structures that can be assigned to a sentence, decreases the demand for consulting of the dictionary, and simplifies dealing with unknown words.
The typical examples are the recognition of BaseNP in English and Chinese~~~In English BNP base noun phrase is defined as simple and non-nesting noun phrases, ie noun phrases that do not contain other noun phrase descendants <TREF>Church, 1988</TREF>~~~After that researches on BNP identification reports promising results for such task in English~~~Observing that the Chinese BNP is different form English, <REF>Zhao  Huang, 1999</REF> puts forward the definition of Chinese BNP in terms of combination of determinative modifier and head noun.
More sophisticated linguistic information comes in several forms, all of which may need to be represented if performance in an automatic categorisation experiment is to be improved~~~Typical examples of linguistically sophisticated annotation include tagging words with their syntactic category although this has not been found to be effective for 1R, lemma of the word eg corpus for corpora, phrasal information eg identifying noun groups and phrases <REF>Lewis 1992c</REF>, <TREF>Church 1988</TREF>, and subject-predicate identification eg <REF>Hindle 1990</REF>~~~For the RAPRA corpus, we currently identify noun groups and adjective groups~~~This is achieved in a manner similar to Churchs 1988 PARTS algorithm used by Lewis 1992bc, in the sense that its main properties are robustness and corpus sensitivity.
For the RAPRA corpus, we currently identify noun groups and adjective groups~~~This is achieved in a manner similar to Churchs 1988 PARTS algorithm used by Lewis 1992bc, in the sense that its main properties are robustness and corpus sensitivity~~~All that is important for this paper is that the technique identifies various groupings of words for example, noun-groups, adjective groups, and so on with a high level of accuracy~~~Major parts of the technique are described in detail in <REF>Finch, 1993</REF>.
problem~~~Excellent methods have been developed for part-of-speech POS tagging using stochastic models trained on partially tagged corpora <TREF>Church, 1988</TREF>; Cutting, <REF>Kupiec, Pedersen  Sibun, 1992</REF>~~~Semantic issues have been addressed, particularly for sense disambiguation, by using large contexts, eg, 50 nearby words <REF>Gale, Church  Yarowsky, 1992</REF> or by reference to on-line dictionaries <REF>Krovetz, 1991</REF>; <REF>Lesk, 1986</REF>; <REF>Liddy  Paik, 1992</REF>; <REF>Zernik, 1991</REF>~~~More recently, methods to work with entirely untagged corpora have been developed which show great promise <REF>Brill  Marcus, 1992</REF>; <REF>Finch  Chater, 1992</REF>; <REF>Myaeng  Li, 1992</REF>; <REF>Schutze, 1992</REF>.
Although methods for unsupervised training of HMMs do exist, training is usually done in a supervised way by estimation of the above probabilities from relative frequencies in the training data~~~The HMM approach to tagging is by far the most studied and applied <TREF>Church 1988</TREF>; <REF>DeRose 1988</REF>; <REF>Charniak 1993</REF>~~~In van <REF>Halteren, Zavrel, and Daelemans 1998</REF> we used a straightforward implementation of HMMs, which turned out to have the worst accuracy of the four competing methods~~~In the present work, we have replaced this by the TnT system we will refer to this tagger as HMM below.
Two-Word Descriptions Three-Word Descriptions Stage Entities Unique Entities Entities Unique Entities POS tagging only 9,079 1,546 2,617 604 After WordNet checkup 1,509 395 81 26  Extraction of candidates for proper nouns~~~After tagging the corpus using the POS part-of-speech tagger <TREF>Church 1988</TREF>, we used a CREP <REF>Duford 1993</REF> regular grammar to first extract all possible candidates for entities~~~These consist of all sequences of words that were tagged as proper nouns NP by POS~~~Our manual analysis showed that out of a total of 2150 entities recovered in this way, 1139 529 are not names of entities.
A trained system would probably be more accurate in classifying new verbs~~~Finally, the lexical ambiguity problem could probably be reduced substantially in the applied context by using a statistical tagging program <REF>Brill 1992</REF>; <TREF>Church 1988</TREF>~~~For addressing basic questions in machine learning of natural language the solutions outlined above are not attractive~~~All of those solutions provide the learner with additional specific knowledge of English, whereas the goal for the machine learning effort should be to replace specific knowledge with general knowledge about the types of regularities to be found in natural language.
to appear, <REF>Hearst 1991</REF>, <REF>Lesk 1986</REF>, <REF>Smadja and McKeown 1990</REF>, <REF>Walker 1987</REF>, <REF>Veronis and Ide 1990</REF>, <REF>Yarowsky 1992</REF>, Zemik 1990, 1991~~~Much of this work offers the prospect that a disambiguation system might be able to input unrestricted text and tag each word with the most likely sense with fairly reasonable accuracy and efficiency, just as part of speech taggers eg , <TREF>Church 1988</TREF> can now input unrestricted text and assign each word with the most likely part of speech with fairly reasonable accuracy and efficiency~~~The availability of massive lexicographic databases offers a promising route to overcoming the knowledge acquisition bottleneck~~~More than thirty years ago, BarI-<REF>Iillel 1960</REF> predicted that it would be futile to write expert-system-like rules by-hand as they had been doing at Georgetown at the time because there would be no way to scale up such rules to cope with unrestricted input.
<REF>Choueka and Lusignan 1985</REF> presented a system for the morphological tagging of large texts that is based on the short context of the word but also depends heavily on human interaction~~~Methods using the short context of a word in order to resolve ambiguity usually categorical ambiguity are very common in English and other languages <REF>DeRose 1988</REF>; <TREF>Church 1988</TREF>; <REF>Karlsson 1990</REF>~~~A system using this approach was developed by Levinger and Ornan in order to serve as a component in their project of morphological disambiguation in Hebrew <REF>Levinger 1992</REF>~~~The main resource, used by this system for disambiguation, is a set of syntactic constraints that were defined manually by the authors and followed two theoretical works that defined short context rules for Hebrew <REF>Pines 1975</REF>; <REF>Albeck 1992</REF>.
Another application which is more difficult in Hebrew than in other languages is text-to-speech systems, which cannot be implemented in Hebrew without first solving the morphological ambiguity, since in many cases different analyses of a word imply different pronunciations~~~A much simpler problem occurs in English, where for some words the correct syntactic tag is necessary for pronunciation <TREF>Church 1988</TREF>~~~The notion that this ambiguity problem in Hebrew is very complicated and that it can be dealt with only by using vast syntactic and semantic knowledge has led researchers to look for solutions involving a considerable amount of human interaction~~~<REF>Ornan 1986</REF> for instance, developed a new writing system for Hebrew, called The Phonemic Script.
Estimating the Lexical Priors for Rare Forms For a common form such as lopen walk a reasonable estimate of the lexical prior probabilities is the MLE, computed over all occurrences of this form~~~So, in the UdB corpus, lopen occurs 92 times as an infinitive and 43 times as a finite plural, so the MLE 1 Even models of disambiguation that make use of context, such as statistical n-gram taggers, often presume some estimate of lexical priors, in addition to requiring estimates of the transition probabilities of sequences of lexical tags <TREF>Church 1988</TREF>; <REF>DeRose 1988</REF>; <REF>Kupiec 1992</REF>, and this again brings up the question of what to do about unseen or low-frequency forms~~~In working taggers, a common approach is simply to apply a uniform small probability to the various senses of unseen or low-frequency forms: this was done in the tagger discussed in <TREF>Church 1988</TREF>, for example~~~156 Baayen and Sproat Lexical Priors for Low-Frequency Forms to> 8 Figure 1 I I I I 0 2 4 6 log frequency class Relative frequency of Dutch infinitives versus finite plurals in the Uit den Boogaart corpus, as a function of the natural log of the frequency of the word forms.
So, in the UdB corpus, lopen occurs 92 times as an infinitive and 43 times as a finite plural, so the MLE 1 Even models of disambiguation that make use of context, such as statistical n-gram taggers, often presume some estimate of lexical priors, in addition to requiring estimates of the transition probabilities of sequences of lexical tags <TREF>Church 1988</TREF>; <REF>DeRose 1988</REF>; <REF>Kupiec 1992</REF>, and this again brings up the question of what to do about unseen or low-frequency forms~~~In working taggers, a common approach is simply to apply a uniform small probability to the various senses of unseen or low-frequency forms: this was done in the tagger discussed in <TREF>Church 1988</TREF>, for example~~~156 Baayen and Sproat Lexical Priors for Low-Frequency Forms to> 8 Figure 1 I I I I 0 2 4 6 log frequency class Relative frequency of Dutch infinitives versus finite plurals in the Uit den Boogaart corpus, as a function of the natural log of the frequency of the word forms~~~The horizontal solid line represents the overall MLE, the relative frequency of the infinitive as computed over all tokens; the horizontal dashed line represents the relative frequency of the infinitive among the hapax legomena.
Statistical anMyses of linguistic data were very popular in the 50s and 60s, mainly, even though not only, for literary types of analyses and for studies on the lexicon <REF>Guiraud 1959</REF>, <REF>Muller 1964</REF>, <REF>Moskovich 1977</REF>~~~Stochastic approaches to linguistic analyses have been strongly reevaluated in the past few years, either for syntactic analysis Gmside et al 1987, <TREF>Church 1988</TREF>, or for NLP applications <REF>Brown et al 1988</REF>, or for semantic analysis <REF>Zemik 1989</REF>, <REF>Smadja 1989</REF>~~~Quantitative not statistical evidence on eg word-sense occurrences in a large corpus have been taken into account for lexicographic descriptions Cobuild 17~~~I llere and in the following we have not translated idiomatic phrases and compounds, because there is no point in giving the literal translation of the single words.
Furthermore, we might expect that some words, such as prepositions and determiners, for example, do not constitute the typical end to an intonational phrase~~~We test these possibilities by examining part-of-speech in a window of four words surrounding each potential phrase break, using Churchs part-of-speech tagger 1988~~~Recall that each intermediate phrase is composed of one or more pitch accents plus a phrase accent, and each intonational phrase is composed of one or more intermediate phrases plus a boundary tone~~~Informal observation suggests that phrase boundaries are more likely to occur in some accent contexts than in others.
Discussion The application of CART techniques to the problem of predicting and detecting phrasing boundaries not only provides a classification procedure for predicting intonational boundaries from text, but it increases our understanding of the importance of several among the numerous variables which might plausibly be related to boundary location~~~In future, we plan to extend the set of variables for analysis to include counts of stressed syllables, automatic NP-detection <TREF>Church, 1988</TREF>, MUTUAL INFORMATION, GENERALIZED MUTUAL INFORMATION scores can serve as indicators of intonational phrase boundaries <REF>Magerman and Marcus, 1990</REF>~~~We will also examine possible interactions among the statistically important variables which have emerged from our initial study~~~CART techniques have worked extremely well at classifying phrase boundaries and indicating which of a set of potential variables appear most important.
The main atvantage of the linguistic approach is that the model is constructed from a linguistic Ioint of view and contains many and complex kinds of knowledge iI1 tim lemning approach, tile most extended tbrmalism is based on n-grains or IIMM~~~In tiffs case, the language inodel can be estimated from a labelled corpus supervised methods <TREF>Church, 1988</TREF>Weisehedel et al , 1993 or from a nonlabelled corpus unsupervised methods Cutting et 21~~~, 1992~~~In the first; case, the model is trained from the relative observed Dequencies.
122 LKarning Techniques These allnoachcs automatically :onstruel; a language model from a labellod alld brackKted corpus~~~The lirst probabilistic approach was proposed in <TREF>Church, 1988</TREF>~~~This method learn; a bigram model for detecting simph3 noun phrasKs on the Brown corpus~~~Civn a sequene of parts of st3eeh as inlug, the Church program inserts the most prolable openings and Kndings of NPs, using a Viterbiqiko.
It shows the descriptive power of low-level morphology-based constraints~~~The most successful achievements so far in the domain of large-scale morphological disambiguation of running text have been those for English reported by <REF>Garside, Leech, and Sampson 1987</REF>, on tagging the LOB corpus, and <TREF>Church 1988</TREF>, on assigning part-of-speech labels and parsing noun phrases~~~Success rates ranging between 95-99 are reported, depending on how success is defined~~~These approaches are probabilistic and based on transitional probabilities calculated from extensive pretagged corpora.
The methods we investi1This gives the Viterbi model <REF>Merialdo, 1994</REF>, which we use here~~~2This version of the method uses Bayes theorem  <TREF>Church, 1988</TREF>~~~Pwdt, o Pt, J gate approach this evaluation implicitly, measuring an examples informativeness as the uncertainty in its classification given the current training data <REF>Seung, Opper, and Sompolinsky, 1992</REF>; <REF>Lewis and Gale, 1994</REF>; <REF>MacKay, 1992</REF>~~~The reasoning is that if an examples classification is uncertain given current training data then the example is likely to contain unknown information useful for classifying similar examples in the future.
Our work focuses on sample selection for training probabilistic classifiers~~~In statistical NLP, probabilistic classifiers are often used to select a preferred analysis of the linguistic structure of a text for example, its syntactic structure <REF>Black et al , 1993</REF>, word categories <TREF>Church, 1988</TREF>, or word senses <REF>Gale, Church, and Yarowsky, 1993</REF>~~~As a representative task for probabilistic classification in NLP, we experiment in this paper with sample selection for the popular and well-understood method of stochastic part-of-speech tagging using Hidden Markov Models~~~We first review the basic approach of committeebased sample selection and its application to partof-speech tagging.
Additionally, there is a slight but not significant improvement of tagging accuracy~~~Statistical part-of-speech disambiguation can be efficiently done with n-gram models <TREF>Church, 1988</TREF>; <REF>Cutting et al , 1992</REF>~~~These models are equivalent to Hidden Markov Models HMMs <REF>Rabiner, 1989</REF> of order n 1~~~The states represent parts of speech categories, tags, there is exactly one state for each category, and each state outputs words of a particular category.
A specialised version of the chunking task is NP CHUNKING or baseNP identification in which the goal is to identify the base noun phrases~~~The first work on this topic was done back in the eighties <TREF>Church, 1988</TREF>~~~The data set that has become standard for evaluation machine learning approaches is the one first used by <REF>Ramshaw and Marcus 1995</REF>~~~It consists of the same training and test data segments of the Penn Treebank as the chunking task respectively sections 15-18 and section 20.
We report in this paper on one application of probabilistic models to language processing, the assignment of part of speech to words in open text~~~The effectiveness of such models is well known <TREF>Church 1988</TREF> and they are currently in use in parsers eg de <REF>Marcken 1990</REF>~~~Our work is an incremental improvement on these models in two ways: 1 We have run experiments regarding the amount of training data needed in moving to a new domain; 2 we have added probabilistic models of word features to handle unknown words effectively~~~We describe POST and its algorithms and then we describe our extensions, showing the results of our experiments.
Using the Viterbi algorithm, we selected the path whose overall probability was highest, and then took the tag predictions from that path~~~We replicated the result <TREF>Church 1988</TREF> that this process is able to predict the parts of speech with only a 3-4 error rate when the possible parts of speech of each the words in the corpus are known~~~This is in fact about the rate of discrepancies among human taggers on the TREEBANK project <REF>Marcus, Santorini  Magerman 1990</REF>~~~22 Quantity of training data While supervised training is shown here to be very effective, it requires a correctly taed corpus.
They mainly differ in the emphasis they give to syntactic and statistical control of the induction process~~~In Church,1988 a well-know purely statistical method for POS tagging is applied to the derivation of simple noun phrases that are relevant in the underlying corpus~~~On the contrary more language oriented methods are those where specialized grammar are used~~~LEXTER Bourigault,1992 extracts maximal length noun phrases mlnp from a corpus, and then applies a special purpose noun phrase parsing to hem in order to focus on significant complex nominals.
There are a number of large tagged corpora available, allowing for a variety of experiments to be run~~~Part-of-speech tagging is an active area of research; a great deal of work has been done in this area over the past few years eg , <REF>Jelinek 1985</REF>; <TREF>Church 1988</TREF>; <REF>Derose 1988</REF>; <REF>Hindle 1989</REF>; <REF>DeMarcken 1990</REF>; <REF>Merialdo 1994</REF>; <REF>Brill 1992</REF>; <REF>Black et al 1992</REF>; <REF>Cutting et al 1992</REF>; <REF>Kupiec 1992</REF>; <REF>Charniak et al 1993</REF>; <REF>Weischedel et al 1993</REF>; <REF>Schutze and Singer 1994</REF>~~~Part-of-speech tagging is also a very practical application, with uses in many areas, including speech recognition and generation, machine translation, parsing, information retrieval and lexicography~~~Insofar as tagging can be seen as a prototypical problem in lexical ambiguity, advances in part-of-speech tagging could readily translate to progress in other areas of lexical, and perhaps structural, ambiguity, such as wordsense disambiguation and prepositional phrase attachment disambiguation.
It has recently become clear that automatically extracting linguistic information from a sample text corpus can be an extremely powerful method of overcoming the linguistic knowledge acquisition bottleneck inhibiting the creation of robust and accurate natural language processing systems~~~A number of part-of-speech taggers are readily available and widely used, all trained and retrainable on text corpora <TREF>Church 1988</TREF>; <REF>Cutting et al 1992</REF>; <REF>Brill 1992</REF>; <REF>Weischedel et al 1993</REF>~~~Endemic structural ambiguity, which can lead to such difficulties as trying to cope with the many thousands of possible parses that a grammar can assign to a sentence, can be greatly reduced by adding empirically derived probabilities to grammar rules <REF>Fujisaki et al 1989</REF>; <REF>Sharman, Jelinek, and Mercer 1990</REF>; <REF>Black et al 1993</REF> and by computing statistical measures of lexical association <REF>Hindle and Rooth 1993</REF>~~~Word-sense disambiguation, a problem that once seemed out of reach for systems without a great deal of handcrafted linguistic and world knowledge, can now in some cases be done with high accuracy when all information is derived automatically from corpora <REF>Brown, Lai, and Mercer 1991</REF>; <REF>Yarowsky 1992</REF>; Gale, Church, and <REF>Yarowsky 1992</REF>; <REF>Bruce and Wiebe 1994</REF>.
However, stochastic taggers have the disadvantage that linguistic information is captured only indirectly, in large tables of statistics~~~Almost all recent work in developing automatically trained part-of-speech taggers has been on further exploring Markovmodel based tagging <REF>Jelinek 1985</REF>; <TREF>Church 1988</TREF>; <REF>Derose 1988</REF>; <REF>DeMarcken 1990</REF>; <REF>Merialdo 1994</REF>; <REF>Cutting et al 1992</REF>; <REF>Kupiec 1992</REF>; <REF>Charniak et al 1993</REF>; <REF>Weischedel et al 1993</REF>; <REF>Schutze and Singer 1994</REF>~~~41 Transformation-based Error-driven Part-of-Speech Tagging Transformation-based part of speech tagging works as follows~~~9 The initial-state annotator assigns each word its most likely tag as indicated in the training corpus.
Extraction of candidates for proper nouns~~~After tagging the corpus using the POS part-of-speech tagger <TREF>Church, 1988</TREF>, we used a CREP <REF>Duford, 1993</REF> regular grammar to first extract all possible candidates for entities~~~These consist of all sequences of words that were tagged as proper nouns NP by POS~~~Our manual analysis showed that out of a total of 2150 entities recovered in this way, 1139 529 are not names of entities.
In the past decade, the speech recognition community has had huge successes in applying hidden Markov models, or HMMs to their problems~~~More recently, the natural language processing community has effectively employed these models for part-ofspeech tagging, as in the seminal <TREF>Church, 1988</TREF> and other, more recent efforts <REF>Weischedel et al , 1993</REF>~~~We would now propose that HMMs have successfully been applied to the problem of name-finding~~~We have built a named-entity NE recognition system using a slightly-modified version of an HMM; we call our system Nymble.
For example, there is less than a 005 chance that the difference between stative and event means for the first four indicators listed 2This test was suggested by Judith Klavans personal communication~~~3Similar baselines for comparison have been used for many classification problems <REF>Duda and Hart, 1973</REF>, eg, part-of-speech tagging <TREF>Church, 1988</TREF>; <REF>Allen, 1995</REF>~~~159 is due to chance~~~Overall, this shows that the differences in stative and event averages are statistically significant for the first seven indicators listed p < 01.
Both types of ambiguity, syntactic and lexical, may cause the system to acquire or use inappropriate patterns~~~This problems is consid ered very important when dealing with a corpus: it was the re,Leon for the substantial human intervention in the procedure of <REF>Grishman et al 1986</REF>, and it is the reason why other techniques use manually tagged corpora eg <TREF>Church 1988</TREF>~~~In practice, however, we have discovered that the problem is not so cruciah semantically vMid patterns have occurred many more times in syntactically unambiguous constructs than in mnbiguous ones~~~Thus, they could be identified without the need of first disambiguating the sentences.
6 PREPROCESSING WITH A PART OF SPEECH TAGGER Phrasal verbs involving the preposition to raise an interesting problem because of the possible confusion with the infinitive marker to~~~We have found that if we first tag every word in the corpus with a part of speech using a method such as <TREF>Church 1988</TREF>, and then measure associations between tagged words, we can identify interesting contrasts between verbs associated with a following preposition toin and verbs associated with a following infinitive marker toto~~~Part of speech notation is borrowed from <REF>Francis and Kucera 1982</REF>; in  preposition; to  infinitive marker; vb  bare verb; vbg  verb  ing; vbd  verb  ed; vbz  verb  s; vbn  verb  en~~~The association ratio identifies quite a number of verbs associated in an interesting way with to; restricting our attention to pairs with a score of 30 or more, there are 768 verbs associated with the preposition toin and 551 verbs with the infinitive marker to/to.
The computational tools available for studying machinereadable corpora are at present still rather primitive~~~These are concordancing programs see Figure 1, which are basically KWIC key word in context; <REF>Aho et al 1988</REF> indexes with additional features such as the ability to extend the context, sort leftward as well as rightward, and so on~~~There is very little interactive software~~~In a typical situation in the lexicography of the 1980s, a lexicographer is giwen the concordances for a word, marks up the printout with colored pens to identify the salient senses, and then writes syntactic descriptions and definitions.
/3 2~~~precision  recall 1 HMM-based Chunk Tagger The idea of using statistics for chunking goes back to <TREF>Church1988</TREF>, who used corpus frequencies to determine the boundaries of simple non-recursive noun phrases~~~Skut and <REF>Brants1998</REF> modified Churchs approach in a way permitting efficient and reliable recognition of structures of limited depth and encoded the structure in such a way that it can be recognised by a Viterbi tagger~~~This makes the process run in time linear to the length of the input string.
Input Text Tokenization Part-of-speech Lookup Descriptor array construction Classification by learning algorithm Text withsentence boundaries disambiguated 31 Tokenization The first stage of the process is lexical analysis, which breaks the input text a stream of characters into tokens~~~The Satz tokenizer is implemented using the UNIX tool LEX <REF>Lesk and Schmidt 1975</REF> and is modeled on the tokenizer used by the PARTS part-of-speech tagger <TREF>Church 1988</TREF>~~~The tokens returned by the LEX program can be a sequence of alphabetic characters, a sequence of digits, 8 or a sequence of one or more non-alphanumeric characters such as periods or quotation marks~~~32 Part-of-Speech Lookup The individual tokens are next assigned a series of possible parts of speech, based on a lexicon and simple heuristics described below.
The lexicon and thus the frequency counts used to calculate the descriptor arrays were derived from the Brown corpus <REF>Francis and Kucera 1982</REF>~~~In initial experiments we used the extensive lexicon from the PARTS part-of-speech tagger <TREF>Church 1988</TREF>, which contains 30,000 words~~~We later experimented with a much smaller lexicon, and these results are discussed in Section 44~~~In Sections 41-49 we describe the results of our experiments with the Satz system using the neural network as the learning algorithm.
We regard our use of probabilities as being consistent with Bauers claim that accounting for semi-productivity is an issue of performance, not competence <REF>Bauer 1983</REF>:71f~~~The frequency with which a given word form is associated with a particular lexical entry ie sense or grammatical realization is often highly skewed; <TREF>Church 1988</TREF> points out that a model of part-of-speech assignment in context will be 90 accurate for English if it simply chooses the lexically most frequent part-of-speech for a given word~~~<REF>Briscoe and Carroll 1995</REF> found in one corpus that there were about 18 times as many instances of believe in the most common subcategorizati0n class as in the 4 least common classes combined~~~In the absence of other factors, it seems very likely that language users utilize frequency information to resolve indeterminacies in both generation and interpretation.
12 Survey of Related Work Chunking has been studied for English and other languages, though not very extensively~~~The earliest work on chunking based on machine learning goes to Church K, 1988 for English~~~<REF>Ramshaw and Marcus, 1995</REF> used transformation based learning using a large annotated corpus for English~~~<REF>Skut and Brants, 1998</REF> modi ed Churchs approach, and used standard HMM based tagging methods to model the chunking process.
27 3 Empirical Comparison We evaluated the similarity functions introduced in the previous section on a binary decision task, using the same experimental framework as in our previous preliminary comparison <REF>Dagan et al , 1999</REF>~~~That is, the data consisted of the verb-object cooccurrence pairs in the 1988 Associated Press newswire involving the 1000 most frequent nouns, extracted via Churchs 1988 and Yarowskys processing tools~~~587,833 80 of the pairs served as a training set from which to calculate base probabilities~~~From the other 20, we prepared test sets as follows: after discarding pairs occurring in the training data after all, the point of similarity-based estimation is to deal with unseen pairs, we split the remaining pairs into five partitions, and replaced each nounverb pair n, vl with a noun-verb-verb triple n, vl, v2 such that Pv2  Pvl.
In this paper, we propose a new space and a new metric for computing this distance~~~Being inspired by the success of noisy-channel-based approaches in applications as diverse as speech recognition <REF>Jelinek, 1997</REF>, part of speech tagging <TREF>Church, 1988</TREF>, machine translation <REF>Brown et al , 1993</REF>, information retrieval <REF>Berger and Lafferty, 1999</REF>, and text summarization <REF>Knight and Marcu, 2002</REF>, we develop a noisy channel model for QA~~~This model explains how a given sentence S A that contains an answer sub-string A to a question Q can be rewritten into Q through a sequence of stochastic operations~~~Given a corpus of questionanswer pairs Q, S A , we can train a probabilistic model for estimating the conditional probability PQ  S A .
7 0 7 3 Word Segmentation Algorithm 31 Statistical Language Model For the language model in Equation 1, we used the part of speech trigram nlodel POS trigranl or 2nd-order HMM~~~It is used,as tagging mode in English <TREF>Church, 1988</TREF>; <REF>Cutting et al , 1992</REF> and morphological analysis nlodel word segmentation and tagging in Japanese <REF>Nagata, 1994</REF>~~~Let the input character sequence be /  ccec  We approxinlate PCby PW, 7, the joint prol>ability of word sequence W  wlw2u, and part of speech sequence   tlte, t,,~~~PW,T is then approximated t>y the product of parts of speech trigram probabilities Ptiti-2, i-l and word output probabilities for given part of speech Pwiltl, 71 pc pw, -- IX pt, lt,-,t,-,p,lt, 5 i1 Ptilti-,e,ti- and /-wlti  are estimated >y computing the relative frequencies of the corresponding events in training corpus a 32 Forward-DP Backward-A Algorithm /sing the language model 5, Japanese morplological analysis can be detined,as finding tile set of word segmentation and parts of speech 1/, 7 that maximizes the joint probability of word sequence and tag sequence PW, 7.
In this seelion, we will outline the three lexicalist, linguistically perspicuous, qualitatiwly different models that we have leveloped a, nd tested~~~21 Model A: Bigram lexieal affinities N-gram tatters like <TREF>Church, 1988</TREF>; lelinek 1985; <REF>Kupiec 1992</REF>; <REF>Merialdo 1990</REF> take the following view of row /, tagged sentctrce enters the worhl~~~Iirst, a setuenee of tags is gnexated aecordittg to a Markov lrocess, with th random choice of ech tag conditioned ou the previous two tags~~~Second, a word is choseu conditional on each tag.
Nevertheless, if a large POS set is specified, the number of rules increases significantly and rule definition becomes highly costly and cumbersome~~~Stochastic taggers use both contextual and morphological information, and the model parameters are usually defined or updated automatically from tagged texts Cerf-Danon and E1-<REF>Beze 1991</REF>; <TREF>Church 1988</TREF>; <REF>Cutting et al 1992</REF>; <REF>Dermatas and Kokkinakis 1988, 1990, 1993, 1994</REF>; <REF>Garside, Leech, and Sampson 1987</REF>; <REF>Kupiec 1992</REF>; Maltese  Department of Electrical Engineering, Wire Communications Laboratory WCL, University of Patras, 265 00 Patras, Greece~~~E-mail: dermataswcleeupatrasgr~~~ 1995 Association for Computational Linguistics Computational Linguistics Volume 21, Number 2 and <REF>Mancini 1991</REF>; <REF>Meteer, Schwartz, and Weischedel 1991</REF>; <REF>Merialdo 1991</REF>; <REF>Pelillo, Moro, and Refice 1992</REF>; <REF>Weischedel et al 1993</REF>; <REF>Wothke et al 1993</REF>.
2 Relation to Previous Works Quite a few works have dealt with extending a given POS tagger, mainly by smoothing it using extra-information about untreated words~~~For example, <TREF>Church, 1988</TREF> uses the simple heuristic of predicting proper nouns from capitalization~~~This method is not applicable to Arabic and Hebrew, which lack typographical marking of proper nouns~~~More advanced methods like those described by Weischedel et al.
In particular, it would be interesting to see if the accuracy ranking of the seven algorithms is affected by a change in the representation~~~Similar comparisons of a range of algorithms should also be performed on other natural language problems such as part-of-speech tagging <TREF>Church, 1988</TREF>, prepositional phrase attachment <REF>Hindle  Rooth, 1993</REF>, anaphora resolution Anoe  <REF>Bennett, 1995</REF>, etc Since the requirements of individual tasks vary, different algorithms may be suitable for different sub-problems in natural language processing~~~87 o 0 : 0 0  / 350 300 250 200 150 1 O0 50 I I I I I ~~~Naive Bales o J 3 Nearest Neighbor --0-- Perceptron -m- C45 ---x   PFOIL-DNF ---1 PFOIL-CNF ---1 PFOIL-DLIST --- I  G    :::::-;    ii212  x2C:222Z/2: 200 400 600 800 1000 1200 Training Examples Figure 3: Testing Time for Line Corpus Conclusions This paper has presented fairly comprehensive experiments comparing seven quite different empirical methods on learning to disambiguate words in context.
As far as coreference resolution is concerned, the goal of these NLP modules is to determine the boundary of the markables, and to provide the necessary information about each markable for subsequent generation of features in the training examples~~~Our part-of-speech tagger is a standard sta285 tistical bigram tagger based on the Hidden Markov Model HMM <TREF>Church, 1988</TREF>~~~Similarly, we built a statistical HMM-based noun phrase identification module where the noun phrase boundaries are determined solely based on the part-of-speech tags assigned to the words in a sentence~~~We also implemented a module that recognizes MUC-style named entities, ie, organization, person, location, date, time, money, and percent.
1 is a typical example of the ambiguities encountered in a running text: little POS ambiguity, but a lot of gender, number and case ambiguity columns 3 to 5~~~485 3 The Model Instead of employing the source-channel paradigm for tagging more or less explicitly present eg in <REF>Merialdo, 1992</REF>, <TREF>Church, 1988</TREF>, <REF>HajiS, HladkA, 1997</REF> used in the past notwithstanding some exceptions, such as Maximum Entropy and rule-based taggers, we are using here a direct approach to modeling, for which we have chosen an exponential probabilistic model~~~Such model when predicting an event  y E Y in a context x has the general form PAC,eYl x  expEiI ifiy,x Zx 3 where fiY, x is the set of size n of binary-valued yes/no features of the event value being predicted and its context, Ai is a weigth in the exponential sense of the feature fi, and the normalization factor Zx is defined naturally as n Zx   exp Aifiy,x 4 yEY i:1 We use a separate model for each ambiguity class AC which actually appeared in the training data of each of the 13 morphological categories 6~~~The final PAC YlX distribution is further smoothed using unigram distributions on subtags again, separately for each category.
For a larger dataset, such as the Canadian Hansards, it was not possible to check the results by hand~~~We used the same procedure that is used in <TREF>Church 1988</TREF>~~~This procedure was developed by Kathryn Baker unpublished~~~79 Computational Linguistics Volume 19, Number 1 fefQ t e E 0 0 0 0 P, o Ol I 0 .
The accuracy of this data has an impact on the tagging accuracy of both the HMM itself and the derived transducer~~~The training of the HMM can be done on either a tagged or untagged corpus, and is not a topic of this paper since it is exhaustively described in the literature <REF>Bahl and Mercer, 1976</REF>; <TREF>Church, 1988</TREF>~~~An HMM can be identically represented by a weighted FST in a straightforward way~~~We are, however, interested in non-weighted transducers.
The first feature represents the part of speech of the word~~~Vve use an in-house statistical tagger based on <TREF>Church, 1988</TREF> to tag the text in which the unknown word occurs~~~The tag set used is a simplified version of the tags used in the machinereadable version of the Oxford Advanced Learners Dictionary OALD~~~The tag set contains just one tag to identify nouns.
The first major use of HMMs for part of speech tagging was in CLAWS <REF>Garside et al , 1987</REF> in the 1970s~~~With the availability of large corpora and fast computers, there has been a recent resurgence of interest, and a number of variations on and alter53 natives to the FB, Viterbi and BW algorithms have been tried; see the work of, for example, Church <TREF>Church, 1988</TREF>, Brill <REF>Brill and Marcus, 1992</REF>; <REF>Brill, 1992</REF>, DeRose <REF>DeRose, 1988</REF> and gupiec <REF>Kupiec, 1992</REF>~~~One of the most effective taggers based on a pure HMM is that developed at Xerox <REF>Cutting et al , 1992</REF>~~~An important aspect of this tagger is that it will give good accuracy with a minimal amount of manually tagged training data.
Specifically speaking, the content chunking contains two subtasks: 1 to recognize the maximum phrase in a sequence of content words; 2 to analyze the hierarchical structure within the phrase down to words~~~Like baseNP chunking<TREF>Church, 1988</TREF>; <REF>Ramshaw  Marcus 1995</REF>, content chunk parsing is also a kind of shallow parsing~~~Content chunk parsing is deeper than baseNP chunking in two aspects: 1 a content chunk may contain verb phrases and other phrases even a full sentence as long as the all the components are content words; 2 it may contain recursive NPs~~~Thus the content chunk can supply more structural information than a baseNP.
From the perspective of using a lexicalized grammar developed for parsing and importing parsing techniques, our method is similar to the following approaches~~~The Fergus system <REF>Bangalore and Rambow, 2000</REF> uses LTAG Lexicalized Tree Adjoining Grammar <TREF>Schabes et al , 1988</TREF> for generating a word lattice containing realizations and selects the best one using a trigram model~~~<REF>White and Baldridge 2003</REF> developed a chart generator for CCG Combinatory Categorial Grammar <REF>Steedman, 2000</REF> and proposed several techniques for efficient generation such as best-first search, beam thresholding and chunking the input logical forms <REF>White, 2004</REF>~~~Although some of the techniques look effective, the models to rank candidates are still limited to simple language models.
Work on the use of synchronous TAGs to capture quantifier scoping possibilities makes use of so-called multicomponent TAGs~~~Finally, the base TAGs may be lexicalized <TREF>Schabes et al , 1988</TREF> or not~~~Once the base formalism has been decided upon we currently are using lexicalized multi-component TAGs with substitution and adjunction, a simple translation strategy from a source string to a target is to parse the string using an appropriate TAG parser for the base formalism~~~Each derivation of the source string can be mapped according to the synchronizing links in the grammar to a target derivation.
Also, the provision of conceptual entities which are incrementally generated by the semantic interpretation process supplies the necessary anchoring points for the continuous resolution of textual anaphora and ellipses <REF>Strube  Hahn, 1995</REF>; <REF>Hahn et al , 1996</REF>~~~The lexical distribution of grammatical knowledge one finds in many lexiealized grammar formalisms eg , LTAGS <TREF>Schabes et al , 1988</TREF> or HPSG <REF>Pollard  Sag, 1994</REF> is still constrained to declarative notions~~~Given that the control flow of text understanding is globally unpredictable and, also, needs to be purposefully adapted to critical states of the analysis eg , cases of severe extragrammaticality, we drive lexicalization to its limits in that we also incorporate procedural control knowledge at the lexical gr,unmar level~~~The specification of lexiealized communication primitives allows heterogeneous and local lorms of interaction among groups of lexical items.
This will allow for easy maintenance and facilitate updates to the grammar~~~1 Motivations Lexicalized tree-adjoining grammar LTAG <TREF>Schabes et al , 1988</TREF>; <REF>Schabes, 1990</REF> is a tree-rewriting formalism used for specifying the syntax of natural languages~~~It combines elementary lexical trees with two operations, adjoining and substitution~~~In a LTAG, lexical itenm are associated with complex syntactic structures in the form of trees that define the various phrase structures they can participate in.
Formally, a derivation tree is represented as a set of dependencies: D   i,  j,r i , where  i is an elementary tree,   i represents a node in  j where substitution/adjunction has occurred, and r i is a label of the applied rule, ie, adjunction or substitution~~~A probability of derivation tree D   i,  j,r i  is generally defined as follows <TREF>Schabes et al , 1988</TREF>; <REF>Chiang, 2000</REF>~~~pD productdisplay i p i   j,r i  Note that each probability on the right represents the syntactic/semantic preference of a dependency of two lexical items~~~We can readily see that the model is very similar to LPCFG models.
That is, the models are still based on decomposition into primitive lexical dependencies~~~Derivation trees, the structural description in LTAG <TREF>Schabes et al , 1988</TREF>, represent the association of lexical items ie, elementary trees~~~In LTAG, all syntactic constraints of words are described in an elementary tree, and the dependencies of elementary trees, ie, a derivation tree, describe the semantic relations of words more directly than lexicalized parse trees~~~For example, Figure 3 has a derivation tree corresponding to the parse tree in Figure 1 2.
A more linguistically motivated approach is to expand the domain of productions downward to incorporate more tree structures~~~The Lexicalized Tree-Adjoining Grammar LTAG formalism <TREF>Schabes et al , 1988</TREF>, <REF>Schabes, 1990</REF>, although not context-free, is the most well-known instance in this category~~~PLTIGs belong to this third category and generate only context-free languages~~~LTAGs and LTIGs are tree-rewriting systems, consisting of a set of elementary trees combined by tree operations.
Thus CCG assigns the following two groupings to John likes apples: 2 John likes apples 3 John likes apples The work on CCG was presented by Mark Steedman in an earlier DARPA SLS Workshop <REF>Steedman, 1989</REF>~~~In this paper, we show how a CCG-like account for coordination can be constructed in the framework of lexicalized tree-adjoining grammars TAGs <REF>Joshi, 1987</REF>; <TREF>Schabes et al , 1988</TREF>; <REF>Schabes, 1990</REF>~~~2~~~In particular, we show how a fixed constituency can be maintained at the level of the elementary trees of lexicalized TAGs and yet be able to achieve the kind of flexibility needed for dealing with the so-called non-constituents.
which cannot be felicitously uttered except in a context where there is something in the discourse that a restriction could apply to~~~Conventional approaches to subcategorization, such as Definite Clause Grammar <REF>Pereira and Warren, 1980</REF>, Categorial Grammar <REF>Ades and Steedman, 1982</REF>, PATR-II <REF>Shieber, 1986</REF>, and lexicalized TAG <TREF>Schabes et al, 1988</TREF> all deal with complementation by including in one form or another a notion of subcategorization frame that specifies a sequence of complement phrases and constraints on them~~~Handling all the possible variations in complement distribution in such formalisms inevitably leads to an explosion in the number of such frames, and a correspondingly more difficult task in porting to a new domain~~~In our approach, on the other hand, it becomes possible to view subcategorization of a lexical item as a set of constraints on the outgoing arcs of its semantic graph node.
Recently there has been a gain in interest in the so-called mildly context-sensitive formalisms Vijay-<REF>Shanker, 1987</REF>; <REF>Weir, 1988</REF>; <REF>Joshi, VijayShanker, and Weir, 1991</REF>; Vijay-<REF>Shanker and Weir, 1993a</REF> that generate only a small superset of context-free languages~~~One such formalism is lexicalized tree-adjoining grammar LTAG Schabes, Abeill, and <REF>Joshi, 1988</REF>; <REF>Abeillfi et al , 1990</REF>; <REF>Joshi and Schabes, 1992</REF>, which provides a number of attractive properties at the cost of decreased efficiency, On6-time in the worst case <REF>VijayShanker, 1987</REF>; <REF>Schabes, 1991</REF>; <REF>Lang, 1990</REF>; <REF>VijayShanker and Weir, 1993b</REF>~~~An LTAG lexicon consists of a set of trees each of which contains one or more lexical items~~~These elementary trees can be viewed as the elementary clauses including their transformational variants in which the lexical items participate.
Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation~~~At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar LFG <REF>Bresnan, 1982</REF>, Lexicalized Tree Adjoining Grammar LTAG <TREF>Schabes et al , 1988</TREF>, Headdriven Phrase Structure Grammar HPSG <REF>Pollard and Sag, 1994</REF> and Combinatory Categorial Grammar CCG <REF>Steedman, 2000</REF>, which represent deep syntactic structures that cannot be expressed in a shallower formalism designed to represent only aspects of surface syntax, such as the dependency formalism used in current mainstream dependency parsing~~~We present a novel framework that combines strengths from surface syntactic parsing and deep syntactic parsing, specifically by combining dependency and HPSG parsing~~~We show that, by using surface dependencies to constrain the application of wide-coverage HPSG rules, we can benefit from a number of parsing techniques designed for high-accuracy dependency parsing, while actually performing deep syntactic analysis.
This paper will concentrate on context-free grammars CFG and their associated parsers~~~However, virtually all Tree Adjoining Grammars TAG, see eg, <TREF>Schabes et al , 1988</TREF> used in NLP applications can almost be seen as lexicalized Tree Insertion Grammars TIG, which can be converted into strongly equivalent CFGs <REF>Schabes and Waters, 1995</REF>~~~Hence, the parsing techniques and tools described here can be applied to most TAGs used for NLP, with, in the worst case, a light over-generation which can be easily and efficiently eliminated in a complementary pass~~~This is indeed what we have achieved with a TAG automatically extracted from Villemonte de <REF>La Clergerie, 2005</REF>s large-coverage factorized French TAG, as we will see in Section 4.
We can thus define lexical transfer rules that avoid the defects of a mere word-to-word approach but still benefit from the simplicity and elegance of a lexical approach~~~We rely on the French and English LTAG grammars Abeille 1988, Abeille 1990 b, Abeilld et al 1990, Abeill6 and Schabes 1989, 1990 that have been designed over the past two years jointly at University of Pennsylvania and University of Paris 7-Jussieu~~~1 Strategy for Machine Translation with LTAGs The idea of using grammars written with lexicalist formalisms for machine translation is not new This research was partially ftmded by ARO grant DAAG29-84-K-0061, DARPA grant N00014-85-K0018, and NSF grant MCS-82-19196 at the University of Pen nsylvania~~~We are indebted to Stuart Shieber for his valuable comments.
3 A TAG Analysis The TAG formalism for a recent introduction, see <REF>Joshi 1987a</REF> is well suited for linguistic description because 1 it provides a larger domain of locality than a CFG or other augmented CFG-based formalisms such as tlPSG or LFG, and 2 it allows factoring of recursion from the domain of dependencies~~~This extended domain of locality, provided by the elementary trees of TAG, allows us to lexicalize a TAG grammar: we can associate each tree in a grammar with a lexical item <TREF>Schabes et al 1988</TREF>, <REF>Schabes 1990</REF> 4~~~The tree will contain the lexical item, and all of its syntac3Some verbs allow scrambling out of their Complements more freely than others~~~It appears that all subject-control verbs and most object-control verbs governing the dative allow scrambling fairly fely, while scrambling with objectcontrol verbs governing the accusative is more restricted cir.
In Section 8 we conclude with some directions for future work~~~2 Lexicalized Tree-Adjoining Grammar Lexicalized Tree-Adjoining Grammar LTAG <TREF>Schabes et al , 1988</TREF>; <REF>Schabes, 1990</REF> consists of ELEMENTARY TREES, with each elementary tree having a lexical item anchor on its frontier~~~An elementary tree serves as a complex description of the anchor and provides a domain of locality over which the anchor can specify syntactic and semantic predicate-argument constraints~~~Elementary trees are of two kinds a INITIAL TREES and b AUXILIARY TREES.
178 6 Comparison to PSG Approaches One feature of word order domains is that they factor ordering alternatives from the syntactic tree, much like feature annotations do for morphological alternatives~~~Other lexicalized grammars collapse syntactic and ordering information and are forced to represent ordering alternatives by lexical ambiguity, most notable L-TAG <TREF>Schabes et al , 1988</TREF> and some versions of CG <REF>Hepple, 1994</REF>~~~This is not necessary in our approach, which drastically reduces the search space for parsing~~~This property is shared by the proposal of <REF>Reape 1993</REF> to associate HPSG signs with sequences of constituents, also called word order domains.
This high coverage allowed us to evaluate the parser in terms of the accuracy of dependency analysis on real-world texts, the evaluation measure that is previously used for more statistically-oriented parsers~~~2 HPSG Head-Driven Phrase Structure Grammar HPSG is classified into lexicalized grammars <TREF>Schabes et al , 1988</TREF>~~~It attempts to model linguistic phenomena by interactions between a small number of grammar rules and a large number of lexical entries~~~Figure 1 shows an example of an HPSG derivation of a Japanese sentence kare ga shinda, which means, He died In HPSG, linguistic entities such as words and phrases are represented by typed feature structures called signs, and the grammaticality of a sentence is verified by applying grammar rules to a sequence of signs.
We also investigate the reason for that difference~~~Various parsing techniques have been developed for lexicalized grammars such as Lexicalized Tree Adjoining Grammar LTAG <TREF>Schabes et al , 1988</TREF>, and Head-Driven Phrase Structure Grammar HPSG <REF>Pollard and Sag, 1994</REF>~~~Along with the independent development of parsing techniques for individual grammar formalisms, some of them have been adapted to other formalisms <TREF>Schabes et al , 1988</TREF>; van <REF>Noord, 1994</REF>; <REF>Yoshida et al , 1999</REF>; <REF>Torisawa et al , 2000</REF>~~~However, these realizations sometimes exhibit quite different performance in each grammar formalism <REF>Yoshida et al , 1999</REF>; <REF>Yoshinaga et al , 2001</REF>.
Various parsing techniques have been developed for lexicalized grammars such as Lexicalized Tree Adjoining Grammar LTAG <TREF>Schabes et al , 1988</TREF>, and Head-Driven Phrase Structure Grammar HPSG <REF>Pollard and Sag, 1994</REF>~~~Along with the independent development of parsing techniques for individual grammar formalisms, some of them have been adapted to other formalisms <TREF>Schabes et al , 1988</TREF>; van <REF>Noord, 1994</REF>; <REF>Yoshida et al , 1999</REF>; <REF>Torisawa et al , 2000</REF>~~~However, these realizations sometimes exhibit quite different performance in each grammar formalism <REF>Yoshida et al , 1999</REF>; <REF>Yoshinaga et al , 2001</REF>~~~If we could identify an algorithmic difference that causes performance difference, it would reveal advantages and disadvantages of the different realizations.
The parser achieves an OGn6-time worst case behavior, OG2n4-time for unambiguous grammars and linear time for a large class of grammars~~~The parser uses the following two-pass parsing strategy originally defined for lexicalized grammars <TREF>Schabes et al , 1988</TREF> which improves its performance in practice <REF>Schabes and Joshi, 1990</REF>:  In the first step the parser will select, the set of structures corresponding to each word in the sentence~~~Each structure can be considered as encoding a set of rules~~~In the second step, the parser tries to see whether these structures can be combined to obtain a wellformed structure.
XTAG runs under Common Lisp and X Window CLX~~~Tree-adjoining grammar TAG <REF>Joshi et al , 1975</REF>; <REF>Joshi, 1985</REF>; <REF>Joshi, 1987</REF> and its lexicalized variant <TREF>Schabes et al , 1988</TREF>; <REF>Schabes, 1990</REF>; <REF>Joshi and Schabes, 1991</REF> are tree-rewriting systems in which the syntactic properties of words are encoded as tree structured-objects of extended size~~~TAG trees can be combined with adjoining and substitution to form new derived trees~~~1 Tree-adjoining grammar differs from more traditional tree-generating systems such as context-free grammar in two ways: 1.
This information is particularly useful for a top-down component of the parser <REF>Schabes and Joshi, 1990</REF>~~~XTAG provides all the utilities required for designing a lexicalized TAG structured as in Schabes et al 1988~~~All the syntactic concepts of lexicalized TAG such as the grouping of the trees in tree families which represents the possible variants on a basic subcategorization frame are accessible through mouse-sensitive items~~~Also, all the operations required to build a grammar such as load trees, define tree families, load syntactic and morphological lexicon can be predefined with a macro-like language whose instructions can be loaded from a file See Figure 5.
See the introduction by Joshi 1987 for an introduction to tree-adjoining grammar~~~We refer the reader to Joshi 1985, Joshi 1987, Kroch and Joshi 1985, Abeill et al 1990a, Abeill 1988 and to Joshi and Schabes 1991 for more information on the linguistic characteristics of TAG such as its lexicalization and factoring recursion out of dependencies~~~2The TAG derivation tree is the basis for semantic interpretation <REF>Shieber and Schabes, 1990b</REF>, generation <REF>Shieber and Schabes, 1991</REF> and machine translation Abeill et al , 1990b since the information given in this data-structure is richer than the one found in the derived tree~~~Furthermore, it is at the level of the derivation tree that ambiguity must be defined.
Interestingly, several studies suggested that the identification of PropBank annotations would require linguistically-motivated features that can be obtained by deep linguistic analysis <REF>Gildea and Hockenmaier, 2003</REF>; <REF>Chen and Rambow, 2003</REF>~~~They employed a CCG <REF>Steedman, 2000</REF> or LTAG <TREF>Schabes et al , 1988</TREF> parser to acquire syntactic/semantic structures, which would be passed to statistical classifier as features~~~That is, they used deep analysis as a preprocessor to obtain useful features for training a probabilistic model or statistical classifier of a semantic argument identifier~~~These results imply the superiority of deep linguistic analysis for this task.
All errors are of course our own~~~As for Lexicalized TAGs, in <TREF>Schabes et al , 1988</TREF> a two step algorithm has been presented: during the first step the trees corresponding to the input string are selected and in the second step the input string is parsed with respect to this set of trees~~~Another paper by <REF>Schabes and Joshi 1989</REF> shows how parsing strategies can take advantage of lexicalization in order to improve parsers performance~~~Two major advantages have been discussed in the cited work: grammar filtering the parser can use only a subset of the entire grammar and bottom-up information further constraints are imposed on the way trees can be combined.
In <REF>Kroch and Joshi, 1985</REF> a detailed discussion of the linguistic relevance of TAGs can be found~~~Lexicalized Tree Adjoining Grammars <TREF>Schabes et al , 1988</TREF> are a refinement of TAGs such that each elementary tree is associated with a lexieal item, called the anchor of the tree~~~Therefore, Lexicalized TAGs conform to a common tendency in modem theories of grammar, namely the attempt to embed grammatical information within lexical items~~~Notably, the association between elementary trees and anchors improves also parsing performance, as will be discussed below.
Early mechanisms of this sort included categorial grammar Bar-<REF>Hillel, 1953</REF> and subcategorization frames <REF>Chomsky, 1965</REF>~~~Other lexicalized formalisms include <TREF>Schabes et al , 1988</TREF>; Meluk, 1988; <REF>Pollard and Sag, 1994</REF>~~~Besides the possible arguments of a word, a natural-language grammar does well to specify possible head words for those arguments~~~Convene requires an NP object, but some NPs are more semantically or lexically appropriate here than others, and the appropriateness depends largely on the NPs head eg , meeting.
A more linguistically motivated approach is to expand the domain of productions downward to incorporate more tree structures~~~The Lexicalized Tree-Adjoining Grammar LTAG formalism <TREF>Schabes et al, 1988</TREF>, <REF>Schabes, 1990</REF> , although not context-free, is the most well-known instance in this category~~~PLTIGs belong to this third category and generate only context-free languages~~~LTAGs and LTIGs are tree-rewriting systems, consisting of a set of elementary trees combined by tree operations.
E87-1042:140~~~TEMPORAL REASONING IN NATURAL LANGUAGE UNDERSTANDING: THE TEMPORAL STRUCTURE OF THE NARRATIVE Alexander Nakhimovsky Department of Computer Science Colgate University Hamilton, NY 13346 USA CSNet: sashacolgate Abstract This paper proposes a new framework for discourse analysis, in the spirit of <TREF>Grosz and Sidner 1986</TREF>, Webber 1987a,b but differentiated with respect to the type or genre of discourse~~~It is argued that different genres call for different representations and processing strategies; particularly important is the distinction between subjective, pefformative discourse and objective discourse, of which narrative is a primary example~~~This paper concentrates on narratives and introduces the notions of temporal focus proposed also in <REF>Webber 1987b</REF> and narrative move.
Preprocessing the Corpus with a Part of Speech Tagger Phrasal verbs involving the preposition to raise an interesting problem because of the possible confusion with the infinitive marker to~~~We have found that if we first tag every word in the corpus with a part of speech using a method such as <REF>Church 1988</REF> or <TREF>DeRose 1988</TREF>, and then measure associations between tagged words, we can identify interesting contrasts between verbs associated with a following preposition toin and verbs associated with a following infinitive marker toto~~~Part of speech notation is borrowed from <REF>Francis and Kucera 1982</REF>; m  preposition; to  infinitive marker; vb  bare verb; vbg  verb  ing; vbd  verb  ed; vbz  verb  s; vbn  verb  en~~~The score identifies quite a number of verbs associated in an interesting way with to; restricting our attention to pairs with a score of 30 or more, there are 768 verbs associated with the preposition toin and 551 verbs with the infinitive marker toto.
Automatic morphological disambiguation is an important component in higher level analysis of natural language text corpora~~~There has been a large number of studies in tagging and morphological disambiguation using various techniques such as statistical techniques, eg, <REF>Church, 1988</REF>; <REF>Cutting et al , 1992</REF>; <TREF>DeRose, 1988</TREF>, constraint-based techniques <REF>Karlsson et al , 1995</REF>; <REF>Voutilainen, 1995b</REF>; Voutilainen, Heikkil/i, and <REF>Anttila, 1992</REF>; <REF>Voutilainen and Tapanainen, 1993</REF>; <REF>Oflazer and KuruSz, 1994</REF>; <REF>Oflazer and Till 1996</REF> and transformation-based techniques <REF>Brilt, 1992</REF>; <REF>Brill, 1994</REF>; <REF>Brill, 1995</REF>~~~This paper presents a novel approach to constraint based morphological disambiguation which relieves the rule developer from worrying about conflicting rule ordering requirements~~~The approach depends on assigning votes to constraints according to their complexity and specificity, and then letting constraints cast votes on matching parses of a given lexical item.
HMMs have long been central in speech recognition <REF>Rabiner, 1989</REF>~~~Their application to partof-speech tagging <REF>Church, 1988</REF>; <TREF>DeRose, 1988</TREF> kicked off the era of statistical NLP, and they have found additional NLP applications to phrase chunking, text segmentation, word-sense disambiguation, and information extraction~~~The algorithm is also important to teach for pedagogical reasons, as the entry point to a family of EM algorithms for unsupervised parameter estimation~~~Indeed, it is an instructive special case of 1 the inside-outside algorithm for estimation of probabilistic context-free grammars; 2 belief propagation for training singly-connected Bayesian networks and junction trees <REF>Pearl, 1988</REF>; <REF>Lauritzen, 1995</REF>; 3 algorithms for learning alignment models such as weighted edit distance; 4 general finitestate parameter estimation <REF>Eisner, 2002</REF>.
1; they closely replicate Brills results 1993b, page 96, allowing for the fact that his tests used more templates, including templates like if one of the three previous tags is A~~~Brills results demonstrate that this approach can outperform the Hidden Markov Model approaches that are frequently used for part-of-speech tagging <REF>Jelinek, 1985</REF>; <REF>Church, 1988</REF>; <TREF>DeRose, 1988</TREF>; <REF>Cutting et al , 1992</REF>; <REF>Weischedel et al , 1993</REF>, as well as showing promise for other applications~~~The resulting model, encoded as a list of rules, is also typically more compact and for some purposes more easily interpretable than a table of HMM probabilities~~~An Incremental Algorithm It is worthwhile noting first that it is possible in some circumstances to significantly speed up the straightforward algorithm described above.
We describe a part-of-speech tagger built on these principles and we suggest a methodology for developing an adequate training corpus~~~In the part-of-speech hterature, whether taggers are based on a rule-based approach <REF>Klein and Simmons, 1963</REF>, <REF>Brill, 1992</REF>, <REF>Voutilainen, 1993</REF>, or on a statistical one <REF>Bahl and Mercer, 1976</REF>, <REF>Leech et al , 1983</REF>, <REF>Merialdo, 1994</REF>, <TREF>DeRose, 1988</TREF>, <REF>Church, 1989</REF>, <REF>Cutting et al , 1992</REF>, there is a debate as to whether more attention should be paid to lexical probabilities rather than contextual ones~~~<REF>Church, 1992</REF> claims that part-of-speech taggers depend almost exclusively on lexical probabilities, whereas other researchers, such as Voutilainen <REF>Karlsson et al , 1995</REF> argue that word ambiguities vary widely in function of the specific text and genre~~~Indeed, part of Churchs argument is relevant if a system is based on a large corpus such as the Brown corpus Francis and Kuera, 1982 which represents one million surface forms of morpho-syntacticaJly disambiguated words from a range of balanced texts.
The work is based on some similarity metrics~~~ Bahl, <REF>Brown, DeSouza and Mercer, 1989</REF>; Brown, Pietra, deSouza and Mercer,1992; <REF>Chang, 1995</REF>; DeRose,1988; <REF>Garside, 1987</REF>; <REF>Hughes, 1994</REF>; Jardino,1993; <REF>Jelinek, Mercer, and Roukos, 1990b</REF>; Wu, <REF>Wang, Yu and Wang, 1995</REF>; <REF>Magerman, 1994</REF>; <REF>McMahon, 1994</REF>; <REF>McMahon, 1995</REF>; <REF>Pereira, 1992</REF>; <REF>Resnik, 1992</REF>; <REF>Zhao, 1995</REF>; <REF>Brill 1993</REF> and <REF>Pop 1996</REF> present a transformation-based tagging~~~Before a part-of-speech tagger can be built, the word classifications are performed to help us choose a set of part-of-speech~~~They use the sum of two relative entropies obtained from neighboring words as the similarity metric to compare two words.
The training is performed on ambiguity classes and not on individual word tokens~~~<REF>Kallgren 1996</REF> gives a more covering description of how XPOST is used on the Swedish material and also sketches the major differences between this algorithm and some others used for tagging, such as PARTS <REF>Church 1988</REF> and VOLSUNGA <TREF>DeRose 1988</TREF>~~~A characteristic tbature of the SUC is its high number of different tags~~~The number of part-ofspeech tags used in the SUC is 21.
Several workers have addressed the problem of tagging text~~~Methods have ranged from locally-operating rules <REF>Greene and Rubin, 1971</REF>, to statistical methods <REF>Church, 1989</REF>; <TREF>DeRose, 1988</TREF>; <REF>Garside, Leech and Sampson, 1987</REF>; <REF>Jelinek, 1985</REF> and back-propagation <REF>Benello, Mackie and Anderson, 1989</REF>; <REF>Nakamura and Shikano, 1989</REF>~~~The statistical methods can be described in terms of Markov models~~~States in a model represent categories clc n is the number of different categories used.
Indeed, recent increased interest in the problem of disambiguating lexical category in English has led to significant progress in developing effective programs for assigning lexical category in unrestricted text~~~The most successful and comprehensive of these are based on probabilistic modeling of category sequence and word category <REF>Church 1987</REF>; <REF>Garside, Leech and Sampson 1987</REF>; <TREF>DeRose 1988</TREF>~~~These stochastic methods show impressive performance: Church reports a success rate of 95 to 99, and shows a sample text with an error rate of less than one percent~~~What may seem particularly surprising is that these methods succeed essentially without reference to syntactic structure; purely surface lexical patterns are involved.
Part-of-Speech Tagging Many of the very same methods are being applied to problems in natural language processing by many of the very same researchers~~~As a result, the empirical approach has been adopted by almost all contemporary part-of-speech programs: <REF>Bahl and Mercer 1976</REF>, <REF>Leech, Garside, and Atwell 1983</REF>, <REF>Jelinek 1985</REF>, <REF>Deroualt and Merialdo 1986</REF>, <REF>Garside, Leech, and Sampson 1987</REF>, <REF>Church 1988</REF>, <TREF>DeRose 1988</TREF>, <REF>Hindle 1989</REF>, Kupiec 1989, 1992, Ayuso et al~~~1990, de<REF>Marcken 1990</REF>, <REF>Karlsson 1990</REF>, <REF>Boggess, Agarwal, and Davis 1991</REF>, <REF>Merialdo 1991</REF>, and <REF>Voutilainen, Heikkila, and Anttila 1992</REF>~~~These programs input a sequence of words, eg, The chair will table the motion, and output a sequence of part-of-speech tags, eg, art noun modal verb art noun.
4 Concluding remarks Though there can be little doubt that the ruling system of bakeoffs actively encourages a degree of oneupmanship, our paper and our software are not offered in a competitive spirit~~~As we said at the out211 set, we dont necessarily believe HunPos to be in any way better than TnT, and certainly the main ideas have been pioneered by <TREF>DeRose 1988</TREF>, <REF>Church 1988</REF>, and others long before this generation of HMM work~~~But to improve the results beyond what a basic HMM can achieve one needs to tune the system, and progress can only be made if the experiments are end to end replicable~~~There is no doubt many other systems could be tweaked further and improve on our results what matters is that anybody could now also tweak HunPos without any restriction to improve the state of the art.
54 29 3 43 40 4 97 69 7 These results are remarkably good, in spite of the fact that many other systems are reported to reach an accuracy of 9697~~~<REF>Garside 1987</REF>, <REF>Marshall 1987</REF>, <TREF>DeRose 1988</TREF>, <REF>Church 1988</REF>, <REF>Ejerhed 1987</REF>, O<REF>Shaughnessy 1989</REF>~~~Those systems, however, all use heavier artillery than MorP, that has been deliberately restricted in accordance with the hypotheses presented above~~~This restrictiveness concerns both the size of the lexicon and the ways of carrying out disambiguation.
Although finite-state machines have been used for part-of-speech tagging <REF>Tapanainen and Voutilainen 1993</REF>; <REF>Silberztein 1993</REF>, none of these approaches has the same flexibility as stochastic techniques~~~Unlike stochastic approaches to part-of-speech tagging <REF>Church 1988</REF>; <REF>Kupiec 1992</REF>; <REF>Cutting et al 1992</REF>; <REF>Merialdo 1990</REF>; <TREF>DeRose 1988</TREF>; <REF>Weischedel et al 1993</REF>, up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired~~~<REF>Recently, Brill 1992</REF> described a rule-based tagger that performs as well as taggers based upon probabilistic models and overcomes the limitations common in rule-based approaches to language processing: it is robust and the rules are automatically ac Mitsubishi Electric Research Laboratories, 201 Broadway, Cambridge, MA 02139~~~E-mail: rocbe/schabesmerlcom.
Research on corpus-based natural language learning and processing is rapidly accelerating following the introduction of large on-line corpora, faster computers, and cheap storage devices~~~Recent work involves novel ways to employ annotated corpus in part of speech tagging <REF>Church 1988</REF> <REF>Derose 1988</REF> and the application of mutual information statistics on the corpora to uncover lexical information <REF>Church 1989</REF>~~~The goal of the research is the construction of robust and portable natural language processing systems~~~The wide range of topics available on the Internet calls for an easily adaptable information extraction system for different domains.
There has recently been a great deal of work exploring methods for automatically training part of speech taggers, as an alternative to laboriously hand-crafting rules for tagging, as was done in the past <REF>Klein and Simmons, 1963</REF>; <REF>Harris, 1962</REF>~~~Almost all of the work in the area of automatically trained taggers has explored Markov-model based part of speech tagging <REF>Jelinek, 1985</REF>; <REF>Church, 1988</REF>; <REF>Derose, 1988</REF>; <REF>DeMarcken, 1990</REF>; <REF>Cutting et al , 1992</REF>; <REF>Kupiec, 1992</REF>; <REF>Charniak et al , 1993</REF>; <REF>Weischedel et al , 1993</REF>; <REF>Schutze and Singer, 1994</REF>; <REF>Lin et al , 1994</REF>; <REF>Elworthy, 1994</REF>; <REF>Merialdo, 1995</REF>~~~2 For a Markov-model based tagger, training consists of learning both lexical probabilities Pwordltag and contextual probabilities Ptagiltagil tagi-n~~~Once trained, a sentence can be tagged by searching for the tag sequence that maximizes the product of lexical and contextual probabilities.
Unknown Words Unknown Common Words Unknown Proper Nouns Tagger Guesser Metrics Error Error Coverage Error Coverage HMM Xerox mean 17851643 30022169 37567270 10785563 63797113 s-error 0484710 0469922 1687396 0613745 1714969 HMM Cascade mean 12378716 21266264 36507909 7776456 64795969 s-error 0917656 0403957 2336381 0853958 2206457 Brill Brill mean 14688501 27411736 38998687 6439525 62160917 s-error 0908172 0539634 2627234 0501082 4010992 Brill Cascade mean 11327863 20986240 37933048 5548990 63816586 s-error 0761576 0480798 2353510 0561009 3775991 the Brown Corpus, we obtained the error rate mean 0 4003093 with the standard error deB0155599~~~This agrees with the results on the closed dictionary ie , without unknown words obtained by other researchers for this class of the model on the same corpus <REF>Kupiec 1992</REF>; <TREF>DeRose 1988</TREF>~~~The Brill tagger showed some better results: error rate mean 0 3327366 with the standard error deBO 123903~~~Although our primary goal was not to compare the taggers themselves but rather their performance with the guessing components, we attribute the difference in their performance to the fact that Brills tagger uses the information about the most likely tag for a word whereas the HMM tagger did not have this information and instead used the priors for a set of POS-tags ambiguity class.
A lot of effort has been devoted in the past to the problem of tagging text, ie assigning to each word the correct tag part of speech in the context of the sentence~~~Two main approaches have generally been considered: rule-based <REF>Klein and Simmons 1963</REF>; <REF>Brodda 1982</REF>; <REF>Paulussen and Martin 1992</REF>; <REF>Brill et al 1990</REF> probabilistic <REF>Bahl and Mercer 1976</REF>; <REF>Debili 1977</REF>; <REF>Stolz, Tannenbaum, and Carstensen 1965</REF>; <REF>Marshall 1983</REF>; <REF>Leech, Garside, and Atwell 1983</REF>; <REF>Derouault and Merialdo 1986</REF>; <TREF>DeRose 1988</TREF>; <REF>Church 1989</REF>; <REF>Beale 1988</REF>; <REF>Marcken 1990</REF>; <REF>Merialdo 1991</REF>; <REF>Cutting et al 1992</REF>~~~More recently, some work has been proposed using neural networks <REF>Benello, Mackie, and Anderson 1989</REF>; <REF>Nakamura and Shikano 1989</REF>~~~Multimedia Communications Department, Institut EURECOM, 2229 Route des Cretes, BP 193, 06904 Valbonne Cedex France; merialdoeurecomfr.
for evaluation at word level, choose the most probable tag for each word in the sentence argmax argmax Wi  t pti  t/W  t  pW, T T:tit where Wi is the tag assigned to word wi by the tagging procedure b in the context of the sentence W, We call this procedure Maximum Likelihood ML tagging~~~It is interesting to note that the most commonly used method is Viterbi tagging see <TREF>DeRose 1988</TREF>; <REF>Church 1989</REF> although it is not the optimal method for evaluation at word level~~~The reasons for this preference are presumably that:  Viterbi tagging is simpler to implement than ML tagging and requires less computation although they both have the same asymptotic complexity  Viterbi tagging provides the best interpretation for the sentence, which is linguistically appealing  ML tagging may produce sequences of tags that are linguistically impossible because the choice of a tag depends on all contexts taken together~~~However, in our experiments, we will show that Viterbi and ML tagging result in very similar performance.
Although methods for unsupervised training of HMMs do exist, training is usually done in a supervised way by estimation of the above probabilities from relative frequencies in the training data~~~The HMM approach to tagging is by far the most studied and applied <REF>Church 1988</REF>; <TREF>DeRose 1988</TREF>; <REF>Charniak 1993</REF>~~~In van <REF>Halteren, Zavrel, and Daelemans 1998</REF> we used a straightforward implementation of HMMs, which turned out to have the worst accuracy of the four competing methods~~~In the present work, we have replaced this by the TnT system we will refer to this tagger as HMM below.
<REF>Choueka and Lusignan 1985</REF> presented a system for the morphological tagging of large texts that is based on the short context of the word but also depends heavily on human interaction~~~Methods using the short context of a word in order to resolve ambiguity usually categorical ambiguity are very common in English and other languages <TREF>DeRose 1988</TREF>; <REF>Church 1988</REF>; <REF>Karlsson 1990</REF>~~~A system using this approach was developed by Levinger and Ornan in order to serve as a component in their project of morphological disambiguation in Hebrew <REF>Levinger 1992</REF>~~~The main resource, used by this system for disambiguation, is a set of syntactic constraints that were defined manually by the authors and followed two theoretical works that defined short context rules for Hebrew <REF>Pines 1975</REF>; <REF>Albeck 1992</REF>.
Most successful methods have followed speech recognition systems <REF>Jelinek, Mercer, and Roukos 1992</REF> and used large corpora to deduce the probability of each part of speech in the current context usually the two previous words--trigrams~~~These methods have reported performance in the range of 95-99 correct by word <TREF>DeRose 1988</TREF>; <REF>Cutting et al 1992</REF>; <REF>Jelinek, Mercer, and Roukos 1992</REF>; <REF>Kupiec 1992</REF>~~~The difference in performance is due to different evaluation methods, different tag sets, and different corpora~~~<REF>See Church 1992</REF> for a survey.
More precisely, assume that the word wh occurs in a sentence W  wlWkwn, and that w is a word we are considering substituting for it, yielding sentence W I Word w is then preferred over wk iff PW > PW, where PW and PW are the probabilities of sentences W and W f respectively~~~1 We calculate PW using the tag sequence of W as an intermediate quantity, and summing, over all possible tag sequences, the probability of the sentence with that tagging; that is: PW   PW, T T where T is a tag sequence for sentence W The above probabilities are estimated as is traditionally done in trigram-based part-of-speech tagging <REF>Church, 1988</REF>; <TREF>DeRose, 1988</TREF>: PW,T  PWITPT  1  HPwiti HPt, lt,2t,l2 i i where T  tltn, and Ptitl-2ti-1 is the prob ability of seeing a part-of-speech tag tl given the two preceding part-of-speech tags ti-2 and ti-1~~~Equations 1 and 2 will also be used to tag sentences W and W  with their most likely part-of-speech sequences~~~This will allow us to determine the tag that 1To enable fair comparisons between sequences of different length as when considering maybe and may be, we actually compare the per-word geometric mean of the sentence probabilities.
More recently, Koskenniemi also used a rule-based approach implemented with finite-state machines <REF>Koskenniemi, 1990</REF>~~~Statistical methods have also been used eg , <TREF>DeRose, 1988</TREF>, <REF>Garside et al , 1987</REF>~~~These provide the capability of resolving ambiguity on the basis of most likely interpretation~~~A form of Markov model has been widely used that assumes that a word depends probabilistically on just its part-of-speech category, which in turn depends solely on the categories of the preceding two words.
Estimating the Lexical Priors for Rare Forms For a common form such as lopen walk a reasonable estimate of the lexical prior probabilities is the MLE, computed over all occurrences of this form~~~So, in the UdB corpus, lopen occurs 92 times as an infinitive and 43 times as a finite plural, so the MLE 1 Even models of disambiguation that make use of context, such as statistical n-gram taggers, often presume some estimate of lexical priors, in addition to requiring estimates of the transition probabilities of sequences of lexical tags <REF>Church 1988</REF>; <TREF>DeRose 1988</TREF>; <REF>Kupiec 1992</REF>, and this again brings up the question of what to do about unseen or low-frequency forms~~~In working taggers, a common approach is simply to apply a uniform small probability to the various senses of unseen or low-frequency forms: this was done in the tagger discussed in <REF>Church 1988</REF>, for example~~~156 Baayen and Sproat Lexical Priors for Low-Frequency Forms to> 8 Figure 1 I I I I 0 2 4 6 log frequency class Relative frequency of Dutch infinitives versus finite plurals in the Uit den Boogaart corpus, as a function of the natural log of the frequency of the word forms.
Its recall is very high 997 of all words receive the correct morphological analysis, but this system leaves 3-7 of all words ambiguous, trading precision for recall~~~157 ena or the linguists abstraction capabilities eg knowledge about what is relevant in the context, they tend to reach a 95-97 accuracy in the analysis of several languages, in particular English <REF>Marshall 1983</REF>; Black et aL 1992; <REF>Church 1988</REF>; <REF>Cutting et al 1992</REF>; de <REF>Marcken 1990</REF>; <TREF>DeRose 1988</TREF>; <REF>Hindle 1989</REF>; <REF>Merialdo 1994</REF>; <REF>Weischedel et al 1993</REF>; <REF>Brill 1992</REF>; <REF>Samuelsson 1994</REF>; Eineborg and Gambick 1994, etc~~~Interestingly, no significant improvement beyond the 97 barrier by means of purely data-driven systems has been reported so far~~~In terms of the accuracy of known systems, the data-driven approach seems then to provide the best model of part-of-speech distribution.
This will partly be based on another important step in the process, namely the construction of constituents, in particular noun phrases and prepositional phrases <REF>Church 1988</REF>, <REF>Kfillgren 1984c</REF>, and partly on a more general algorithm that for pairs or longer sequences of tags calculates the relative probability of alternative tag assignments~~~The principles behind such algorithms are known, but they have never been tried on Swedish material <TREF>DeRose 1988</TREF>, <REF>Marshall 1987</REF>, <REF>EegOlofsson 1985</REF>~~~An indispensable step in the disambiguation process is the assignment of clause boundaries, which presupposes established constituents at the same time as it forms an important basis for disambiguating chains of tags~~~Methods for this are being tested out on Swedish material <REF>Ejerhed 1989</REF>.
There are a number of large tagged corpora available, allowing for a variety of experiments to be run~~~Part-of-speech tagging is an active area of research; a great deal of work has been done in this area over the past few years eg , <REF>Jelinek 1985</REF>; <REF>Church 1988</REF>; <REF>Derose 1988</REF>; <REF>Hindle 1989</REF>; <REF>DeMarcken 1990</REF>; <REF>Merialdo 1994</REF>; <REF>Brill 1992</REF>; <REF>Black et al 1992</REF>; <REF>Cutting et al 1992</REF>; <REF>Kupiec 1992</REF>; <REF>Charniak et al 1993</REF>; <REF>Weischedel et al 1993</REF>; <REF>Schutze and Singer 1994</REF>~~~Part-of-speech tagging is also a very practical application, with uses in many areas, including speech recognition and generation, machine translation, parsing, information retrieval and lexicography~~~Insofar as tagging can be seen as a prototypical problem in lexical ambiguity, advances in part-of-speech tagging could readily translate to progress in other areas of lexical, and perhaps structural, ambiguity, such as wordsense disambiguation and prepositional phrase attachment disambiguation.
However, stochastic taggers have the disadvantage that linguistic information is captured only indirectly, in large tables of statistics~~~Almost all recent work in developing automatically trained part-of-speech taggers has been on further exploring Markovmodel based tagging <REF>Jelinek 1985</REF>; <REF>Church 1988</REF>; <REF>Derose 1988</REF>; <REF>DeMarcken 1990</REF>; <REF>Merialdo 1994</REF>; <REF>Cutting et al 1992</REF>; <REF>Kupiec 1992</REF>; <REF>Charniak et al 1993</REF>; <REF>Weischedel et al 1993</REF>; <REF>Schutze and Singer 1994</REF>~~~41 Transformation-based Error-driven Part-of-Speech Tagging Transformation-based part of speech tagging works as follows~~~9 The initial-state annotator assigns each word its most likely tag as indicated in the training corpus.
Statistical taggers usually work as follows: First, each word in the input word string 1471,   , W, is assigned all possible tags according to the lexicon, thereby creating a lattice~~~A dynamic programming technique is then used to find tag the sequence 5/,, , that maximizes PT1,,Tn I Wl,   , Wn  tt  IIPTk T1,,Tk-1;Wl,,Wn kl 1:1 PTk Tk-NI,,Tk-1; VIZk  7 PT wk PTk k-Nl,, k-l  kl fl PTk PT, Tk-N,, - PWk I Tk : PWk Since the maximum does not depend on the factors PWk, these can be omitted, yielding the standard statistical PoS tagging task: max - PTk IU-V,,Tk-JPWk JT TI,,T, tl This is well-described in for example <TREF>DeRose 1988</TREF>~~~We thus have to estimate the two following sets of probabilities:  Lexical probabilities: The probability of each tag T i conditional on the word W that is to be tagged, pr I I wr~~~i Often the converse probabilities PW are given instead, but we will for reasons soou to become apparent use the former formulation.
Recent research advances may lead to the development of viable book indexing methods for Chinese books~~~These include the availability of efficient and high precision word segmentation methods for Chinese text <REF>Chang et al , 1991</REF>; <REF>Sproat and Shih, 1990</REF>; <REF>Wang et al , 1990</REF>, the availability of statistical analysis of a Chinese corpus <REF>Liu et al , 1975</REF> and large-scale electronic Chinese dictionaries with partof-speech information <REF>Chang et al , 1988</REF>; BDC, 1992, the corpus-based statistical part-of-speech tagger <REF>Church, 1988</REF>; <TREF>DeRose, 1988</TREF>; <REF>Beale, 1988</REF>, as well as phrasal and clausal analyzers <REF>Church 1988</REF>; <REF>Ejerhed 1990</REF> 2~~~Problem description As being pointed out in <REF>Salton, 1988</REF>, back-of-book indexes may consist of more than one word that are derived from a noun phrase~~~Given the text of a book, an indexing system, must perform some kind of phrasal and statistical analysis in order to produce a list of candidate indexes and their occurrence statistics in order to generate indexes as shown in Figure 1 which is an excerpt from the reconstruction of indexes of a book on transformational grammar for Mandarin Chinese <REF>Tang, 1977</REF>.
In this paper, we report on experimental work dealing with the part-of-speech tagging of a corpus of transcribed spoken Swedish~~~The tagger used implements a standard probabilistic biclass model see, e g, <TREF>DeRose 1988</TREF> trained on a tagged subset of the Stockhohn-Ume Corpus of written Swedish <REF>Ejerhed et al 1992</REF>~~~Given that the transcriptions contain many modifications of standard orthography in order to capture spoken language variants, reductions, etc~~~a special lexicon had to be developed to map spoken langnage variants onto their canonical written language forms.
Several approaches have been proposed to construct automatic taggers~~~Most work on statistical methods has used n-gram models or Hidden Markov Model-based taggers eg <REF>Church, 1988</REF>; <TREF>DeRose, 1988</TREF>; <REF>Cutting et al 1992</REF>; <REF>Merialdo, 1994</REF>, etc~~~In 14 these approaches, a tag sequence is chosen for a sentence that maximizes the product of lexical and contextual probabilities as estimated from a tagged corpus~~~In rule-based approaches, words are assigned a tag based on a set of rules and a lexicon.
Categorical ambiguity, however, is of a different kind and is resolved in a different way~~~For the purposes of the present paper, it will be assumed that only content words are at issue, and that the syntactic category of all content words in the text that is under study can be determined automatically <REF>Church, 1988</REF>; <TREF>DeRose, 1988</TREF>~~~The problem is simply to decide which sense of a content word--noun, verb, adjective, or adverb---is appropriate in a given linguistic context~~~It will also be assumed that sense resolution for individual words can be accomplished on the basis of information about the irnrnediate linguistic context.
Purely rule-based techniques seemed too brittle for dealing with the variety of constructions, the long sentences averaging 29 words per sentence, and the degree of unexpected input~~~Statistical models based on local information eg , <TREF>DeRose 1988</TREF>; <REF>Church 1988</REF> might operate effectively in spite of sentence length and unexpected input~~~To see whether our four hypotheses in italics above effectively addressed the four concerns above, we chose to test the hypotheses on two well-known problems: ambiguity both at the structural level and at the part-of-speech level and inferring syntactic and semantic information about unknown words~~~Guided by the past success of probabilistic models in speech processing, we have integrated probabilistic models into our language processing systems.
We report in Section 2 on our experiments on the assignment of part of speech to words in text~~~The effectiveness of such models is well known <TREF>DeRose 1988</TREF>; <REF>Church 1988</REF>; <REF>Kupiec 1989</REF>; <REF>Jelinek 1985</REF>, and they are currently in use in parsers eg de <REF>Marcken 1990</REF>~~~Our work is an incremental improvement on these models in three ways: 1 Much less training data than theoretically required proved adequate; 2 we integrated a probabilistic model of word features to handle unknown words uniformly within the probabilistic model and measured its contribution; and 3 we have applied the forward-backward algorithm to accurately compute the most likely tag set~~~In Section 3, we demonstrate that probability models can improve the performance of knowledge-based syntactic and semantic processing in dealing with structural ambiguity and with unknown words.
Trento, Italy, pages 133-140, Association for Computational Linguistics~~~Steven J <TREF>DeRose 1988</TREF>~~~Grammatical Category Disambiguation by Statistical Optimization~~~Computational Linguistics, 141 :31-39.
The first major use of HMMs for part of speech tagging was in CLAWS <REF>Garside et al , 1987</REF> in the 1970s~~~With the availability of large corpora and fast computers, there has been a recent resurgence of interest, and a number of variations on and alter53 natives to the FB, Viterbi and BW algorithms have been tried; see the work of, for example, Church <REF>Church, 1988</REF>, Brill <REF>Brill and Marcus, 1992</REF>; <REF>Brill, 1992</REF>, DeRose <TREF>DeRose, 1988</TREF> and gupiec <REF>Kupiec, 1992</REF>~~~One of the most effective taggers based on a pure HMM is that developed at Xerox <REF>Cutting et al , 1992</REF>~~~An important aspect of this tagger is that it will give good accuracy with a minimal amount of manually tagged training data.
Different accessibility relations can be modeled, eg to distinguish a local context for resolving reflexive anaphors like himself  from a global context <REF>Kruijff, 2001</REF>~~~Finally, the rich temporal ontology underlying models of tense and aspect such as <TREF>Moens and Steedman 1988</TREF> can be captured using the sorting strategy~~~Earlier work like <REF>Blackburn and Lascarides 1992</REF> already explored such ideas~~~HLDS employs hybrid logic to integrate Moens and Steedmans notion of the event nucleus directly into meaning representations.
However, at least 30 of the dates and times in the MUC test were fixed-format ones occurring in document headers, trailers, and copyright notices~~~ Finally, there is a large body of work, eg, <TREF>Moens and Steedman 1988</TREF>, <REF>Passoneau 1988</REF>, <REF>Webber 1988</REF>, <REF>Hwang 1992</REF>, <REF>Song and Cohen 1991</REF>, that has focused on a computational analysis of tense and aspect~~~While the work on event chronologies is based on some of the notions developed in that body of work, we hope to further exploit insights from previous work~~~Conclusion We have developed a temporal annotation specification, and an algorithm for resolving a class of time expressions found in news.
Other aspects of our ontology are designed following proposals by <REF>Jackendoff 1990</REF>, in particular his analysis of movement events~~~2 <TREF>Moens and Steedman 1988</TREF> also use this term, but they restrict it to momentaneous events~~~Unfortunately, the terminology used in the literature for these kinds of categories varies so much that a standardization seems out of reach~~~404 Stede Verb Alternations event1 fill conf---- > not-full -state-1    f > pa>btination  fill-state-2   water-1  value > full Figure 2 SitSpec representing a fill-event.
As their central feature we take them to always involve some change of state: the building loses its integrity, the book comes into existence, or gets finished~~~<REF>While Bach 1986</REF> did not investigate the internal structure of events, others suggested that this needs to be done eg , <TREF>Moens and Steedman 1988</TREF>; <REF>Parsons 1990</REF>~~~<REF>Pustejovsky 1991</REF> treated Vendlerian accomplishments and achievements as transitions from a state Qy to NOT-Qy, and suggested that accomplishments in addition have an intrinsic agent performing an activity that brings about the change of state~~~We follow this line, but modify it in some ways.
In this paper, we show how one can find and exploit approximate solutions for both of these problems by capitalizing on the occurrences of certain lexicogrammatical constructs~~~Such constructs can include tense 96 and aspect <TREF>Moens and Steedman, 1988</TREF>; <REF>Webber, 1988</REF>; <REF>Lascarides and Asher, 1993</REF>, certain patterns of pronominalization and anaphoric usages <REF>Sidner, 1981</REF>; <REF>Grosz and Sidner, 1986</REF></REF>; <REF>Sumita et al , 1992</REF>; <REF>Grosz, Joshi, and Weinstein, 1995</REF>,/t-clefts <REF>Delin and Oberlander, 1992</REF>, and discourse markers or cue phrases <REF>Ballard, Conrad, and Longacre, 1971</REF>; <REF>Halliday and Hasan, 1976</REF>; <REF>Van Dijk, 1979</REF>; <REF>Longacre, 1983</REF>; <REF>Grosz and Sidner, 1986</REF></REF>; <REF>Schiffrin, 1987</REF>; <REF>Cohen, 1987</REF>; <REF>Redeker, 1990</REF>; <REF>Sanders, Spooren, and Noordman, 1992</REF>; <REF>Hirschberg and Litman, 1993</REF>; <REF>Knott, 1995</REF>; <REF>Fraser, 1996</REF>; <REF>Moser and Moore, 1997</REF>~~~In the work described here, we investigate how far we can get by focusing our attention only on discourse markers and lexicogrammatical constructs that can be detected by a shallow analysis of natural language texts~~~The intuition behind our choice relies on the following facts:  Psycholinguistic and other empirical research <REF>Kintsch, 1977</REF>; <REF>Schiffrin, 1987</REF>; <REF>Segal, Duchan, and Scott, 1991</REF>; <REF>Cahn, 1992</REF>; <REF>Sanders, Spooren, and Noordman, 1992</REF>; <REF>Hirschberg and Litman, 1993</REF>; <REF>Knott, 1995</REF>; <REF>Costermans and Fayol, 1997</REF> has shown that discourse markers are consistently used by human subjects both as cohesive ties between adjacent clauses and as macroconnectors between larger textual units.
The feature specification of this ompositionally derived accomplishment is therefore identical to that of a sentence containing a telic accomplishment verb, such as destroy~~~According to many researchers, knowledge of lexical aspect--how verbs denote situations as developing or holding in time-may be used to interpret event sequences in discourse <REF>Dowty, 1986</REF>; <TREF>Moens and Steedman, 1988</TREF>; <REF>Passoneau, 1988</REF>~~~In particular, Dowty suggests that, absent other cues, a relic event is interpreted as completed before the next event or state, as with ran into lhe room in 4a; in contrast, atelic situations, such as run, was hungry in 4b and 4 are interpreted as contemporaneous with the following situations: fell and made a pizza, respectively~~~4 a Mary ran into the room.
In 10, s is the consequent state of the event of John greeting Max, and it holds at the time t which precedes now~~~So our semantics of the perfect is like that in <TREF>Moens and Steedman 1988</TREF>: a perfect transforms an event into a consequent state, and asserts that the consequent state holds~~~The pluperfect of a state, such as 11, therefore, is assumed to first undergo a transformation into an event~~~11 John had loved Mary.
Other genres of text might depend on other kinds of structuring devices~~~Changes in time scale or time, as we redefined the category, may require world knowledge reasoning to recoguize but are often indicated by either cue words and phrases eg , n/he years ago ,  a year, for months; several months ago, a change in grammatical time of the verb eg , past tense versus present tense, or changes in aspect eg , atomic versus extended events versus states as defined by <TREF>Moens  Steedman 1988</TREF>~~~In considering how time change might affect anaphoric expression choice, we consider the choice for the first mention of a discourse entity in a sentence where that entity has recently been referred to in the discourse~~~Our hypothesis is that: Changes in time reliably signal changes of the thread in newspaper articles; definite descriptions should appear when the current reference to a discourse entity is in a different thread from the last reference to that entity and pronouns should occur when the previous mention is in the same thread 3.
The imperfective point of view brought by IMP imposes a change of point of view on the term of the eventuality~~~As for accomplishments, we can assume that they can be decomposed into several stages, according to <TREF>Moens and Steedman, 1988</TREF>: first a preparatory phase, second a culmination or achievement we are not concerned here with the result state~~~We can then say that IMP refers only to the preparatory phase, so that the term of the eventuality loses all relevance~~~This explains the so-called imperfective paradox: it is possible to use IMP even though the eventuality never reaches its term: 6 a I1 traversait la rue quand la voiture la 6cras6 He was crossing the street when the car hit him b  I1 traversa la rue quand la voiture la 6cras6 He crossed the street when the car hit him As for achievements, we can assume that they are reduced to a culmination.
Secondly, it has been argued that A when B permits many possible temporal relationships between the eventualities denoted by A and B cf~~~<TREF>Moens and Steedman 1988</TREF>; its for this reason that 2c can be interpreted as denoting El; 260 but given this permissiveness, why is 2d not as acceptable as 2c~~~3 The basic explanation: temporal presuppositions The basic explanation for the inappropriateness of 2b and 2d is actually quite simple~~~Sentences containing temporal connectives are presuppositional: the temporal clause introduces an eventuality that must be presupposed to have occurred, for the sentence as a whole to have a truth-value cf.
Second, with when, the almost equal distribution of preposed and postposed tokens suggests either free variation of the two patterns or different uses of the two patterns, with each use favoring a different pattern~~~The latter would accord with a theoretical distinction that has been made between postposed when expressing a purely temporal relation between the two clauses, and preposed when expressing a contingent relation between them <TREF>Moens and Steedman, 1988</TREF>~~~Integrated evidence from the PTB and PropBank may help distinguish the two possibilities~~~Third, there is a striking contrast between the patterning of although and even though, especially if one assumes that even though like even when, even after, even if, etc.
Weneedamuchbettermodelofhowtocommunicate time, and how this communication depends on the semantics and linguistic expression of the events being described~~~An obvious first step, which we are currently working on, is to include a linguisticallymotivatedtemporalontology <REF>MoensandSteedman, 1988</REF>, which will be separate from the existing domain ontology~~~We also need better techniques for communicating the temporal relationships between events in cases where they are not listed in chronological order <REF>Oberlander and Lascarides, 1992</REF>~~~6 Discussion Two discourse analysts from Edinburgh University, Dr Andy McKinlay and Dr Chris McVittie, kindly examined and compared some of the human and BT45 texts.
The domain is limited to trajectoryof-motion events specified by the verbs run, jog, sit is worth noting that as an alternative to positing a lexical ambiguity, one could just as easily invoke a coercion operator on an event predicate Pz mapping it to the process predicate he~~~plurPx  e, which would bring the present treatment more in line with <TREF>Moens and Steedman 1988</TREF> and <REF>Jackendoff 1991</REF>~~~plod, and walk; the locative prepositions to, towards, from, away from, along, eastwards, westwards, and to and back; various landmarks; the distance adverbials n miles; the frequency adverbials twice and n times; and finally the temporal adverbials for and in~~~Trajectory-of-motion events are modeled as continuous constant rate changes of location in one dimension of the TRAJECTOR relative to one or more LANDMARKS following <REF>Regier 1992</REF> in his use of Langackers 1987 terminology.
2 3 Theory 31 Ontology Various authors including <REF>Link, 1983</REF>, <REF>Bach, 1986</REF>, <REF>Krifka, 1989</REF>, <REF>Eberle, 1990</REF> have proposed modeltheoretic treatments in which a parallel ontological distinction is made between substances and things, processes and events, etc A similarly parallel distinction is employed here, but in a rather different way: unlike the above treatments, the present account models substances, processes, and other such entities as abstract kinds whose realizations vary in amount~~~As such, the approach developed here may be seen as building upon the work of <REF>Carlson 1977</REF> and his successors; it also represents one way to further formalize the intuitions found in <TREF>Moens and Steedman 1988</TREF> and <REF>Jackendoff 1991</REF>~~~<REF>Following Schubert and Pelletier 1987</REF>, the present account distinguishes individuals from kinds, but not from stages or quantities~~~Extending their ontology, the same distinction is assumed to hold not only in the domain of materials but also in the domain of eventualities, and derivatively in the domains of space and time as well.
Note that in a terminal DRS ready for an embedding test, all the auxiliary Rpts disappear do not participate in the embedding~~~The perfect is analyzed by using the notion of a nucleus <TREF>Moens and Steedman, 1988</TREF> to account for the inner structure of an eventuality~~~A nucleus is defined as a structure containing a preparatory process, culmination and consequent state~~~The categorization of verb phrases into different aspectual classes can be phrased in terms of which part of the nucleus they refer to.
Moreover, the semantic category of these features can also play a role~~~For example, Sue played the piano is nonc,lminated, while Sue played the sonata signifies a c-lminated event this example comes from <TREF>Moens and Steedman 1988</TREF>~~~32 Classes of Ambiguous Verbs Placing aspectually ambiguous verbs into semantic categories will help predict how these verbs combine with their arguments to determine aspectual class~~~This is because many verbs with related meanings combine with their arguments in similar ways.
For example, shaw denotes a state in, H/ lumbar puncture showed evidence of white cells, but denotes an event in, He showed me the photographs~~~This ambiguity presents a diculty for automatically classifying a verb because the aspectual class of a clause is a function of several clausal constituents in addition to the main verb <REF>Dowry, 1979</REF>; <TREF>Moens and Steedman, 1988</TREF>; <REF>Pustejovsky, 1991</REF>~~~However, previous work that numerically evaluates aspectual classification has looked at verbs in isolation <REF>Klavans and Chodorow, 1992</REF>; <REF>Siegel, 1997</REF>~~~10 The verb have is particularly problematic.
For example, I made a fire is culminated, whereas, I gazed at the sunset is non-culminated~~~Aspectual classification is necessary for interpreting temporal modifiers and assessing temporal entailments <TREF>Moens and Steedman, 1988</TREF>; <REF>Dorr, 1992</REF>; <REF>Klavans, 1994</REF>, and is therefore a necessary component for applications that perform certain language interpretation, summarization, information retrieval, and machine translation tasks~~~Aspectual classification is a diflqcult problem because many verbs, like have, are aspectually ambiguous~~~In this paper, I demonstrate that verbs can be disambiguated according to aspect by the semantic category of the direct object.
Finally, Section 8 provides conclusions and describes future work~~~2 Aspect in Natural Language Aspectual classification is a key component of models that assess temporal constraints between clauses <TREF>Moens and Steedman, 1988</TREF>; <REF>Hwang and Schubert, 1991</REF>; <REF>Dorr, 1992</REF>; <REF>Hitzeman et al , 1994</REF>~~~For example, stativity must be identified to detect temporal constraints between clauses connected with when~~~For example, in interpreting I, I She had good strength when objectively tested.
In 26i, the event is associated with the features d,t,-a, whereas, in 26ii the event is associated with the features d,t,a~~~According to <REF>Bennett et al , 1990</REF> in the spirit of <TREF>Moens and Steedman, 1988</TREF>, predicates are allowed to undergo an atomicity coercion in which an inherently non-atomic predicate such as dio may become atomic under certain conditions~~~These conditions are language-specific in nature, ie, they depend on the lexical-semantic structure of the predicate in question~~~Given the featural scheme that is imposed on top of the lexical-semantic framework, it is easy to specify coercion functions for each language.
In light of these observations, the lexicM-semantic structure adopted for UNITRAN is an augmented form of Jackendoffs representation in which events are distinguished from states as before, but they are further subdivided into activities, achievements, and accomplishments~~~The subdivision is achieved by means of three features proposed by <REF>Bennett et al , 1990</REF> following the framework of <TREF>Moens and Steedman, 1988</TREF> in the spirit of <REF>Dowty, 1979</REF> and <REF>Vendler, 1967</REF>: dynamic ie , events vs states, as in the Jackendoff framework, :t:telic i e, culminative events transitions vs nonculminative events activities, and atomic ie , point events vs extended events~~~This featural system is imposed on top of the lexical-semantic framework proposed by Jackendoff~~~For example, the primitive GO would be annotated with the features d,t,-a for the verb destroy, but d,t,a for the verb obliterate, thus providing the appropriate distinction for cases such as 12.
Figure 2 relates the four types of lexical-semantic frameworks outlined above~~~Note that the system of features proposed by <REF>Bennett et al , 1990</REF> and <TREF>Moens and Steedman, 1988</TREF> provide the finest tuning given that five distinct categories of predicates are identified by the feature settings~~~This system is essentially equivalent to the Dowty/Vendler proposal, but features are used to distinguish the categories more precisely~~~In the next section, we will see how the tense and aspect structure described in section 21 and the lexicM-semantic representation described in this section are combined to provide the framework for generating a target-language surface form.
2b John finished drawing the circle~~~<REF>Dowty 1986</REF> and <TREF>Moens and Steedman 1988</TREF> decisively questioned the coherence of the class of achievement verbs, arguing that not all of them are non-durative~~~As noted above, Vendler identifies punctual events through the conjunction of the positive at and negative finish tests~~~However, they do not always yield comparable results : 3a 3b 4a 4b Karpov beat Kasparov at 1000 PM The Allies beat Germany at I000 PM  Karpov finished beating Kasparov The Allies finished beating Germany.
They trigger a change-of-state COS, henceforth, result states RSs, henceforth being entailments of CoSs~~~<TREF>Moens and Steedman 1988</TREF>, <REF>Smith 1991</REF>, <REF>Pustejovsky 1995</REF>, and others argue that it is a defining property of telic events~~~They should therefore include an undergoer argument, whose CoS determines the telicity of the event ie , it acts as a measuring-out argument~~~<REF>Tenny 1987</REF> thus claims that telic events require such an argument, which she calls an affected argument.
The subdivision is achieved by means of three features proposed by Bennett etal~~~1990 following the framework of <TREF>Moens and Steedman 1988</TREF>: -t-dynamic ie , events vs states, as in the Jackendoff framework, telic ie , culminative events transitions vs noneulminative events activities, and -I-atomic ie , point events vs extended events~~~We impose this system of features on top of the current lexical-semantic framework~~~For example, the lexical entry for all three verbs, ransack, obliterate, and destroy, would contain the following lexical-semantic representation: 6 Event CAUSE Thing X, Event GOLoc Thing X, Position TOLoc X John, Property DESTROYED The three verbs would then be distinguished by annotating this representation with the aspectual features d,t,-a for the verb ransack, d,t,-a for the verb destroy, and d,t,a for the verb obliterate, thus providing the appropriate distinction for cases such as 4.
SUMMARY This paper has examined a two-level knowledge representation model for machine translation that integrates aspectual information based on theories by <REF>Bach 1986</REF>, <REF>Comrie 1976</REF>, <REF>Dowty 1979</REF>, mourelatos 1981, <REF>Passonneau 1988</REF>, Pustejovsky 1988, 1989, 1991, and <REF>Vendler 1967</REF>, and more recently by Bennett et al~~~1990 and <TREF>Moens and Steedman 1988</TREF>, with lexicalsemantic information based on Jackendoff 1983, 1990~~~We have examined the question of cross-linguistic applicability showing that the integration of aspect with lexical-semantics is especially critical in machine translation when there are a large number of temporal connectives and verbal selection/realization possibilities that may be generated from a lexical semantic representation~~~Furthermore, we have illustrated that the selection/realization processes may be parameterized, by means of selection charts and coercion functions, so that the processes may operate uniformly across more than one language.
S: Juan le dio una puflaJada a Marls John gave a knife-wound to Mary S: Juan le dio pufialadas a Marls John gave knife-wounds to Mary b Duratlve Divergence, E: John met/knew Mary 4 S: Juan coaoci6 a Marls John met Mary S: Juan conoci a Mrfa John knew Merit Figure 1: Three Levels of MT Divergences et el~~~1990 have examined aspect and verb semantics within the context of machine translation in the spirit of <TREF>Moens and Steedman 1988</TREF>~~~This paper borrows from, and extends, these ideas by demonstrating how this theoretical framework might be adapted for crosslinguistic applicability~~~The framework has been tested within the context of an interlingual machine translation system and is currently being used as the basis for extraction of aspectual information from corpora.
Although there have been quite a few studies on individual aspects of sentence planning, little attention has been paid to the interaction between the various tasks--exceptions are <REF>Rambow and Korelsky 1992</REF> and <REF>Wanner and Hovy 1996</REF>--and in particular to the role of marker choice in the overall sentence planning process~~~There exists a large body of research in NLU on analysing the temporal structure of texts, including the role of temporal markers, though again restricted to English <TREF>Moens and Steedman 1988</TREF>; <REF>Lascarides and Oberlander 1993</REF>; <REF>Hitzeman et al 1995</REF>~~~We turn to these studies when it comes to identifying the information that needs to be assembled for representing temporal markers~~~3 Linguistic perspective: Describing temporal markers Selecting an appropriate German temporal marker given two events in a temporal relationship requires detailed knowledge of the semantic, pragmatic and syntactic properties that characterize temporal markers.
edu Abstract Verbal and compositional lexical aspect provide the underlying temporal structure of events~~~Knowledge of lexical aspect, eg, atelicity, is therefore required for interpreting event sequences in discourse <REF>Dowty, 1986</REF>; <TREF>Moens and Steedman, 1988</TREF>; <REF>Passoneau, 1988</REF>, interfacing to temporal databases <REF>Androutsopoulos, 1996</REF>, processing temporal modifiers <REF>Antonisse, 1994</REF>, describing allowable alternations and their semantic effects <REF>Resnik, 1996</REF>; <REF>Tenny, 1994</REF>, and selecting tense and lexical items for natural language generation <REF>Dorr and Olsen, 1996</REF>; <REF>Klavans and Chodorow, 1992</REF>, cf~~~<REF>Slobin and Bocaz, 1988</REF>~~~We show that it is possible to represent lexical aspect--both verbal and compositional--on a large scale, using Lexical Conceptual Structure LCS representations of verbs in the classes cataloged by <REF>Levin 1993</REF>.
Finally, we illustrate how knowledge of lexical aspect facilitates the interpretation of events in NLP applications~~~Knowledge of lexical aspect--how verbs denote situations as developing or holding in time--is required for interpreting event sequences in discourse <REF>Dowty, 1986</REF>; <TREF>Moens and Steedman, 1988</TREF>; <REF>Passoneau, 1988</REF>, interfacing to temporal databases <REF>Androutsopoulos, 1996</REF>, processing temporal modifiers <REF>Antonisse, 1994</REF>, describing allowable alternations and their semantic effects <REF>Resnik, 1996</REF>; <REF>Tenny, 1994</REF>, and for selecting tense and lexical items for natural language generation Dorr and Olsen~~~1996: <REF>Klavans and Chodorow, 1992</REF>, cf~~~<REF>Slobin and Bocaz, 1988</REF>.
How does 37b get its interpretation~~~As with 36d, the relevant elements of 37b can be represented as   then R   after S  turn right on County Line   e 3 :turn-rightyou, county line and the unresolved interpretation of 37b is thus  xafterx, EVe 3  aftere 3, EV 560 Computational Linguistics Volume 29, Number 4 As for resolving EV, in a well-known article, <TREF>Moens and Steedman 1988</TREF> discuss several ways in which an eventuality of one type eg , a process can be coerced into an eventuality of another type eg , an accomplishment, which Moens and Steedman call a culminated process~~~In this case, the matrix argument of then the eventuality of turning right on County Line can be used to coerce the process eventuality in 37b into a culminated process of going west on Lancaster Avenue until County Line~~~We treat this coercion as a type of associative or bridging inference, as in the examples discussed in section 31.
The alternatives arise when more than one event can be used~~~The temporal ontology is based on a recent theory of temporal semantics developed by <TREF>Moens and Steedman 1988</TREF>~~~This allows a modular representation of the semantics of temporal adverbials like until and by, and also aids in the generation of tense and aspect~~~This system looks at the mechanics of how the alternatives can be generated from the initial data, but we will have less to say about choosing between them.
21 Aspectual Categories of Verbs A number of aspectually oriented lexical-semantic representations have been proposed~~~Ve adopt and extend the feature-based framework proposed by <REF>Bennett et al , 1990</REF> in the spirit of <TREF>Moens and Steedman, 1988</TREF>~~~They uses three features: dynamic, telic, and atomic~~~We add two more features: process and gradual.
In principle, it is possible for this second module to detect aspectual transformations that apply to any input clause, independent of the manner in which the core constituents interact to produce its fundamental aspectual class~~~600 Siegel and McKeown Improving Aspectual Classification 26 Applications of Aspectual Classification Aspectual classification is a required component of applications that perform natural language interpretation, natural language generation, summarization, information retrieval, and machine translation tasks <TREF>Moens and Steedman 1988</TREF>; <REF>Klavans and Chodorow 1992</REF>; <REF>Klavans 1994</REF>; <REF>Dorr 1992</REF>; <REF>Wiebe et al 1997</REF>~~~These applications require the ability to reason about time, ie, temporal reasoning~~~Assessing temporal relationships is a prerequisite for inferring sequences of medical procedures in medical domains.
598 Siegel and McKeown Improving Aspectual Classification Table 3 Several aspectual entailments~~~If a clause occurring: necessarily entails: then it must be: in past progressive tense as argument of stopped in simple present tense past tense reading past tense reading the habitual reading Nonculminated Event Nonculminated Event or State Event 23 Interpreting Temporal Connectives and Modifiers Several researchers have developed models that incorporate aspectual class to assess temporal constraints between connected clauses <REF>Hwang and Schubert 1991</REF>; <REF>Schubert and Hwang 1990</REF>; <REF>Dorr 1992</REF>; <REF>Passonneau 1988</REF>; <TREF>Moens and Steedman 1988</TREF>; <REF>Hitzeman, Moens, and Grover 1994</REF>~~~For example, stativity must be identified to detect temporal constraints between clauses connected with when~~~For example, in interpreting, 7 She had good strength when objectively tested.
An understanding system can recognize the aspectual transformations that have affected a clause only after establishing the clauses fundamental aspectual category~~~Linguistic models motivate the division between a module that first detects fundamental aspect and a second that detects aspectual transformations <REF>Hwang and Schubert 1991</REF>; <REF>Schubert and Hwang 1990</REF>; <REF>Dorr 1992</REF>; <REF>Passonneau 1988</REF>; <TREF>Moens and Steedman 1988</TREF>; <REF>Hitzeman, Moens, and Grover 1994</REF>~~~In principle, it is possible for this second module to detect aspectual transformations that apply to any input clause, independent of the manner in which the core constituents interact to produce its fundamental aspectual class~~~600 Siegel and McKeown Improving Aspectual Classification 26 Applications of Aspectual Classification Aspectual classification is a required component of applications that perform natural language interpretation, natural language generation, summarization, information retrieval, and machine translation tasks <TREF>Moens and Steedman 1988</TREF>; <REF>Klavans and Chodorow 1992</REF>; <REF>Klavans 1994</REF>; <REF>Dorr 1992</REF>; <REF>Wiebe et al 1997</REF>.
Some aspectual markers such as the pseudocleft and many manner adverbs test for intentional events in particular not all events in general, and therefore are not compatible with all events, eg, I died diligently~~~Aspectual coercion such as iteration can allow a punctual event to appear in the progressive, eg She was sneezing for a week point  process  culminated process 4 <TREF>Moens and Steedman 1988</TREF>~~~The predictive power of some indicators is uncertain, since several measure phenomena that are not linguistically constrained by any aspectual category, eg, the present tense, durative for-PPs, frequency and notnever indicators~~~Therefore, the predictive power of individual linguistic indicators is incomplete; only the subset of verbs that adhere to the respective constraints or trends can be correctly classified.
the second sentence describes a state, which begins before the event described by the first sentence~~~Aspectual classification is also a necessary prerequisite for interpreting certain adverbial adjuncts, as well as identifying temporal constraints between sentences in a discourse <TREF>Moens and Steedman 1988</TREF>; <REF>Dorr 1992</REF>; <REF>Klavans 1994</REF>~~~In addition, it is crucial for lexical choice and tense selection in machine translation <TREF>Moens and Steedman 1988</TREF>; <REF>Klavans and Chodorow 1992</REF>; <REF>Klavans 1994</REF>; <REF>Dorr 1992</REF>~~~Table 1 sunnarizes the three aspectual distinctions, which compose five aspectual categories.
Some aspectual auxiliaries also perform an aspectual transformation of the clause they modify, eg, 11 I finished staring at it culminated process~~~Aspectual coercion, a second type of aspectual transformation, can take place when a clause is modified by an aspectual marker that violates an aspectual constraint <TREF>Moens and Steedman 1988</TREF>; <REF>Pustejovsky 1991</REF>~~~In this case, an alternative interpretation of the clause is inferred which satisfies the aspectual constraint~~~For example, the progressive marker is constrained to appear with an extended event.
E-mail: evscscolumbiaedu t Computer Science Dept , 1214 Amsterdam Ave , New York, NY 10027~~~E-mail: kathycscolumbiaedu  2001 Association for Computational Linguistics Computational Linguistics Volume 26, Number 4 Aspectual classification is necessary for interpreting temporal modifiers and assessing temporal entailments <TREF>Moens and Steedman 1988</TREF>; <REF>Dorr 1992</REF>; <REF>Klavans 1994</REF> and is therefore a required component for applications that perform certain natural language interpretation, generation, summarization, information retrieval, and machine translation tasks~~~Each of these applications requires the ability to reason about time~~~A verbs aspectual category can be predicted by co-occurrence frequencies between the verb and linguistic phenomena such as the progressive tense and certain temporal modifiers <REF>Klavans and Chodorow 1992</REF>.
Aspectual classification is also a necessary prerequisite for interpreting certain adverbial adjuncts, as well as identifying temporal constraints between sentences in a discourse <TREF>Moens and Steedman 1988</TREF>; <REF>Dorr 1992</REF>; <REF>Klavans 1994</REF>~~~In addition, it is crucial for lexical choice and tense selection in machine translation <TREF>Moens and Steedman 1988</TREF>; <REF>Klavans and Chodorow 1992</REF>; <REF>Klavans 1994</REF>; <REF>Dorr 1992</REF>~~~Table 1 sunnarizes the three aspectual distinctions, which compose five aspectual categories~~~In addition to the two distinctions described in the previous section, atomicity distinguishes punctual events eg , She noticed the picture on the wall from extended events, which have a time duration eg , She ran to the store.
Therefore, if it appears with an atomic event, eg, 12 He hiccupped point, the event is transformed to an extended event, eg, 13 He was hiccupping process~~~in this case with the iterated reading of the clause <TREF>Moens and Steedman 1988</TREF>~~~25 The First Problem: Fundamental Aspect We define fundamental aspectual class as the aspectual class of a clause before any aspectual transformations or coercions~~~That is, the fundamental aspectual category is the category the clause would have if it were stripped of any and all aspectual markers that induce an aspectual transformation, as well as all components of the clauses pragmatic context that induce a transformation.
Aspect in Natural Language Because, in general, the sequential order of clauses is not enough to determine the underlying chronological order, aspectual classification is required for interpreting 596 Siegel and McKeown Improving Aspectual Classification Table 1 Aspectual classes~~~This table is adapted from Moens and Steedman 1988, p 17~~~Culminated Nonculminated EVENTS punctual extended CULMINATION CULMINATED PROCESS recognize build a house POINT PROCESS hiccup run, swim, walk STATES understand even the simplest narratives in natural language~~~For example, consider: 1 Sue mentioned Miami event.
THE IMPERFECTIVE PARADOX AND TRAJECTORY-OF-MOTION EVENTS  Michael White Department of Computer and Information Science University of Pennsylvania Philadelphia, PA, USA mwhit el inc c is upenn, edu Abstract In the first part of the paper, I present a new treatment of THE IMPERFICTIVE PARADOX <REF>Dowty 1979</REF> for the restricted case of trajectoryof-motion events~~~This treatment extends and refines those of <TREF>Moens and Steedman 1988</TREF> and <REF>Jackendoff 1991</REF>~~~In the second part, I describe an implemented algorithm based on this treatment which determines whether a specified sequence of such events is or is not possible under certain situationally supplied constraints and restrictive assumptions~~~Bach 1986:12 summarizes THE IMPERFECTIVE PARADOX <REF>Dowty 1979</REF> as follows: how can we characterize the meaning of a progressive sentence like la 17 on the basis of the meaning of a simple sentence like lb 18 when la can be true of a history without lb ever being true.
<REF>White 1993</REF>~~~5Much as in <TREF>Moens and Steedman 1988</TREF> and <REF>Jackendoff 1991</REF>, the introduction of gr is necessary to avoid having an ill-sorted formula~~~284 the manner of motion supplied by a verb, it does nevertheless permit one to consider factors such as the normal speed as well as the meanings of the prepositions 10, lowards, etc By making two additional restrictive assumptions, namely that these events be of constant velocity and in one dimension, I have been able to construct and implement an algorithm which determines whether a specified sequence of such events is or is not possible under certain situationally supplied constraints~~~These constraints include the locations of various landmarks assumed to remain stationary and the minimum, maximum, and normal rates associated with various manners of motion eg running, jogging for a given individual.
Capitalizing on Bachs insight, I present in the first part of the paper a new treatment of the imperfective paradox which relies on the possibility of having actual events standing in the part-of relation to hypothetical super-events~~~This treatment extends and refines those of <TREF>Moens and Steedman 1988</TREF> and <REF>Jackendoff 1991</REF>, at least for the restricted case of trajectory-of-motion events~~~1 In particular, the present treatment correctly accounts not only for what 2a fails to entail -namely, that John eventually reaches the museum -but also for what 2a does in fact entail -namely, that John follows by jogging at least an initial part of a path that leads to the museum~~~In the second part of the paper, I briefly describe an implemented algorithm based on this theoretical treatment which determines whether a specified sequence of trajectory-of-motion is or is not possible under certain situationally supplied constraints and restrictive assumptions.
1 SonyCorp~~~hasheavilypromotedtheVideoWalkman since the products introduction last summer, 2 but Bob Gerson, video editor of This Week in Consumer Electronics, says 3 Sony conceives of 8mm as a family of products, camcorders and VCR decks,  SE classification is a fundamental component in determining the discourse mode of texts <REF>Smith, 2003</REF> and, along with aspectual classification, for temporal interpretation <TREF>Moens and Steedman, 1988</TREF>~~~It may be useful for discourse relation projection and discourse parsing~~~Though situation entities are well-studied in linguistics, they have received very little computational treatment.
The imperfective point of view brought by IMP imposes a change of point of view on the term of the eventuality~~~As for accomplishments, we can assume that they can be decomposed into several stages, according to <TREF>Moens and Steedman, 1988</TREF>: first  preparatory phase, second a cuhnination or achievement we are not concerned here with the result state~~~We can then say that IMP refers only to the preparatory phase, so that the term of the eventuality loses all relevance~~~This explains the so-called imperfective paradox: it is possible to use IMP even though the eventnality never reaches its term: 6 a I1 traversait la rue quand la voiture la ras6 He was crossing the street when the car hit him b  I1 traversa la rue quand la voiture la 6cras6 Ile crossed the street when the car hit him As for achievements, we can assume that they are reduced to a culmination.
Three machine learning methods are compared for this task: decision tree induction, a genetic algorithm, and log-linear regression~~~The ability to distinguish states, eg, Mark seems happy, from events, eg, Rende ran down the street, is a necessary prerequisite for interpreting certain adverbial adjuncts, as well as identifying temporal constraints between sentences in a discourse <TREF>Moens and Steedman, 1988</TREF>; <REF>Doff, 1992</REF>; <REF>Klavans, 1994</REF>~~~Furthermore, stativity is the first of three fundamental temporal distinctions that compose the aspectual class of a clause~~~Aspectual classification is a necessary component for a system that analyzes temporal constraints, or performs lexical choice and tense selection in machine translation <TREF>Moens and Steedman, 1988</TREF>; <REF>Passonneau, 1988</REF>; <REF>Doff, 1992</REF>; <REF>Klavans, 1994</REF>.
Furthermore, stativity is the first of three fundamental temporal distinctions that compose the aspectual class of a clause~~~Aspectual classification is a necessary component for a system that analyzes temporal constraints, or performs lexical choice and tense selection in machine translation <TREF>Moens and Steedman, 1988</TREF>; <REF>Passonneau, 1988</REF>; <REF>Doff, 1992</REF>; <REF>Klavans, 1994</REF>~~~Researchers have used empirical analysis of corpora to develop linguistically-based numerical indicators that aid in aspectual classification <REF>Klavans and Chodorow, 1992</REF>; <REF>Siegel and McKeown, 1996</REF>~~~Specifically, this technique takes advantage of linguistic constraints that pertain to aspect, eg, only clauses that describe an event can appear in the progressive.
And that they should be seen as the result of an attempt to take a good metaphor too literally~~~<TREF>Moens and Steedman 1988</TREF> conceived temporal adverbials as functions which coerce their inputs to the appropriate type, by a loose sic~~~analogy with type-coercion in programming languages~~~Under this perspective, aspectual shift is triggered by a conflict between the aspectual type of the situation to be modified and the aspectual constraint set by the temporal preposition heading the modifier1 Coercion operators, then, are thought to adapt the verbal input on the level of model-theoretical interpretation by mapping one sort of situation onto another.
By using TAGs we get the additional benefit of an existing parser that yields derivations and derived trees fiom which we can construct the compositional semantics of a given sentence~~~We decompose each event E into a tripartite structure in a manner similar to <TREF>Moens and Steedman 1988</TREF>, introducing a time function for each predicate to specify whether the predicate is true in the preparatory dringE, cuhnination erdE, or consequent resll:E stage of an event~~~hfitial trees capture tile semantics of the basic senses of verbs in each class~~~For example, many IThese restrictions are more like preferences that generate a preferred reading of a sentence.
However, this incompleteness is also a consequence of the linguistic characteristics of various indicators~~~For example:  Aspectual coercion such as iteration compromises indicator measurements <TREF>Moens and Steedman, 1988</TREF>~~~For example, a punctual event appears with the progressive in, She was sneezing for a week~~~point --, process --.
For example, I made a fire is culminated, since a new state is introduced something is made, whereas, I gazed at the sunset is non-culminated~~~Aspectual classification is necessary for interpreting temporal modifiers and assessing temporal entailments <REF>Vendler, 1967</REF>; <REF>Dowty, 1979</REF>; <TREF>Moens and Steedman, 1988</TREF>; <REF>Dorr, 1992</REF>, and is therefore a necessary component for applications that perform certain natural language interpretation, natural language generation, summarization, information retrieval, and machine translation tasks~~~Aspect introduces a large-scale, domaindependent lexical classification problem~~~Although an aspectual lexicon of verbs would suffice to classify many clauses by their main verb only, a verbs primary class is often domaindependent <REF>Siegel, 1998b</REF>.
112 Table 1: Aspectual classes~~~This table comes from Moens and Steedman <TREF>Moens and Steedman, 1988</TREF>~~~Culm EVENTS STATES punctual extended CULM CULM PROCESS recognize build a house NonPOINT PROCESS Culm hiccup run, swim understand 2 Aspect in Natural Language Table 1 summarizes the three aspectual distinctions, which compose five aspectual categories~~~In addition to the two distinctions described in the previous section, atomicity distinguishes events according to whether they have a time duration punctual versus extended.
This can effect not only the semantic interpretation of the text itself, but also translation and the choice of adverb~~~3 3Many of these issues are discussed in the CL Special Issue on Tense and Aspect <REF>June, 1988</REF> in articles by Hinniche, Moens and Steedman, Nakhimovsky, Passoneau, and Webber~~~For example, Passoneau demonstrates how, without an ccurate specification of the pectual tendencies of the verb coupled with the effect of temporal and aspectual adjuncts, messages, which tend to be in the present tense, ttre not correctly understood nor generated in the PUNDIT system~~~For instance, the pressure is low must be interpreted at statlve, whereas the pump operates must be interpreted as a process.
It is also clear that events are not undifferentiated masses, but rather have subparts that can be picked out by the choice of phrase type or the addition of adverbial phrases~~~<TREF>Moens  Steedman 1988</TREF> identify three constituents to an event nucleus, a preparatory process, culmination, and consequent state, whereas <REF>Nakhimovsky 1988</REF> identifies five: preparatory, initial, body, final, result, exemplified by the following: 1 15~~~When the children crossed the road, a they waited for the teacher to give a signal b they stepped onto its concrete surface as if it were about to swallow them up~~~c they were nearly hit by a car d they reached the other side stricken with fear.
I will call this level the event structure of a word cf~~~<REF>Pustejovsky 1991</REF>; <TREF>Moens and Steedman 1988</TREF>~~~The event structure of a word is one level of the semantic specification 419 Computational Linguistics Volume 17, Number 4 for a lexical item, along with its argument structure, qualia structure, and inheritance structure~~~Because it is recursively defined on the syntax, it is also a property of phrases and sentences.
Weather would seem selfcontained, but change, creation and stative are not semantic fields at all~~~Stative belongs to the Aktionsart categorisation of verbs distinguishing it from verbs of activity, achievement and accomplishment, which is orthogonal to the categorisation of verbs into semantic fields <REF>Vendler, 1967</REF>, <TREF>Moens  Steedman 1988</TREF>, <REF>Amaro, 2006</REF>~~~Moreover, a verb can belong to more than one Aktionsart category, as these apply to verbs in contexts~~~33 Suggested Revision of Categories Among verbs, the level of arbitrariness and incorrectness of the WordNet categories seems greater than that of the relations.
Differences in annotation could be due to the differences in interpretations of the event; however, we found that the vast majority of radically different judgments can be categorized into a relatively small number of classes~~~Some of these correspond to aspectual features of events, which have been intensively investigated eg , <REF>Vendler, 1967</REF>; <REF>Dowty, 1979</REF>; <TREF>Moens and Steedman, 1988</TREF>; <REF>Passonneau, 1988</REF>~~~We then developed guidelines to cover those cases see the next section~~~22 Event Classes Action vs State: Actions involve change, such as those described by words like speaking, gave, and skyrocketed.
Events of type eat differ from those of type run in the way the result d is brought about~~~This can be illustrated by means of the notion of a nucleus-structure Moens/<REF>Steedman 1988</REF>~~~A nucleus-structure consists of three parts: the inception-point IP, the development-portion DP and the culmination-point CP~~~Nucleus-Structure e S S  IP DP CP Figure 1 The result  is evaluated at each part of the nucleus-structure.
Further, we do not have a theory of the interaction of temporal interpretation with aspect~~~<REF>See Dowty, 1979</REF>; <REF>Dowty, 1986</REF>; <REF>Moens, 1987</REF>; <TREF>Moens and Steedman, 1988</TREF>; <REF>Nakhimovsky, 1988</REF>; and <REF>Passoneau, 1988</REF>~~~found in these works~~~Section 5 provides a more detailed comparison with <REF>Yip 1986</REF> and <REF>Hornstein 1990</REF>.
The decisive units for this selection are phases and boundaries <REF>Bickel, 1996</REF>~~~Presuming a tripartite event structure <TREF>Moens and Steedman, 1988</TREF> consisting of a preparation phase dynamic phase  dyn , a culmination point boundary  and a consequent state static phase  stat , there are three possibilities for aspect to select~~~English and Turkish both have  dyn -selecting aspectual markers, Turkish also a marker for explicit  stat selection; Russian pf aspect explicitly selects ~~~The unmarked members of the aspectual oppositions may assert anything else  Russian ipf aspect may assert anything but the explicit selection of a boundary.
2 For the problem with multi-sentence discourses, and the threads that sentences continue, we use an implementation of temporM centering <REF>Kameyama et al , 1993</REF>; <REF>Poesio, 1994</REF>~~~This is a technique similar to the type of centering used for nominal anaphora <REF>Sidner, 1983</REF>; <TREF>Grosz et al , 1983</TREF>~~~Centering assumes that discourse understanding requires some notion of aboutness~~~While nominal centering assumes there is one object that the current discourse is about, temporal centering assumes that there is one thread that the discourse is currently following, and that, in addition to tense and aspect constraints, there is a preference for a new utterance to continue a thread which has a parallel tense or which is semantically related to it and a preference to continue the current thread rather than switching to another thread.
Passonneau to appear examined some of the few claims relating discourse anaphoric noun phrases to global discourse structure in the Pear corpus~~~Resuits included an absence of correlation of segmental structure with centering <TREF>Grosz et al , 1983</TREF>; <REF>Kameyama, 1986</REF>, and poor correlation with the contrast between full noun phrases and pronouns~~~As noted in <REF>Passonneau and Litman, 1993</REF>, the NP features largely reflect Passonneaus hypotheses that adjacent utterances are more likely to contain expressions that corefer, or that are inferentially linked, if they occur within the same segment; and that a definite pronoun is more likely than a full NP to refer to an entity that was mentioned in the current segment, if not in the previous utterance~~~33 Evaluation The segmentation algorithms presented in the next two sections were developed by examining only a training set of narratives.
Any communicative act, be it spoken, written, gestured, or system-initiated, can give rise to DEs~~~As a discourse progresses, an adequate discourse model must represent the relevant entities, and the relationships between them <REF>Grosz and Sidner, 1986</REF>, A speaker may then felicitously refer anaphorically to an object subject to focusing or centering constraints <TREF>Grosz et al , 1983</TREF>, <REF>Sidner 1981, 1983</REF>, <REF>Brennan et al 1987</REF>  if there is an existing DE representing it, or if a corresponding DE may be directly inferred from an existing DE~~~For example, the utterance Every senior in Milford High School has a car gives rise to at least 3 entities, describable in English as the seniors in Milford High School, Milford High School, and the set of cars each of which is owned by some senior in Milford High School~~~These entities may then be accessed by the following next utterances, respectively: They graduate in June.
This is, to our knowledge, the first implementation of Webbers DE generation ideas~~~We designed the 243 algorithms and structures necessary to generate discourse entities from our logical representation of the meaning of utterances, and from pointing gestures, and currently use them in Januss <REF>Weischedel et al , 1987</REF>, BSN, 1988 pronoun resolution component, which applies centering techniques <TREF>Grosz et al , 1983</TREF>, <REF>Sidner 1981, 1983</REF>, <REF>Brennan et al 1987</REF> to track and constrain references~~~Janus has been demonstrated in the Navy domain for DARPAs Fleet Command Center Battle Management Program FCCBMP, and in the Army domain for the Air Land Battle Management Program ALBM~~~2 Meaninq Representation for DE Generation Webber found that appropriate discourse entities could be generated from the meaning representation of a sentence by applying rules to the representation that are strictly structural in nature, as long as the representation reflects certain crucial aspects of the sentence.
Its identification needs peech act cal;egorization of sentences~~~This topic-based approach is in contrast to Kameyamas,Japanese version <REF>Kameyama 1985</REF>, <REF>Kameyama 1986</REF> of tbcus-based spproach to anaphora by <TREF>Grosz et al 1983</TREF>~~~In her framewock, subjecthood and predicate deixis play the principal role, and the fact that topic provides the most important clue to anaphora identification in actual spoken Japanese discourse is not utilized eexplicitly~~~,-L3 Extension of topic introduction One of the pobems with the topicobased approach is that topics reerred to by zero pronouns are not always e:pliitiy marked by the topic postposition wa.
The parser can be modified to simulate other types of machines such LRk-like or SLR-like automata~~~It can also be extended to handle unification based grammars using a similar method as that employed by <TREF>Shieber 1985</TREF> for extending Earleys algorithm~~~Furthermore, the algorithm can be tuned to a particular grammar and therefore be made more efficient by carefully determinizing portions of the nondeterministic machine while making sure that the number of states in not increased~~~These variants lead to more efficient parsers than the one based on the basic non-deterministic push-down machine.
Finally note that in cases where substantial material has to be supplied, as it were, by the target grammar eg if a transitive verb is supplied but no object, then Definition 3 would allow arbitrary lexicalisations, giving rise to a very large number of permissible outputs~~~If this is felt to be problem, then estricting in the sense of <TREF>Shieber 1985</TREF> the subsumption test in the second half of Definition 3 to ignore the values of certain features, ie pred, would bepstraight-forward~~~This would have the effect of producing a single, exemplary lexicalisation for each significantly different ie different ignoring differences under pred structure which satisfies the minimaximal requirements~~~II4 A Problem with the Mini-maximal Approach One potential problem clearly arises with this approach.
So, top-down constraints must be weakened in order for parsing to be guaranteed to terminate~~~In order to solve the nontermination problem, <TREF>Shieber 1985</TREF> proposes restrictor, a statically predefined set of features to consider in propagation, and restriction, a filtering function which removes the features not in restrictor from top-down expectation~~~However, not only does this approach fail to provide a method to automatically generate the restrictor set, it may weaken the predicative power of top-down expectation more than necessary: a globally defined restrictor can only specify the least common features for all propagation paths~~~In this paper, a general method of maximizing top-down constraints is proposed.
Manual detection is also problematic: when a grammar is large, particularly if semantic features are included, complete detection is nearly impossible~~~As for the techniques developed so far which partially solve prediction nontermination eg , <TREF>Shieber 1985</TREF>; <REF>Haas 1989</REF>; <REF>Samuelsson 1993</REF>, they do not apply to nonminimal derivations because nonminimal derivations may arise without left recursion or recursion in general s One way is to define p to filter out all features except the context-free backbone of predictions~~~However, this severely restricts the range of possible instantiations of Shiebers algorithm~~~9 A third possibility is to manually fix the grammar so that nonminimal derivations do not occur, as we noted in Section 4.
However, simple subsumption may filter out valid parses for some grammars, thus sacrificing completeness~~~7 Another possibility is to filter out problematic features in the Prediction step by using the function p However, automatic detection of such features ie , automatic derivation of p is undecidable for the same reason as the prediction nontermination problem caused by left recursion for unification grammars <TREF>Shieber 1985</TREF>~~~Manual detection is also problematic: when a grammar is large, particularly if semantic features are included, complete detection is nearly impossible~~~As for the techniques developed so far which partially solve prediction nontermination eg , <TREF>Shieber 1985</TREF>; <REF>Haas 1989</REF>; <REF>Samuelsson 1993</REF>, they do not apply to nonminimal derivations because nonminimal derivations may arise without left recursion or recursion in general s One way is to define p to filter out all features except the context-free backbone of predictions.
1~~~Unification grammar is a term often used to describe a family of feature-based grammar formalisms, including GPSG <REF>Gazdar et al 1985</REF>, PATR-II <REF>Shieber 1986</REF>, DCG <REF>Pereira and Warren 1980</REF>, and HPSG <REF>Pollard and Sag 1994</REF>~~~In an effort to formalize the common elements of unification-style grammars, <REF>Shieber 1992</REF> developed a logic for describing them, and used this logic to define an abstract parsing algorithm~~~The algorithm uses the same set of operations as Earleys 1970 algorithm for context-free grammars, but modified for unification grammars.
It is more so because 1 there is no restriction such as that there should be only one zero morpheme within an S clause, and 2 the stack is useless because zero morphemes are independent morphemes and are not bound to other morphemes comparable to wh-words~~~<TREF>Shieber 1985</TREF> proposes a more efficient approach to gaps in the PATR-II formalism, extending Earleys algorithm by using restriction to do top-down filtering~~~While an approach to zero morphemes similar to Shiebers gap treatment is possible, we can see one advantage of ours~~~That is, our approach does not depend on what kind of parsing algorithm we choose.
However, if features are carefully selected so as to increase the amount of pruning done by the chart, the net effect may 583 Computational Linguistics Volume 19, Number 4 be that even though the grammar allows more types of constituents, the chart may end up with fewer instances~~~It is interesting to compare this technique to the restriction proposal in <TREF>Shieber 1985</TREF>~~~Both approaches select functional features to be moved forward in processing order in the hope that some processing will be pruned~~~Shiebers approach changes the processing order of functional constraints so that some of them are processed top-down instead of bottom-up.
For the experiments discussed in the final section all goal-weakening operators were chosen by hand, based on small experiments and inspection of the goal table and item table~~~Even if goal weakening is reminiscent of Shiebers 1985 restriction operator, the rules of the game are quite different: in the case of goal weakening, as much information as possible is removed without risking nontermination of the parser, whereas in the case of Shiebers restriction operator, information is removed until the resulting parser terminates~~~For the current version of the grammar of OVIS, weakening the goal category in such a way that all information below a depth of 6 is replaced by fresh variables eliminates the problem caused by the absence of the occur check; moreover, this goal-weakening operator reduces parsing times substantially~~~In the latest version, we use different goal-weakening operators for each different functor.
Depending on the properties of a particular grammar, it may, for example, be worthwhile to restrict a given category to its syntactic features before attempting to solve the parse-goal of that category~~~Shiebers 1985 restriction operator can be used here~~~Thus we essentially throw some information away before an attempt is made to solve a memorized goal~~~For example, the category xA, B, f A, B, gA,hB, i C   may be weakened into: xA,B,f ,,g, If we assume that the predicate weaken/2 relates a term t to a weakened version tw, such that tw subsumes t, then 15 is the improved version of the parse predicate: parsewithweakening Cat, P0, P, E0, E  15 weakenCat,WeakenedCat, parseWeakenedCat,P0,P,E0,E, CatWeakenedCat.
Therefore, we generally cannot use all information available in the grammar but rather we should compute a weakened version of the linking table~~~This can be accomplished, for example, by replacing all terms beyond a certain depth by anonymous variables, or by other restrictors <TREF>Shieber 1985</TREF>~~~Secondly, the use of a linking table may give rise to spurious ambiguities~~~Consider the case in which the category we are trying to parse can be matched against two different items in the linking table, but in which case the predicted head-category may turn out to be the same.
One can obtain similar results for the class of grammars whose context-free backbone is finitely ambiguous--what <REF>Pereira and Warren 1983</REF> called the offline-parsable grammars~~~However, as <TREF>Shieber 1985b</TREF> observed, this class of grammars excludes many linguistically interesting grammars that do not use atomic category symbols~~~230 The present parser as opposed to the table-building algorithm is much like those in the literature~~~Like nearty all parsers using term unification, it is a special case of Earley deduction <REF>Pereira and Warren 1985</REF>.
<REF>Sato and Tamaki 1984</REF> proposed to analyze the behavior of Prolog programs, including parsers, by using something much like a weak prediction table~~~To guarantee that the table was finite, they restricted the depth of terms occurring in the table <TREF>Shieber 1985b</TREF> offered a more selective approach--his program predicts only those features chosen by the user as most useful for prediction~~~<REF>Pereira and Shieber 1987</REF> discuss both approaches~~~We will present a variation of Shiebers ideas that depends on using a sorted language.
Any general parsing method for definite clause grammar will enter an infinite loop in some cases, and it is the task of the grammar writer to avoid this~~~Generalized phrase structure grammar avoids the problem because it has only the formal power of context-free grammar <REF>Gazdar et al 1985</REF>, but according to <TREF>Shieber 1985a</TREF> this is not adequate for describing human language~~~Lexical functional grammar employs a better solution~~~A lexical functional grammar must include a finitely ambiguous context-free grammar, which we will call the context-free backbone <REF>Barton 1987</REF>.
We then define the set of terms in a standard way~~~All unification in this paper is unification of terms, as in <REF>Robinson 1965</REF>--not graphs or other structures, as in much recent work <TREF>Shieber 1985b</TREF>~~~A unification grammar is a five-tuple G  S, ,r T, P, Z where S is a set of sorts, ,r an S-ranked alphabet, T a finite set of terminal symbols, and Z a function letter of arity e in ,r~~~Z is called the start symbol of the grammar the standard notation is S not Z, but by bad luck that conflicts with standard notation for the set of sorts.
One could devise more elaborate examples, but this one suffices to make the point: not every natural unification grammar has an obvious context-free backbone~~~Therefore it is useful to have a parser that does not require us to find a context-free backbone, but works directly on a unification grammar <TREF>Shieber 1985b</TREF>~~~We propose to guarantee that the parsing problem is solvable by restricting ourselves to depth-bounded grammars~~~A unification grammar is depth-bounded if for every L > 0 there is a D > 0 such that every parse tree for a sentential form of L symbols has depth less than D In other words, the depth of a tree is bounded by the length of the string it derives.
The grammar is depth-bounded because the depth of a tree is a linear function of the length of the string it derives~~~A similar grammar can derive the crossed serial dependencies of Swiss German, which according to <TREF>Shieber 1985a</TREF> no context-free grammar can derive~~~It is clear where the extra formal power comes from: a contextfree grammar has a finite set of nonterminals, but a unification grammar can build arbitrarily large nonterminal symbols~~~It remains to show that there is a parsing algorithm for depth-bounded unification grammars.
Some additional penalty may also have been incurred by not using dotted grammar rules to generate reductions, as in standard leftcorner parsing algorithms~~~2 There are important differences between the technique for limited prediction in this parser, and other techniques for limited prediction such as Shiebers notion of restriction <TREF>Shieber, 1985</TREF> which we also use~~~In methods such as Shiebers, predictions are weakened in ways that can result in an overall gain in efficiency, but predictions nevertheless must be dynamically generated for every phrase that is built bottom-up~~~In our log version 314.
In addition, the parser maintains a skeletal copy of the chart in which edges are labeled only by the nonterminal symbols contained in their context-free backbone, which gives us more efficient indexing of the full grammar rules~~~Other optimizations include using one-word look-ahead before adding new predictions, and using restrictors <TREF>Shieber, 1985</TREF> to increase the generality of the predictions~~~Comparison with Other Parsers Table 1 compares the average number of edges, average number of predictions, and average parse times 1 in seconds per utterance for the limited 1All parse times given in this paper were produced on a Sun SPARCstation 10/51, running Quintus Pro111 For grammar with start symbol , phrase structure rules P, lexicon L, context-independent categories CI, and context-dependent categories CD; and for word string w  wlwn: Variant Edges Preds Secs Bottom-Up 1191 0 146 Limited Left-Context 203 25 10 Left-Corner 112 78 40 Table h Comparison of Syntax-Only Parsers if  E CD, predictT, 0; addemptycategories 0 ; for i from I to n do foreach C such that C--wi EL do addedgetochartC, i-i, i ; makenewpredictionsC, ii, i ; findnew-reductionsC, il,i end addemptycategories i ; end sub findmew-reductionsB, j, k  foreach A and a such that A- B 6 P do foreach i such that i  match, j do if A 6 CD and predictedA,i or A 6 CI addedgetochartA, i, k; makenewpredictionsA, i, k ; findnewreductionsA, i, k ; end end  sub addemptycategoriesi  foreach A such that A - e E P do if A 6 CD and predictedA,/ or A 6 CI addedgetochartA, i, i ; makenewpredictionsA, i, i ; findnewreductionsA, i, i ; end  sub makenewpredictionsA, i, j  foreach Aft E Predictionsi do predict fl, j end foreach H - ABfl 6 P such that H 6 CI and B E CD and fl 6 CI do predict B, j end foreach H -- AB 6 P such that H E CD and B E CD and fl E CI and predictedH, i or H left-corner-of C and predictedC, i do predict B, j end Figure 1: Limited Left-Context Algorithm left-context parser with those for a variant equivalent to a bottom-up parser when all categories are context independent and for a variant equivalent to a left-corner parser when all categories are context dependent~~~The tests were performed on a set of 194 utterances chosen at random from the ARPA ATIS corpus MADCOW, 1992, using a broad-coverage syntactic grammar of English having 84 coverage of the test set.
The situation becomes more complicated when we move to unification-based grammars, since there may be an unbounded number of different categories appearing in the accessible stack states~~~In the system implemented here we used restriction <TREF>Shieber, 1985</TREF> on the stack states to restrict attention to a finite number of distinct stack states for any given stack depth~~~Since the restriction operation maps a stack state to a more general one, it produces a finite-state approximation which accepts a superset of the language generated by the original unification grammar~~~Thus for general constraint-based grammars the language accepted by our finite-state approximation is not guaranteed to be either a superset or a subset of the language generated by the input grammar.
Since the size of the state sets possible with finite partitioning is now finite, the algorithm always terminates~~~After establishing a correspondence between attribute and unification grammar UG, we may see that the technique of restriction used by <TREF>Shieber 1985</TREF> in his extended algorithm is related to finite partitioning on attribute domains, in fact a particular case which takes advantage of the more structured attribute domains of UG~~~For attribute grammar, given that the domains involved are more general eg , the integers, finite partitioning is the required device~~~5.
1~~~Earleys 1970 algorithm is a general algorithm for context-free languages, widely used in natural language processing <REF>King, 1983</REF>; <TREF>Shieber, 1985</TREF> and syntactic pattern recognition <REF>Fu, 1982</REF>, where the full generative power of context-free grammar is required~~~The original algorithm and its common implementations, however, assume the atomic symbols of context-free grammars, thus limiting its applicability to systems with attributed symbols, or attribute grammars <REF>Knuth, 1968</REF>~~~Attribute grammar is an elegant formalization of the augmented context-free grammars characteristic of most current NLP systems.
However, their particular realization of the technique is severely restricted for NLP applications, since it uses a deterministic one-path LR algorithm, applicable only to semantically unambiguous grammars~~~<REF>Pereira and Warren 1983</REF> and <TREF>Shieber 1985</TREF> present v6rsions of Earleys algorithm for unification grammars, in which unification is the sole operation responsible for attribute evaluation~~~However, given the high computational cost of unification, important differences between attribute and unification grammars in their respective attribution domains and functions Correa, forthcoming, and the more general nature of attribute grammars in this regard, it is of interest to investigate the extension of Earleys algorithm directly to the main subclasses of attribute grammar~~~The paper is organized as follows: Section 2 presents pieliminary elements, including a definition of attribute grammar and Earleys algorithm.
Attribute grammar is an elegant formalization of the augmented context-free grammars characteristic of most current NLP systems~~~It is more general than members of the family of unification-based grammar formalisms <REF>Kay, 1985</REF>; <REF>Shieber, 1986</REF>, mainly in that it allows and encourages the use of simpler attribution functions than unification for the definition of attribute values, and hence can lead to computationally efficient grammatical definitions, while maintaining the advantages of a well-understood declarative formalism~~~Attribute grammar has been used in the past by the author to define computational models of Chomskys Government-binding theory, from which practical parsing programs were developed <REF>Correa, 1987a</REF>~~~Many systems based on Earleys algorithm have a clear division between the phases of syntactic and semantic analysis.
The parsing problem for offline parsable grammars ts solvable~~~Yet these grammars apparently have enough formal power to describe natural language at least, they can describe the crossed-serial dependencies of Dutch and Swiss German, which are presently the most widely accepted example of a construction that goes beyond context-free grammar <TREF>Shieber 1985a</TREF>~~~Suppose that the variable M ranges over integers, and the function letter s denotes the successor function~~~Consider the rule 1 pM --- psM A grammar containing this rule cannot be offline parsable, because erasing the arguments of the top-level terms in the rule gives 2 p --- p which immediately leads to infinite ambiguity.
These ideas can be generalized to other forms of unification~~~Consider dag unification as in <TREF>Shieber 1985b</TREF>~~~Given a set S of sorts, assign a sort to each label and to each atomic dag~~~The arity of a label is a set of sorts not a sequence of sorts as in term unification.
This does not deny that compilation methods may be able to convert a grammar into a program that generates without termination problems~~~In fact, the partial execution techniques described by two of us <REF>Pereira and Shieber 1985</REF> could form the basis of a compiler built by partial execution of the new algorithm we propose below relative to a grammar~~~However, the compiler will not generate a program that generates top-down, as Strzalkowskis does~~~v c,k,mj V mj I zag V k,m V e,k saw  helpen voeren help feed Figure 2 Schematic of Verb Subeategorization Lists for Dutch Example.
But even this ad hoc solution is problematic, as there may be no principled bound on the size of the subcategorization list~~~For instance, in analyses of Dutch cross-serial verb constructions <REF>Evers 1975</REF>; <REF>Huybrechts 1984</REF>, subcategorization lists may be concatenated by syntactic rules Moort32 Computational Linguistics Volume 16, Number 1, <REF>March 1990</REF> Shieber et al Semantic Head-Driven Grammar gat 1984; <REF>Fodor et al 1985</REF>; <REF>Pollard 1988</REF>, resulting in arbitrarily long lists~~~Consider the Dutch sentence dat Jan Marie de oppasser de olifanten zag helpen that John Mary the keeper the elephants saw help voeren feed that John saw Mary help the keeper feed the elephants The string of verbs is analyzed by appending their subcategorization lists as in Figure 2~~~Subcategorization lists under this analysis can have any length, and it is impossible to predict from a semantic structure the size of its corresponding subcategorization list merely by examining the lexicon.
The symptom is an ordering paradox in the sorting~~~For example, the complements rule given by <TREF>Shieber 1985a</TREF> in the PATR-II formalism VP 1 -- VP 2 X VPl head  VP2 head VP2 syncat first  X <VP2 syncat rest  VPI syncat can be encoded as the DCG rule: vpHead, Syncat/VP ->,Head, Compl/LFlSyncat/VP, Compl/LF~~~Top-down generation using this rule will be forced to expand the lower VP before its complement, since LF is uninstantiated initially~~~Any of the reordering methods must choose to expand the child VP node first.
where the cat i are terms representing the grammatical category of an expression and its subconstituents~~~Terminal symbols are introduced into rules by enclosing them in list brackets, for example sbar/S --> that, s/S Such rules can be translated into Prolog directly using a difference list encoding of string positions; we assume readers are familiar with this technique <REF>Pereira and Shieber, 1985</REF>~~~Because we concentrate on the relationship between expressions in a language and their logical forms, we will assume that the category terms have both a syntactic and a semantic component~~~In particular, the infix function symbol / will be used to form categories of the form Syn/Sem where Syn is the syntactic category of the expression and Sere is an encoding of its semantics as a logical form; the previous rule uses this notation, for example.
This is the correlate of the link relation used in left-corner parsers with top-down filtering; we direct the reader to the discussion by Matsumoto et al~~~1983 or <REF>Pereira and Shieber 1985</REF> for further information~~~applicable  non  chain  ruleRoot, Pivot, RHS : semantics ofroot andpivot ere serae node semanticsRoot, Sem, node semanticsPivot, Sere,  choose a nonchain rue non  chain  ruloLHS, RHS,   whose lhs matches the pivot unifyPivot, LHS,  make sttre tile categories can connect chained nodesPivot, Root~~~A chain rule is applicable to connect a pivot to a root if the pivot can serve as the semantic head of the rule and the left-hand side of the rule is appropriate for linking to the root.
The reason is simple~~~Consider, for example, a grammar with a gap-threading treatment of wh-movement <REF>Pereira 1981</REF>; <REF>Pereira and Shieber 1985</REF>, which might include the rule npAffr, npAgr/SemJX-X/Sem -->  ~~~stating that an NP with agreement Agr and semantics Sere can be empty provided that the list of gaps in the NP can be represented as the difference list npAgr/SemlX-X, that is, the list containing an NP gap with the same agreement features Agr~~~Because the above rule is a nonchain rule, it will be considered when trying to generate any nongap NP, such as the proper noun np3-sing, G-G/john.
Tile algorithm described in this paper is an attempt to resolve these problems in a satisfactory manner~~~Although we believe that this algorithm could be seen as an instance of a uniform architecture for parsing and generation--just as tile extended Earley parser <TREF>Shieber, 1985b</TREF> and the bottom-up generator were instances of the generalized Earley deduction architecture--our efforts to date have been aimed foremost toward the development of the algorithm for generation alone~~~We will mention efforts toward this end in Section 5~~~11 APPLICABILITY OF THE ALGORITHM As does the Earley-based generator, the new algorithm assumes that the grammar is a unification-based or logic grammar with a phrase structure backbone and complex nonterminals.
441 Identifying Functional Strata Manually Normally, the grammarian knows which information needs to be made explicit~~~Hence, instead of differentiating between the linguistic strata sYN and SEM, we let the linguist identify which constraints filter and which only serve as a means for representation; see also <TREF>Shieber, 1985</TREF>~~~In contrast to the separation along linguistic levels, this approach adopts a functional view, cutting across linguistic strata~~~On this view, the syntactic constraints together with, eg, semantic selection constraints would constitute a subgrammar.
Though theoretically very attractive, codescription has its price: i the grammar is difficult to modularize due to the fact that the levels constrain each other mutually and ii there is a computational overhead when parsers use the complete descriptions~~~Problems of these kinds which were already noted by <TREF>Shieber, 1985</TREF> motivated tile research described here~~~The goal was to develop more flexible ways of using codescriptive grammars than having them applied by a parser with full informational power~~~The underlying observation is that constraints in such grammars can play different roles:  Genuine constraints which relate directly to tile grammaticality wellformedness of the input.
Furthermore, the need to perform nondestructive unification means that a large proportion of the processing time is spent copying feature structures~~~One approach to this problem is to refine parsing algorithms by developing techniques such as restrictions, structure-sharing, and lazy unification that reduce the amount of structure that is stored and hence the need for copying of features structures <TREF>Shieber, 1985</TREF>; <REF>Pereira, 1985</REF>; <REF>Karttunen and Kay, 1985</REF>; <REF>Wroblewski, 1987</REF>; <REF>Gerdemann, 1989</REF>; <REF>Godden, 1990</REF>; <REF>Kogure, 1990</REF>; <REF>Emele, 1991</REF>; <REF>Tomabechi, 1991</REF>; <REF>Harrison and Ellison, 1992</REF>~~~While these techniques can yield significant improvements in performance, the generality of unification-based grammar formalisms means that there are still cases where expensive processing is unavoidable~~~This approach does not address the fundamental issue of the tradeoff between the descriptive capacity of a formalism and its computational power.
Original Earley prediction works on category symbols~~~An answer to these problems was presented by <TREF>Shieber 1985</TREF> who proposed to do Earley prediction on the basis of some finite quotient of all constituent DAGs which can be specified by the grammar writer~~~Another example for the influence of the CUG efforts on the development of PATR is a new template notation introduced by Lauri Karttunen in his Interlisp-D version of PATR~~~Since categorial grammars exhibit an extensive embedding of categories within other categories, it is useful to unify templates not only with the whole lexical DAG but also with its categorial subgraphs.
The sohltion to this problem is to define a finite number of equivalence classes into which the infinite uumber of nnnterminals inay be sorted~~~Fhese ,lasses may be established in a number of ways; the one we have adopted in that presented by Harrison and Ellison,  992 which builds on l;he work of <TREF>Shieber, 1985</TREF>: it introduces the nol;ion of a negative restrictor to define equivalence classes~~~In this solution a predefined portion of a category a specific set of paths is discarded when determining whether a category belongs to an equivalence :lass or not~~~For instance, in the above example we could define the negative restrictor to be orth.
It turns out to be the case that only in this way the effect of top-down filtering will pay-off against the increased overhead of having to check the left-corner table~~~6 Some Results The performance of the parsing algorithms discussed in the preceding sections a bottom-up parser for UG BU, a top-down parser for UG of <TREF>Shieber, 1985</TREF> TD, a top-down parser operating on an instantiated grammar TD/1, and a bottom-up parser with topdown filtering operating on an instantiated grammar BU/LC were tested on two experimental CUGs, one implementing the morphosyntactic features of German N Ps, and one implementing the syntax of WH-questions in Dutch by means of a gap-threading mechanism~~~Some illustrative results are listed in Tables 1 and 2~~~Sentencel Sentence2 items sees items sees TD: 93 59 160 105 TD/I: 45 20 68 25 BU: 68 20 120 30 Bu/ c: 12 o6 53 o9 Table1: German For German, an ideal restrictor R was < l > II  cat,val, arg, or dir.
In terms of parse times the two algorithms are almost equivalent~~~Comparing our results with those of <TREF>Shieber 1985</TREF> and <REF>Haas 1989</REF>, we see that in all cases top-down filtering may reduce the size of the chart significantly~~~<REF>Whereas Haas 1989</REF> found that top-down filtering never helps to actually decrease parse times in a bottom-up parser, we have found at least one example German where top-down filtering is useful~~~183 7 Conclusions There is a trend in modern linguistics to replace grammars that are completely language specific by grammars which combine universal rules and principles with language specific parameter settings, lexicons, etc This trend can be observed in such diverse frameworks as Lexical Functional Grammar, Government-Binding Theory, Head-driven Phrase Structure Grammar and Categorial Grammar.
CHART PARSING OF UNIFICATION GRAMMAR UG~~~Parsing methods for context-free grammar can be extended to unification-based grammar formalisms see <TREF>Shieber, 1985</TREF> or <REF>Haas, 1989</REF>, and therefore they can in principle be used to parse CUG~~~A chart-parser scans a sentence from left to right, while entering items, representing partial derivations, in a chart~~~Assume that items are represented as Prolog terms of the form itemBegin, End, LH S, Parsed, ToParse, where LHS is a feature-structure and Parsed and ToParse contain lists of feature-structures.
Contrary to bottomup parsing, however, the adaptation of a top-down algorithm for UG requires some special care~~~For UGs which lack a so-called context-free back-bone, such as CUG, the top-down prediction step can only be guaranteed to terminate if we make use of restriction, as defined in <TREF>Shieber 1985</TREF>~~~Top-down prediction with a restrictor R where R is a finite set of paths through a feature-structure amounts to the following: Restriction The restriction of a feature-structure F relative to a restrictor R is the most specific feature-structure F  E F, such that every path in F j has either an atomic value or is an element of R Predictor Step For each item, End, LHS, Parsed, Next I ToParse such that Rjve, is the restriction of Next relative to R, and each rule RNe:t  RHS, add itemi,i, Rge:t, , RHS~~~Restriction can be used to develop a top-down chart parser for CUG in which the top-down prediction step terminates.
The algorithm described in this paper is an attempt to resolve these problems in a satisfactory manner~~~Although we believe that this algorithm could be seen as an instance of a uniform architecture for parsing and generation--just as the extended Earley parser <TREF>Shieber, 1985b</TREF> and the bottom-up generator were instances of the generalized Earley deduction architecture our efforts to date have been aimed foremost toward the development of the algorithm for generation alone~~~We will have little to say about its relation to parsing, leaving such questions for later research1 2 Applicability of the Algorithm As does the Earley-based generator, the new algorithm assumes that the grammar is a unificationbased or logic grammar with a phrase-structure backbone and complex nonterminMs~~~Furthermore, and again consistent with previous work, we assume that the nonterminals associate to the phrases they describe logical expressions encoding their possible meanings.
This is the correlate of the link relation used in left-corner parsers with topdown filtering; we direct the reader to the discussion by Matsumoto et al~~~1983 or Pereira and Shieber 1985, p 182 for further information~~~applicablenonchainrule Root, Pivot, RHS :7o semantics of root and pivot are same nodesemantics Root, Sem, nodesemanticsPivot, Sem, o choose a nonchain rule nonehainrulerHS, RttS, whose lhs matches the pivot unifyPivot, LHS, make sure the categories can connect chainednodesPivot, Root~~~A chain rule is applicable to connect a pivot to a root if the pivot can serve as the semantic head of the rule and the left-hand-side of the rule is appropriate for linking to the root.
In particular, in order to derive a finite CF grammar, we will need to consider only those features that have a finite number of possible values, or at least consider only finitely many of the possible values for infinitely valued features~~~We can use the technique of restriction <TREF>Shieber 1985</TREF> to remove these features from our feature structures~~~Removing these features may give us a more permissive language model, but it will still be a sound approximation~~~The experimental results reported in this paper are based on a grammar under development at RIACS for a spoken dialogue interface to a semi-autonomous robot, the Personal Satellite Assistant PSA.
The situation becomes more complicated when we move to unification-based grammars, since there may be an unbounded number of different categories appearing in the accessible stack states~~~In the system implemented here we used restriction <TREF>Shieber, 1985</TREF> on the stack states to restrict attention to a finite number of distinct stack states for any given stack depth~~~Since the restriction operation maps a stack state to a more general one, it produces a finite-state approximation which accepts a superset of the language generated by the original unification grammar~~~Thus for general constraint-based grammars the language accepted by our finite-state aptroximation is not guaranteed to be either a superset or a subset of the language generated by the input grammar.
The rule builds up infinitely large subcategorization lists of which eventually only one is to be matched against the subcategorization list of, eg, the lexical entry for buys~~~Though this rule is not cyclic, it becomes cyclic upon off-line abstraction: magicvp VForm, CSem I3, SSem : magicvp VForm, CSem2l, SSem  Through trimming this magic rule, eg, given a bounded term depth <REF>Sato and Tamaki, 1984</REF> or a restrictor <TREF>Shieber, 1985</TREF>, constructing an abstract unfolding tree reveals the fact that a cycle results from the magic rule~~~This information can then be used to discard the culprit~~~312 Indexing Removing the direct or indirect cycles from the magic part of the compiled grammar does eliminate the necessity of subsumption checking in many cases.
As a result of the explicit representation of filtering we do not need to postpone abstraction until run-time, but can trim the magic predicates off-line~~~One can consider this as bringing abstraction into the logic as the definite clause representation of filtering is weakened such that only a mild form of connectedness results which does not affect completeness <TREF>Shieber, 1985</TREF>~~~Consider the following magic rule: magicvpVForm, CgemlArgs, SSem :magicvp VForm, Args, SSem  This is the rule that is derived from the headrecursive vp rule when the partially specified subcategorization list is considered as filtering information cf~~~, fn.
More specifically, magic generation falls prey to non-termination in the face of head recursion, ie, the generation analog of left recursion in parsing~~~This necessitates a dynamic processing strategy, ie, memoization, extended with an abstraction function like, eg, restriction <TREF>Shieber, 1985</TREF>, to weaken filtering and a subsumption check to discard redundant results~~~It is shown that for a large class of grammars the subsumption check which often influences processing efficiency rather dramatically can be eliminated through fine-tuning of the magic predicates derived for a particular grammar after applying an abstraction function in an off-line fashion~~~Unfolding can be used to eliminate superfluous filtering steps.
In the categorial grammar example only x/3 goals are memoized and thus only these goals incur the cost of table management~~~The abstraction step, which is used in most memoizing systems including complex feature grammar chart parsers where it is somewhat confusingly called restriction, as in <TREF>Shieber 1985</TREF>, receives an elegant treatment in a CLP approach; an abstracted goal is merely one in which not all of the equality constraints associated with the variables appearing in the goal are selected with that goal~~~2 For example, because of the backward application rule and the left-to-right evaluation our parser uses, eventually it will search at every left string position for an uninstantiated category the variable Y in the clause, we might as well abstract all memoized goals of the form xC, L, R to x, L, , ie, goals in which the category and right string position are uninstantinted~~~Making the equality constraints explicit, we see that the abstracted goal is obtained by merely selecting the underlined subset of these below: xXl,X2, X3,Xl  C, X2  L, Xa  R While our formal presentation does not discuss abstraction since it can be implemented in terms of constraint selection as just described, because our implementation uses the underlying Prologs unification mechanism to solve equality constraints over terms, it provides an explicit abstraction operation.
24 Top-Down Predictive Linking The aim of our proposal is to define equivalence relations that keep the linking relation finite while also preventing it from being too restrictive; this turns the linking relation into a weakpredietion table in the sense of Haas 1989: 227ff~~~Like Shieber 1985, 1992 with the notion of restriction, we confine our attention to a subset of specifications; in particular, we can define a feature structure that subsumes all VP-type feature structures of Shiebers recursive subcategorization rules~~~But unlike Shieber, our restrictors are computed automatically by building the generalization of the occurrences ofleftrecursive categories in a grammar~~~The intuitive idea is that we consider categories to be left recursive if their tokens can be unified rather than being identical, as in the case of atoms; we then use their generalization, or greatest lower bound, as a common denominator defining an equivalence relation.
A better solution, which we have adopted from <REF>Kilbury 1990</REF>, is to introduce rule numbers, which are then used to define a purely filtering linking relation~~~This amounts to the simplest case of the restriction technique of <TREF>Shieber 1985</TREF>~~~Only when a link between numerical pointers is first found is the linking relation between feature structures used to instantiate information~~~3 Consequences of Predictive Linking What is the advantage of predictive linking as discussed above in 2.
Such formalisms typically include a contextfree CF base, which allows the use of parsing algorithms designed for CF languages despite the fact that complex-feature-based formalisms are essentially more powerful than CF grammars~~~However, such an adaptation of CF algorithms involves their extension to possibly infinite nonterminal domains, which, as <TREF>Shieber 1985</TREF> and <REF>Haas 1989</REF> have shown, is nontrivial~~~Various CF algorithms make use of a binary relation between a goal category and the category of a constituent phrase or word which either has just been parsed or is to be parsed next~~~Different terms have been used to designate this relation; <REF>Kay 1980</REF> speaks of reachability, while Pereira/<REF>Shieber 1987</REF> and others before them use the term linking for the relation.
1990 have discussed similar techniques in the context of semantichead-driven generation, we are concerned here with parsing~~~We view the linking relation not simply as a filter to increase efficiency within the domain of syntactic analysis--this aspect is stressed by <TREF>Shieber 1985</TREF> and other investigators such as <REF>Bouma 1991</REF>--but rather as a device for the top-down predictive instantiation of information, as Shieber et al~~~1990 have shown for semantic-head-driven generation~~~In this paper we are concerned especially with morphosyntactic information and illustrate the relevance of predictive linking for morphological analysis and for the analysis of unknown or new lexical items.
Whatever term one takes, an important aspect of the relation is that it can be used to reduce the search space of possible syntactic analyses at an earlier point in parsing and thus serves to improve the efficiency of a parser~~~Shieber 1985, 1992 follows established terminology in speaking of top-down filtering in connection with the prediction step of the Earley algorithm~~~His central notion of restriction, whereby a restrictor is a finite subset of the paths specified in a feature structure, is related to the technique we introduce here, since both guarantee the finiteness of an otherwise possibly infinite domain of complex categories, but Shiebers restrictors are specified manually~~~We propose a general algorithmic method of compilation that avoids manual specification.
In particular, declaring certain category-valued features so that they cannot take variable values may lead to nontermination in the backbone construction for some grammars~~~However, it should be possible to restrict the set of features that are considered in category-valued features in an analogous way to Shiebers 1985 restrictors for Earleys 1970 algorithm, so that a parse table can still be constructed~~~36 Ted Briscoe and John Carroll Generalized Probabilistic LR Parsing 4~~~Building LR Parse Tables for Large NL Grammars The backbone grammar generated from the ANLT grammar is large: it contains almost 500 distinct categories and more than 1600 productions.
This requirement places a greater load on the grammar writer and is inconsistent with most recent unification-based grammar formalisms, which represent grammatical categories entirely as feature bundles eg <REF>Gazdar et al 1985</REF>; <REF>Pollard and Sag 1987</REF>; <REF>Zeevat, Calder, and Klein 1987</REF>~~~In addition, it violates the principle that grammatical formalisms should be declarative and defined independently of parsing procedure, since different definitions of the CF portion of the grammar will, at least, effect the efficiency of the resulting parser and might, in principle, lead to nontermination on certain inputs in a manner similar to that described by <TREF>Shieber 1985</TREF>~~~In what follows, we will assume that the unification-based grammars we are considering are represented in the ANLT object grammar formalism <REF>Briscoe et al 1987</REF>~~~This formalism is a notational variant of Definite Clause Grammar eg <REF>Pereira and Warren 1980</REF>, in which rules consist of a mother category and one or more daughter categories, defining possible phrase structure configurations.
In contrast to the symbols in context-free grammars, feature structures in unification-based grammars often include information encoding part of the derivation history, most notably semantics~~~In order to achieve successful packing rates, feature restriction <TREF>Shieber, 1985</TREF> is used to remove this information during creation of the packed parse forest~~~During the unpacking phase, which operates only on successful parse trees, these features are unified back in again~~~For their experiments with efficient subsumptionbased packing, <REF>Oepen and Carroll, 2000</REF> experimented with different settings of the packing restrictor for the English Resource Grammar ERG <REF>Copestake and Flickinger, 2000</REF>: they found that good packing rates, and overall good performance during forest creation and unpacking were achieved, for the ERG, with partial restriction of the semantics, eg keeping index features unrestricted, since they have an impact on external combinatorial potential, but restricting most of the internal MRS representation, including the list of elementary predications and scope constraints.
5 Choosing the Grammar Restrictor and Parsing Strategy In order for the subsumption relation to apply meaningfully to HPSG signs, two conditions must be met~~~Firstly, parse tree construction must not be duplicated in the feature structures by means of the HPSG DTRS feature but be left to the parser ie recorded in the chart; this is achieved in a standard way by feature structure restriction <TREF>Shieber, 1985</TREF> applied to all passive edges~~~Secondly, the processing of constraints that do not restrict the search space but build up new often semantic structure should be postponed, since they are likely to interfere with subsumption~~~For example, analyses that differ only with respect to PP attachment would have the same syntax, but differences in semantics may prevent them being packed.
Attention is restricted here to approximations of context-free grammars because context-free languages are the smallest class of formal language that can realistically be applied to the analysis of natural language~~~Techniques such as restriction <TREF>Shieber, 1985</TREF> can be used to construct context-free approximations of many unification-based formalisms, so techniques for constructing finite-state approximations of context-free grammars can then be applied to these formalisms too~~~2 Finite-state calculus A finite-state calculus or finite automata toolkit is a set of programs for manipulating finite-state automata and the regular languages and transducers that they describe~~~Standard operations include intersection, union, difference, determinisation and minimisation.
The ELU tormalism provides a generalization of the template facility of PATR-II, the relational abstractions, which are statements abstracting over sets of constraint equations~~~These statements 5 Restrictors are also used to restrict the search space in parsing see <TREF>Shieber 1985</TREF>~~~fbe use of linking information in generation was first proposed by van <REF>Noord 1988</REF>~~~may receive multiple and mcursive definitions.
Problems in the prediction step of the Earley parser used for unification-based formalisms no longer exist~~~The use of restrictors as proposed by <TREF>Shieber 1985</TREF> is no longer necessary and the difficulties caused by treating subcategorization as a feature is no longer a problem~~~By assuming that the number of structures associated with a lexical item is finite, since each structure has a lexical item attached to it, we implicitly make the assumption that an input string of finite length cannot be syntactically infinitely ambiguous~~~Since the trees are produced by the input string, the parser can use information that might be non-local to guide the search.
The resulting structures form equivalence classes, since they abstract from word-specific information, such as FORM or STEM~~~The abstraction is specified by means of a restrictor <TREF>Shieber, 1985</TREF>, the so-called lexicon restrictor~~~After that, the grammar rules are instantiated by unification, using the abstracted lexicon entries and resulting in derivation trees of depth 1~~~The rule restrictor is applied to each resulting feature structure FS, removing all information contained only in the daughters of a rule.
head-dtr syn  counter 1   Then, this can generate an infinite sequence of signs, each of which contains a part,  counter <bar, ba, r,,bar l and is not equivalent to any previously generated sign~~~In order to resolve this difficulty, we apply tim restriction <TREF>Shieber, 1985</TREF> to a rule schemata and a lexical entry, and split the feature structure F  fsR of a rule schema R or a lexical entry F  l, into two, namely, coreF and subF such that F  coreF U subF~~~The definition of the restriction here is given as follows~~~Definition 5 paths For arty node n in a feature structure F, pathsn,F is a set of all the paths that reaches n from the root of F Definition 6 Restriction Schema A restriction schema rs is a set of paths.
However, these models are inadequate for language interpretation, since they cannot express the relevant syntactic and semantic regularities~~~Augmented phrase structure grammar APSG formalisms, such as unification-based grammars <TREF>Shieber, 1985a</TREF>, can express many of those regularities, but they are computationally less suitable for language modeling, because of the inherent cost of computing state transitions in APSG parsers~~~The above problems might be circumvented by using separate grammars for language modeling and language interpretation~~~Ideally, the recognition grammar should not reject sentences acceptable by the interpretation grammar and it should contain as much as reasonable of the constraints built into the interpretation grammar.
procedure CLOSUREI; begin repeat for each item <AwBx> in I, and each production C-y such that C is unifiable with B and <CBy> is not in I do add <CB--,y> to I; until no more items can be added to I; return I end; procedure NEXT-SI,C for each category C that appears to the right ; of the dot in items begin let J be the set of items <A-wBx> such that <AwBx> is in I and B is unifiable with C; return CLOSUREJ end; Figure 6~~~Preliminary CLOSURE/NEXT-S Procedures The preliminary CLOSURE procedure Unifies the lhs of a predicted production, ie -71 Cy, and the category the prediction is made flom, ieB This approach is essentially top-down lrOlagation of instantiated features and well documented by <TREF>Shieber 1985</TREF> in the context of Earleys algorithm~~~A new item added to the state, <CB--,~~~y>, is not the production C--,y, but its partial instantiation, y is also instantiated to be y as a result of the unification CB if C and some members of y share tags.
That is, instantiation of productions introduces the nontermination problem of left-recursive productions to the procedure, as well as to the Predictor Step of Earleys algorithm~~~To overcome this problem, <TREF>Shieber 1985</TREF> proposes restrictor, which specifies a maximum depth of feature-based categories~~~When the depth of a category in a predicted item exceeds the limit imposed by a restrictor, further instantiation of the category in new items is prohibited~~~The Predictor Step eventually halts when it starts creating a new item whose feature specification within the depth allowed by the resuictor is identical to, or subsumed by, a previous one.
The situation is different for active chart items since daughters can affect their siblings~~~To be independent from a-certain grammatical theory or implementation, we use restrictors similar to <TREF>Shieber, 1985</TREF> as a flexible and easyto-use specification to perform this deletion~~~A positive restrictor is an automaton describing the paths in a feature structure that will remain after restriction the deletion operation, 3There are refinements of the technique which we have implemented and which in practice produce additional benefits; we will report these in a subsequent paper~~~Briefly, they involve an improvement to th e path collection method, and the storage of other information besides types in the vectors.
Estimates of the object-grammar size for typical systems vary from hundreds or thousands 3 up to trillions of rules <REF>Shieber 1983</REF>:4~~~With some formalisms, the context-free object-grammar approach is not even possible because the object grammar would be infinite <TREF>Shieber 1985</TREF>:145~~~Grammar size matters beyond questions of elegance and clumsiness, for it typically affects processing complexity~~~<REF>Berwick and Weinberg 1982</REF> argue that the effects of grammar size can actually dominate complexity for a relevant range of input lengths.
That is, for the reference determination, the subject roles of the candidates referent within a discourse segment will be checked intheflrstplace~~~Thisflndingsupportswell the suggestion in centering theory that the grammaticalrelationsshouldbeusedasthe key criteria to rank forward-looking centers in the process of focus tracking <TREF>Brennan et al , 1987</TREF>; <REF>Grosz et al , 1995</REF>~~~3~~~candi Pron and candi NoAntecedent are to be examined in the cases when the subject-role checking fails, which conflrms the hypothesis in the S-List model by <REF>Strube 1998</REF> that co-refereing candidates would have higher preference than other candidates in the pronoun resolution.
The S-List model <REF>Strube, 1998</REF>, for example, assumes that a co-referring candidate is a hearer-old discourse entity and is preferred to other hearer-new candidates~~~In the algorithms based on the centering theory <TREF>Brennan et al , 1987</TREF>; <REF>Grosz et al , 1995</REF>, if acandidateanditsantecedentarethebackwardlooking centers of two subsequent utterances respectively, the candidate would be the most preferred since the CONTINUE transition is always ranked higher than SHIFT or RETAIN~~~In this paper, we present a supervised learning-based pronoun resolution system which incorporates coreferential information of candidates in a trainable model~~~For each candidate, we take into consideration the properties of its antecedents in terms of features henceforth backward features, and use the supervised learning method to explore their in uences on pronoun resolution.
The first class is characterized by adaptations of previously known reference algonthms eg~~~<REF>Lappin and Leass, 1994</REF>, <TREF>Brennan et al , 1987</TREF> the scarce syntactic and semantic knowledge available m an w system eg~~~<REF>Kameyama, 1997</REF>~~~The second class is based on statistical and machine learning techniques that rely on the tagged corpora to extract features of the coreferential relations eg.
One of the most unusual features of centering theory is that the notions of utterance, previous utterance, ranking, and realization used in the definitions above are left unspecified, to be appropriately defined on the basis of empirical evidence, and possibly in a different way for each language~~~As a result, centering theory is best viewed as a cluster of theories, each of which specifies the parameters in a different ways: eg, ranking has been claimed to depend on grammatical function <REF>Kameyama, 1985</REF>; <TREF>Brennan et al , 1987</TREF>, on thematic roles <REF>Cote, 1998</REF>, and on the discourse status of the CFs <REF>Strube and Hahn, 1999</REF>; there are at least two definitions of what counts as previous utterance <REF>Kameyama, 1998</REF>; <REF>Suri and McCoy, 1994</REF>; and realization can be interpreted either in a strict sense, ie, by taking a CF to be realized in an utterance only if an NP in that utterance denotes that CF, or in a looser sense, by also counting a CF as realized if it is referred to indirectly by means of a bridging reference <REF>Clark, 1977</REF>, ie, an anaphoric expression that refers to an object which wasnt mentioned before but is somehow related to an object that already has, as in the vase the handle see, eg, the discussion in <REF>Grosz et al , 1995</REF>; <REF>Walker et al , 1998b</REF>~~~3 METHODS The fact that so many basic notions of centering theory do not have a completely specified definition makes empirical verification of the theory rather difficult~~~Because any attempt at directly annotating a corpus for utterances and their CBs is bound to force the annotators to adopt some specification of the basic notions of the theory, previous studies have tended to study a particular variant of the theory <REF>Di Eugenio, 1998</REF>; <REF>Kameyama, 1998</REF>; <REF>Passonneau, 1993</REF>; <REF>Strube and Hahn, 1999</REF>; <REF>Walker, 1989</REF>.
Another important factor in pronoun resolution is the grammatical role of the antecedent~~~The role hierarchy used in centering <TREF>Brennan et al , 1987</TREF>; <REF>Grosz et al , 1995</REF> ranks subjects over direct objects over indirect objects over others~~~<REF>Lappin and Leass 1994</REF> provide a more elaborate model which ranks NP complements and NP adjuncts lowest~~~Two other distinctions in their model express a preference of rhematic2 over thematic arguments: Existential subjects, which follow the verb, rank very high, between subjects and direct objects.
Similarly, we can assess other strategies of sentence ordering that have been proposed in the literature~~~Hard-core centering approaches only deal with the last sentence <TREF>Brennan et al , 1987</TREF>~~~In Negra, these approaches can consequently have at most a success rate of 442~~~Performance is particularly low with possessive pronouns which often only have antecedents in the current sentence.
While it appears that our existing linguistic bias set will be of use, we believe that the CBL system will benefit from additional linguistic biases~~~Centering constraints see <TREF>Brennan et al , 1987</TREF>, for example, can be encoded as linguistic biases and applied to the pronoun resolution task to increase system performance~~~Furthermore, we have focused on applying the linguistic bias approach to feature set selection for case-based learning algorithms only~~~In future work, we plan to investigate the use of the approach for feature selection in conjunction with other standard machine learning algorithms.
A number of studies have developed refinements and extensions of the theory eg~~~<TREF>Brennan et al , 1987</TREF>; <REF>Kameyama, 1986</REF>; <REF>Strube and Hahn, 1996</REF>; <REF>Walker et al , 1998</REF>, but few have attempted to extend the model to multi-party discourse cf~~~<REF>Brennan, 1998</REF>; <REF>Walker, 1998</REF>~~~For dialog systems, the benefits of using centering theory include improved reference resolution and generation of more coherent referring expressions.
Separately, the SPG also handles referring expression generation by converting proper names to pronouns when they appear in the previous utterance~~~The rules are applied locally, across adjacent sequences of utterances <TREF>Brennan et al , 1987</TREF>~~~Referring expressions are manipulated in the d-trees, either intrasententially during the creation of the sp-tree, or intersententially, if the full sp-tree contains any period operations~~~The third and fourth sentences for Alt 13 in Figure 4 show the conversion of a named restaurant Carmines to a pronoun.
Pragmatic level Working together, surface patterns and possessive relationships can deal with many PPAs found in our corpus, but we still have two problems to be solved: semantic ambiguity among two or more acceptable candidates and abstract anaphors/antecedents, which cannot be solved by simply applying possessive relationship rules~~~For these cases, and possibly for some other cases not included in previous rules, we suggest a pragmatic factor, adapted from S Brennans et al 1987 centering algorithm~~~Although sentence center plays a crucial role in many works in anaphor resolution, usually limiting the number of candidates to be considered, we notice that, because PPAs can refer to almost any NP in the sentence rather than, for example, personal pronouns, which are often related to the sentence center, pragmatic knowledge plays only a secondary but still important role in our approach~~~We have adapted basic aspects of center algorithm, considering subject/object preference, and domain concepts preference, 1012 suggested by R <REF>Mitkov 1996</REF>, aiming to estimate the most probable center for intrasentential PPAs.
In Section 3, I introduce my model, its only data structure, the S-list, and the accompanying algorithm~~~In Section 4, I compare the results of my algorithm with the results of the centering algorithm <TREF>Brennan et al , 1987</TREF> with and without specifications for complex sentences <REF>Kameyama, 1998</REF>~~~2 A Look Back: Centering The centering model describes the relation between the focus of attention, the choices of referring expressions, and the perceived coherence of discourse~~~The model has been motivated with evidence from preferences for the antecedents of pronouns <REF>Grosz et al , 1983</REF>; 1995 and has been applied to pronoun resolution Brennan et al.
Their approach has been proven as the point of departure for a new model which is valid for English as well~~~The use of the centering transitions in Brennan et als 1987 algorithm prevents it from being applied incrementally cf~~~<REF>Kehler 1997</REF>~~~In my approach, I propose to replace the functions of the backward-looking center and the centering transitions by the order among the elements of the list of salient discourse entities S-list.
RANK by transition orderings~~~To illustrate this algorithm, we consider example 1 <TREF>Brennan et al , 1987</TREF> which has two different final utterances ld and ld~~~Utterance ld contains one pronoun, utterance ld t two pronouns~~~We look at the interpretation of ld and ldt.
Table 1 illustrates the Pour transitions that are detined according to diese constraints~~~<TREF>Brennan et al , 1987</TREF></TREF> proposes a default ordering on transitions which correlates with discourse coherence: CONTINUE is preferred to RETAIN is prelbrred to SMOOTH-SHIFT is pre2The version of centering I presem; here is tiom <TREF>Brennan et al , 1987</TREF></TREF>~~~352 H CbUu  CbWn, Ct,Un 7 CtW,,~~~1 n CbU,  CtU, XNTINUI,; SMOOTII-SItlH CbU,,  CpU, IETAIN IIUIlI-SIIIFT Tatle 1: Ceutering Transitions ferred to IIIII-SIIllT.
Although they report that their method estimates over 90 of zero subjects correctly, there are several difficulties including the fact that the test corpus is identical with the corpus from which the pragmatic constraints are extracted, and the fact that there are so many rules46 rules to estimate 175 sentences~~~As for the identifying method available in general discourses, the centering theory<TREF>Brennan et al , 1987</TREF>; <REF>Walker et al , 1990</REF> and the property sharing theory<REF>Kameyama, 1988</REF> are proposed~~~The important feature of these theories is the fact that it is independent of the type of discourse~~~However, according to our experimental result, it seems that these kinds of theory do not estimate zero subjects in high precision for manual sentences 3.
This preference can be explained in terms of salience from the point of view of the centering theory~~~The latter proposes the ranking subject, direct object, indirect object <TREF>Brennan et al 1987</TREF> and noun phrases which are parts of prepositional phrases are usually indirect objects~~~Collocation pattern preference This preference is given to candidates which have an identical collocation pattern with a pronoun 2,0~~~The collocation preference here is restricted to the patterns noun phrase pronoun, verb and verb, noun phrase pronoun.
Grosz et al list seven such costraints, three of which can be directly evaluated~~~Even though we are not following here the distinction between constraints and rules introduced in <REF>Brennan, Friedman, and Pollard 1987</REF>, we will use for these three claims the names Brennan et al gave them, by which they are now best known: Constraint 1 Strong: All utterances of a segment except for the first have exactly one CB~~~Rule 1 GJW95: If any CF is pronominalized, the CB is Rule 2 GJW95: Sequences of continuations are preferred over sequences of retains, which are preferred over sequences of shifts~~~231 Constraint 1, Topic Uniqueness, and Entity Coherence.
162 In effect, we use this probability information to identify the topic of the segment with the belief that the topic is more likely to be referred to by a pronoun~~~The idea is similar to that used in the centering approach <TREF>Brennan et al , 1987</TREF> where a continued topic is the highest-ranked candidate for pronominalization~~~Given the above possible sources of informar tion, we arrive at the following equation, where Fp denotes a function from pronouns to their antecedents: Fp  argmaxP Ap  alp, h, l, t, l, so, d A where Ap is a random variable denoting the referent of the pronoun p and a is a proposed antecedent~~~In the conditioning events, h is the head constituent above p, l r is the list of candidate antecedents to be considered, t is the type of phrase of the proposed antecedent always a noun-phrase in this study, I is the type of the head constituent, sp describes the syntactic structure in which p appears, dspecifies the distance of each antecedent from p and M is the number of times the referent is mentioned.
A number of studies have developed refinements and extensions of the theory eg~~~Brennan et at, 1987; <REF>Kameyama, 1986</REF>; <REF>Strube and Hahn, 1996</REF>; <REF>Walker et al, 1998</REF>, but few have attempted to extend the model to mul party discourse cf~~~<REF>Brennan, 1998</REF>; <REF>Walker, 1998</REF>~~~For dialog systems, the benefits of using centering theory include improved reference resolution and generation of more coherent referring expressions.
In Section 3, I introduce my model, its only data structure, the S-list, and the accompanying algorithm~~~In Section 4, I compare the results of my algorithm with the results of the centering algorithm <TREF>Brennan et al, 1987</TREF> with and without specifications for complex sentences <REF>Kamcyama, 1998</REF>~~~2 A Look Back: Centering The centering model describes the relation between the focus of attention, the choices of referring expressions, and the perceived coherence of discourse~~~The model has been motivated with evidence from preferences for the antecedents of pronouns <REF>Grosz et al, 1983</REF>; 1995 and has been applied to pronoun resolution Brennan et al.
Their approach has been proven as the point of departure for a new model which is valid for English as well~~~The use of the centering transitions in Brennan et als 1987 algorithm prevents it from being applied incrementally cf~~~<REF>Kehler 1997</REF>~~~In my approach, I propose to replace the functions of the backward-looking center and the centering transitions by the order among the elements of the list of salient discourse entities S-list.
RANK by transition orderings~~~To illustrate this algorithm, we consider example 1 <TREF>Brennan et al, 1987</TREF> which has two different final utterances ld and ld~~~Utterance ld contains one pronoun, utterance ld t two pronouns~~~We look at the interpretation of ld and ld.
Mitkovs knowledge-poor pronoun resolution method <REF>Mitkov, 1998</REF>, for example, uses the scores from a set of antecedent indicators to rank the candidates~~~And centering algorithms <TREF>Brennan et al , 1987</TREF>; <REF>Strube, 1998</REF>; <REF>Tetreault, 2001</REF>, sort the antecedent candidates based on the ranking of the forward-looking or backwardlooking centers~~~In recent years, supervised machine learning approaches have been widely used in coreference resolution <REF>Aone and Bennett, 1995</REF>; <REF>McCarthy, 1996</REF>; <REF>Soon et al , 2001</REF>; <REF>Ng and Cardie, 2002a</REF>, and have achieved significant success~~~Normally, these approaches adopt a single-candidate model in which the classifier judges whether an antecedent candidate is coreferential to an anaphor with a confidence value.
Any communicative act, be it spoken, written, gestured, or system-initiated, can give rise to DEs~~~As a discourse progresses, an adequate discourse model must represent the relevant entities, and the relationships between them <REF>Grosz and Sidner, 1986</REF>, A speaker may then felicitously refer anaphorically to an object subject to focusing or centering constraints <REF>Grosz et al , 1983</REF>, <REF>Sidner 1981, 1983</REF>, <TREF>Brennan et al 1987</TREF>  if there is an existing DE representing it, or if a corresponding DE may be directly inferred from an existing DE~~~For example, the utterance Every senior in Milford High School has a car gives rise to at least 3 entities, describable in English as the seniors in Milford High School, Milford High School, and the set of cars each of which is owned by some senior in Milford High School~~~These entities may then be accessed by the following next utterances, respectively: They graduate in June.
This is, to our knowledge, the first implementation of Webbers DE generation ideas~~~We designed the 243 algorithms and structures necessary to generate discourse entities from our logical representation of the meaning of utterances, and from pointing gestures, and currently use them in Januss <REF>Weischedel et al , 1987</REF>, BSN, 1988 pronoun resolution component, which applies centering techniques <REF>Grosz et al , 1983</REF>, <REF>Sidner 1981, 1983</REF>, <TREF>Brennan et al 1987</TREF> to track and constrain references~~~Janus has been demonstrated in the Navy domain for DARPAs Fleet Command Center Battle Management Program FCCBMP, and in the Army domain for the Air Land Battle Management Program ALBM~~~2 Meaninq Representation for DE Generation Webber found that appropriate discourse entities could be generated from the meaning representation of a sentence by applying rules to the representation that are strictly structural in nature, as long as the representation reflects certain crucial aspects of the sentence.
Pragmatic level Working together, surface patterns and possessive relationships can deal with many PPAs found in our corpus, but we still have two problems to be solved: semantic ambiguity among two or more acceptable candidates and abstract anaphors/antecedents, which cannot be solved by simply applying possessive relationship rules~~~For these cases, and possibly for some other cases not included in previous rules, we suggest a pragmatic factor, adapted from S Brennans et al 1987 centering algorithm~~~Although sentence center plays a crucial role in many works in anaphor resolution, usually limiting the number of candidates to be considered, we notice that, because PPAs can refer to almost any NP in the sentence rather than, for example, personal pronouns, which are often related to the sentence center, pragmatic knowledge plays only a secondary but still important role in our approach~~~We have adapted basic aspects of center algorithm, considering subject/object preference, and domain concepts preference, 1012 suggested by R <REF>Mitkov 1996</REF>, aiming to estimate the most probable center for intrasentential PPAs.
The result is as follows: UI : Cb f a, Cp  a U2:Cba, Cpb U3 : Cb f b, Cp  b U4 : Cb  b, Cp  b In terms of the conventional transitions this works out as U/U2: RETIN U2/U3: SMOOTH SHIFT us/u: COnTINUa This is consistent with Strube and Hahns 1996 observation that a IIrAIN transition ideally predicts a SMOOTH sswr in the following utterance~~~<REF>Brenuan et al 1987</REF> make a very similar claim: A computational system for 9e-emtion would try to plan a retention as a signal of an impending shift, so that after a retention, a shift would be preferred rather than a continuation~~~<REF>Grosz et al 1995</REF> give the following example of the Am SHIFT pattern: 5 a John has had trouble arranging his vacation~~~b He Cb; John cannot find anyone to take over his responsibilities.
I suggmtthat these results should be treated with some caution since it is not dear that the authors have the same assumptions about the claims of CT or that what they are testing directly reflects formulations of CT in the more theoretical literature~~~For instance <REF>Passoneau 1998</REF> refers to two variantS of CT: Version A based on <TREF>Brennan et al 1987</TREF> and Version B taken from <REF>Kameyama et al 1993</REF>~~~Passoneau does nt address the issue of direct vs indirect realisation and it appears from the examples given that she only takes account of entities realised by a full NP or possibly null pronoun~~~The analysis according to Version B results in a count of 52 NULL transitions, ie no Cb, which gives the impression that CT is in fact a rather poor measure of coherence, It is probable that a higher measure might have been obtained if Passoneau had allowed entities to be added to the U/s by inference, as discussed in Brennan et al, op cit.
Centering theory C is a theory of discourse structure which models the interaction of cohesion and salience in the internal organlsation of a text~~~The main assumptions of the theory as presented by Gross et a11995 GJW, <TREF>Brennan et al 1987</TREF> rare: 1~~~For each utterance in a discourse there is precisely one entity which is the centre of attention or center~~~2.
Take the Cb as given and plan the realisation of Ui- to make this entity the highestranked~~~The first strategy is clearly appropriate for interpretation cf <TREF>Brennan et al 1987</TREF> but for generation the issue is less clear-cut~~~Either the generator interprets its own output to designate Cb in terms of the grammatical structure of the previous utterance, in which case there have to be separate principles for deciding on the grammatical structure, or Cb is independently defined in the text plan and this information is used to plan the sentence structure~~~According to the pipelining principle information cannot flow backwards between tasks.
The function for resolving IPAs ResolveIpa has similarly been tested on texts, where APAs were excluded~~~We have compared the obtained results with those obtained by testing bfp <TREF>Brennan et al , 1987</TREF> and str98 <REF>Strube, 1998</REF>~~~In all tests the intrasentential anaphors have been manually resolved~~~Expletive and cataphoric uses of pronouns have been marked and excluded from the tests.
Thus, even if there exists a perfect theory, it might not work well with noisy input, or it would not cover all the anaphoric phenomena~~~1Walker <REF>Walker, 1989</REF> compares Brennan, Friedman aad Pollards centering approach <TREF>Brennan et al , 1987</TREF> with Hobbs algorithm <REF>Hohbs, 1976</REF> on a theoretical basis~~~These requirements have motivated us to develop robust, extensible, and trainable anaphora resolution systems~~~Previously <REF>Aone and McKee, 1993</REF>, we reported our data-driven multilingual anaphora resolution system, which is robust, exteusible, and manually trainable.
But, more generally, we must have a theory that is able to handle all cases of pronoun use~~~A pronoun interpretation algorithm based on centering which relied on centering transition preferences was developed in Brennan et aL 1987 Using transition preferences in a pronoun generation rule would cover more cases of pronoun use than is covered by Rule 1, but the application of such transition preferences also proved unhelpful in explaining pronoun patterns in our corpus~~~<REF>Reichman 1985</REF> and <REF>Grosz  Sidner 1986</REF> indicate that discourse segmentation has an effect on the linguistic realization of referring expressions~~~While this is intuitively appealing, it is unclear how to apply this to the generation problem in part because it is unclear how to define discourse segments to a generation system.
This distinction underlies my proposals about the attentional consequences of pitch accents when applied to pronominals, in particular, that while most pitch accents may weaken or reinforce a cospecifiers status as the center of attention, a contrastively stressed pronominal may force a shift, even when contraindicated by textual features~~~To predict and track the center of attention in discourse, theories of centering <REF>Grosz et al , 1983</REF>; <TREF>Brennan et al , 1987</TREF>; <REF>Grosz et al , 1989</REF> and immediate focus <REF>Sidner, 1986</REF> rely on syntactic and grammatical features of the text such as pronominalization and surface sentence position~~~This may be sufficient for written discourse~~~For oral discourse, however, we must also consider the way intonation affects the interpretation of a sentence, especially the cases in which it alters the predictions of centering theories.
3~~~Processing Complex Sentences: A Reason for Extending Focusing Algorithms Although complex sentences are prevalent in written English, most other local focusing research focusing: Sidner 1979 and Carter 1987; centering: Grosz, Joshi, and Weinstein 1983, 1995, Brennan, Friedman, and Pollard 1987, Walker 1989, 1993, Kameyama 1986 2, Walker, Iida, and Cote 1994, Brennan 1998, Kameyama, Passonneau, and Poesio 1993, Linson 1993 and Hoffman 1998; and PUNDIT: Dahl 1986, Palmer et al 1986, and Dahl and Ball 1990 did not explicitly and/or adequately address how to process complex sentences~~~Thus, there is a need to extend focusing algorithms~~~An exception to this rule is the work of <REF>Strube 1996</REF> which applies functionalinformation-structure-based criteria on a per-clause basis, <REF>Kameyama 1998</REF>, and <REF>Strube 1998</REF>.
Overview of the data used~~~5 Preliminary Model Overviews The models evaluated in this paper are based on Centering Theory <REF>Grosz et al , 1995</REF>; <REF>Grosz  Sidner, 1986</REF> and the algorithms devised by Brennan and colleagues 1987 and adapted by <REF>Tetreault 2001</REF>~~~We examine a language-only model based on Tetreaults Left-Right Centering LRC model, a visual-only model that uses a measure of visual salience to rank the objects in the visual field as possible referential anchors, and an integrated model that balances the visual information along with the linguistic information to generate a ranked list of possible anchors~~~51 The Language-Only Model We chose the LRC algorithm <REF>Tetreault, 2001</REF> to serve as the basis for our language-only model.
Together this work suggests that the interlocutors shared visual context has a major impact on their patterns of referring behavior~~~Yet, a number of discourse-based models of reference primarily rely on linguistic information without regard to the surrounding visual environment eg , see <TREF>Brennan et al , 1987</TREF>; <REF>Hobbs, 1978</REF>; <REF>Poesio et al , 2004</REF>; <REF>Strube, 1998</REF>; <REF>Tetreault, 2005</REF>~~~Recently, multi-modal models have emerged that integrate visual information into the resolution process~~~However, many of these models are restricted by their simplifying assumption of communication via a command language.
This preference can be explained in terms of salience from the point of view of the centering theory~~~The latter proposes the ranking subject, direct object, indirect object <TREF>Brennan et al 1987</TREF> and noun phrases which are parts of prepositional phrases are usually indirect objects~~~Collocation pattern preference This preference is given to candidates which have an identical collocation pattern with a pronoun 2,0~~~The collocation preference here is restricted to the patterns noun phrase pronoun, verb and verb, noun phrase pronoun.
Since the constraints are eflective in the lifferent target from ours, the accuracy of identifying the referents of zero pronouns would be improved much more by using both of his constraints and the constraint we proposed~~~As for the identifying method available in general discourses, the centering theory<TREF>Brennan et al , 1987</TREF>; <REF>Walker et al , 1990</REF> and the property sharing theory<REF>Kameyama, 1988</REF> are proposed~~~Although this kind of theory has a good point that it is independent of the type o17 discourse, the linguistic constraints specitic to expressions like the pragmatic constraints l/roposed by Dohsaka or us are more accurate than theirs when the speeitlc constraints are applicable~~~3 General ontology in manuals and prinmry constraints In this section, we consider the general ontology which can be used in,dl types of manuals.
S, O and X Table 1B~~~The members of the CF list are ranked according to their grammatical role <TREF>Brennan et al , 1987</TREF> and their position in the grid3 The derived sequence of CF lists can then be used to compute other important Centering concepts:  The CB, ie the referent that links the current CF list with the previous one such as microsoft in b~~~Transitions <TREF>Brennan et al , 1987</TREF> and NOCBs, that is, cases in which two subsequent CF lists do not have any referent in common~~~Violations of CHEAPNESS <REF>Strube and Hahn, 1999</REF>, COHERENCE and SALIENCE <REF>Kibble and Power, 2000</REF>.
The members of the CF list are ranked according to their grammatical role <TREF>Brennan et al , 1987</TREF> and their position in the grid3 The derived sequence of CF lists can then be used to compute other important Centering concepts:  The CB, ie the referent that links the current CF list with the previous one such as microsoft in b~~~Transitions <TREF>Brennan et al , 1987</TREF> and NOCBs, that is, cases in which two subsequent CF lists do not have any referent in common~~~Violations of CHEAPNESS <REF>Strube and Hahn, 1999</REF>, COHERENCE and SALIENCE <REF>Kibble and Power, 2000</REF>~~~22 Metrics of coherence <REF>Karamanis 2003</REF> assumes a system which receives an unordered set of CF lists as its input and uses a metric to output the highest scoring ordering.
2 <unit finitefinite-yes idu210> <ne idne410 gfsubj>144</ne> is <ne idne411 gfpredicate> a torc</ne> </unit>~~~The ranking of the CFs other than the CP is defined according to the following preference on their gf <TREF>Brennan et al , 1987</TREF>: obj>iobj>other~~~CFs with the same gf are ranked according to the linear order of the corresponding NPs in the utterance~~~The second column of Table 1 shows how the utterances in example 1 are automatically translated by the scripts developed by Poesio et al.
It is often claimed in current work on in natural language generation that the constraints on felicitous text proposed by the theory are useful to guide text structuring, in combination with other factors see <REF>Karamanis, 2003</REF> for an overview~~~However, how successful Centerings constraints are on their own in generating a felicitous text structure is an open question, already raised by the seminal papers of the theory <TREF>Brennan et al , 1987</TREF>; <REF>Grosz et al , 1995</REF>~~~In this work, we explored this question by developing an approach to text structuring purely based on Centering, in which the role of other factors is deliberately ignored~~~In accordance with recent work in the emerging field of text-to-text generation <REF>Barzilay et al , 2002</REF>; <REF>Lapata, 2003</REF>, we assume that the input to text structuring is a set of clauses.
However, in this work we are treating CF lists as an abstract representation Following again the terminology in <REF>Kibble and Power 2000</REF>, we call the requirement that CBn be the same as CBn1 the principle of coherence and the requirement that CBn be the same as CPn the principle of salience~~~Each of these principles can be satisfied or violated while their various combinations give rise to the standard transitions of Centering shown in Table 2; Poesio et als scripts compute these violations6 We also make note of the preference between these transitions, known as Centerings Rule 2 <TREF>Brennan et al , 1987</TREF>: continue is preferred to retain, which is preferred to smoothshift, which is preferred to rough-shift~~~Finally, the scripts determine whether CBn is the same as CPn1, known as the principle of cheapness <REF>Strube and Hahn, 1999</REF>~~~The last column of Table 1 shows the violations of cheapness denoted with an asterisk in 17 23 Evaluating the coherence of a text and text structuring The statistics about transitions computed as just discussed can be used to determine the degree to which a text conforms with, or violates, Centerings principles.
P99-1079:56~~~Analysis of Syntax-Based Pronoun Resolution Methods Joel R Tetreault University of Rochester Department of Computer Science Rochester, NY, 14627 tetreaulcs, rochester, edu Abstract This paper presents a pronoun resolution algorithm that adheres to the constraints and rules of Centering Theory <REF>Grosz et al , 1995</REF> and is an alternative to Brennan et als 1987 algorithm~~~The advantages of this new model, the Left-Right Centering Algorithm LRC, lie in its incremental processing of utterances and in its low computational overhead~~~The algorithm is compared with three other pronoun resolution methods: Hobbs syntax-based algorithm, Strubes S-list approach, and the BFP Centering algorithm.
The noteworthy results were that Hobbs and LRC performed the best~~~The aim of this project is to develop a pronoun resolution algorithm which performs better than the <TREF>Brennan et al 1987</TREF> algorithm 1 as a cognitive model while also performing well empirically~~~A revised algorithm Left-Right Centering was motivated by the fact that the BFP algorithm did not allow for incremental processing of an utterance and hence of its pronouns, and also by the fact that it occasionally imposes a high computational load, detracting from its psycholinguistic plausibility~~~A second motivation for the project is to remedy the dearth of empirical results on pronoun resolution methods.
5~~~Identify Transition with the Cb and Cf resolved, use the criteria from <TREF>Brennan et al , 1987</TREF> to assign the transition~~~It should be noted that BFP makes use of Centering Rule 2 <REF>Grosz et al , 1995</REF>, LRC does not use the transition generated or Rule 2 in steps 4 and 5 since Rule 2s role in pronoun resolution is not yet known see <REF>Kehler 1997</REF> for a critique of its use by BFP~~~Computational overhead is avoided since no anchors or auxiliary data structures need to be produced and filtered.
The RETAIN is motivated as it enables a cheap SMOOTH SHIFT, and so we need a way of evaluating the whole sequence CONTINUE-RETAIN-SHIFT verSUS CONTINUE-CONTINUE-SHIFT~~~: :24,:Ceaateringin :NLG CT has developed primarily in the context of natural language interpretation, focussing on anaphora resolution see eg, <TREF>Brennan et al 1987</TREF>~~~Curiously, NLG researchers have tended to overlook GJWs proposal that Rule 2 provides a constraint on speakers, and on natural-language generation systems To empirically test the claim made by Rule 2 requires examination of differences in inference load of alternative multi-utterance sequences that differentially realize the same content~~~GJW, p 215.
This is also referred to as the backward-looking center or Cb~~~The notion of salience for the purposes of centering theory is most commonly defined according to a hierarchy of grammatical roles: SUBJECT > DIRECT OBJECT > INDIRECT OBJECT > OTHERS see eg, <TREF>Brennan et al 1987</TREF> For alternative approaches see eg, <REF>Strube and Hahn 1999</REF>, <REF>Walker et al 1994</REF>~~~2~~~There is a preference for consecutive utterances within a discourse segment to keep the same entity as the center, and for the center to be realised as Subject or preferred center Cp.
Cheapness is satisfied by a transition pair Un-1, Un, Un, Unl if the preferred center of Un is the Cb of Unl For example, this test is satisfied by a RETAIN-SHIFT sequence but not by CONTINUE-SHIFT, so it is predicted that the former pattern will be used to introduce a new center~~~This claim is consistent with the findings of <REF>Brennan 1998</REF>, <TREF>Brennan et al 1987</TREF>~~~If we consider examples la-e below, the sequence cd-e , including a RETAIN-SHIFT sequence, reads more fluently than c-d-e even though the latter scores better according to the canonical ranking~~~a John has had trouble arranging his vacation.
We found that it is much easier to annotate the building blocks of a theory of the local focus, and then use scripts to automatically compute the CB~~~There are two advantages to this approach: first of all, agreement on the building blocks is much easier to reach than agreement on the CBin our preliminary experiments we didnt go beyond   6 when trying to directly identify the CB using the definitions from <TREF>Brennan et al , 1987</TREF>~~~And secondly, this approach makes it possible to compute the CB according to different ways of instantiating what we call the parameters of Centering eg , ranking~~~We developed such scripts for the work discussed in <REF>Poesio et al , 2004b</REF>; they can be tested on the web site associated with that paper, http://cswwwessexacuk/staff/poesio/ cbc/.
It uses a hierarchy of grammatical roles quite similar to that of RAP, but this role hierarchy does not directly influence antecedent selection~~~Whereas th e hierarchy in RAP contributes to a multi-dimensional measure of the relative salience of all antecedent candidates, in <TREF>Brennan et al 1987</TREF>, it is used only to constrain the choice of the backward-looking center, Cb, of an utterance~~~It does not serve as a general preference measure for antecedence~~~The items in the forward center list, Cf, are ranked according to the hierarchy of grammatical roles.
The ranking of the Cf members is determined by the salience status of the entities in the utterance and mayvary crosslinguistically~~~Kameyama 28198529 and Brennan et al 28198729 proposed that the Cf ranking for English is determined by grammatical function as follows: 28229 Rule for ranking of forward-looking centers: SUBJ3EIND~~~OBJ3EOBJ3EOTHERS Later crosslinguistic studies based on empirical work 28<REF>Di Eugenio, 1998</REF>; <REF>Turan, 1995</REF>; <REF>Kameyama, 1985</REF>29 determined the following detailed ranking, with QIS standing for quanti0Ced inde0Cnite subjects 28people, everyone etc29 and PRO-ARB 28we, you29 for arbitrary plural pronominals~~~28329Revised rule for the ranking of forward-looking centers: SUBJ3EIND.
For example, as pointed out in Bonnie Webbers Penn presentation, there is a distinction between Tree Adjunction Grammar TAG as a linguistic theory and the several algorithms that have been used to implement TAG parsers: Extended CKY parser, Extended Earley parser, Two-pass extended Earley parser based on lexicalized TAGs, and a DCG parser using lexicalized TAGs~~~There is also a distinction between Centering as a theory for resolving anaphoric pronouns <REF>Joshi and Weinstein 1981</REF>; <REF>Gross et al 1983</REF>, and the attempts to use a centering approach to resolving pronouns in an implementation <TREF>Brennan et al 1987</TREF>~~~In addition, one way of looking inside a system is to look at the performance of one or more modules or components~~~Which components are obtained depends on the nature of the decomposition of the system.
PFNOCB, a second baseline, which enhances MNOCB with a global constraint on coherence that <REF>Karamanis, 2003</REF> calls the PageFocus PF~~~PFBFP which is based on PF as well as the original formulation of CT in <TREF>Brennan et al , 1987</TREF>~~~PFKP which makes use of PF as well as the recent reformulation of CT in <REF>Kibble and Power, 2000</REF>~~~<REF>Karamanis et al , 2004</REF> report that PFNOCB outperformed MNOCB but was overtaken by PFBFP and PFKP.
The function for resolving IPAsResolveIpa has similarly been tested on texts, where APAswereexcluded~~~We have compared the obtained results with those obtained by testing bfp <TREF>Brennan et al , 1987</TREF> and str98 <REF>Strube, 1998</REF>~~~In all tests the intrasentential anaphors have been manually resolved and expletive and cataphoric uses of pronouns have been marked and excluded from the test~~~Dialogue act units were marked and classified by three annotators following <REF>Eckert and Strube, 2000</REF>.
three arguments~~~Finally, the measure MBFP <TREF>Brennan et al , 1987</TREF> uses a lexicographic ordering on 4-tuples which indicate whether the transition is a CONTINUE, RETAIN, SMOOTH-SHIFT, or ROUGHSHIFT~~~cT and all four functions it is computed from take three arguments because the classification depends on COHERENCE~~~As the first transition in the discourse is coherent by default it has no Cb, we can compute cI by distinguishing RETAIN and CONTINUE via SALIENCE.
This is in contrast to theories of global coherence, which can consider relations between larger chunks of the discourse and eg structures them into a tree <REF>Mann and Thompson, 1988</REF>; <REF>Marcu, 1997</REF>; <REF>Webber et al , 1999</REF>~~~Measures of local coherence specify which ordering of the sentences makes for the most coherent discourse, and can be based eg on Centering Theory <REF>Walker et al , 1998</REF>; <TREF>Brennan et al , 1987</TREF>; <REF>Kibble and Power, 2000</REF>; <REF>Karamanis and Manurung, 2002</REF> or on statistical models <REF>Lapata, 2003</REF>~~~But while formal models of local coherence have made substantial progress over the past few years, the question of how to efficiently compute an ordering of the sentences in a discourse that maximises local coherence is still largely unsolved~~~The fundamental problem is that any of the factorial number of permutations of the sentences could be the optimal discourse, which makes for a formidable search space for nontrivial discourses.
Based on these concepts, CT classifies the transitions between subsequent utterances into different types~~~Table 1 shows the most common classification into the four types CONTINUE, RETAIN, SMOOTH-SHIFT, and ROUGH-SHIFT, which are predicted to be less and less coherent in this order <TREF>Brennan et al , 1987</TREF>~~~<REF>Kibble and Power 2000</REF> define three further classes of transitions: COHERENCE and SALIENCE, which are both defined in Table 1 as well, and NOCB, the class of transitions for which Cbui is undefined~~~Finally, a transition is considered to satisfy the CHEAPNESS constraint <REF>Strube and Hahn, 1999</REF> if Cbui  Cpui1.
One more filtering criterion is mutual information MI, which reflects the relatedness of two terms in their combination , kj ww  To keep a relation  kji wwwP, we require , kj ww be a meaningful combination~~~We use the following pointwise MI <TREF>Church and Hanks 1989</TREF>:  ,log, kj kj kj wPwP wwPwwMI  We only keep meaningful combinations such that 0, >kj wwMI  By these filtering criteria, we are able to reduce considerably the number of biterms and triterms~~~For example, on a collection of about 200MB, with a vocabulary size of about 148K, we selected only about 27M useful biterms and about 137M triterms, which remain tractable~~~33 Probability of Biterms In LM used in IR, each query term is attributed the same weight.
33 Scoring the semantic similarity of word pairs Measuring the semantic similarity of words on the basis of raw corpus data is obviously a much harder task than measuring the orthographic similarity of words~~~Mutual information first introduced to computational linguistics by <TREF>Church and Hanks 1989</TREF> is one of many measures that seems to be roughly correlated to the degree of semantic relatedness between words~~~The mutual information between two words A and B is given by: IA;B  log PrA;BPrAPrB 1 Intuitively, the larger the deviation between the empirical frequency of co-occurrence of two words and the expected frequency of co-occurrence if they were independent, the more likely it is that the occurrence of one of the two words is not independent from the occurrence of the other~~~Brown et alii 1990 observed that when mutual information is computed in a bi-directional fashion, and by counting co-occurrences of words within a 4Most of the pairs in this block  78  are actually morphologically related.
2In the case of an interrupted collocation, words can be separated by an arbitrary number of words, whereas 71 sin:e,:hey assumed that a collocation is a se-,lun:e of adjacent words that frequently apl:,ar tgether~~~<TREF>Church and Hanks, 1989</TREF> delhw:I ;t collocation as a pair of correlated words :mi,,set mutual information to evaluate such ,xi:a,1 :orrelations of word pairs of length two~~~They retrieved interrupted word pairs, as well as,minterrupted word pairs~~~<REF>Haruno et al , 1996</REF>,:onstructed collocations by combining adjacent n-grams with high value of mutual information.
Because of their low ambiguity and high specificity, these words are also particularly useful for conceptualizing a knowledge domain or for supporting the creation of a domain ontology~~~Candidate terminological expressions are usually captured with more or less shallow techniques, ranging from stochastic methods <TREF>Church and Hanks 1989</TREF>; <REF>Yamamoto and Church 2001</REF> to more sophisticated syntactic approaches <REF>Jacquemin 1997</REF>~~~155 Navigli and Velardi Learning Domain Ontologies WordNet domain corpus contrastive corpora terminology extraction candidate extraction terminology filtering semantic interpretation semantic disambiguation identification of taxonomic relations identification of conceptual relations Inductive learner Natural Language Processor ontology integration and updating 3 2 1 Lexical Resources Domain Concept Forest Figure 3 The architecture of OntoLearn~~~Obviously, richer syntactic information positively influences the quality of the result to be input to the statistical filtering.
Afterward, if a target adjective sense was not resolved, semantic indicator attributes were applied; no individual indicator nouns were used~~~The semantic attributes that were applied were animate, body part, color, concrete, human, and text type; <TREF>Church and Hanks 1989</TREF> had pointed to two of these attributes, person and body part also time, previously mentioned above in a seemingly casual listing of just five attributes potentially useful for describing the lexico-syntactic regularities of noun-verb relations~~~Table 1 shows that these few, general attributes cover almost three-quarters of all instances of the target adjectives~~~Disambiguation by these syntactic and semantic attributes is effectively as reliable as disambiguation using significant indicator nouns: having three apparent errors in disambiguation is not significantly worse than the errorless performance of the significant indicator nouns in the 100-sentence samples.
Content words that have a close syntactic relation to one another are useful candidates for examination and are intuitively more likely to bear a close semantic relation than words that are near one another but are not related syntactically~~~One much-studied example is the semantic relation between a verb and its arguments eg , <REF>Boguraev et al 1989</REF>; <TREF>Church and Hanks 1989</TREF>; Braden-<REF>Harder 1991</REF>; <REF>Hindle and Rooth 1991</REF>~~~Discrimination among senses of adjectives based on the nouns they modify or of which they are predicated has been the subject of less intensive and systematic study~~~Determining the potential of this line of evidence is the focus of this paper.
Like path coreference, semantic compatibility can be considered a form of world knowledge needed for more challenging pronoun resolution instances~~~We encode the semantic compatibility between a noun and its parse tree parent and grammatical relationship with the parent using mutual information MI <TREF>Church and Hanks, 1989</TREF>~~~Suppose we are determining whether ham is a suitable antecedent for the pronoun it in eat it~~~We calculate the MI as: MIeat:obj, ham  log Preat:obj:hamPreat:objPrham Although semantic compatibility is usually only computed for possessive-noun, subject-verb, and verb-object relationships, we include 121 different kinds of syntactic relationships as parsed in our news corpus3 We collected 488 billion parent:rel:node triples, including over 327 million possessive-noun values, 129 billion subject-verb and 877 million verb-direct object.
This line of research was motivated by a series of successful applications of mutual information statistics to other problems in natural language processing~~~In the last decade, research in speech recognition <REF>Jelinek 1985</REF>, noun classification <REF>Hindle 1988</REF>, predicate argument relations <TREF>Church  Hanks 1989</TREF>, and other areas have shown that mutual information statistics provide a wealth of information for solving these problems~~~22 Mutual Information Statistics The mutual information statistic <REF>Fano 1961</REF> is a measure of the interdependence of two signals in a message~~~It is a function of the probabilities of the two events: Mz, u  log u xzPvy In this paper, the events x and y will be part-of-speech n-grams instead of single parts-of-speech, as in some earlier work.
The idea behind it is that similar words tend to co-occur in certain patterns~~~Considerable efforts have been devoted to measure word similarity based on cooccurrence frequency of two words in a window <TREF>Church and Hanks, 1989</TREF>; <REF>Turney, 2001</REF>; <REF>Terra and Clarke, 2003</REF>; <REF>Matsuo et al, 2006</REF>~~~In addition to the classical window-based technique, some studies investigated the use of lexico-syntactic patterns eg, X or Y to get more accurate co-occurrence statistics <REF>Chilovski and Pantel, 2004</REF>; <REF>Bollegala et al, 2007</REF>~~~These two approaches are complementary with each other, because they are founded on different hypotheses and utilize different corpus statistics.
For example, Wilks et al~~~1989 use this ratio as a criterion for establishing links between words in a semantic network; <TREF>Church and Hanks 1989</TREF> use the logarithm of this ratio as a measure for word association~~~14 Justeson and Katz Co-occurrences of Antonymous Adjectives Under this formulation of the co-occurrence theory, acquiring the lexical relation of antonymy requires a certain amount of training for the association, and as the frequency of adjectives declines, so must the frequency of training for its associations~~~On the whole, then, very infrequent training should result in weaker associations; more generally, adjective frequency should correlate with the strength of lexical associations.
In the past, for this purpose a number of measures have been proposed~~~They were based on mutual information <TREF>Church  Hanks, 1989</TREF>, conditional probabilities <REF>Rapp, 1996</REF>, or on some standard statistical tests, such as the chi-square test or the loglikelihood ratio <REF>Dunning, 1993</REF>~~~For the purpose of this paper, we decided to use the loglikelihood ratio, which is theoretically well justified and more appropriate for sparse data than chi-square~~~In preliminary experiments it also led to slightly better results than the conditional probability measure.
Finally, methods and strategies for handling low-frequency data are suggested~~~The measures2  Mutual Information a0a2a1  <TREF>Church and Hanks, 1989</TREF>, the log-likelihood ratio test <REF>Dunning, 1993</REF>, two statistical tests: t-test and a3a5a4 -test, and co-occurrence frequency  are applied to two sets of data: adjective-noun AdjN pairs and preposition-noun-verb PNV triples, where the AMs are applied to PN,V pairs~~~See section 3 for a description of the base data~~~For evaluation of the association measures, a6 -best strategies section 41 are supplemented with precision and recall graphs section 42 over the complete data sets.
6 Probabifities were estimated using the Penn Treebank version of the Brown corpus~~~The pairs come from an example given by <TREF>Church and Hanks 1989</TREF>, illustrating the words that human subjects most frequently judged as being associated with the word doctor~~~The word sick also appeared on the list, but is excluded here because it is not a noun~~~Word 1 Word 2 doctor nurse doctor lawyer doctor man doctor medicine doctor hospital doctor health doctor sickness Similarity Most Informative Subsumer 94823 health professional 72240 professional person 29683 person, individual 10105 <entity 10105 <entity 00 virtual root 00 virtual root Doctors are minimally similar to medicine and hospitals, since these things are all instances of something having concrete existence, riving or nonliving WordNet class ent ty, but they are much more similar to lawyers, since both are kinds of professional people, and even more similar to nurses, since both are professional people working specifically within the health professions.
By no means an exhaustive list, the most commonly cited ranking and scoring algorithms are HITS <REF>Kleinberg 1998</REF> and PageRank <REF>Page et al 1998</REF>, which rank hyperlinked documents using the concepts of hubs and authorities~~~The most well-known keyword scoring methods within the IR community are the tf-idf <REF>Salton and McGill 1983</REF> and pointwise mutual information <TREF>Church and Hanks 1989</TREF> measures, which put more importance on matching keywords that occur frequently in a document relative to the total number of documents that contain the keyword by normalizing term frequencies with inverse document frequencies~~~Various methods including tf-idf have been comparatively evaluated by <REF>Salton and Buckley 1987</REF>~~~Creating nbest lists using the above algorithms produce result sets where each result is considered independently.
41 EIIR: Expected Independent Information Ranking Model Baseline Model Recall the task definition from Section 3~~~Finding a property r that most reduces the uncertainty in a query set Q can be modeled by measuring the strength of association between r and Q <REF>Following Pantel and Lin 2002</REF>, we use pointwise mutual information pmi to measure the association strength between two events q and r, where q is a term in Q and r is syntactic dependency, as follows <TREF>Church and Hanks 1989</TREF>:      N fqc N rwc N rqc Ff Ww rqpmi       , , , log,  41 where cq,r is the frequency of r in the feature vector of q as defined in Section 32, W is the set of all words in our corpus, F is the set of all syntactic dependencies in our corpus, and N   WwFf fwc , is the total frequency count of all features of all words~~~We estimate the association strength between a property r and a set of terms Q by taking the expected pmi between r and each term in Q as:       Qq rqpmiqPrQpmi ,,  42 where Pq is the probability of q in the corpus~~~Finally, the EIIR model chooses an n-best list by selecting the n properties from R that have highest pmiQ, r.
32 Contexts The context in which a word appears often imposesconstraintsonthesemantictypeoftheword~~~This basic idea has been exploited by many proposals for distributional similarity and clustering, eg, <TREF>Church and Hanks, 1989</TREF>; <REF>Lin, 1998</REF>; <REF>Pereira et al , 1993</REF>~~~Similar to <REF>Lin and Pantel 2001</REF>, we define the contexts of a word to be the undirected paths in dependency trees involving that word at either the beginning or the end~~~The following diagram shows an example dependency tree: Which city hosted the 1988 Winter Olympics.
RB, RBR, or RBS VB, VBD, VBN, or VBG anything The second step is to estimate the semantic orientation of the extracted phrases, using the PMI-IR algorithm~~~This algorithm uses mutual information as a measure of the strength of semantic association between two words <TREF>Church  Hanks, 1989</TREF>~~~PMI-IR has been empirically evaluated using 80 synonym test questions from the Test of English as a Foreign Language TOEFL, obtaining a score of 74 <REF>Turney, 2001</REF>~~~For comparison, Latent Semantic Analysis LSA, another statistical measure of word association, attains a score of 64 on the 3 http://wwwcsjhuedu/brill/RBT114tarZ 4 <REF>See Santorini 1995</REF> for a complete description of the tags.
same 80 TOEFL questions <REF>Landauer  Dumais, 1997</REF>~~~The Pointwise Mutual Information PMI between two words, word1 and word2, is defined as follows <TREF>Church  Hanks, 1989</TREF>: pword1  word2 PMIword1, word2  log2 pword1 pword2 1 Here, pword1  word2 is the probability that word1 and word2 co-occur~~~If the words are statistically independent, then the probability that they co-occur is given by the product pword1 pword2~~~The ratio between pword1  word2 and pword1 pword2 is thus a measure of the degree of statistical dependence between the words.
Thus we introduce d-bigram which is a bigram cooccurrence information concerning the distance<REF>Tsutsumi et al , 1993</REF>~~~Expression 1 calculates the score between two neighboring letters; UKi  E E Mwj,wid;d  x,qd 1 dl j-i--d--1 where wl as an eveN;, d as the distance between two eveN;s, dmax as the maximum distance used in the processing we set drnax - 5, and gd as the weight fimction on distance for this system gd  d-2<REF>Sano et al , 1996</REF>, to decrease tile influence of tile d-bigrams when the distance get longer <TREF>Church and Hanks, 1989</TREF>~~~When calculating the linking score between the letters wi and Wil, tile d-bigram information of the letter pairs around tim target two such as wi-l, wi2; 3 are added~~~Expression 2 calculates the mutual information between two events with d-bigram data; v; d d -2 where x, y as events, d for the distance between two events, and Px as the probability.
Linking Score Expression 2 is tbr calculating the linking score between two letters in a sentence ~~~Z 2 d:-:l ji-d-1 dmax : max distance used wl : the i-th letter in the sentence w gd : a certain weight for iV// concerning distance between letters The information between two remote words has less nmaning in a sentence when it comes to the semantic analysis<TREF>Church and Hanks, 1989</TREF>~~~According to the idea we put gd in the expression so that nearer pair can be more effective in calculating the score of the sentence~~~ hi,, I I I--1 B C  F G H Figure 3: Calculation of Linking Score A pair of far-away letters do not have strong relation between each other, neither syntactically nor semantically.
Word alignments that are shared by many different words are most probably mismatches~~~For this experiment we used Pointwise Mutual Information I <TREF>Church and Hanks, 1989</TREF>~~~IW, f  log PW, fPWPf,where W is the target word PW is the probability of seeing the word Pf is the probability of seeing the feature PW,f is the probability of seeing the word and the feature together~~~33 Word Alignment The multilingual approach we are proposing relies on automatic word alignment of parallel corpora from Dutch to one or more target languages.
For statistical features, previous work Section 2 suggests that the mutual information between the decision tokens xL0 and xR0 may be appropriate~~~The log of the pointwise mutual information <TREF>Church and Hanks, 1989</TREF> between the decision-boundary tokens xL0, xR0 is: MIxL0, xR0  log PrxL0xR0Prx L0PrxR0 This is equivalent to the sum: log CxL0xR0  log K log CxL0 log CxR0~~~For web-based features, the counts C~~~can be taken as a search engines count of the number of pages containing the term.
Tile focus of much of this work was to develop the methods themselves~~~<TREF>Church and Hanks 1989</TREF> explored tile use of mutual information statistics in ranking co-occurrences within five-word windows~~~<REF>Smadja 1992</REF> gathered co-occurrences within fiveword windows to find collocations, particularly in specific domains~~~<REF>Hindle 1990</REF> classified nouns on the basis of co-occurring patterns of subjectverb and verb-object pairs.
Collocations have been studied by computational linguists in different contexts~~~For instance, there is a substantial body of papers on the extraction of frequently co-occurring words from corpora using statistical methods eg , <REF>Choueka et al , 1983</REF>, <TREF>Church and Hanks, 1989</TREF>, <REF>Smadja, 1993</REF> to list only a few~~~These authors focus on techniques for providing material that can be used in other processing tasks such as x The research rcpmlcd in this paper was undmtaken as the project Collocations and the Lexicalisation of Semantic Operations ET10/75~~~Financial contributions weir by the Commission of the European Community, Association Suissetra Geneva and Oxford University Press.
Hence, greater the frequency, the more is the likelihood of the expression to be a MWE~~~612 Point-wise Mutual Information a16  Point-wise Mutual information of a collocation <TREF>Church and Hanks, 1989</TREF> is defined as, a16a18a17a19a11a21a20a23a22a25a24a27a26 a15a28a17a19a11a2a20a23a22a25a24a30a29a31a15a28a17a33a32a34a20a35a32a36a24 a15a28a17a19a11a2a20a35a32a36a24a30a29a37a15a28a17a33a32a34a20a23a22a25a24 where, a11 is the verb and a22 is the object of the collocation~~~The higher the Mutual information of a collocation, the more is the likelihood of the expression to be a MWE~~~613 Least mutual information difference with similar collocations a38  This feature is based on Lins work <REF>Lin, 1999</REF>.
Various statistical measures have been suggested for ranking expressions based on their compositionality~~~Some of these are Frequency, Mutual Information <TREF>Church and Hanks, 1989</TREF>, distributed frequency of object <REF>Tapanainen et al , 1998</REF> and LSA model <REF>Baldwin et al , 2003</REF> <REF>Schutze, 1998</REF>~~~In this paper, we define novel measures both collocation based and context based measures to measure the relative compositionality of MWEs of V-N type see section 6 for details~~~Integrating these statistical measures should provide better evidence for ranking the expressions.
These ranks are then compared with the human ranking~~~<REF>Breidt, 1995</REF> has evaluated the usefulness of the Point-wise Mutual Information measure as suggested by <TREF>Church and Hanks, 1989</TREF> for the extraction of V-N collocations from German text corpora~~~Several other measures like Log-Likelihood <REF>Dunning, 1993</REF>, Pearsons a2a4a3 <REF>Church et al , 1991</REF></REF>, Z-Score <REF>Church et al , 1991</REF></REF>, Cubic Association Ratio MI3, etc , have been also proposed~~~These measures try to quantify the association of two words but do not talk about quantifying the non-compositionality of MWEs.
However, the focus was more on balancing the effect of query expansion techniques such that different concepts in the query were equally benefited~~~Mutual information has been used previously in <TREF>Church and Hanks, 1989</TREF> to identify collocations of terms for identifying semantic relationships in text~~~Experiments were confined to bigrams~~~The use of MaST over a graph of mutual information values to incorporate the most significant dependencies between terms was first noted in <REF>Rijsbergen, 1979</REF>.
Mutual Information is attractive because it is not only easy to compute, but also takes into consideration corpus statistics and semantics~~~The mutual information between two terms <TREF>Church and Hanks, 1989</TREF> can be calculated using Equation 2~~~Ix,y  log nx,y N nx N ny N 2 nx,y is the number of times terms x and y occurred within a term window of 100 terms across the corpus, while nx and ny are the frequencies of x and y in the collection of size N terms~~~To tackle the situation where we have an arbitrary number of variables terms we extend the twovariable case to the multivariate case.
1~~~Mutual <REF>Information Church and Hanks 1989</REF> discussed the use of the mutual information statistic in order to identify a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type content word/content word to lexico-syntacfic co-occurrence constraints between verbs and prepositions content word/function word~~~Mutual information, lx;y, compares the probability of observing word x and word y together the joint probability with the probabilities of observing x and y independently chance~~~lx;y - log 2 Px,y ex ey If there is a genuine association between x and y, then the joint probability Px,y will he much larger than chance Px Py, and consequently lx;y >> 0, as illustrated in the table below.
75 Some Interesting Associations with Doctor in the 1987 AP Corpus N  15 million; w  6 Ix; y fx y fx x fly Y 2~~~Phrasal Verbs 80 24 111 honorary 621 doctor 80 16 1105 doctors 44 dentists 84 60 1105 doctors 241 nurses 71 16 1105 doctors 154 treating 67 12 275 examined 621 doctor 66 12 1105 doctors 317 treat 64 50 621 doctor 1407 bills 64 12 621 doctor 350 visits 63 38 1105 doctors 676 hospitals 61 12 241 nurses 1105 doctors Associations with Doctor Some Less Interesting -13 12 621 doctor 73785 with -14 82 284690 a 1105 doctors 14 24 84716 is 1105 doctors <TREF>Church and Hanks 1989</TREF> also used the mutual information statistic in order to identify phrasal verbs, following up a remark by Sinclair: How common are the phrasal verbs with set~~~Set is particularly rich in making combinations with words like about, in, up, out, on, off, and these words are themselves very common~~~How likely is set off to occur.
Extraction of V-N-Collocations from Text Corpora: A Feasibility Study for German Elisabeth Breidt Seminar fiir Sprachwissenschaft University of Tiibingen Kleine Wilhelmstr~~~113, D-72074 Tiibingen breidtarbucklesnsneuphilologieuni-tuebingende Abstract The usefulness of a statistical approach suggested by <TREF>Church and Hanks 1989</TREF> is evaluated for the extraction of verb-noun V-N collocations from German text corpora~~~Some motivations for the extraction of V-N collocations from corpora are given and a couple of differences concerning the German language are mentioned that have implications on the applicability of extraction methods developed for English~~~We present precision and recall results for V-N collocations with support verbs and discuss the consequences for further work on the extraction of collocations from German corpora.
Collocations present an area that is important both for lexicography to improve their coverage in modern dictionaries as well as for lexical acquisition in computational linguistics, where the goal is to build either large reusable lexical databases LDBs or specific lexica for specialized NLP-applications~~~We have tested the statistical approach Mutual Information MI, brought up by <TREF>Church and Hanks 1989</TREF> for linguistics, for a semiautomatic extraction of verb-noun V-N collocations from untagged German text corpora~~~We try to answer the question how much can be done with an untagged corpus and what might be gained by lemmatizing, POS-tagging or even superficial parsing~~~<REF>Choueka 1988</REF> describes how to automatically extract word combinations from English corpora as a preselection of collocation candidates to ease a lexicographers search for collocations.
AER  MergePos 054 045 049 05101  MergeMI 055 045 050 05045 Table 6: Results using the compositionality based features pressions of various types~~~Some of them are Frequency, Point-wise mutual information <TREF>Church and Hanks, 1989</TREF>, Distributed frequency of object <REF>Tapanainen et al , 1998</REF>, Distributed frequency of object using verb information <REF>Venkatapathy and Joshi, 2005</REF></REF>, Similarity of object in verbobject pair using the LSA model <REF>Baldwin et al , 2003</REF>, <REF>Venkatapathy and Joshi, 2005</REF></REF> and Lexical and Syntactic fixedness <REF>Fazly and Stevenson, 2006</REF>~~~These features have largely been evaluated by the correlation of the compositionality value predicted by these measures with the gold standard value suggested by human judges~~~It has been shown that the correlation of these measures is higher than simple baseline measures suggesting that these measures represent compositionality quite well.
In the past, various measures have been suggested for measuring the compositionality of multi-word expressions~~~Some of these are mutual information <TREF>Church and Hanks, 1989</TREF>, distributed frequency <REF>Tapanainen et al , 1998</REF> and Latent Semantic Analysis LSA model <REF>Baldwin et al , 2003</REF>~~~Even though, these measures have been shown to represent compositionality quite well, compositionality itself has not been shown to be useful in any application yet~~~In this paper, we explore this possibility of using the information about compositionality of MWEs verb based for the word alignment task.
Our main aim is here to investigate the use of long-distance semantic dependencies to dynamically adapt the prediction to the current semantic context of communication~~~Similar work has been done by <REF>Li and Hirst 2005</REF> and <REF>Matiasek and Baroni 2003</REF>, who exploit Pointwise Mutual Information PMI; <TREF>Church and Hanks, 1989</TREF>~~~Trnka et al~~~2005 dynamically interpolate a high number of topic-oriented models in order to adapt their predictions to the current topic of the text or conversation.
More is to be learned from the fact that you can drink wine than from the fact that you can drink it even though there are more clauses in our sample with  as an object of drink than with wine~~~To capture this intuition, we turn, following <TREF>Church and Hanks 1989</TREF>, to mutual information see <REF>Fano 1961</REF>~~~The mutual information of two events lx y is defined as follows: Px y lxy  log2 Px Py where Px y is the joint probability of events x and y, and Px and Py axe the respective independent probabilities~~~When the joint probability Px y is high relative to the product of the independent probabilities, I is positive; when the joint probability is relatively low, I is negative.
The aim of this measure is to indicate the relatedness between two elements composing a pair~~~Mutual information has been positively used in many NLP tasks such as collocation analysis <TREF>Church and Hanks, 1989</TREF>, terminology extraction <REF>Damerau, 1993</REF>, and word sense disambiguation <REF>Brown et al , 1991</REF>~~~3 Experimental Evaluation As many other corpus linguistic approaches, our entailment detection model relies partially on some linguistic prior knowledge the expected structure of the searched collocations, ie, the textual entailment patterns and partially on some probability distribution estimation~~~Only a positive combination of both these two ingredients can give good results when applying and evaluating the model.
In the first stage, pairwise lexical relations are retrieved using only statistical information~~~This stage is comparable to <TREF>Church and Hanks 1989</TREF> in that it evaluates a certain word association between pairs of words~~~As in <TREF>Church and Hanks 1989</TREF>, the words can appear in any order and they can be separated by an arbitrary number of other words~~~However, the statistics we use provide more information and allow us to have more precision in our output.
Example sentences containing the two words in the two possible positions are:  The provision is aimed at making a hostile takeover prohibitively expensive by enabling Borg Warners stockholders to buy the The pill would make a takeover attempt more expensive by allowing the retailers shareholders to buy more company stock Let us note that this filtering method is an original contribution of our work~~~Other works such as <TREF>Church and Hanks 1989</TREF> simply focus on an evaluation of the correlation of appearance of a pair of words, which is roughly equivalent to condition C1~~~See next section~~~However, taking note of their pattern of appearance allows us to filter out more irrelevant collocations with C2 and C3.
Finally, at a more general level, although disambiguation was originally considered as a performance task, the collocations retrieved have not been used for any specific computational task~~~<TREF>Church and Hanks 1989</TREF> describe a different set of techniques to retrieve collocations~~~A collocation as defined in their work is a pair of correlated words~~~That is, a collocation is a pair of words that appear together more often than expected.
Collocations in the lexicographic meaning are only dealt with in the lexical approach~~~Aside from the work we present in this paper, most of the work carried out within the lexical approach has been done in computer-assisted lexicography by <REF>Choueka, Klein, and Neuwitz 1983</REF> and Church and his colleagues <TREF>Church and Hanks 1989</TREF>~~~Both works attempted to automatically acquire true collocations from corpora~~~Our work builds on Chouekas, and has been developed contemporarily to Churchs.
This stage is comparable to <TREF>Church and Hanks 1989</TREF> in that it evaluates a certain word association between pairs of words~~~As in <TREF>Church and Hanks 1989</TREF>, the words can appear in any order and they can be separated by an arbitrary number of other words~~~However, the statistics we use provide more information and allow us to have more precision in our output~~~The output of this first stage is then passed in parallel to the next two stages.
This limitation is intrinsic to the technique used since mutual information scores are defined for two items~~~The second limitation is that many collocations identified in <TREF>Church and Hanks 1989</TREF> do not really identify true collocations, but simply pairs of words that frequently appear together such as the pairs doctor-nurse, doctor-bill, doctor-honorary, doctors-dentists, doctors-hospitals, etc These co-occurrences are mostly due to semantic reasons~~~The two words are used in the same context because they are of related meanings; they are not part of a single collocational construct~~~The work we describe in the rest of this paper is along the same lines of research.
The two words are often used together because they are associated with the same context rather than for pure structural reasons~~~Many collocations retrieved in <TREF>Church and Hanks 1989</TREF> were of this type, as they retrieved doctors-dentists, doctors-nurses, doctorbills, doctors-hospitals, nurses-doctor, etc , which are not collocations in the sense defined above~~~Such collocations are not of interest for our purpose, although they could be useful for disambiguation or other semantic purposes~~~Condition C2 filters out exactly this type of collocations.
In this case, we are interested in collocations between the head of a PP complement, a preposition and the head of the phrase being postmodified~~~In general, these words will not be adjacent in the text, so it will not be possible to use existing approaches unmodified eg <TREF>Church and Hanks 1989</TREF>, because these apply to adjacent words in unanalyzed text~~~<REF>Hindle and Rooth 1991</REF> report good results using a mutual information measure of collocation applied within such a structurally defined context, and their approach should carry over to our framework straightforwardly~~~One way of integrating structural collocational information into the system presented above would be to make use of the semantic component of the ANLT grammar This component pairs logical forms with each distinct syntactic analysis that represent, among other things, the predicate-argument structure of the input.
Finally, we write the conditional probability as a function of : Pci  1jsi1,si  11  n1    n where   PH1P H0  Psi1,siPs i1Psi  Psijsi1Ps i The conditional probability, Pci  1jsi1,si is a mapping g from  2 0,1 to p 2 0, 1~~~Beginning with <TREF>Church and Hanks, 1989</TREF>, numerous authors have used the pointwise mutual information between pairs of words to analyze word co-locations and associations~~~This ratio tells us whether si1 and si co-occur more or less often than would be expected by chance alone~~~Consider, for example, the tags DT determiner and NN noun, and the four possible ordered tagpairs.
Since we use grammatical context, the feature set is considerably larger than the simple word based proximity feature set for the newspaper corpus~~~43 Calculating Feature Vectors Having collected all nouns and their features, we now proceed to construct feature vectors and values for nouns from both corpora using mutual information <TREF>Church and Hanks, 1989</TREF>~~~We first construct a frequency count vector Ce  ce1,ce2,,cek, where k is the total number of features and cef is the frequency count of feature f occurring in word e Here, cef is the number of times word e occurred in context f We then construct a mutual information vector MIe  mie1,mie2,,miek for each word e, where mief is the pointwise mutual information between word e and feature f, which is defined as: mief  log cef Nsummationtext n i1 cif N  summationtextk j1 cej N 6 where n is the number of words and N  5We perform this operation so that we can compare the performance of our system to that of <REF>Pantel and Lin 2002</REF>~~~summationtextn i1 summationtextm j1 cij is the total frequency count of all features of all words.
The SO of a phrase is determined based upon the phrases pointwise mutual information PMI with the words excellent and poor~~~PMI is defined by <TREF>Church and Hanks 1989</TREF> as follows: a0a2a1a4a3a6a5a8a7a10a9a12a11a13a7a15a14a17a16a19a18a21a20a23a22a25a24 a14a27a26a29a28 a5a8a7a10a9a19a30a31a7a15a14a17a16 a28 a5a8a7 a9 a16 a28 a5a8a7 a14 a16a33a32 1 where a28 a5a8a7a10a9a19a30a31a7a15a14a12a16 is the probability that a7a34a9 and a7a35a14 co-occur~~~The SO for a a28a37a36a39a38a41a40a29a42a44a43 is the difference between its PMI with the word excellent and its PMI with the word poor The method used to derive these values takes advantage of the possibility of using the World Wide Web as a corpus, similarly to work such as <REF>Keller and Lapata, 2003</REF>~~~The probabilities are estimated by querying the AltaVista Advanced Search engine1 for counts.
To solve the problem, we make use of a kind of cooperative evolution strategy to design an evolutionary algorithm~~~Word compositions have long been a concern in lexicography<REF>Benson et al 1986</REF></REF>; <REF>Miller et al 1995</REF>, and now as a specific kind of lexical knowledge, it has been shown that they have an important role in many areas in natural language processing, eg, parsing, generation, lexicon building, word sense disambiguation, and information retrieving, etceg , <REF>Abney 1989, 1990</REF>; <REF>Benson et al 1986</REF></REF>; <REF>Yarowsky 1995</REF>; <TREF>Church and Hanks 1989</TREF>; Church, <REF>Gale, Hans, and Hindle 1989</REF>~~~But due to the huge number of words, it is impossible to list all compositions between words by hand in dictionaries~~~So an urgent problem occurs: how to automatically acquire word compositions.
While bound compositions are not predictable, ie, their reasonableness cannot be derived from the syntactic and semantic properties of the words in them<REF>Smadja 1993</REF>~~~Now with the availability of large-scale corpus, automatic acquisition of word compositions, especially word collocations from them have been extensively studiedeg , <REF>Choueka et al 1988</REF>; <TREF>Church and Hanks 1989</TREF>; <REF>Smadja 1993</REF>~~~The key of their methods is to make use of some statistical means, eg, frequencies or mutual information, to quantify the compositional strength between words~~~These methods are more appropriate for retrieving bound compositions, while less appropriate for retrieving free ones.
The classifier can also be used to rank these vectors according to their relative compositionality~~~3 Related <REF>Work Church and Hanks 1989</REF> proposed a measure of association called Mutual Information 9~~~Mutual Information MI is the logarithm of the ratio between the probability of the two words occurring together and the product of the probability of each word occurring individually~~~The higher the MI, the more likely are the words to be associated with each other.
Dictionaries produced by hand always substantially lag real language use~~~The last two points do not argue against the use of existing dictionaries, but show that the incomplete information that they provide needs to be supplemented with further knowledge that is best collected automatically The desire to combine hand-coded and automatically learned knowledge 1A point made by <TREF>Church and Hanks 1989</TREF>~~~Arbitrary gaps in listing can be smoothed with a program such as the work presented here~~~For example, among the 27 verbs that most commonly cooccurred with from, Church and Hanks found 7 for which this 235 suggests that we should aim for a high precision learner even at some cost in coverage, and that is the approach adopted here.
They are estimated by using Table 2~~~C8B4CRCYD4D3D7B5 BP CUB4CRBND4D3D7B5 CUB4CRBND4D3D7B5B7CUB4BMCRBND4D3D7B5 C8B4CRCYD2CTCVB5 BP CUB4CRBND2CTCVB5 CUB4CRBND2CTCVB5B7CUB4BMCRBND2CTCVB5 PMI based polarity value Using PMI, the strength of association between CR and positive sentences and negative sentences is defined as follows <TREF>Church and Hanks, 1989</TREF>~~~C8C5C1B4CRBND4D3D7B5 BP D0D3CV BE C8B4CRBND4D3D7B5 C8B4CRB5C8B4D4D3D7B5 C8C5C1B4CRBND2CTCVB5 BP D0D3CV BE C8B4CRBND2CTCVB5 C8B4CRB5C8B4D2CTCVB5 PMI based polarity value is defined as their difference~~~This idea is the same as <REF>Turney, 2002</REF>.
The reason for choosing this measure is that it can be used to compute the distance between any two co-occurrence vectors independent of any information about other words~~~This is in contrast to many other measures, eg, <REF>Lin 1998</REF>, which use the co-occurrences of features with other words to compute a weighting function such as mutual information MI <TREF>Church and Hanks, 1989</TREF>~~~Since we only have corpus data for the target phrases, it is not possible for us to use such a measure~~~However, the -skew divergence measure has been shown <REF>Weeds, 2003</REF> to perform comparably with measures which use MI, particularly for lower frequency target words.
If the absolute value of the relative distance in a sentence for a feature and an opinion word is less than Minimum-Offset, they are considered contextdependent~~~Many methods have been proposed to measure the co-occurrence relation between two words such as  2 Church and Mercer,1993 , mutual information <TREF>Church and Hanks, 1989</TREF></TREF>; <REF>Pantel and Lin, 2002</REF>, t-test <TREF>Church and Hanks, 1989</TREF></TREF>, and loglikelihood Dunning,1993~~~In this paper a revised formula of mutual information is used to measure the association since mutual information of a lowfrequency word pair tends to be very high~~~Table 1 gives the contingency table for two words or phrases w 1  and  w 2 , where A is the number of reviews where w 1  and w 2  co-occur; B indicates the number of reviews where w 1  occurs but does not co-occur with w 2 ; C denotes the number of reviews where w 2  occurs but does not co-occur with w 1 ; D is number of reviews where neither w 1  nor w 2  occurs; N  A  B  C  D With the table, the revised formula of mutual information is designed to calculate the association of w 1 with w 2  as formula 1.
Other types of phrases~~~Many efficient techniques exist to extract multiword expressions, collocations, lexical units and idioms <TREF>Church and Hanks, 1989</TREF>; <REF>Smadja, 1993</REF>; <REF>Dias et al , 2000</REF>; <REF>Dias, 2003</REF>~~~Unfortunately, very few have been applied to information retrieval with a deep evaluation of the results~~~Maximal Frequent Sequences.
An inter-domain entropy IDE measure will be proposed for this purpose~~~2 Conventional Clustering View for Constructing Lexicon Trees One conventional way to construct the lexicon hierarchy from web corpora is to collect the terms in all web documents and measure the degree of word association between word pairs using some well-known association metrics <TREF>Church and Hanks, 1989</TREF>; <REF>Smadja et al , 1996</REF> as the distance measure~~~Terms of high association are then clustered bottom-up using some clustering techniques to build the hierarchy~~~The clustered hierarchy is then submitted to lexicographers to assign a semantic label to each sub-cluster.
The definition can be easily extended to a set of expressions T Given a pair vt and vh we define the following entailment strength indicator Svt,vh~~~Specifically, the measure Snomvt,vh is derived from point-wise mutual information <TREF>Church and Hanks, 1989</TREF>: Snomvt,vh  log pvt,vhnompv tpvhpers 3 where nom is the event of having a nominalized textual entailment pattern and pers is the event of having an agentive nominalization of verbs~~~Probabilities are estimated using maximum-likelihood: pvt,vhnom  fCPnomvt,vhf C uniontextP nomvprimet,vprimeh, 852 pvt  fCFvt/fCuniontextFv, and pvhpers  fCFagentvh/fCuniontextFagentv~~~Counts are considered useful when they are greater or equal to 3.
We then construct a mutual information vector MIe  mi e1, mi e2, , mi em  for each word e, where mi ef is the pointwise mutual information between word e and feature f, which is defined as: N c N c N c ef m j ej n i if ef mi       1 1 log 1 where n is the number of words and N    n i m j ij c 11 is the total frequency count of all features of all words~~~Mutual information is commonly used to measure the association strength between two words <TREF>Church and Hanks 1989</TREF>~~~A well-known problem is that mutual information is biased towards infrequent elements/features~~~We therefore multiply mi ef with the following discounting factor: 1,min,min 1 11 11                        m j jf n i ei m j jf n i ei ef ef cc cc c c 2 32 Phase II Following <REF>Pantel and Lin 2002</REF>, we construct a committee for each semantic class.
This is the basis for Sadlers Analogical Semantics <REF>Sadler 1989</REF>, which according to his report has not proved effective~~~His results may be improved if more sophisticated methods and larger corpora are used to establish similarity between words such as in <TREF>Hindle 1990</TREF>~~~In particular, an enhancement of our disambiguation method, using similarity-based estimation <REF>Dagan, Marcus, and Markovitch 1993</REF>, was evaluated recently~~~In this evaluation the applicability of the disambiguation method was increased by 15, with only a slight decrease in the precision.
 1994 Association for Computational Linguistics Computational Linguistics Volume 20, Number 4 knowledge can be found in the use of statistical data on the occurrence of lexical relations in large corpora eg , <REF>Grishman, Hirschman, and Nhan 1986</REF>~~~The use of such relations mainly relations between verbs or nouns and their arguments and modifiers for various purposes has received growing attention in recent research <REF>Church and Hanks 1990</REF>; <REF>Zernik and Jacobs 1990</REF>; <TREF>Hindle 1990</TREF>; <REF>Smadja 1993</REF>~~~More specifically, two recent works have suggested using statistical data on lexical relations for resolving ambiguity of prepositional phrase attachment <REF>Hindle and Rooth 1991</REF> and pronoun references <REF>Dagan and Itai 1990, 1991</REF>~~~Clearly, statistics on lexical relations can also be useful for target word selection.
The use of such relations mainly relations between verbs or nouns and their arguments and modifiers for various purposes has received growing attention in recent research <REF>Church and Hanks 1990</REF>; <REF>Zernik and Jacobs 1990</REF>; <TREF>Hindle 1990</TREF>; <REF>Smadja 1993</REF>~~~More specifically, two recent works have suggested using statistical data on lexical relations for resolving ambiguity of prepositional phrase attachment <REF>Hindle and Rooth 1991</REF> and pronoun references <REF>Dagan and Itai 1990, 1991</REF>~~~Clearly, statistics on lexical relations can also be useful for target word selection~~~Consider, for example, the Hebrew sentence extracted from the foreign news section of the daily Ha-<REF>Aretz, September 1990</REF> transcripted to Latin letters: 1 Nose ze mana mi-shtei ha-mdinot mi-lahtom al hoze shalom.
We present an empirical evaluation on the task of attaching partof and causation relations, showing an improvement on F-score over a baseline model~~~NLP researchers have developed many algorithms for mining knowledge from text and the Web, including facts <REF>Etzioni et al 2005</REF>, semantic lexicons <REF>Riloff and Shepherd 1997</REF>, concept lists <REF>Lin and Pantel 2002</REF>, and word similarity lists <TREF>Hindle 1990</TREF>~~~Many recent efforts have also focused on extracting binary semantic relations between entities, such as entailments <REF>Szpektor et al 2004</REF>, is-a <REF>Ravichandran and Hovy 2002</REF>, part-of <REF>Girju et al 2003</REF>, and other relations~~~The output of most of these systems is flat lists of lexical semantic knowledge such as Italy is-a country and orange similar-to blue.
There have been many approachs to automatic detection of similar words from text~~~Our method is similar to <TREF>Hindle, 1990</TREF>, <REF>Lin, 1998</REF>, and <REF>Gasperin, 2001</REF> in the use of dependency relationships as the word features~~~Another approach used the words distribution to cluster the words <REF>Pereira, 1993</REF>, and Inoue <REF>Inoue, 1991</REF> also used the word distributional information in the Japanese-English word pairs to resolve the polysemous word problem~~~Wu <REF>Wu, 2003</REF> shows one approach to collect synonymous collocation by using translation information.
3 http:// wwwcisupennedu/ treebank/ rank candidate 1 batt 2 batterie 3 bat 4 BTBTBTBT cover 5 BTY 6 batterry 7 BT BT BT BT BT BT BT BT BT BT adapter 8 bezel 9 BT BT BT BT BT BT BT BT BT BT cheque 10 BTBTBTBT screw Table 3: batterys Synonymous Expression Candidates from the Entire Corpus Author A rank candidate 1 battery 2 controller 3 BT BT BT BT BT BT BT BT Cover 4 APM 5 BTBTBTBT screw 6 mark 7 BT BT BT BT BT BT BT BT BT BT cheque 8 diskette 9 checkmark 10 boot Author B rank candidate 1 batt 2 form 3 protector 4 DISKETTE 5 Mwave 6 BT BT BT BT BT BT BT BT BT BT adapter 7 mouse 8 BT BT BT BT BT BT BT BT BT BT cheque 9 checkmark 10 process Table 4: Noise Candidates from Each Authors Corpus word~~~The words we want to aggregate for text analysis are not rigorous synonyms, but the role is the same, so we have to consider the syntactic relation based on the assumptions that words with the same role tend to modify or be modified by similar words <TREF>Hindle, 1990</TREF>; <REF>Strzalkowski, 1992</REF>~~~On the other hand, window-based techniques are not suitable for our data, because the documents are written by several authors who have a variety of different writing styles eg selecting different prepositions and articles~~~Therefore we consider only syntactic features: dependency pairs, which consist of nouns, verbs, and their relationships.
All digits in both patterns and sentences are replaced with a common marker, such 810 that any two numerical values with the same number of digits will overlap during matching~~~Many methods have been proposed to compute distributional similarity between words, eg, <TREF>Hindle, 1990</TREF>, <REF>Pereira et al , 1993</REF>, <REF>Grefenstette, 1994</REF> and <REF>Lin, 1998</REF>~~~Almost all of the methods represent a word by a feature vector, where each feature corresponds to a type of context in which the word appeared~~~They differ in how the feature vectors are constructed and how the similarity between two feature vectors is computed.
et al, 1999; <REF>Torisawa, 2002</REF>~~~Others proposed distributional similarity measures between words <TREF>Hindle, 1990</TREF>; <REF>Lin, 1998</REF>; <REF>Lee, 1999</REF>; <REF>Weeds et al, 2004</REF>~~~Once such similarity is defined, it is trivial to perform clustering~~~On the other hand, some researchers utilized co-occurrence for word clustering.
In fact, corpus linguistics became a popular research field because of the claim that shallow techniques could overcome the lexical coverage bottleneck of traditional NLP techniques~~~Among the applications of collocational analysis for lexical acquisition are: the derivation of syntactic disambiguation cues <REF>Basili et al 1991, 1993a</REF>; <REF>Hindle and Rooths 1991</REF>,1993; <REF>Sekine 1992</REF> <REF>Bogges et al 1992</REF>, sense preference <REF>Yarowski 1992</REF>, acquisition of selectional restrictions <REF>Basili et al 1992b, 1993b</REF>; <REF>Utsuro et al 1993</REF>, lexical preference in generation <REF>Smadjia 1991</REF>, word clustering <REF>Pereira 1993</REF>; <TREF>Hindle 1990</TREF>; <REF>Basili et al 1993c</REF>, etc In the majority of these papers, even though the precedent or subsequent statistical processing reduces the number of accidental associations, very large corpora 10,000,000 words are necessary to obtain reliable data on a large enough number of words~~~In addition, most papers produce a performance evaluation of their methods but do not provide a measure of the coverage, ie the percentage of cases for which their method actually provides a right or wrong solution~~~It is quite common that results are discussed only for 10-20 cases.
This is known as the Distributional Hypothesis in linguistics <REF>Harris, 1968</REF>~~~For example, the words test and exam are similar because both of them follow verbs such as administer, cancel, cheat on, conduct,  and both of them can be preceded by adjectives such as academic, comprehensive, diagnostic, difficult,  Many methods have been proposed to compute distributional similarity between words <TREF>Hindle, 1990</TREF>; <REF>Pereira et al , 1993</REF>; <REF>Grefenstette, 1994</REF>; <REF>Lin, 1998</REF>~~~Almost all of the methods represent a word by a feature vector where each feature corresponds to a type of context in which the word appeared~~~They differ in how the feature vectors are constructed and how the similarity between two feature vectors is computed.
This is known as the Distributional Hypothesis in linguistics <REF>Harris, 1968</REF>~~~For example, the words test and exam are similar because both of them can follow verbs such as administer, cancel, cheat on, conduct, etc Many methods have been proposed to compute distributional similarity between words, eg, <TREF>Hindle, 1990</TREF>; <REF>Pereira et al , 1993</REF>; <REF>Grefenstette, 1994</REF>; <REF>Lin, 1998</REF>~~~Almost all of the methods represent a word by a feature vector where each feature corresponds to a type of context in which the word appeared~~~They differ in how the feature vectors are constructed and how the similarity between two feature vectors is computed.
<REF>Schutze 1998</REF> used bag-of-words contexts for sense discrimination~~~<TREF>Hindle 1990</TREF> grouped nouns into thesaurus-like lists based on the similarity of their syntactic contexts~~~Our approach is similar with the difference that we do not group noun arguments into finite categories, but instead leave the category boundaries blurry and allow overlaps~~~The DDNs are essentially a form of world knowledge which we extract automatically and apply to VSD.
or the cooccurrence of two words within a limited distance in the context~~~Statistical data about these various cooccurrence relations is employed for a variety of applications, such as speech recognition <REF>Jelinek, 1990</REF>, language generation <REF>Smadja and McKeown, 1990</REF>, lexicography <REF>Church and Hanks, 1990</REF>, machine translation Brown et al , ; <REF>Sadler, 1989</REF>, information retrieval <REF>Maarek and Smadja, 1989</REF> and various disambiguation tasks <REF>Dagan et al , 1991</REF>; <REF>Hindle and Rooth, 1991</REF>; <REF>Grishman et al , 1986</REF>; <REF>Dagan and Itai, 1990</REF>~~~A major problem for the above applications is how to estimate the probability of cooccurrences that were not observed in the training corpus~~~Due to data sparseness in unrestricted language, the aggregate probability of such cooccurrences is large and can easily get to 25 or more, even for a very large training corpus <REF>Church and Mercer, 1992</REF>.
To account for this problem we developed a simple heuristic that searches for words that are potentially similar to w, using thresholds on mutual information values and frequencies of cooccurrence pairs~~~The search is based on the property that when computing simwl, w2, words that have high mutual information values 5The nominator in our metric resembles the similarity metric in <TREF>Hindle, 1990</TREF>~~~We found, however, that the difference between the two metrics is important, because the denominator serves as a normalization factor~~~with both wl and w2 make the largest contributions to the value of the similarity measure.
Semantic variation is rarely studied in specialized domains~~~Works on word similarity and word sense disambiguation are generally based on statistical methods designed for large or even very large corpora <TREF>Hindle, 1990</TREF>; <REF>Agirre and Rigau, 1996</REF>~~~Therefore, they cannot be applied for technical documents which usually are medium size corpora~~~However, dealing with already linguistic filtered data, <REF>Assadi, 1997</REF> aims at statistically build rough clusters supposing that similar candidate terms have similar expansions.
We applied both a neural network model and a linguistic method, that is syntactic information, to a large corpora and extracted necessary information~~~To extract semantic information of words such as synonyms and antonyms from corpora, previous research used syntactic structures <TREF>Hindle 1990</TREF>, <REF>Hatzivassiloglou 1993</REF> and <REF>Tokunaga 1995</REF>, response time to associate synonyms and antonyms in psychological experiments <REF>Gross 1989</REF>, or extracting related words automatically from corpora <REF>Grefensette 1994</REF>~~~Most lexical classification is based on parts of speech, as they have very important semantic information~~~For examples, typically, an adjective refers to an attribute, a verb refers to a motion or an event, and a noun refers to an object.
3 RELATED WORK Interest in extracting lexical and especially collocational information from text has risen dramatically in the last two years, as sufficiently large corpora and sufficiently cheap computation have become available~~~Three recent papers in this area are <REF>Church and Hanks 1990</REF>, <TREF>Hindle 1990</TREF>, and <REF>Smadja and McKeown 1990</REF>~~~The latter two are concerned exclusively with collocation relations between open-class words and not with grammatical properties~~~Church is also interested primarily in open-class collocations, but he does discuss verbs that tend to be followed by infinitives within his mutual information framework.
Following the idea proposed in Harris Distributional Hypothesis <REF>Harris, 1985</REF>, that words occurring in similar contexts are semantically similar, many works have used different definitions of context to identify various types of semantic similarity~~~<TREF>Hindle 1990</TREF> uses a mutual-information based metric derived from the distribution of subject, verb and object in a large corpus to classify nouns~~~Pereira et al~~~1993 cluster nouns according to their distribution as direct objects of verbs, using information-theoretic tools the predecessors of the tools we use in this work.
Distributional similarity represents the relatedness of two words by the commonality of contexts the words share, based on the distributional hypothesis <REF>Harris, 1985</REF>, which states that semantically similar words share similar contexts~~~A number of researches which utilized distributional similarity have been conducted, including <TREF>Hindle, 1990</TREF>; <REF>Lin, 1998</REF>; <REF>Geffet and Dagan, 2004</REF> and many others~~~Although they have been successful in acquiring related words, various parameters such as similarity measures and weighting are involved~~~As Weeds et al.
In our experiments we set   095~~~32 m,n-cousin Classification The classifier for learning coordinate terms relies on the notion of distributional similarity, ie, the idea that two words with similar meanings will be used in similar contexts <TREF>Hindle, 1990</TREF>~~~We extend this notion to suggest that words with similar meanings should be near each other in a semantic taxonomy, and in particular will likely share a hypernym as a near parent~~~Our classifier for m,n-cousins is derived from the algorithm and corpus given in <REF>Ravichandran et al , 2005</REF>.
We agree with the differential definition of semantics : the meaning of the morpho-lexical units is not defined by reference to a concept, but rather by contrast with other units <REF>Rastier et al , 1994</REF>~~~In fact, we are considering word usage rather than word meanin <REF>Zernik, 1990</REF> following in this the distributional point of view, see <REF>Harris, 1968</REF>, <TREF>Hindle, 1990</TREF>~~~Statistical or probabilistic methods are often used to extract semantic clusters from corpora in order to build lexical resources for ANLP tools <TREF>Hindle, 1990</TREF>, <REF>Zernik, 1990</REF>, <REF>Resnik, 1993</REF>, or for automatic thesaurus generation <REF>Grefenstette, 1994</REF>~~~We use similar techniques, enriched by a preliminaxy morpho-syntaztic analysis, in order to perform knowledge acquisition and modeling for a specific task eg : electrical network planning.
In fact, we are considering word usage rather than word meanin <REF>Zernik, 1990</REF> following in this the distributional point of view, see <REF>Harris, 1968</REF>, <TREF>Hindle, 1990</TREF>~~~Statistical or probabilistic methods are often used to extract semantic clusters from corpora in order to build lexical resources for ANLP tools <TREF>Hindle, 1990</TREF>, <REF>Zernik, 1990</REF>, <REF>Resnik, 1993</REF>, or for automatic thesaurus generation <REF>Grefenstette, 1994</REF>~~~We use similar techniques, enriched by a preliminaxy morpho-syntaztic analysis, in order to perform knowledge acquisition and modeling for a specific task eg : electrical network planning~~~Moreover, we are dealing with language for specific purpose texts and not with general texts.
Our approach avoids hand-crafting a set of spe11 ci c indicator features; we simply use the distribution of the pronouns context~~~Our method is thus related to previous work based on <REF>Harris 1985</REF>s distributional hypothesis2 It has been used to determine both word and syntactic path similarity <TREF>Hindle, 1990</TREF>; <REF>Lin, 1998a</REF>; <REF>Lin and Pantel, 2001</REF>~~~Our work is part of a trend of extracting other important information from statistical distributions~~~<REF>Dagan and Itai 1990</REF> use the distribution of a pronouns context to determine which candidate antecedents can  t the context.
The hypothesis states that words that occur in the same contexts tend to have similar meaning~~~Researchers have mostly looked at representing words by their surrounding words <REF>Lund and Burgess 1996</REF> and by their syntactical contexts <TREF>Hindle 1990</TREF>; <REF>Lin 1998</REF>~~~However, these representations do not distinguish between the different senses of words~~~Our framework utilizes these principles and representations to induce disambiguated feature vectors.
2a~~~If the bound is too tight to allow the correct parse of some sentence, we would still like to allow an accurate partial parse: a sequence of accurate parse fragments <TREF>Hindle, 1990</TREF>; <REF>Abney, 1991</REF>; <REF>Appelt et al , 1993</REF>; <REF>Chen, 1995</REF>; <REF>Grefenstette, 1996</REF>~~~Furthermore, we would like to use the fact that some fragment sequences are presumably more likely than others~~~Our partial parses will look like the one in Fig.
<REF>Smadja 1992</REF> gathered co-occurrences within fiveword windows to find collocations, particularly in specific domains~~~<TREF>Hindle 1990</TREF> classified nouns on the basis of co-occurring patterns of subjectverb and verb-object pairs~~~<REF>Hatzivassiloglou and MeKeown 1993</REF> clustered adjectives into semantic classes, and Pereira et al~~~1993 clustered nouns on their appearance ill verb-object pairs.
Also, the patterns are learned with the specific goal of scaling to the terascale see Table 2~~~22 Co-occurrence-based approaches The second class of algorithms uses cooccurrence statistics <TREF>Hindle 1990</TREF>, <REF>Lin 1998</REF>~~~These systems mostly employ clustering algorithms to group words according to their meanings in text~~~Assuming the distributional hypothesis <REF>Harris 1985</REF>, words that occur in similar grammatical contexts are similar in meaning.
4 Towards an adequate similarity esfimatation for the building of ontologies The comparison with the similarity score of <TREF>Hindle, 1990</TREF> shows that SYCLADE similarity indicator is specifically relevant for ontology bootstrap and tuning~~~Hindle uses the observed frequencies within a specific syntactic pattern subject/verb, and verb/object to derive a cooccu,> rence score which is an estimate of mutual information <REF>Church and Hanks, 1990</REF>~~~We adapted this score to noun phrase patterns However the similarity measures based on cooccurrence scores and nominal phrase patterns are less relevant for an ontological analysis~~~The subgraph of the chirurgical acts words, which is easy to identify from the SYCLADE graph fig.
Automatic exploration of a sublanguage corpus constitutes a first step towards identifying the semantic classes and relationships which are relevant for this sublanguage~~~In the past five years, important research on the automatic acquisition of word classes based on lexical distribution has been published <REF>Church and Hanks, 1990</REF>; <TREF>Hindle, 1990</TREF>; <REF>Smadja, 1993</REF>; Greinstette, 1994; <REF>Grishman and Sterling, 1994</REF>~~~Most of these approaches, however, need large or even very large corpora in order for word classes to be discovered 1 whereas it is often the case that the data to be processed are insufficient to provide reliable lexical intbrmation~~~In other words, it is not always possible to resort to statistical methods.
2 Simplifying parse trees to classify words 21 The need for normalized syntactic contexts As Hindles work proves it, among others <REF>Grishman and Sterling, 1994</REF>; <REF>Grefenstette, 1994</REF>:, the mere existence of robust syntactic parsers makes it possible to parse large corpora in order to automate the discovery of syntactic patterns in the spirit of Harriss distributional hypothesis~~~Itowever, Harris methodology implies also to simplify and transform each parse tree 2, so as to obtain so-called elementary sentences exhibiting the main conceptual classes for the domain Sager lIaor instance, Hindle <TREF>Hindle, 1990</TREF> needs a six million word corpus in order to extract noun similarities from predicate-argunlent structures~~~2Changing passive into active sentences, using a verb instead of a nominalization, and so on~~~490 NP NPa AP4 I I Nr As I I stenose serre NPo PP2 Pa NP6 I de D9 NPlo le NPll AP12 t NPla AP14 A15 I I I N Ar gauche I I tronc eorninun Iigure 1: Parse tree for stenose serre de le hone commun gauche et al , 1987.
Although Stairmand <REF>Stairmand, 1997</REF> and Richardson <REF>Richardson and Smeaton, 1995</REF> have proposed the use of WordNet in information retrieval, they did not used WordNet in the query expansion framework~~~Our predicate-argument structure-based thesatmis is based on the method proposed by Hindie <TREF>Hindle, 1990</TREF>, although Hindle did not apply it to information retrieval~~~Instead, he used mutual information statistics as a Similarity coefficient, wheras we used the Dice coefficient for normalization purposes~~~Hindle only extracted the subject-verb and the object-verb predicatearguments, while we also extract adjective-noun predicate-arguments.
Among many kinds of lexical relations, synonyms are especially useful ones, having broad range of applications such as query expansion technique in information retrieval and automatic thesaurus construction~~~Various methods <TREF>Hindle, 1990</TREF>; <REF>Lin, 1998</REF>; <REF>Hagiwara et al , 2005</REF> have been proposed for synonym acquisition~~~Most of the acquisition methods are based on distributional hypothesis <REF>Harris, 1985</REF>, which states that semantically similar words share similar contexts, and it has been experimentally shown considerably plausible~~~However, whereas many methods which adopt the hypothesis are based on contextual clues concerning words, and there has been much consideration on the language models such as Latent Semantic Indexing <REF>Deerwester et al , 1990</REF> and Probabilistic LSI <REF>Hofmann, 1999</REF> and synonym acquisition method, almost no attention has been paid to what kind of categories of contextual information, or their combinations, are useful for word featuring in terms of synonym acquisition.
However, whereas many methods which adopt the hypothesis are based on contextual clues concerning words, and there has been much consideration on the language models such as Latent Semantic Indexing <REF>Deerwester et al , 1990</REF> and Probabilistic LSI <REF>Hofmann, 1999</REF> and synonym acquisition method, almost no attention has been paid to what kind of categories of contextual information, or their combinations, are useful for word featuring in terms of synonym acquisition~~~For example, <TREF>Hindle 1990</TREF> used cooccurrences between verbs and their subjects and objects, and proposed a similarity metric based on mutual information, but no exploration concerning the effectiveness of other kinds of word relationship is provided, although it is extendable to any kinds of contextual information~~~<REF>Lin 1998</REF> also proposed an information theorybased similarity metric, using a broad-coverage parser and extracting wider range of grammatical relationship including modifications, but he didnt further investigate what kind of relationships actually had important contributions to acquisition, either~~~The selection of useful contextual information is considered to have a critical impact on the performance of synonym acquisition.
Some of our initial data suggest that the hypothesis of deep semantic selection may in fact be correct, as well as indicating what the nature of the coercion rules may be~~~Using techniques described in <REF>Church and Hindle 1990</REF>, <REF>Church and Hanks 1990</REF>, and <REF>Hindle and Rooth 1991</REF>, Figure 4 shows some examples of the most frequent V-O pairs from the AP corpus~~~Corpus studies confirm similar results for weakly intensional contexts such as the complement of coercive verbs such as veto~~~These are interesting because regardless of the noun type appearing as complement, it is embedded within a semantic interpretation of the proposal to, thereby clothing the complement within an intensional context.
While full automatic extraction of semantic collocations is not yet feasible, some recent research in related areas is promising~~~<TREF>Hindle 1990</TREF> reports interesting results of this kind based on literal collocations, where he parses the corpus <REF>Hindle 1983</REF> into predicate-argument structures and applies a mutual information measure <REF>Fano 1961</REF>; <REF>Magerman and Marcus 1990</REF> to weigh the association between the predicate and each of its arguments~~~For example, as a list of the most frequent objects for the verb drink in his corpus, Hindle found beer, tea, Pepsi, and champagne~~~Based on the distributional hypothesis that the degree of shared contexts is a similarity measure for words, he develops a similarity metric for nouns based on their substitutability in certain verb contexts.
<REF>Although Stairmand 1997</REF> and <REF>Richardson 1995</REF> proposed the use of WordNet in information retrieval, they did not use WordNet in the query expansion framework~~~Our syntactic-relation-based thesaurus is based on the method proposed by <TREF>Hindle 1990</TREF>, although Hindle did not apply it to information retrieval~~~Hindle only extracted subject-verb and object-verb relations, while we also extract adjective-noun and noun-noun relations, in the manner of <REF>Grefenstette 1994</REF>, who applied his 99 Proceedings of EACL 99 Table 3: Average non-interpolated precision for expansion using single or combined thesauri~~~Topic Type Base Title 01175 Description 01428 All 01976 Expanded with WordNet Roget Syntac Cooccur Combined only only only only method 01276 01236 01386 01457 02314 86 52  179 240 969 01509 0,1477 01648 01693 02645 57 34 154 185 852 02010 01999 02131 02191 02724 17 12 78 108 378 syntactically-based thesaurus to information retrieval with mixed results.
232 Syntactically-based Thesaurus In contrast to the previous section, this method attempts to gather term relations on the basis of linguistic relations and not document cooccurrence statistics~~~Words appearing in similax grammatical contexts are assumed to be similar, and therefore classified into the same class <REF>Lin, 1998</REF>; <REF>Grefenstette, 1994</REF>; <REF>Grefenstette, 1992</REF>; <REF>Ruge, 1992</REF>; <TREF>Hindle, 1990</TREF>~~~First, all the documents are parsed using the Apple Pie Parser~~~The Apple Pie Parser is a natural language syntactic analyzer developed by Satoshi Sekine at New York University <REF>Sekine and Grishman, 1995</REF>.
The problemis that for large enough corpora the number of possible joint events is much larger than the number of event occurrences in the corpus, so many events are seen rarely or never, making their frequency counts unreliable estimates of their probabilities~~~<TREF>Hindle 1990</TREF> proposed dealing with the sparseness problem by estimating the likelihood of unseen events from that of similar events that have been seen~~~For instance, one may estimate the likelihood of a particular direct object for a verb from the likelihoods of that direct object for similar verbs~~~This requires a reasonable definition of verb similarity and a similarity estimation method.
With seemingly endless amounts of textual data at our disposal, we have a tremendous opportunity to automatically grow semantic term banks and ontological resources~~~To date, researchers have harvested, with varying success, several resources, including concept lists <REF>Lin and Pantel 2002</REF>, topic signatures <REF>Lin and Hovy 2000</REF>, facts <REF>Etzioni et al 2005</REF>, and word similarity lists <TREF>Hindle 1990</TREF>~~~Many recent efforts have also focused on extracting semantic relations between entities, such as entailments <REF>Szpektor et al 2004</REF>, is-a <REF>Ravichandran and Hovy 2002</REF>, part-of <REF>Girju et al 2006</REF>, and other relations~~~The following desiderata outline the properties of an ideal relation harvesting algorithm:  Performance: it must generate both high precision and high recall relation instances;  Minimal supervision: it must require little or no human annotation;  Breadth: it must be applicable to varying corpus sizes and domains; and  Generality: it must be applicable to a wide variety of relations ie , not just is-a or part-of.
We are not aware of other work that uses such collocations as we do~~~Features identified using distributional similarity have previously been used for syntactic and semantic disambiguation <TREF>Hindle 1990</TREF>; <REF>Dagan, Pereira, and Lee 1994</REF> and to develop lexical resources from corpora <REF>Lin 1998</REF>; <REF>Riloff and Jones 1999</REF>~~~We are not aware of other work identifying and using density parameters as described in this article~~~Since our experiments, other related work in NLP has been performed.
Typical feature weighting functions include the logarithm of the frequency of word-feature cooccurrence <REF>Ruge, 1992</REF>, and the conditional probability of the feature given the word within probabilistic-based measures <REF>Pereira et al , 1993</REF>, <REF>Lee, 1997</REF>, <REF>Dagan et al , 1999</REF>~~~Probably the most widely used association weight function is point-wise Mutual Information MI <REF>Church et al , 1990</REF>, <TREF>Hindle, 1990</TREF>, <REF>Lin, 1998</REF>, <REF>Dagan, 2000</REF>, defined by:  ,log, 2 fPwP fwPfwMI  A known weakness of MI is its tendency to assign high weights for rare features~~~Yet, similarity measures that utilize MI showed good performance~~~In particular, a common practice is to filter out features by minimal frequency and weight thresholds.
Finally, a novel feature weighting and selection function is presented, which yields superior feature vectors and better word similarity performance~~~Distributional Similarity has been an active research area for more than a decade <TREF>Hindle, 1990</TREF>, <REF>Ruge, 1992</REF>, <REF>Grefenstette, 1994</REF>, <REF>Lee, 1997</REF>, <REF>Lin, 1998</REF>, <REF>Dagan et al , 1999</REF>, <REF>Weeds and Weir, 2003</REF>~~~Inspired by Harris distributional hypothesis <REF>Harris, 1968</REF>, similarity measures compare a pair of weighted feature vectors that characterize two words~~~Features typically correspond to other words that co-occur with the characterized word in the same context.
Let Seenrp be the set of seen headwords for an argument rp of a predicate p Then we model the selectional preference S of rp for a possible headword w0 as a weighted sum of the similarities between w0 and the seen headwords: Srpw0  summationdisplay wSeenrp simw0,wwtrpw simw0,w is the similarity between the seen and the potential headword, and wtrpw is the weight of seen headword w Similarity simw0,w will be computed on the generalization corpus, again on the basis of extracted tuples p,rp,w~~~We will be using the similarity metrics shown in Table 1: Cosine, the Dice and Jaccard coefficients, and <TREF>Hindles 1990</TREF> and <REF>Lins 1998</REF> mutual information-based metrics~~~We write f for frequency, I for mutual information, and Rw for the set of arguments rp for which w occurs as a headword~~~In this paper we only study corpus-based metrics.
Computing MI scores is by now a standard procedure for measuring the co-occurrence between objects relative to their overall occurrence~~~MI is defined in general as follows: y I ix y  log2 Px Py We can use this definition to derive an estimate of the connectedness between words, in terms of collocations <REF>Smadja, 1993</REF>, but also in terms of phrases and grammatical relations <TREF>Hindle, 1990</TREF>~~~For instance the co-occurrence of verbs and the heads of their NP objects iN: size of the corpus, ie the number of stems: N Cobj v n  log2 /v /n N N All nouns are now classified by running a similaxity measure over their MI scores and the MI scores of each CoRELEx class~~~For this we use the Jaccard measure that compares objects relative to the attributes they share <REF>Grefenstette, 1994</REF>.
Evaluation of automatically generated lexical resources is a difficult problem~~~In <TREF>Hindle, 1990</TREF>, a small set of sample results are presented~~~In <REF>Smadja, 1993</REF>, automatically extracted collocations are judged by a lexicographer~~~In <REF>Dagan et al, 1993</REF> and <REF>Pereira et al, 1993</REF>, clusters of similar words are evaluated by how well they are able to recover data items that are removed from the input corpus one at a time.
We also constructed several other thesauri using the same corpus, but with the similarity measures in Figure 1~~~The measure simHindle is the same as the similarity measure proposed in <REF>Hindie, 1990</REF>, except that it does not use dependency triples with negative mutual information~~~The measure simHindle r is the same as simHindle except that all types of dependency relationships are used, instead of just subject and object relationships~~~The measures simcosine, simdice and simJacard are versions of similarity measures commonly used in information retrieval Frakes and Baeza-<REF>Yates, 1992</REF>.
and Conclusion There have been many approaches to automatic detection of similar words from text corpora~~~Ours is 772 similar to <REF>Grefenstette, 1994</REF>; <TREF>Hindle, 1990</TREF>; <REF>Ruge, 1992</REF> in the use of dependency relationship as the word features, based on which word similarities are computed~~~Evaluation of automatically generated lexical resources is a difficult problem~~~In <TREF>Hindle, 1990</TREF>, a small set of sample results are presented.
The similarity is usually calculated from a thesaurus~~~Since a handmade thesaurus is not slfitahle for machine use, and expensive to compile, automatical construction ofa thesaurus has been attempted using corpora <TREF>Hindle, 1990</TREF>~~~llowever, the thesaurus constructed by such ways does not contain so many nouns, and these nouns are specified by the used corpus~~~In other words, we cannot construct the general thesaurus from only a corpus.
In all, we obtained 2,708,135 bits of generalized cooccurrence data, which consisted of 115,330 types~~~23 Measuring the similarity between classes step 3 In step 3, we measure the similarity between two primitive classes by using the method given by Hindle <TREF>Hindle, 1990</TREF>~~~First, we define the nmtual information MI of a verb v and a primitive class C as follows~~~ZmY2 M ,Clogs N eq1 N N In the above equation, N is the total number of cooccurrence data bits, and fv and fC are the frequency of v and C in the whole cooccurrence data set respectively, and fv, C is the frequency of the cooccurrence data C, wo, v.
In the next section, we proceed to apply this technique for generating noun similarity lists~~~4 Building Noun Similarity Lists A lot of work has been done in the NLP community on clustering words according to their meaning in text <TREF>Hindle, 1990</TREF>; <REF>Lin, 1998</REF>~~~The basic intuition is that words that are similar to each other tend to occur in similar contexts, thus linking the semantics of words with their lexical usage in text~~~One may ask why is clustering of words necessary in the first place.
The UMass/MUC-3 parser would clearly need additional mechanisms to handle the ensuing part of speech and 7Other parsing errors occurred throughout the training set, but only those instances where the antecedent was not recognized as a constituent and the wh-word had an anteceden0 were discarded~~~8Interestingly, in work on the automated classification of nouns, <TREF>Hindle, 1990</TREF> also noted problems with empty words that depend on their complements for meaning~~~221 word sense disambiguation problems~~~However, recent research in these areas indicates that automated approaches for these tasks may be feasible see, for example, Brown, Della Pietra, <REF>Della Pietra,  Mercer, 1991</REF> and l-<REF>Iindle, 1983</REF>.
The corpus is relatively small it contains approximately 450,000 words and 18,750 sentences~~~In comparison, most corpus-based algorithms employ substantially larger corpora eg , 1 million words de <REF>Marcken, 1990</REF>, 25 million words <REF>Brent, 1991</REF>, 6 million words <TREF>Hindle, 1990</TREF>, 13 million words <REF>Hindle,  Rooth, 1991</REF>~~~Relative pronoun processing is especially important for the MUC-3 corpus because approximately 25 of the sentences contain at least one relative pronoun~~~3 In fact, the relative pronoun who occurs in approximately 1 out of every 10 sentences.
In previous research, word meanings have been statistically modeled based on syntactic information derived from a corpus~~~<TREF>Hindle 1990</TREF> used noun-verb syntactic relations, and <REF>Hatzivassiloglou and McKeown 1993</REF> used coordinated adjective-adjective modifier pairs~~~These methods are useful for the organization of words deep within a hierarchy, but do not seem to provide a solution for the top levels of the hierarchy~~~To find an objective hierarchical word structure, we utilize the complementary similarity measure CSM, which estimates a one-to-many relation, such as superordinatesubordinate relations <REF>Hagita and Sawaki 1995</REF>, <REF>Yamamoto and Umemura 2002</REF>.
Do we really need to fully parse the texts in every application~~~Some researchers apply shallow or partial parsers <REF>Smadja, 1991</REF>; <TREF>Hindle, 1990</TREF> to acquiring specific patterns from texts~~~These tell us that it is not necessary to completely parse the texts for some applications~~~This paper will propose a probabilistic partial parser and incorporate linguistic knowledge to extract noun phrases.
We also constructed several other thesauri using the same corpus, but with the similarity measures in Figure 1~~~The measure simHinate is the same as the similarity measure proposed in <TREF>Hindle, 1990</TREF>, except that it does not use dependency triples with negative mutual information~~~The measure simHindle,, is the same as simHindle except that all types of dependency relationships are used, instead of just subject and object relationships~~~The measures simcosine, simdice and simdacard are versions of similarity measures commonly used in information retrieval Frakes and Baeza-<REF>Yates, 1992</REF>.
and Conclusion There have been many approaches to automatic detection of similar words from text corpora~~~Ours is 772 similar to <REF>Grefenstette, 1994</REF>; <TREF>Hindle, 1990</TREF>; <REF>Ruge, 1992</REF> in the use of dependency relationship as the word features, based on which word similarities are computed~~~Evaluation of automatically generated lexical resources is a difficult problem~~~In <TREF>Hindle, 1990</TREF>, a small set of sample results are presented.
Evaluation of automatically generated lexical resources is a difficult problem~~~In <TREF>Hindle, 1990</TREF>, a small set of sample results are presented~~~In <REF>Smadja, 1993</REF>, automatically extracted collocations are judged by a lexicographer~~~In <REF>Dagan et al , 1993</REF> and Pereira et al ,  993, clusters of similar words are evaluated by how well they are able to recover data items that are removed from the input corpus one at a time.
2 Acquiring syntactic associations Clustered association data are collected by first extracting from the corpus all the syntactically related word pairs~~~Combining statistical and parsing methods has been done by <TREF>Hindle, 1990</TREF>; Hindle and Rooths,1991 and <REF>Smadja and McKewon, 1990</REF>; Smadja,1991~~~The novel aspect of our study is that we collect not only operational pairs, but triples, such as Nprep N, VprepN etc In fact, the preposition convey important information on the nature of the semantic link between syntactically related content words~~~By looking at the preposition, it is possible to restrict the set of semantic relations underlying a syntactic relation eg forpurpose,beneficiary.
The results of these studies have important applications in lexicography, to detect lexicosyntactic regularities <REF>Church and Hanks, 1990</REF>1 Calzolari and Bindi,1990, such as, for example support verbs eg make-decision prepositional verbs eg rely-upon idioms, semantic relations eg partof and fixed expressions eg kick the bucket~~~In Hindle,1990; <REF>Zernik, 1989</REF>; Webster el <REF>Marcus, 1989</REF> cooccurrence analyses augmented with syntactic parsing is used for the purpose of word classification~~~All these studies are based on th strong assumption that syntactic similarity in wor patterns implies semantic similarity~~~In Guthrie el al , 1991, sets of consistently contiguous word, neighbourhood are extracted from machinereadable dictionaries, to help semantic disambiguation in information retrieval.
In <REF>Smadja, 1989</REF>, <REF>Zernik and Jacobs, 1990</REF>, the associations are filtered by selecting the word pairs x,y whose frequency of occurrence is above fks, where f is the average appearance, s is the standard deviation, and k is an empirically determined factor~~~<TREF>Hindle, 1990</TREF>; Hindle and Rooths,1991 and <REF>Smadja, 1991</REF> use syntactic markers to increase the significance of the data~~~<REF>Guthrie et al , 1991</REF> uses the subject classification given in machine-readable dictionaries eg economics, engineering, etc~~~to reinforce cooccurence links.
More sophisticated linguistic information comes in several forms, all of which may need to be represented if performance in an automatic categorisation experiment is to be improved~~~Typical examples of linguistically sophisticated annotation include tagging words with their syntactic category although this has not been found to be effective for 1R, lemma of the word eg corpus for corpora, phrasal information eg identifying noun groups and phrases <REF>Lewis 1992c</REF>, <REF>Church 1988</REF>, and subject-predicate identification eg <TREF>Hindle 1990</TREF>~~~For the RAPRA corpus, we currently identify noun groups and adjective groups~~~This is achieved in a manner similar to Churchs 1988 PARTS algorithm used by Lewis 1992bc, in the sense that its main properties are robustness and corpus sensitivity.
In NLP, representing verb semantics with their thematic roles is a consolidated practice, even though theoretical researches <REF>Pustejovski 1991</REF> propose more rich and formal representation frameworks~~~More recent papers <TREF>Hindle 1990</TREF>, <REF>Pereira and Tishby 1992</REF> proposed to cluster nouns on the basis of a metric derived from the distribution of subject, verb and object in the texts~~~Both papers use as a source of information large corpora, but differ in the type of statistical approach used to determine word similarity~~~These studies, though valuable, leave several open problems: 70 1 A metric of conceptual closeness based on mere syntactic similarity is questionable, particularly if applied to verbs.
1993 and <REF>Lee 1999</REF>, among others~~~We use the cosine similarity measure for windowbased contexts and the following commonly used similarity measures for the syntactic vector space: <TREF>Hindles 1990</TREF> measure, the weighted Lin measure <REF>Wu and Zhou, 2003</REF>, the -Skew divergence measure <REF>Lee, 1999</REF>, the Jensen-Shannon JS divergence measure <REF>Lin, 1991</REF>, Jaccards coef cient van <REF>Rijsbergen, 1979</REF> and the Confusion probability <REF>Essen and Steinbiss, 1992</REF>~~~The Jensen-Shannon measure JS x1, x2  summationtext yY summationtext xx1,x2 parenleftbigg P yx log parenleftbigg Pyx 1 2 Pyx1Pyx2 parenrightbiggparenrightbigg subsequently performed best for our task~~~We compare the different ranking methodologies and data sets with respect to a manually-de ned gold standard list of 20 goal-type verbs and 20 nouns.
First, this approach leverages a little a priori grammatical knowledge using statistical inference~~~Most work on corpora of naturally occurring language 244 Michael R Brent From Grammar to Lexicon either uses no a priori grammatical knowledge <REF>Brill and Marcus 1992</REF>; <REF>Ellison 1991</REF>; <REF>Finch and Chater 1992</REF>; <REF>Pereira and Schabes 1992</REF>, or else it relies on a large and complex grammar <REF>Hindle 1990, 1991</REF>~~~One exception is <REF>Magerman and Marcus 1991</REF>, in which a small grammar is used to aid learning~~~1 A second difference is that the work reported here uses inferential rather than descriptive statistics.
In other words, it uses statistical methods to infer facts about the language as it exists in the minds of those who produced the corpus~~~Many other projects have used statistics in a way that summarizes facts about the text but does not draw any explicit conclusions from them <REF>Finch and Chater 1992</REF>; <TREF>Hindle 1990</TREF>~~~On the other hand, <REF>Hindle 1991</REF> does use inferential statistics, and <REF>Brill 1992</REF> recognizes the value of inference, although he does not use inferential statistics per se~~~Finally, many other projects in machine learning of natural language use input that is annotated in some way, either with part-of-speech tags <REF>Brill 1992</REF>; <REF>Brill and Marcus 1992</REF>; <REF>Magerman and Marcus 1990</REF> or with syntactic brackets <REF>Pereira and Schabes 1992</REF>.
This is the basis for Sadlers <REF>Analogical Semantics 1989</REF> which has not yet proved effective~~~His results may be improved if more sophisticated techniques and larger corpora are used to establish similarity between words such as in <TREF>Hindle, 1990</TREF>~~~Conflicting data~~~In very few cases two alternatives were supported equally by the statistical data, thus preventing a selection.
Consequently, a possible though partial alternative to using manually constructed knowledge can be found in the use of statistical data on the occurrence of lexical relations in large corpora~~~The use of such relations mainly relations between verbs or nouns and their arguments and modifiers for various purposes has received growing attention in recent research <REF>Church and Hanks, 1990</REF>; <REF>Zernik and Jacobs, 1990</REF>; <TREF>Hindle, 1990</TREF>~~~More specifically, two recent works have suggested to use statistical data on lexical relations for resolving ambiguity cases of PP-attachment <REF>Hindle and Rooth, 1990</REF> and pronoun references <REF>Dagan and Itai, 1990a</REF>; <REF>Dagan and Itai, 1990b</REF>~~~Clearly, statistical methods can be useful also for target word selection.
The use of such relations mainly relations between verbs or nouns and their arguments and modifiers for various purposes has received growing attention in recent research <REF>Church and Hanks, 1990</REF>; <REF>Zernik and Jacobs, 1990</REF>; <TREF>Hindle, 1990</TREF>~~~More specifically, two recent works have suggested to use statistical data on lexical relations for resolving ambiguity cases of PP-attachment <REF>Hindle and Rooth, 1990</REF> and pronoun references <REF>Dagan and Itai, 1990a</REF>; <REF>Dagan and Itai, 1990b</REF>~~~Clearly, statistical methods can be useful also for target word selection~~~Consider, for example, the Hebrew sentence extracted from the foreign news section of the daily <REF>Haaretz, September 1990</REF> transcripted to Latin letters.
Many studies extract synonyms from large monolingual corpora by using context information around targetterms<REF>CroachandYang, 1992</REF>; <REF>ParkandChoi, 1996</REF>; <REF>Waterman, 1996</REF>; <REF>Curran, 2004</REF>~~~Some researchers <TREF>Hindle, 1990</TREF>; <REF>Grefenstette, 1994</REF>; <REF>Lin, 1998</REF> classify terms by similarities based on their distributional syntactic patterns~~~These methods often extract not only synonyms, but also semantically related terms, such as antonyms, hyponyms and coordinate terms such as cat and dog Some studies make use of bilingual corpora or dictionaries to nd synonyms in a target language <REF>Barzilay and McKeown, 2001</REF>; <REF>Shimohata and Sumita, 2002</REF>; <REF>Wu and Zhou, 2003</REF>; <REF>Lin et al, 2003</REF>~~~Lin et al.
Semantic Relatedness Information~~~There bas recently been work in the detection of semantically related nouns via, for example, shared argument structures <TREF>Hindle 1990</TREF>, and shared dictionary definition context Wilks e al 1990~~~These approaches attempt to infer relationships among exical terms by looking at very large text samples and determining which ones are related in a statistically significant way~~~The technique introduced in this paper can be seen as having a similar goal but an entirely different approach, since only one sample need be found in order to determine a salient relationship and that sample may be infrequently occurring or nonexistent.
However, many studies investigate synonym extraction from only one resource~~~The most frequently used resource for synonym extraction is large monolingual corpora <TREF>Hindle, 1990</TREF>; <REF>Crouch and Yang, 1992</REF>; <REF>Grefenstatte, 1994</REF>; <REF>Park and Choi, 1997</REF>; <REF>Gasperin et al , 2001</REF> and <REF>Lin, 1998</REF>~~~The methods used the contexts around the investigated words to discover synonyms~~~The problem of the methods is that the precision of the extracted synonymous words is low because it extracts many word pairs such as cat and dog, which are similar but not synonymous.
It has two sources of evidence: the similarity of the strings themselves ie , edit distance and the similarity of the assertions they appear in~~~This second source of evidence is sometimes referred to as distributional similarity <TREF>Hindle, 1990</TREF>~~~Section 32 presents a simple model for predicting whether a pair of strings co-refer based on string similarity~~~Section 33 then presents a model called the Extracted Shared Property ESP Model for predicting whether a pair of strings co-refer based on their distributional similarity.
3 It is worth noting at this point that there are several well-known measures from the NLP literature that we have omitted from our experiments~~~Arguably the most widely used is the mutual information <TREF>Hindle, 1990</TREF>; <REF>Church and Hanks, 1990</REF>; <REF>Dagan et al , 1995</REF>; <REF>Luk, 1995</REF>; D <REF>Lin, 1998a</REF>~~~It does not apply in the present setting because it does not measure the similarity between two arbitrary probability distributions in our case, PVIn  and PVIm, but rather the similarity between a joint distribution PX1,X2 and the corresponding product distribution PX1PX2~~~Hamming-type metrics <REF>Cardie, 1993</REF>; <REF>Zavrel and Daelemans, 1997</REF> are intended for data with symbolic features, since they count feature label mismatches, whereas we are dealing feature Values that are probabilities.
Furthermore, this effort is repeated when a system is ported to another domain~~~This criticism leads us to automatic approaches for building thesauri from large corpora <REF>Hirschman et al , 1975</REF>; <TREF>Hindle, 1990</TREF>; <REF>Hatzivassiloglou and McKeown, 1993</REF>; <REF>Pereira et al , 1993</REF>; Tokunaga et aL, 1995; <REF>Ushioda, 1996</REF>~~~Past attempts have basically taken the following steps <REF>Charniak, 1993</REF>~~~1 extract word co-occurrences 2 define similarities distances between words on the basis of co-occurrences 3 cluster words on the basis of similarities The most crucial part of this approach is gathering word co-occurrence data.
The Distributional Hypothesis <REF>Harris 1985</REF> states that words that occur in the same contexts tend to be similar~~~There have been many approaches to compute the similarity between words based on their distribution in a corpus <TREF>Hindle 1990</TREF>; <REF>Landauer and Dumais 1997</REF>; <REF>Lin 1998</REF>~~~The output of these programs is a ranked list of similar words to each word~~~For example, Lins approach outputs the following similar words for wine and suit: wine: beer, white wine, red wine, Chardonnay, champagne, fruit, food, coffee, juice, Cabernet, cognac, vinegar, Pinot noir, milk, vodka, suit: lawsuit, jacket, shirt, pant, dress, case, sweater, coat, trouser, claim, business suit, blouse, skirt, litigation,  The similar words of wine represent the meaning of wine.
A proper filter must be able to access information in the text using any word of a set of similar words~~~A number of knowledge-rich <REF>Jacobs and Rau, 1990</REF>, <REF>Calzolari and Bindi, 1990</REF>, <REF>Mauldin, 1991</REF> and knowledge-poor <REF>Brown et al , 1992</REF>, <TREF>Hindle, 1990</TREF>, <REF>Ruge, 1991</REF>, <REF>Grefenstette, 1992</REF> methods have been proposed for recognizing when words are similar~~~The knowledge-rich approaches require either a conceptual dependency representation, or semantic tagging of the words, while the knowledge-poor approaches require no previously encoded semantic information, and depend on frequency of co-occurrence of word contexts to determine similarity~~~Evaluations of results produced by the above systems are often been limited to visual verification by a human subject or left to the human reader.
3 Distributional Word Similarity Words that tend to appear in the same contexts tend to have similar meanings <REF>Harris 1968</REF>~~~For example, the words corruption and abuse are similar because both of them can be subjects of verbs like arouse, become, betray, cause, continue, cost, exist, force, go on, grow, have, increase, lead to, and persist, etc, and both of them can modify nouns like accusation, act, allegation, appearance, and case, etc Many methods have been proposed to compute distributional similarity between words, eg, <TREF>Hindle, 1990</TREF>, <REF>Pereira et al 1993</REF>, <REF>Grefenstette 1994</REF> and <REF>Lin 1998</REF>~~~Almost all of the methods represent a word by a feature vector where each feature corresponds to a type of context in which the word appeared~~~84 31 Proximity-based Similarity It is natural to use dependency relationship Meluk, 1987 as features, but a parser has to be available.
The underlying idea is based largely on the central claim of the distributional hypothesis <REF>Harris 1968</REF>, that is: The meaning of entities, and the meaning of grammatical relations among them, is related to the restriction of combinations of these entities relative to other entities~~~This hypothesized relationship between distributional similarity and semantic similarity has given rise to a large body of work on automatic thesaurus generation <TREF>Hindle 1990</TREF>; <REF>Grefenstette 1994</REF>; <REF>Lin 1998a</REF>; <REF>Curran and Moens 2002</REF>; <REF>Kilgarriff 2003</REF>~~~There are inherent problems in evaluating automatic thesaurus extraction techniques, and much research assumes a gold standard that does not exist see Kilgarriff 2003 and Weeds 2003 for more discussion of this~~~A further problem for distributional similarity methods for automatic thesaurus generation is that they do not offer any obvious way to distinguish between linguistic relations such as synonymy, antonymy, and hyponymy see Caraballo 1999 and Lin et al 2003 for work on this.
As can be seen, similarity between neighbor sets is significantly higher at high recall settings low  within the model than at highprecision settings high , which suggests that dist  has high-recall CR characteristics~~~45 Hindles <REF>Measure Hindle 1990</REF> proposed an MI-based measure, which he used to show that nouns could be reliably clustered based on their verb co-occurrences~~~We consider the variant of 458 Weeds and Weir Co-occurrence Retrieval Figure 1 Variation with parameters  and  in development set mean similarity between neighbor sets of the additive t-test based CRM and of dist   Hindles Measure proposed by <REF>Lin 1998a</REF>, which overcomes the problem associated with calculating MI for word-feature combinations that do not occur: sim hind w 1, w 2   summationdisplay Tw 1 Tw 2  minIc, w 1 , Ic, w 2  38 where Tw 1  c : Ic, n > 0~~~This expression is the same as the numerator in the expressions for precision and recall in the difference-weighted MI-based CRM: P dw mi w 1, w 2   summationtext TP Iw 1, c  minIw 1, c,Iw 2, c Iw 1, c summationtext Fw 1  Iw 1, c  summationtext TP minIw 1, c, Iw 2, c summationtext Fw 1  Iw 1, c 39 R dw mi w 1, w 2   summationtext TP Iw 2, c  minIw 2, c,Iw 1, c Iw 2, c summationtext Fw 2  Iw 2, c  summationtext TP minIw 2, c, Iw 1, c summationtext Fw 2  Iw 2, c 40 since TP  Tw 1   Tw 2 .
A statistical technique using a language model that assigns a zero probability to these previously unseen events will rule the correct parse or interpretation of the utterance impossible~~~Similarity-based smoothing <TREF>Hindle 1990</TREF>; <REF>Brown et al 1992</REF>; <REF>Dagan, Marcus, and Markovitch 1993</REF>; <REF>Pereira, Tishby, and Lee 1993</REF>; <REF>Dagan, Lee, and Pereira 1999</REF> provides an intuitively appealing approach to language modeling~~~In order to estimate the probability of an unseen co-occurrence of events, estimates based on seen occurrences of similar events can be combined~~~For example, in a speech recognition task, we might predict that cat is a more likely subject of growl than the word cap, even though neither co-occurrence has been seen before, based on the fact that cat is similar to words that do occur as the subject of growl eg , dog and tiger, whereas cap is not.
statistical factors, although statistical factors are calculated in terms of the predicate argument structure in which each noun appears~~~Predicate argument structures, which consist of complements case filler nouns and case markers and verbs, have also been used in the task of noun classification <TREF>Hindle 1990</TREF>~~~This can be expressed by Equation 3, where ff is the vector for the noun in question, and items ti represent the statistics for predicate argument structures including n ff  h, t2,, ti  3 In regard to ti, we used the notion of TF~~~IDF <REF>Salton and McGill 1983</REF>.
2 Previous Work There have been several approaches to automatically discovering lexico-semantic information from text <REF>Hearst 1992</REF>; <REF>Riloff and Shepherd 1997</REF>; <REF>Riloff and Jones 1999</REF>; <REF>Berland and Charniak 1999</REF>; <REF>Pantel and Lin 2002</REF>; <REF>Fleischman et al 2003</REF>; <REF>Girju et al 2003</REF>~~~One approach constructs automatic thesauri by computing the similarity between words based on their distribution in a corpus <TREF>Hindle 1990</TREF>; <REF>Lin 1998</REF>~~~The output of these programs is a ranked list of similar words to each word~~~For example, Lins approach outputs the following top-20 similar words of orange: D peach, grapefruit, yellow, lemon, pink, avocado, tangerine, banana, purple, Santa Ana, strawberry, tomato, red, pineapple, pear, Apricot, apple, green, citrus, mango A common problem of such lists is that they do not discriminate between the senses of polysemous words.
First, most of theln assume that the input corpora me aligned sentence by sentence, which reduces their applicability remarkably~~~Although a number of automatic sentence alignment methods have been proposed <TREF>Brown et al 1991</TREF> ; <REF>Gale  Church 1991</REF> b; <REF>Kay  Roscheisen 1993</REF>; <REF>Chen 1993</REF>, they are not very reliable for real noisy bilingual texts~~~Second, the statistical methods usually require a very large corpus as their input~~~However, it is not easy to obtain a very large corpus.
In fact, it could be argued that, ultimately, text alignment is no easier than the more general problem of natural language understanding~~~In addition, most research efforts were directed towards the easiest problem, that of sentence-to-sentence alignment <TREF>Brown et al , 1991</TREF>; <REF>Gale and Church, 1991</REF>; <REF>Debili, 1992</REF>; Kay and lscheisen, 1993; <REF>Simard et al , 1992</REF>; <REF>Simard and Plamondon, 1996</REF>~~~Alignment at the word and term level, which is extremely useful for applications such as lexieal resource extraction, is still a largely unexplored research area<REF>Melamed, 1997</REF>~~~In order to live up to the expectations of the 711 various application fields, alignment technology will therefore have to improve substantially.
Several automatic methods have been proposed for this task in recent years~~~However, most of these methods address only the sub-problem of alignment <REF>Catizone et al 1989</REF>, <TREF>Brown et al 1991</TREF>, <REF>Gale  Church 1991</REF>, <REF>Debili  Sammouda 1992</REF>, <REF>Simard et al 1992</REF>, Kay  R<REF>Sscheisen 1993</REF>, <REF>Wu 1994</REF>~~~Alignment algorithms assume the availability of text unit boundary information and their output has less expressive power than a general bitext map~~~The only published solution to the more difficult general bitext mapping problem <REF>Church 1993</REF> can err by several typeset pages.
There are several papers in the literature about bitext alignment~~~The algorithms that seem to work best rely on the high correlation between the lengths of corresponding sentences <TREF>Brown et al 1991</TREF>, <REF>Gale  Church 1991</REF>~~~However, these algorithms can fumble in bitext sections that contain many sentences of very similar length, like this vote record: English French Mr McInnis~~~Yes.
SIMR borrows several insights from previous work~~~<REF>Like Gale  Church 1991</REF> and Brown et al~~~1991, SIMR relies on the high correlation between the lengths of mutual translations~~~Like charalign <REF>Church 1993</REF>, SIMR infers bitext maps from likely points of correspondence between the two texts, points that are plotted in a two-dimensional space of possibilities.
Many software products which aid human translators now contain sentence alignment tools as an aid to speeding up editing and terminology searching~~~Various methods have been developed for sentence alignment which we can categorise as either lexical such as <REF>Chen, 1993</REF>, based on a large-scale bilingual lexicon; statistical such as <TREF>Brown et al , 1991</TREF> <REF>Church, 1993</REF><REF>Gale and Church, 1903</REF>Kay and R<REF>Ssheheisen, 1993</REF>, based on distributional regularities of words or byte-length ratios and possibly inducing a bilingual lexicon as a by-product, or hybrid such as <REF>Utsuro et al , 1994</REF> <REF>Wu, 1994</REF>, based on some combination of the other two~~~Neither of the pure approaches is entirely satisfactory for the following reasons:  Text volume limits the usefulness of statistical approaches~~~We would often like to be able to align small amounts of text, or texts from various domains which do not share the same statistical properties.
pruning translation alternatives for query translation~~~Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction <REF>Smadja, 1993</REF>; <REF>Fung and Wu, 1994</REF>, phrasal translation <REF>Smadja et al , 1996</REF>; <REF>Kupiec, 1993</REF>; <REF>Wu, 1995</REF>; <REF>Dagan and Church, 1994</REF>, target word selection <REF>Liu and Li, 1997</REF>; <REF>Tanaka and Iwasaki, 1996</REF>, domain word translation <REF>Fung and Lo, 1998</REF>; <REF>Fung, 1998</REF>, sense disambiguation <TREF>Brown et al , 1991</TREF>; <REF>Dagan et al , 1991</REF>; <REF>Dagan and Itai, 1994</REF>; <REF>Gale et al , 1992a</REF>; <REF>Gale et al , 1992b</REF>; <REF>Gale et al , 1992c</REF>; <REF>Shiitze, 1992</REF>; <REF>Gale et al , 1993</REF>; <REF>Yarowsky, 1995</REF>, and even recently for query translation in cross-language IR as well <REF>Ballesteros and Croft, 1998</REF>~~~Co-occurrence statistics is collected from either bilingual parallel and 334 non-parallel corpora <REF>Smadja et al , 1996</REF>; <REF>Kupiec, 1993</REF>; <REF>Wu, 1995</REF>; <REF>Tanaka and Iwasaki, 1996</REF>; <REF>Fung and Lo, 1998</REF>, or monolingual corpora <REF>Smadja, 1993</REF>; <REF>Fung and Wu, 1994</REF>; <REF>Liu and Li, 1997</REF>; <REF>Shiitze, 1992</REF>; <REF>Yarowsky, 1995</REF>~~~As we noted in <REF>Fung and Lo, 1998</REF>; <REF>Fung, 1998</REF>, parallel corpora are rare in most domains.
Strategies that use system-external dictionaries, finally, can only be used if a large-enough dictionary exists for a specific language pair~~~22 Word Alignment Aligning below the sentence level is usually done using statistical models for machine translation <TREF>Brown et al , 1991</TREF>; <REF>Brown et al , 1993</REF>; <REF>Hiemstra, 1996</REF>; <REF>Vogel et al , 1999</REF> where any word of the targetlanguageistakentobeapossibletranslation for each source language word~~~The probability of some target language word to be a translation of a source language word then depends on the frequency with which both co-occur at the same or similar positions in the parallel corpus~~~The probabilities are estimated from the using the EM-algorithm1, and a Viterbi search is carried out to compute the most probable sequence of word translation pairs.
A number of techniques for aligning sentences in parallel corpora have been proposed~~~<REF>Gale  Church 1991</REF>; <TREF>Brown et al 1991</TREF>; <REF>Wu 1994</REF> used sentence length as the basic feature for alignment~~~<REF>Kay  Roscheisen 1993</REF>; and <REF>Chen 1993</REF> used lexical information for sentence alignment~~~Models combining length and lexicon information were proposed in <REF>Zhao and Vogel, 2002</REF>; <REF>Moore 2002</REF>.
To support machine translation, parallel sentences should be extracted from the mined parallel documents~~~However, current sentence alignment models, <TREF>Brown et al 1991</TREF>; <REF>Gale  Church 1991</REF>; <REF>Wu 1994</REF>; Chen 489 1993; <REF>Zhao and Vogel, 2002</REF>; etc~~~are targeted on traditional textual documents~~~Due to the noisy nature of the web documents, parallel web pages may consist of non-translational content and many out-of-vocabulary words, both of which reduce sentence alignment accuracy.
Nonetheless, they were able to align phrases in French with the English words that produce them as illustrated in their Figure 3~~~More recently, <REF>Gale and Church 1991a</REF> describe an algorithm similar to the one described in Brown et al~~~1988~~~Like Brown et al , they consider only the simultaneous appearance of words in pairs of sentences that are translations of one another.
If we reinterpret the Viterbi alignment to mean the most probable alignment that we can find rather than the most probable alignment that exists, then a similarly reinterpreted Viterbi training algorithm still converges~~~We have already used this algorithm successfully as a part of a system to assign senses to English and French words on the basis of the context in which they appear <REF>Brown et al 1991a, 1991b</REF>~~~We expect to use it in models that we develop beyond Model 5~~~293 Computational Linguistics Volume 19, Number 2 63 Multi-Word Cepts In Models 1-5, we restrict our attention to alignments with cepts containing no more than one word each.
P93-1001:25~~~Charalign: A Program for Aligning Parallel Texts at the Character Level Kenneth Ward Church ATT Bell Laboratories 600 Mountain Avenue Murray Hill NJ, 07974-0636 kwc researchattcom Abstract There have been a number of recent papers on aligning parallel texts at the sentence level, eg, <TREF>Brown et al 1991</TREF>, Gale and Church to appear, <REF>Isabelle 1992</REF>, Kay and R/Ssenschein to appear, <REF>Simard et al 1992</REF>, <REF>WarwickArmstrong and Russell 1990</REF>~~~On clean inputs, such as the Canadian Hansards, these methods have been very successful at least 96 correct by sentence~~~Unfortunately, if the input is noisy due to OCR and/or unknown markup conventions, then these methods tend to break down because the noise can make it difficult to find paragraph boundaries, let alone sentences.
In fact, it could be argued that, ultimately, text alignment is no easier thal the more general problem of natural laalguage understanding~~~In addition, most research efforts were directed towards the easiest problem, that of sentence-to-sentence alignment <TREF>Brown et al, 1991</TREF>; <REF>Gale and Church, 1991</REF>; <REF>Debili, 1992</REF>; Kay and R<REF>Sscheisen, 1993</REF>; <REF>Simard et al, 1992</REF>; <REF>Simard and Plamondon, 1996</REF>~~~Alignment at the word and term level, which is extremely useful for applications such as lexical resource extraction, is still a largely unexplored research area<REF>Melamed, 1997</REF>~~~In order to live up to the expectations of the 711 various application fields, alignment technology will therefore have to improve substantially.
Furthermore, a glossing algorithm can be used for lexical selection in a full-fledged machine translation MT system~~~Many corpus-based MT systems require parallel corpora <REF>Brown et al , 1990</REF>; <TREF>Brown et al , 1991</TREF>; <REF>Gale and Church, 1991</REF>; <REF>Resnik, 1999</REF>~~~<REF>Kikui 1999</REF> used a word sense disambiguation algorithm and a non-paralM bilingual corpus to resolve translation ambiguity~~~In this paper, we present a word-for-word glossing algorithm that requires only a source language corpus.
Hence, we use a slightly different framework~~~We view a bilingual corpus as a sequence of sentence beads <TREF>Brown et al , 1991b</TREF>, where a sentence bead corresponds to an irreducible group of sentences that align with each other~~~For example, the correct alignment of the bilingual corpus in Figure 2 consists of the sentence bead El; F1 followed by the sentence bead E2; ;2, F3~~~We can represent an alignment 4 of a corpus as a sequence of sentence beads Epl; Fpl, Ep2; F,, where the E and F can be zero, one, or more sentences long.
For the remnant, we allow only 1-1, 1-2, 1-3, 1-4, 2-2 as pMrs of the numbers of sentences: xi,yi e 1,1,1,2,2,1,1,3, 3, 1, 1,4, 4, 1, 2, 2 This optimization problem is solvable as a standard problem in dynamic programming~~~Dynamic programming is applied to bilingual sentence alignment in most of previous works <TREF>Brown et al , 1991</TREF>; <REF>Gate and Church, 1993</REF>; <REF>Chen, 1993</REF>~~~1078 4 Word Correspondence Estimation In this section, first we describe estimation flmctions based-on co-occnrrence frequencies~~~Then, we show how to incorporate word correspondence information available in bilingual dictionaries and to estimate word correspondences not included in bilingual dictionaries.
One of the major approaches to analyzing bilingual texts is the statistical approach~~~The statistical approach involves the following: alignment of bilingual texts at the sentence level nsing statistical techniques eg <TREF>Brown, Lai and Mercer 1991</TREF>, <REF>Gale and Church 1993</REF>, <REF>Chen 1993</REF>, and Kay and R<REF>Sscheisen 1993</REF>, statistical machine translation models eg Brown, Cooke, Pietra, Pietra et al~~~1990, finding character-level / word-level / phrase-level correspondences from bilingual texts eg <REF>Gale and Church 1991</REF>, <REF>Church 1993</REF>, and <REF>Kupiec 1993</REF>, and word sense disambiguation for MT eg <REF>Dagan, Itai and Schwall 1991</REF>~~~In general, the statistical approach does not use existing hand-written bilingual dictionaries, and depends solely upon statistics.
In general, the statistical approach does not use existing hand-written bilingual dictionaries, and depends solely upon statistics~~~For example, sentence alignment of bilingual texts are performed just by measuring sentence lengths in words or in characters <TREF>Brown et al , 1991</TREF>; <REF>Gale and Church, 1993</REF>, or by statistically estimating word level correspondences <REF>Chen, 1993</REF>; Kay and R<REF>Sscheisen, 1993</REF>~~~The statistical approach analyzes unstructured sentences in bilingual texts, and it is claimed that the results are useful enough in real applications such as machine translation and word sense disambiguation~~~However, structured bilingual sentences are undoubtedly more informative and important for future natural language researches.
It remains to be seen how we can also make use of the multilingual texts as NLP resources~~~In the years since the appearance of the first papers on using statistical models for bilingual lexicon compilation and machine translation<REF>Brown et al , 1993</REF>; <TREF>Brown et al , 1991</TREF>; <REF>Gale and <REF>Church, 1993</REF></REF>; <REF>Church, 1993</REF>; <REF>Simard et al , 1992</REF>, large amount of human effort and time has been invested in collecting parallel corpora of translated texts~~~Our goal is to alleviate this effort and enlarge the scope of corpus resources by looking into monolingual, comparable texts~~~This type of texts are known as nonparallel corpora.
In this way, we can ignore the context of the collocations and their translations, and base our decisions only on the patterns of co-occurrence of each collocation and its candidate translations across the entire corpus~~~This approach is quite different from those adopted for the translation of single words <REF>Klavans and Tzoukermann 1990</REF>; <REF>Dorr 1992</REF>; <REF>Klavans and Tzoukermann 1996</REF>, since for single words polysemy cannot be ignored; indeed, the problem of sense disambiguation has been linked to the problem of translating ambiguous words <TREF>Brown et al 1991</TREF>; <REF>Dagan, Itai, and Schwall 1991</REF>; <REF>Dagan and Itai 1994</REF>~~~The assumption of a single meaning per collocation was based on our previous experience with English collocations <REF>Smadja 1993</REF>, is supported for less opaque collocations by the fact that their constituent words tend to have a single sense when they appear in the collocation <REF>Yarowsky 1993</REF>, and was verified during our evaluation of Champollion Section 7~~~We construct a mathematical model of the events we want to correlate, namely, the appearance of any word or group of words in the sentences of our corpus, as follows: To each group of words G, in either the source or the target language, we map a binary random variable Xc that takes the value 1 if G appears in a particular sentence and 0 if not.
Smadja, McKeown, and Hatzivassiloglou Translating Collocations for Bilingual Lexicons 2~~~Related Work The recent availability of large amounts of bilingual data has attracted interest in several areas, including sentence alignment <REF>Gale and Church 1991b</REF>; <TREF>Brown, Lai, and Mercer 1991</TREF>; <REF>Simard, Foster and Isabelle 1992</REF>; <REF>Gale and Church 1993</REF>; <REF>Chen 1993</REF>, word alignment <REF>Gale and Church 1991a</REF>; <REF>Brown et al 1993</REF></REF>; <REF>Dagan, Church, and Gale 1993</REF>; <REF>Fung and McKeown 1994</REF>; <REF>Fung 1995b</REF>, alignment of groups of words <REF>Smadja 1992</REF>; <REF>Kupiec 1993</REF>; van der <REF>Eijk 1993</REF>, and statistical translation <REF>Brown et al 1993</REF></REF>~~~Of these, aligning groups of words is most similar to the work reported here, although, as we shall show, we consider a greater variety of groups than is typical in other research~~~In this section, we describe work on sentence and word alignment and statistical translation, showing how these goals differ from our own, and then describe work on aligning groups of words.
Nevertheless, top 20 candidate output give a 509 average increase in accuracy on human translator performance~~~Despite a surge in research using parallel corpora for various machine translation tasks <REF>Brown et al 1993</REF>,<TREF>Brown et al 1991</TREF>; <REF>Gale  <REF>Church 1993</REF></REF>; <REF>Church 1993</REF>; <REF>Dagan  Church 1994</REF>; <REF>Simard et al 1992</REF>; <REF>Chen 1993</REF>; <REF>Melamed 1995</REF>; <REF>Wu  Xia 1994</REF>; <REF>Wu 1994</REF>; Smadja et aI~~~1996, the amount of available bilingual parallel corpora is still relatively small in comparison to the large amount of available monolingual text~~~It ks unlikely that one can find parallel corpora in any given domain in electronic form.
Here we regard the context of a word as the preceding and following non-stop words; our approach can easily be extended to other types of contextual features~~~This model is trained on approximately 5 million sentence pairs of Hansard Canadian parliamentary and UN proceedings which have been aligned on a sentence-by-sentence basis by the methods of <TREF>Brown et al , 1991</TREF>, and then further aligned on a word-by-word basis by methods similar to <REF>Brown et al , 1993</REF>~~~The French::English model can be described by simply interchanging English and French notation above~~~It is trained separately on the same training data, using identical procedures.
<REF>As Chen 1993</REF> points out, dynamic programming is particularly susceptible to deletions occurring in one of the two languages~~~Thus, dynamic programming based sentence alignment algorithms rely on paragraph anchors <TREF>Brown et al 1991</TREF> or lexical information, such as cognates <REF>Simard 1992</REF>, to maintain a high accuracy rate~~~These methods are not robust with respect to non-literal translations and large deletions <REF>Simard 1996</REF>~~~This paper presents a new approach based on image processing IP techniques, which is immune to such predicaments.
11 2042 2007 2024 7976 3532 1065 1637 5982 bnnrule 8488 2504 3868 6132 8655 830 1514 4538 nnrule 6589 6329 6457 3543 3589 3543 3566 5850 Table 1: Trial set results~~~tences <REF>Gale and Church, 1991</REF>; <TREF>Brown et al , 1991</TREF>~~~The observation can be codified as a distance between the word at position i on the LHS and the word at position j on the RHS Dleni,j  1  4  Lli  LrjLl i  Lrj2 1 where Lli is the length of the token at position i on the LHS~~~Note that Dlen is similar to a normalized harmonic mean, ranging from 0 to 10, with the minimum achieved when the lengths are the same.
However, since Chinese text consists of an unsegmented character stream without marked word boundaries, it would not be possible to count the number of words in a sentence without first parsing it~~~Although it has been suggested that lengthbased methods are language-independent <REF>Gale  Church 1991</REF>; <TREF>Brown et al 1991</TREF>, they may in fact rely to some extent on length correlations arising from the historical relationships of the languages being aligned~~~If translated sentences share cognates, then the character lengths of those cognates are of course correlated~~~Grammatical similarities between related languages may also produce correlations in sentence lengths.
Our report concerns three related topics: 1 progress on the HKUST English-Chinese Parallel Bilingual Corpus; 2 experiments addressing the applicability of Gale  Churchs 1991 lengthbased statistical method to the task of alignment involving a non-Indo-European language; and 3 an improved statistical method that also incorporates domain-specific lexical cues~~~INTRODUCTION Recently, a number of automatic techniques for aligning sentences in parallel bilingual corpora have been proposed Kay  R<REF>Sscheisen 1988</REF>; Catizone e al 1989; <REF>Gale  Church 1991</REF>; <TREF>Brown et al 1991</TREF>; <REF>Chen 1993</REF>, and coarser approaches when sentences are difficult to identify have also been advanced <REF>Church 1993</REF>; Dagan e al 1993~~~Such corpora contain the same material that has been translated by human experts into two languages~~~The goal of alignment is to identify matching sentences between the languages.
In other words, the only feature of Lt and L2 that affects their alignment probability is their length~~~Note that there are other length-based alignment methods 81 that measure length in number of words instead of characters <TREF>Brown et al 1991</TREF>~~~However, since Chinese text consists of an unsegmented character stream without marked word boundaries, it would not be possible to count the number of words in a sentence without first parsing it~~~Although it has been suggested that lengthbased methods are language-independent <REF>Gale  Church 1991</REF>; <TREF>Brown et al 1991</TREF>, they may in fact rely to some extent on length correlations arising from the historical relationships of the languages being aligned.
Their method thus yields some incorrect noun phrases that will not be proposed by a tagger, but on the other hand does not miss noun phrases that may be missed due to tagging errors~~~3 Bilingual Task: An Application for Word Alignment 31 Sentence and word alignment Bilingual alignment methods <REF>Warwick et al , 1990</REF>; <TREF>Brown et al , 1991a</TREF>; <REF>Brown et al , 1993</REF>; <REF>Gale and Church, 1991b</REF>; <REF>Gale and Church, 1991a</REF>; <REF>Kay and Roscheisen, 1993</REF>; <REF>Simard et al , 1992</REF>; <REF>Church, 1993</REF>; <REF>Kupiec, 1993a</REF>; <REF>Matsumoto et al , 1993</REF>; <REF>Dagan et al , 1993</REF>~~~have been used in statistical machine translation <REF>Brown et al , 1990</REF>, terminology research and translation aids <REF>Isabelle, 1992</REF>; <REF>Ogden and Gonzales, 1993</REF>; van der <REF>Eijk, 1993</REF>, bilingual lexicography <REF>Klavans and Tzoukermann, 1990</REF>; <REF>Smadja, 1992</REF>, word-sense disambiguation <TREF>Brown et al , 1991b</TREF>; <REF>Gale et al , 1992</REF> and information retrieval in a multilingual environment <REF>Landauer and Littman, 1990</REF>~~~Most alignment work was concerned with alignment at the sentence level.
3 Bilingual Task: An Application for Word Alignment 31 Sentence and word alignment Bilingual alignment methods <REF>Warwick et al , 1990</REF>; <TREF>Brown et al , 1991a</TREF>; <REF>Brown et al , 1993</REF>; <REF>Gale and Church, 1991b</REF>; <REF>Gale and Church, 1991a</REF>; <REF>Kay and Roscheisen, 1993</REF>; <REF>Simard et al , 1992</REF>; <REF>Church, 1993</REF>; <REF>Kupiec, 1993a</REF>; <REF>Matsumoto et al , 1993</REF>; <REF>Dagan et al , 1993</REF>~~~have been used in statistical machine translation <REF>Brown et al , 1990</REF>, terminology research and translation aids <REF>Isabelle, 1992</REF>; <REF>Ogden and Gonzales, 1993</REF>; van der <REF>Eijk, 1993</REF>, bilingual lexicography <REF>Klavans and Tzoukermann, 1990</REF>; <REF>Smadja, 1992</REF>, word-sense disambiguation <TREF>Brown et al , 1991b</TREF>; <REF>Gale et al , 1992</REF> and information retrieval in a multilingual environment <REF>Landauer and Littman, 1990</REF>~~~Most alignment work was concerned with alignment at the sentence level~~~Algorithms for the more difficult task of word alignment were proposed in <REF>Gale and Church, 1991a</REF>; <REF>Brown et al , 1993</REF></REF>; <REF>Dagan et al , 1993</REF> and were applied for parameter estimation in the IBM statistical machine translation system <REF>Brown et al , 1993</REF></REF>.
Part-ofspeech taggers are used in a few applications, such as speech synthesis <REF>Sproat et al , 1992</REF> and question answering <REF>Kupiec, 1993b</REF>~~~Word alignment is newer, found only in a few places <REF>Gale and Church, 1991a</REF>; <REF>Brown et al , 1993</REF>; <REF>Dagan et al , 1993</REF>~~~It is used at IBM for estimating parameters of their statistical machine translation prototype Brown et Authors current address: Dept of Mathematics and Computer Science, Bar Ilan University, Ramat Gan 52900, Israel~~~al, 1993.
Most alignment work was concerned with alignment at the sentence level~~~Algorithms for the more difficult task of word alignment were proposed in <REF>Gale and Church, 1991a</REF>; <REF>Brown et al , 1993</REF></REF>; <REF>Dagan et al , 1993</REF> and were applied for parameter estimation in the IBM statistical machine translation system <REF>Brown et al , 1993</REF></REF>~~~Previously translated texts provide a major source of information about technical terms~~~<REF>As Isabelle 1992</REF> argues, Existing translations contain more solutions to more translation problems than any other existing resource.
Aligning English-Chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages~~~A number of alignment techniques have been proposed, varying from statistical methods <TREF>Brown et al , 1991</TREF>; <REF>Gale and Church, 1991</REF> to lexical methods Kay and R<REF>Sscheisen, 1993</REF>; <REF>Chen, 1993</REF>~~~The method we adopted is t h a t of Simard et al~~~1992.
At finer-grained levels, methods are more sophisticated and combine linguistic clues with statistical ones~~~Statistical alignment methods at sentence level have been thoroughly investigated <REF>Gale  Church, 1991a</REF>/ 1991b ; <TREF>Brown et al , 1991</TREF> ; <REF>Kay  Rscheisen, 1993</REF>~~~Others use various linguistic information <REF>Simard et al , 1992</REF> ; <REF>Papageorgiou et al , 1994</REF>~~~Purely statistical alignment methods are proposed at word level <REF>Gale  Church, 1991a</REF> ; <REF>Kitamura  Matsumoto, 1995</REF>.
Existing lexicon compilation methods <REF>Kupiec 1993</REF>; <REF>Smadja  McKeown 1994</REF>; <REF>Kumano  Hirakawa 1994</REF>; <REF>Dagan et al 1993</REF>; <REF>Wu  Xia 1994</REF> all attempt to extract pairs of words or compounds that are translations of each other from previously sentencealigned, parallel texts~~~However, sentence alignment <TREF>Brown et al 1991</TREF>; Kay  R<REF>Sscheisen 1993</REF>; <REF>Gale  <REF>Church 1993</REF></REF>; <REF>Church 1993</REF>; <REF>Chen 1993</REF>; <REF>Wu 1994</REF> is not always practical when corpora have unclear sentence boundaries or with noisy text segments present in only one language~~~Our proposed algorithm for bilingual lexicon acquisition bootstraps off of corpus alignment procedures we developed earlier <REF>Fung  Church 1994</REF>; <REF>Fung  McKeown 1994</REF>~~~Those procedures attempted to align texts by finding matching word pairs and have demonstrated their effectiveness for Chinese/English and Japanese/English.
It can resolve the alignment problem on real bilingual text~~~There have been a number of papers on aligning parallel texts at the sentence level in the last century, eg, <TREF>Brown et al 1991</TREF>; <REF>Gale and Church, 1993</REF>; <REF>Simard et al 1992</REF>; <REF>Wu DeKai 1994</REF>~~~On clean inputs, such as the Canadian Hansards and the Hong Kang Hansards, these methods have been very successful~~~Church, Kenneth W, 1993; <REF>Chen, Stanley, 1993</REF> proposed some methods to resolve the problem in noisy bilingual texts.
Therefore, sentence mapping algorithms need not worry about crossing correspondences~~~<REF>In 1991</REF>, two teams of researchers independently discovered that sentences can be accurately aligned by matching sequences 310 with similar lengths <REF>Gale  Church, 1991a</REF>; <TREF>Brown et al , 1991</TREF>~~~Soon thereafter, <REF>Church 1993</REF> found that bitext mapping at the sentence level is not an option for noisy bitexts found in the real world~~~Sentences are often difficult to detect, especially where punctuation is missing due to OCR errors.
bank were surrendered by banc~~~SENT fair Although there has been some previous work on the sentence alignment, eg, <TREF>Brown, Lai, and Mercer, 1991</TREF>, <REF>Kay and Rtscheisen, 1988</REF>, Catizone et al , to appear, the alignment task remains a significant obstacle preventing many potential users from reaping many of the benefits of bilingual corpora, because the proposed solutions are often unavailable, unreliable, and/or computationally prohibitive~~~The align program is based on a very simple statistical model of character lengths~~~The model makes use of the fact that longer sentences in one language tend to be translated into longer sentences in the other language, and that shorter sentences tend to be translated into shorter sentences.
The result shows that the use of dependency relation helps to acquire interesting translation patterns~~~Since the advent of statistical methods in Machine qhanslation, the bilingual sentence alignmerit <TREF>Brown et al , 1991</TREF> or word alignment <REF>Dagan et al , 1992</REF> have been explored and achieved numerous success over the last decade~~~In coN;rasl,, fewer resull;s are reported in phraselevel correspondence~~~As word sequences are not translated literally a word for a word, acquiring phraseqevel correspondence still remains an important problem to be exploited.
Automatic sentence alignment approaches face two kinds of difficulties: robustness and accuracy~~~A number of automatic sentence alignment techniques have been proposed <REF>Kay and Rscheisen, 1993</REF>; <REF>Gale and Church, 1991</REF>; <TREF>Brown et al , 1991</TREF>; <REF>Debili and Samouda, 1992</REF>; <REF>Papageorgiou et al , 1994</REF>; <REF>Gaussier, 1995</REF>; <REF>Melamed, 1996</REF>; <REF>Fluhr et al , 2000</REF>~~~73 The method proposed in <REF>Kay and Rscheisen, 1993</REF> is based on the assumption that in order for the sentences in a translation to correspond, the words in them must correspond~~~In other words, all necessary information and in particular, lexical mapping is derived from the to-be-aligned texts themselves.
In other words, all necessary information and in particular, lexical mapping is derived from the to-be-aligned texts themselves~~~In <REF>Gale and Church, 1991</REF> and <TREF>Brown et al , 1991</TREF>, the authors start from the fact that the length of a source text sentence is highly correlated with the length of its target text translation: short sentences tend to have short translations, and long sentences tend to have long translations~~~The method proposed in <REF>Debili and Sammouda, 1992</REF> is based on the preliminary alignment of words using a conventional bilingual lexicon and the method described in <REF>Papageorgiou et al , 1994</REF> added grammatical labeling based on the assumption that the same parts of speech tend to be employed in the translation~~~In this paper, we present a sentence aligner which is based on a cross-language information retrieval approach and combines different information sources bilingual lexicon, sentence length and sentence position.
To do such kinds of researches, the most impmlant task is to align the bilingual texts~~~Many length-based alignment algorithms have been proposed <TREF>Brown et al , 1991</TREF>; <REF>Gale and Church, 1991a</REF>~~~The correct rates are good~~~However, the languages they processed belong to occidental family.
It remains to be seen how we can also make use of the multilingual texts as NLP resources~~~In the years since the appearance of the first papers on using statistical models for bilingual lexicon compilation and machine translation<REF>Brown et al, 1993</REF>; <TREF>Brown et al, 1991</TREF>; <REF>Gale and <REF>Church, 1993</REF></REF>; <REF>Church, 1993</REF>; <REF>Simard et al, 1992</REF>, large amount of human effort and time has been invested in collecting parallel corpora of translated texts~~~Our goal is to alleviate this effort and enlarge the scope of corpus resources by looking into monolingual, comparable texts~~~This type of texts are known as nonparallel corpora.
The technique for acquiring various kinds of rules such as translation rules, grammar rules, dictionary entries and so on from bilingual corpora needs to include several kinds of sub-techniques; identification of aligned sentence pairs which consist of pairs of one language sentence and translation equivalents of the sentence sentence alignment; identification of equivalent words/phrases pairs from aligned sentence pairs word alignment; and extraction of rules such as translation rules, grammar rules, dictionary entries and so on from identified aligned sentence pairs and equivalent word/phrase pairs~~~Several methods have been proposed with regard to aligning sentences <TREF>Brown et al , 1991</TREF>; <REF>Gale and Church, 1991</REF>; <REF>Haruno and Yamazaki, 1996</REF>; Kay and losche/sen, 1993, alJsnlng words <REF>Church, 1993</REF>; <REF>Kupiec, 1993</REF>; <REF>Matsumoto et al , 1993</REF>; <REF>Wu, 1995</REF>; <REF>Yamada et al , 1996</REF> and acquiring rules from bilingual corpora <REF>Dagan et al , 1991</REF>; <REF>Dagan and Church, 1994</REF>; <REF>Fung and Church, 1994</REF>; Tana, 1994; <REF>Yamada et al , 1995</REF>~~~From the point of view of the extraction of resolution rules of zero pronouns, a technique to identify zero pronouns in a sentence in one language and their antecedents in a translation from aligned sentence pairs is needed~~~But there is currently no method to identify zero pronouns and their antecedents automatically within bilingual corpora.
Therefore, we decided to extract NE translation pairs from content-aligned corpora, such as multilingual broadcast news articles including new NEs daily, which are not literally translated~~~Sentential alignment <TREF>Brown et al , 1991</TREF>; <REF>Gale and Church, 1993</REF>; <REF>Kay and Roscheisen, 1993</REF>; <REF>Utsuro et al , 1994</REF>; <REF>Haruno and Yamazaki, 1996</REF> is commonly used as a starting point for finding the translations of words or expressions from bilingual corpora~~~However, it is not always possible to correspond non-parallel corpora in sentences~~~Past statistical methods for non-parallel corpora <REF>Fung and Yee, 1998</REF> are not valid for finding translations of words or expressions with low frequency.
Many software products which aid human translators now contain sentence alignment tools as an aid to speeding up editing and terminology searching~~~Various methods have been developed for sentence alignment which we can categorise as either lexical such as <REF>Chen, 1993</REF>, based on a large-scale bilingual lexicon; statistical such as <TREF>Brown et al, 1991</TREF> <REF>Church, 1993</REF>Gale and <REF>Church, 1993</REF><REF>Kay and Rhshcheisen, 1993</REF>, based on distributional regularities of words or byte-lengdl ratios and possibly inducing a bilingual lexicon as a by-product, or hybrid such as <REF>Vtsuro et al, 1994</REF> <REF>Wu, 1994</REF>, based on some combination of the other two~~~Neither of the pure approaches is entirely satisfactory for the following reasons:  Text volume limits the usefulness of statistical approaches~~~We would often like to be able to align small amounts of text, or texts from various domains which do not share the same statistical properties.
The performance tends to deteriorate significantly when these approaches are applied to noisy complex corpora with sentence omission or insertion, less literal translation~~~There are basically three kinds of approaches on sentence alignment: the length-based approach <REF>Gale  Church 1991</REF> and <TREF>Brown et al 1991</TREF>, the lexical approach key  <REF>Roscheisen 1993</REF>, and the combination of them <REF>Chen 1993</REF>, <REF>Wu 1994</REF> and <REF>Langlais 1998</REF>, etc~~~The first published algorithms for aligning sentences in parallel texts are length-based approach proposed by <REF>Gale  Church 1991</REF> and <REF>Brow et al 1991</REF>~~~Based on the observation that short sentences tend to be translated as short sentences and long sentences as long sentences, they calculate the most likely sentence correspondences as a function of the relative length of the candidates.
For Brn and GC, the search method was based on the one used by Moore, ie, searching within a growing diagonal band~~~Using this search method meant that no prior segmentation of the corpora was needed <REF>Moore, 2002</REF>, either in terms of aligned paragraphs <REF>Gale and Church, 1991</REF>, or some aligned sentences as anchors <TREF>Brown et al , 1991</TREF>~~~We would have liked to study the effect of linguistic distance more systematically, but we couldnt get equivalent manually-checked aligned parallel corpora for other pairs of languages~~~We have to rely on the reported results for other language pairs, but those results, as mentioned before, do not mention the conditions of testing which we are considering for our evaluation and, therefore, cannot be directly compared to our results for English-Hindi.
Sentence length methods were based on the intuition that the length of a translated sentence is likely to be similar to that of the source sentence~~~Brown, Lai and Mercer <TREF>Brown et al , 1991</TREF> used word count as the sentence length, whereas Gale and Church <REF>Gale and Church, 1991</REF> used character count~~~Brown, Lai and Mercer assumed prior alignment of paragraphs~~~Gale and Church relied on some previously aligned sentences as anchors.
1~~~Motivation There have been quite a number of recent papers on parallel text: Brown et al 1990, 1991, 1993, <REF>Chen 1993</REF>, <REF>Church 1993</REF>, <REF>Church et al 1993</REF>, <REF>Dagan et al 1993</REF>, Gale and Church 1991, 1993, <REF>Isabelle 1992</REF>, <REF>Kay and Rgsenschein 1993</REF>, <REF>Klavans and Tzoukermann 1990</REF>, <REF>Kupiec 1993</REF>, <REF>Matsumoto 1991</REF>, <REF>Ogden and Gonzales 1993</REF>, <REF>Shemtov 1993</REF>, <REF>Simard et al 1992</REF>, <REF>WarwickArmstrong and Russell 1990</REF>, Wu to appear~~~Most of this work has been focused on European language pairs, especially English-French~~~It remains an open question how well these methods might generalize to other language pairs, especially pairs such as English-Japanese and EnglishChinese.
This is compared with the previous rate of about 30 terms per hour using charaligns output, and an extremely lower rate before alignment tools were available~~~4 Conclusions Compared with other word alignment algorithms <REF>Brown et al , 1993</REF>; <REF>Gale and Church, 1991a</REF>, wordalign does not require sentence alignment as input, and was shown to produce useful alignments for small and noisy corpora~~~Its robustness was achieved by modifying Brown et als Model 2 to handle an initial rough alignment, reducing the number of parameters and introducing a dependency between alignments of adjacent words~~~Taking the output of charalign as input, wordalign produces significantly better, word7 Offset from correct alignment 0 1 2 3 4 Percentage 605 108 75 52 16 Accumulative percentage 605 713 788 84 856 Table 2: Wordaligns precision on noisy input, scanned by an OCR device.
To deal with these robustness issues, <REF>Church 1993</REF> developed a character-based alignment method called charalign~~~The method was intended as a replacement for sentence-based methods eg , <TREF>Brown et al , 1991a</TREF>; <REF>Gale and Church, 1991b</REF>; <REF>Kay and Rosenschein, 1993</REF>, which are very sensitive to noise~~~This paper describes a new program, called wordalign, that starts with an initial rough alignment eg , the output of charalign or a sentence-based alignment method, and produces improved alignments by exploiting constraints at the word-level~~~The alignment algorithm consists of two steps: 1 estimate translation probabilities, and 2 use these probabilities to search for most probable alignment path.
More importantly, because wordalign and charalign were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at ATT Language Line Services, a commercial translation service, to help them with difficult terminology~~~Aligning parallel texts has recently received considerable attention <REF>Warwick et al , 1990</REF>; <TREF>Brown et al , 1991a</TREF>; <REF>Gale and Church, 1991b</REF>; <REF>Gale and Church, 1991a</REF>; <REF>Kay and Rosenschein, 1993</REF>; <REF>Simard et al , 1992</REF>; <REF>Church, 1993</REF>; <REF>Kupiec, 1993</REF>; <REF>Matsumoto et al , 1993</REF>~~~These methods have been used in machine translation <REF>Brown et al , 1990</REF>; <REF>Sadler, 1989</REF>, terminology research and translation aids <REF>Isabelle, 1992</REF>; <REF>Ogden and Gonzales, 1993</REF>, bilingual lexicography <REF>Klavans and Tzoukermann, 1990</REF>, collocation studies <REF>Smadja, 1992</REF>, word-sense disambiguation <TREF>Brown et al , 1991b</TREF>; <REF>Gale et al , 1992</REF> and information retrieval in a multilingual environment <REF>Landauer and Littman, 1990</REF>~~~The information retrieval application may be of particular relevance to this audience.
Aligning parallel texts has recently received considerable attention <REF>Warwick et al , 1990</REF>; <TREF>Brown et al , 1991a</TREF>; <REF>Gale and Church, 1991b</REF>; <REF>Gale and Church, 1991a</REF>; <REF>Kay and Rosenschein, 1993</REF>; <REF>Simard et al , 1992</REF>; <REF>Church, 1993</REF>; <REF>Kupiec, 1993</REF>; <REF>Matsumoto et al , 1993</REF>~~~These methods have been used in machine translation <REF>Brown et al , 1990</REF>; <REF>Sadler, 1989</REF>, terminology research and translation aids <REF>Isabelle, 1992</REF>; <REF>Ogden and Gonzales, 1993</REF>, bilingual lexicography <REF>Klavans and Tzoukermann, 1990</REF>, collocation studies <REF>Smadja, 1992</REF>, word-sense disambiguation <TREF>Brown et al , 1991b</TREF>; <REF>Gale et al , 1992</REF> and information retrieval in a multilingual environment <REF>Landauer and Littman, 1990</REF>~~~The information retrieval application may be of particular relevance to this audience~~~It would be highly desirable for users to be able to express queries in whatever language they chose and retrieve documents that may or may not have been written in the same language as the query.
That is, one cannot pick two anchor points, one in each text, and have the program align the corresponding regions above and below the anchor points~~~See <TREF>Brown et al , 1991</TREF> for discussion of an alternative~~~This is not necessarily a problem either, and can be worked around~~~Lastly, it does not give usable results on texts which are not absolutely parallel.
One way to remedy this situation is to adapt and retrain the system parameters based on bilingual data from the same source or at least a closely related source~~~A bilingual sentence alignment program <REF>Gale and Church, 1991</REF>, and <TREF>Brown et al , 1991</TREF> is the crucial part in this adaptation procedure, in that it collects bilingual document pairs from the Internet, and identifies sentence pairs, which should have a high likelihood of being correct translations of each other~~~The set of identified bilingual parallel sentence pairs is then added to the training set for parameter reestimation~~~As is well known, text mined from the Internet is very noisy.
20 The method of acquiring parameters from ambiguous occurrences in a corpus, relying on the spreading of noise, can be used in many contexts~~~For example, it was used for acquiring statistics for disambiguating prepositional phrase attachments, counting ambiguous occurrences of prepositional phrases as representing both noun-pp and verb-pp constructs <TREF>Hindle and Rooth 1991</TREF>~~~588 Ido Dagan and Alon Itai Word Sense Disambiguation vides a useful source of a sense tagged corpus~~~<REF>Gale, Church, and Yarowsky 1992a</REF> have also exploited this resource for achieving large amounts of testing and training materials.
The use of such relations mainly relations between verbs or nouns and their arguments and modifiers for various purposes has received growing attention in recent research <REF>Church and Hanks 1990</REF>; <REF>Zernik and Jacobs 1990</REF>; <REF>Hindle 1990</REF>; <REF>Smadja 1993</REF>~~~More specifically, two recent works have suggested using statistical data on lexical relations for resolving ambiguity of prepositional phrase attachment <TREF>Hindle and Rooth 1991</TREF> and pronoun references <REF>Dagan and Itai 1990, 1991</REF>~~~Clearly, statistics on lexical relations can also be useful for target word selection~~~Consider, for example, the Hebrew sentence extracted from the foreign news section of the daily Ha-<REF>Aretz, September 1990</REF> transcripted to Latin letters: 1 Nose ze mana mi-shtei ha-mdinot mi-lahtom al hoze shalom.
Content words that have a close syntactic relation to one another are useful candidates for examination and are intuitively more likely to bear a close semantic relation than words that are near one another but are not related syntactically~~~One much-studied example is the semantic relation between a verb and its arguments eg , <REF>Boguraev et al 1989</REF>; <REF>Church and Hanks 1989</REF>; Braden-<REF>Harder 1991</REF>; <TREF>Hindle and Rooth 1991</TREF>~~~Discrimination among senses of adjectives based on the nouns they modify or of which they are predicated has been the subject of less intensive and systematic study~~~Determining the potential of this line of evidence is the focus of this paper.
or the cooccurrence of two words within a limited distance in the context~~~Statistical data about these various cooccurrence relations is employed for a variety of applications, such as speech recognition <REF>Jelinek, 1990</REF>, language generation <REF>Smadja and McKeown, 1990</REF>, lexicography <REF>Church and Hanks, 1990</REF>, machine translation Brown et al , ; <REF>Sadler, 1989</REF>, information retrieval <REF>Maarek and Smadja, 1989</REF> and various disambiguation tasks <REF>Dagan et al , 1991</REF>; <TREF>Hindle and Rooth, 1991</TREF>; <REF>Grishman et al , 1986</REF>; <REF>Dagan and Itai, 1990</REF>~~~A major problem for the above applications is how to estimate the probability of cooccurrences that were not observed in the training corpus~~~Due to data sparseness in unrestricted language, the aggregate probability of such cooccurrences is large and can easily get to 25 or more, even for a very large training corpus <REF>Church and Mercer, 1992</REF>.
Earlier work that is of particular relevance considered the importance of relations between lexical heads for disambiguation in parsing~~~<REF>See Hindle and Rooth 1991</REF> for one of the earliest pieces of research on this topic in the context of prepositional-phrase attachment ambiguity~~~For work that uses lexical relations for parse disambiguation all with very promising resultssee Sekine et al~~~1992, Jones and Eisner 1992a, 1992b, and <REF>Alshawi and Carter 1994</REF>.
Some of our initial data suggest that the hypothesis of deep semantic selection may in fact be correct, as well as indicating what the nature of the coercion rules may be~~~Using techniques described in <REF>Church and Hindle 1990</REF>, <REF>Church and Hanks 1990</REF>, and <TREF>Hindle and Rooth 1991</TREF>, below are some examples of the most frequent V-O pairs from the AP corpus~~~Counts for objects of begin/V: 205 begin/V career/O 176 begin/V day/O 159 begin/V work/O 140 begin/V talk/O 120 begin/V campaign/O 113 begin/V investigation/O 106 begin/V process/O 92 begin/V program/O 8S begin/V operation/O 86 begin/V negotiation/O 66 begin/V strike/O 64 begin/V production/O 59 begin/V meeting/O 89 begin/V term/O 50 begin/V visit/O 45 begin/V test/O 39 begin/V construction/O 31 begin/V debate/O 29 begin/V trial/O Corpus studies confirm similar results for weakly intensional contexts <REF>Pustejovsky 1991</REF> such as the complement of coercive verbs such as veto~~~These are interesting because regardless of the noun type appearing as complement, it is embedded within a semantic interpretation of the proposal to, thereby clothing the complement within an intensional context.
Some of our initial data suggest that the hypothesis of deep semantic selection may in fact be correct, as well as indicating what the nature of the coercion rules may be~~~Using techniques described in <REF>Church and Hindle 1990</REF>, <REF>Church and Hanks 1990</REF>, and <TREF>Hindle and Rooth 1991</TREF>, Figure 4 shows some examples of the most frequent V-O pairs from the AP corpus~~~Corpus studies confirm similar results for weakly intensional contexts such as the complement of coercive verbs such as veto~~~These are interesting because regardless of the noun type appearing as complement, it is embedded within a semantic interpretation of the proposal to, thereby clothing the complement within an intensional context.
In fact, a large amount of ambiguity such as coordination, Noun-Noun compounds and relative clause attachment can be resolved using such a kind of information, and relations can provide a useful interface between syntax and semantics~~~<TREF>Hindle and Rooth, 1991</TREF> had shown the use of dependency in Prepositional Phrase disambiguation, and the experimental results reported in <REF>Hockenmaier, 2003</REF> demonstrate that a language model which encodes a rich notion of predicate argument structure eg including long-range relations arising through coordination can significantly improve the parsing performances~~~Moreover, the notion of predicate argument structure has been advocated as useful in a number of dierent large-scale languageprocessing tasks, and the RS is a convenient intermediate representation in several applications see <REF>Bosco, 2004</REF> for a survey on this topic~~~For instance, in Information Extraction relations allows for recognizing dierent guises in which an event can appear regardless of the several dierent syntactic patterns that can be used to specify it <REF>Palmer et al , 2001</REF> 2 In Question Answering, systems usually use forms of relation-based structured representations of the input texts ie questions and answers and try to match those representations see eg.
There has not been a general method proposed to date, however, that learns dependencies between case slots~~~Methods of resolving ambiguities have been based, for example, on the assumption that case slots are mutually independent <TREF>Hindle and Rooth 1991</TREF>, or at most two case slots are dependent <REF>Collins and Brooks 1995</REF>~~~In this article, we propose an efficient and general method of learning dependencies between case frame slots~~~2.
Obviously, if we assume that case slots are independent, then we only need to compare PflyXfrom  1 and PietXfrom  1~~~This is nearly equivalent to the disambiguafion method proposed by <TREF>Hindle and Rooth 1991</TREF>~~~Their method, referred to here as the Lexical Association LA method, actually compares the two probabilities by means of hypothesis testing~~~Specifically, it calculates the so-called t-score, which is a statistic about the difference between the two probabilities.
We use the definition of lexical likelihood described above to avoid this problem~~~4 32 The data sparseness problem Hindle  Rooth have previously proposed resolving pp-attachment ambiguities with two-word probabilities <TREF>Hindle and Rooth, 1991</TREF>, eg, Pwithlicecream,Pwithleat, but these are not accurate enough to represent lexical preference~~~For example, in the sentences, Britain reopened the embassy in December, Britain reopened the embassy in Teheran, 10 the pp-attachment sites of the two prepositional phrases are different~~~The attachment sites would be determined to be the same, however, if we were to use two-word probabilities c:f<REF>Resnik, 1993</REF>, and thus the ambiguity of only one of the sentences can be resolved.
There have been a number of methods proposed to perform structural disambiguation using probability models, many of which have proved to be quite effective <REF>Alshawi and Carter, 1995</REF>; <REF>Black et al , 1992</REF>; Briscoe and Carroll~~~1993; <REF>Chang et al , 1992</REF>; <REF>Collins and Brooks, 1995</REF>; <REF>Fujisaki, 1989</REF>; <TREF>Hindle and Rooth, 1991</TREF>; <REF>Hindle and Rooth, 1993</REF>; <REF>Jelinek et al , 1990</REF>; <REF>Magerman and Marcus, 1991</REF>; <REF>Magerman, 1995</REF>; <REF>Ratnaparkhi et al , 1994</REF>; <REF>Resnik, 1993</REF>; <REF>Su and Chang, 1988</REF>~~~Although each of the disambiguation methods proposed to date has its merits, none resolves the disambiguation problem completely satisfactorily~~~We feel that it is necessary to devise a new method that unifies the above two approaches, ie, to implement psycholinguistic principles of disambiguation on the basis of a probabilistic methodology.
Thus our problem involves the following three subproblems: a resolving structural ambiguities based on LPR in terms of probabilistic representations, b resolving structural ambiguities based on RAP and ALPP in terms of probabilistic representations, and c combining the two~~~For subproblem a, we have devised a new method, based on LPR, which has some good properties not shared by the methods proposed so far <REF>Alshawi and Carter, 1995</REF>; <REF>Chang et al , 1992</REF>; <REF>Collins and Brooks, 1995</REF>; <TREF>Hindle and Rooth, 1991</TREF>; <REF>Ratnaparkhi et al , 1994</REF>; <REF>Resnik, 1993</REF>~~~In <REF>Li and Abe, 1995</REF>, we have described this method in detail~~~In the present paper, we mainly describe our solutions to subproblems b and c.
If we employ the Maximum Likelihood Estimator, we may find most of the parameters are estimated to be 0: a problem often referred to, in statistical natural language processing, as the data sparseness problem~~~the motivation for using the two-word probabilities in <TREF>Hindle and Rooth, 1991</TREF> appears to be a desire to avoid the data sparseness problem~~~ One may expect this problem to be less severe in the future, when more data are available~~~However, as data size increases, new words may appear, and the number of parameters that need to be estimated may increase as well.
We expect that the knowledge to be extracted will not only be useful for disambiguating sentences but also will contribute to discovering ontological classes in given subject domains~~~Though several studies with similar objectives have been reported <REF>Church, 1988</REF>, <REF>Zernik and Jacobs, 1990</REF>, <REF>Calzolari and Bindi, 1990</REF>, <REF>Garside and Leech, 1985</REF>, <TREF>Hindle and Rooth, 1991</TREF>, <REF>Brown et al , 1990</REF>, they require that sample corpora be correctly analyzed or tagged in advance~~~It must be a training corpus, which is tagged or parsed by human or it needs correspondence between two language corpora~~~Because their preparation needs a lot of manual assistance or an unerring tagger or parser, this requirement makes their algorithm, troublesome in actual application environments.
In general, these words will not be adjacent in the text, so it will not be possible to use existing approaches unmodified eg <REF>Church and Hanks 1989</REF>, because these apply to adjacent words in unanalyzed text~~~<TREF>Hindle and Rooth 1991</TREF> report good results using a mutual information measure of collocation applied within such a structurally defined context, and their approach should carry over to our framework straightforwardly~~~One way of integrating structural collocational information into the system presented above would be to make use of the semantic component of the ANLT grammar This component pairs logical forms with each distinct syntactic analysis that represent, among other things, the predicate-argument structure of the input~~~In the resolution of PP attachment and similar ambiguities, it is collocation at this level of representation that appears to be most relevant.
The corpus is relatively small it contains approximately 450,000 words and 18,750 sentences~~~In comparison, most corpus-based algorithms employ substantially larger corpora eg , 1 million words de <REF>Marcken, 1990</REF>, 25 million words <REF>Brent, 1991</REF>, 6 million words <REF>Hindle, 1990</REF>, 13 million words <REF>Hindle,  Rooth, 1991</REF>~~~Relative pronoun processing is especially important for the MUC-3 corpus because approximately 25 of the sentences contain at least one relative pronoun~~~3 In fact, the relative pronoun who occurs in approximately 1 out of every 10 sentences.
For example, we can learr that make decision is a better choice than, say 96 have decision or take decision~~~<REF>Hindle and Rooths, 1991</REF> proposes that a syntactic disambiguation criterion can be gathered by comparing the probability of occurrence of nounpreposition and verb-preposition pairs in V NP PP structures~~~In general word associations are collected by extracting word pairs in a -5 window~~~In <REF>Calzolari and Bindi, 1990</REF>, <REF>Church and Hanks, 1990</REF> the significance of an association x,y is measured by the mutual information Ix,y, ie the probability of observing x and y together, compared with the probability of observing x and y independently.
2 Acquiring syntactic associations Clustered association data are collected by first extracting from the corpus all the syntactically related word pairs~~~Combining statistical and parsing methods has been done by <REF>Hindle, 1990</REF>; Hindle and Rooths,1991 and <REF>Smadja and McKewon, 1990</REF>; Smadja,1991~~~The novel aspect of our study is that we collect not only operational pairs, but triples, such as Nprep N, VprepN etc In fact, the preposition convey important information on the nature of the semantic link between syntactically related content words~~~By looking at the preposition, it is possible to restrict the set of semantic relations underlying a syntactic relation eg forpurpose,beneficiary.
In <REF>Smadja, 1989</REF>, <REF>Zernik and Jacobs, 1990</REF>, the associations are filtered by selecting the word pairs x,y whose frequency of occurrence is above fks, where f is the average appearance, s is the standard deviation, and k is an empirically determined factor~~~<REF>Hindle, 1990</REF>; Hindle and Rooths,1991 and <REF>Smadja, 1991</REF> use syntactic markers to increase the significance of the data~~~<REF>Guthrie et al , 1991</REF> uses the subject classification given in machine-readable dictionaries eg economics, engineering, etc~~~to reinforce cooccurence links.
Much of the overhead and inefficiency comes from the fact that the lexical and structural ambiguity of natmal language input can only be dealt with using limited context information available to the parser~~~Partial parsing techniques have been used with a considerable success in processing large volumes of text, for example ATTs Fidditch <TREF>Hindle and Rooth, 1991</TREF> parsed 13 million words of Associated Press news messages, while MITs parser de <REF>Marcken, 1990</REF> was used to process the 1 million word Lancaster/Oslo/Bergen LOB corpus~~~In both cases, the parsers were designed to do partial processing only, that is, they would never attempt a complete analysis of certain constructions, such as the attachment of pp-adjuncts, subordinate clauses, or coordinations~~~This kind of partial analysis may be sufficient in some applications because of a relatively high precision of identifying correct syntactic dependencies.
The experiments were performed within an information retrieval system so that the final recall and precision statistics were used to rnealurc effectiwmess of the panmr~~~a <TREF>Hindle and Rooth 1991</TREF> and <REF>Church and Hanks 1990</REF> used partial parses generated by Fidditch to study word urrtnc patterns m syntactic contexts~~~ACRES DE COLING-92, NANTES, 23-28 AOr 1992 1 9 8 PROC~~~OF COL1NG-92.
FUTURE DIRECTIONS This paper presented one method of learning subcategorizations, but there are other approaches one might try~~~For disambiguating whether a PP is subcategorized by a verb in the V NP PP environment, <TREF>Hindle and Rooth 1991</TREF> used a t-score to determine whether the PP has a stronger association with the verb or the preceding NP~~~This method could be usefully incorporated into my parser, but it remains a special-purpose technique for one particular ease~~~Another research direction would be making the parser stochastic as well, rather than it being a categorical finite state device that runs on the output of a stochastic tagger.
This successful reuse indicates that lexical preference between prepositions and function words is largely independent of text type~~~<TREF>Hindle and Rooth, 1991</TREF> first proposed solving the prepositional attachment task with the help of statistical information, and also defined the prevalent formulation as a binary decision problem with three words involved~~~<REF>Ratnaparkhi et al , 1994</REF> 228 extended the problem instances to quadruples by also considering the kernel noun of the PP, and used maximum entropy models to estimate the preferences~~~Both supervised and unsupervised training procedures for PP attachment have been investigated and compared in a number of studies, with supervised methods usually being slightly superior <REF>Ratnaparkhi, 1998</REF>; <REF>Pantel and Lin, 2000</REF>, with the notable exception of <REF>Volk, 2002</REF>, who obtained a worse accuracy in the supervised case, obviously caused by the limited size of the available treebank.
One reason for this is that many rule-driven syntax analyzers provide no obvious way to integrate uncertain, statistical information into their decisions~~~Another is the traditional emphasis on PP attachment as a binary classification task; since <TREF>Hindle and Rooth, 1991</TREF>, research has concentrated on resolving the ambiguity in the category pattern VNPN, ie predicting the PP attachment to either the verb or the first noun~~~It is often assumed that the correct attachment is always among these 223 two options, so that all problem instances can be solved correctly despite the simplification~~~This task is sufficient to measure the relative quality of different probability models, but it is quite different from what a parser must actually do: It is easier because the set of possible answers is pre-filtered so that only a binary decision remains, and the baseline performance for pure guessing is already 50.
This task is sufficient to measure the relative quality of different probability models, but it is quite different from what a parser must actually do: It is easier because the set of possible answers is pre-filtered so that only a binary decision remains, and the baseline performance for pure guessing is already 50~~~But it is harder because it does not provide the predictor with all the information needed to solve many doubtful cases; <TREF>Hindle and Rooth, 1991</TREF> found that human arbiters consistently reach a higher agreement when they are given the entire sentence rather than just the four words concerned~~~Instead of the accuracy of PP attachers in the isolated decision between two words, we investigate the problem of situated PP attachment~~~In this task, all nouns and verbs in a sentence are potential attachment points for a preposition; the computer must find suitable attachments for one or more prepositions in parallel, while building a globally coherent syntax structure at the same time.
Verb CPU Time second Average Number of Generalized Levels eat 100 52 buy 066 46 fly 111 60 operate 090 50 Average 092 52 many probabilistic methods proposed in the literature to address the PP-attachment problem using lexical semantic knowledge which, in our view, can be classified into three types~~~The first approach <REF>Hindle and Rooth 1991, 1993</REF> takes doubles of the form verb, prep and nounl, prep, like those in Table 9, as training data to acquire semantic knowledge and judges the attachment sites of the prepositional phrases in quadruples of the form verb, nounl, prep, noun2 eg, see, girl, with, telescope--based on the acquired knowledge~~~<TREF>Hindle and Rooth 1991</TREF> proposed the use of the lexical association measure calculated based on such doubles~~~More specifically, they estimate Pprep I verb and Pprep  noun1, and calculate the so-called t-score, which is a measure of the statistical significance of the difference between Pprep I verb and Pprep  nounl.
The first approach <REF>Hindle and Rooth 1991, 1993</REF> takes doubles of the form verb, prep and nounl, prep, like those in Table 9, as training data to acquire semantic knowledge and judges the attachment sites of the prepositional phrases in quadruples of the form verb, nounl, prep, noun2 eg, see, girl, with, telescope--based on the acquired knowledge~~~<TREF>Hindle and Rooth 1991</TREF> proposed the use of the lexical association measure calculated based on such doubles~~~More specifically, they estimate Pprep I verb and Pprep  noun1, and calculate the so-called t-score, which is a measure of the statistical significance of the difference between Pprep I verb and Pprep  nounl~~~If the t-score indicates that the former probability is significantly larger, 233 Computational Linguistics Volume 24, Number 2 Table 9 Example input data as doubles.
The generalization step is needed in order to represent the input case frame instances more compactly as well as to judge the degree of acceptability of unseen case frame instances~~~For the extraction problem, there have been various methods proposed to date, which are quite adequate <TREF>Hindle and Rooth 1991</TREF>; <REF>Grishman and Sterling 1992</REF>; <REF>Manning 1992</REF>; <REF>Utsuro, Matsumoto, and Nagao 1992</REF>; <REF>Brent 1993</REF>; <REF>Smadja 1993</REF>; <REF>Grefenstette 1994</REF>; <REF>Briscoe and Carroll 1997</REF>~~~The generalization problem, in contrast, is a more challenging one and has not been solved completely~~~A number of methods for generalizing values of a case frame slot for a verb have been  CC Media Res.
236 Li and Abe Generalizing Case Frames Table 13 Results of PP-attachment disambiguation~~~Coverage Accuracy Default 100 562 MDL  Default 100 822 SA  Default 100 767 LA  Default 100 807 LAt  Default 100 781 TEL 100 824 We also implemented the exact method proposed by <TREF>Hindle and Rooth 1991</TREF>, which makes disambiguation judgement using the t-score~~~Figure 10 shows the result as LAt, where the threshold for t-score is set to 128 significance level of 90 percent~~~From Figure 10 we see that with respect to accuracy-coverage curves, MDL outperforms both SA and LA throughout, while SA is better than LA Next, we tested the method of applying a default rule after applying each method.
We estimate Pnoun2 I verb, prep and Pnoun2 I nount, prep from training data consisting of triples, and compare them: If the former exceeds the latter by a certain margin we attach it to verb, else if the latter exceeds the former by the same margin we attach it to noun1~~~In our experiments, described below, we compare the performance of our proposed method, which we refer to as MDL, against the methods proposed by <TREF>Hindle and Rooth 1991</TREF>, <REF>Resnik 1993b</REF>, and <REF>Brill and Resnik 1994</REF>, referred to respectively as LA, SA, and TEL~~~Data Set~~~We used the bracketed corpus of the Penn Treebank Wall Street Journal corpus <REF>Marcus, Santorini, and Marcinkiewicz 1993</REF> as our data.
Examples of such efforts include work on the induction of synchronous grammars <REF>Wu and Wong, 1998</REF>; <REF>Chiang, 2005</REF> and learning multilingual lexical resources <REF>Genzel, 2005</REF>~~~Another thread of work using cross-lingual links has been in word-sense disambiguation, where senses of words can be defined based on their translations <TREF>Brown et al, 1991</TREF>; <REF>Dagan et al, 1991</REF>; <REF>Resnik and Yarowsky, 1997</REF>; <REF>Ng et al, 2003</REF>~~~When annotations for a task of interest are available in a source language but are missing in the target language, the annotations can be projected across a parallel corpus <REF>Yarowsky et al, 2000</REF>; <REF>Diab and Resnik, 2002</REF>; <REF>Pado and Lapata, 2006</REF>; <REF>Xi and Hwa, 2005</REF>~~~In fact, projection methods have been used to train highly accurate part-of-speech taggers <REF>Yarowsky and Ngai, 2001</REF>; <REF>Feldman et al, 2006</REF>.
Finally, in Section 7 we present a comparative analysis of statistical sense disambiguation methods and then conclude in Section 8~~~2 A similar observation underlies the use of parallel bilingual corpora for sense disambiguation <TREF>Brown et al 1991</TREF>; <REF>Gale, Church, and Yarowsky 1992</REF>~~~As we explain in Section 7, these corpora are a form of a manually tagged corpus and are more difficult to obtain than monolingual corpora~~~Erroneously, the preliminary publication of our method <REF>Dagan, Itai, and Schwall 1991</REF> was cited several times as requiring a parallel bilingual corpus, 565 Computational Linguistics Volume 20, Number 4 2.
Sense disambiguation thus resembles many other decision tasks, and not surprisingly, several common decision algorithms were employed in different works~~~These include a Bayesian classifier <REF>Gale, Church, and Yarowsky 1993</REF> and a distance 589 Computational Linguistics Volume 20, Number 4 metric between vectors <REF>Schiitze 1993</REF>, both inspired from methods in information retrieval; the use of the flip-flop algorithm for ordering possible informants about the preferred sense, trying to maximize the mutual information between the informant and the ambiguous word <TREF>Brown et al 1991</TREF>; and the use of confidence intervals to establish the degree of confidence in a certain preference, combined with a constraint propagation algorithm the current paper~~~At the present stage of research on sense disambiguation, it is difficult to judge whether a certain decision algorithm is significantly superior to others~~~21 Yet, these decision models can be characterized by several criteria, which clarify the similarities and differences between them.
It seems, however, that Brown et al expect that target word selection would be determined mainly by translation probabilities the second factor in the above term, which should be derived from a bilingual corpus <REF>Brown et al 1990</REF>, p 79~~~This view is reflected also in their elaborate method for target word selection <TREF>Brown et al 1991</TREF>, in which better estimates of translation probabilities are achieved as a result of word sense disambiguation~~~Our method, on the other hand, incorporates only 592 Ido Dagan and Alon Itai Word Sense Disambiguation target language probabilities and ignores any notion of translation probabilities~~~It thus demonstrates a possible trade-off between these two types of probabilities: using more informative statistics of the target language may compensate for the lack of translation probabilities.
Model parameters are automatically estimated using a corpus of translation pairs~~~TMs have been used for statistical machine translation <REF>Berger et al , 1996</REF>, word alignment of a translation corpus <REF>Melamed, 2000</REF>, multilingual document retrieval <REF>Franz et al , 1999</REF>, automatic dictionary construction <REF>Resnik and Melamed, 1997</REF>, and data preparation for word sense disambiguation programs <TREF>Brown et al , 1991</TREF>~~~Developing a better TM is a fundamental issue for those applications~~~Researchers at IBM first described such a statistical TM in <REF>Brown et al , 1988</REF>.
A third difference concerns the granularity of WSD attempted, which one can illustrate in terms of the two levels of semantic distinctions found in many dictionaries: homograph and sense see Section 31~~~Like <REF>Cowie, Guthrie, and Guthrie 1992</REF>, we shall give results at both levels, but it is worth pointing out that the targets of, say, work using translation equivalents eg , <TREF>Brown et al 1991</TREF>; <REF>Gale, Church, and <REF>Yarowsky 1992</REF>c</REF>; and see Section 23 and Roget categories <REF>Yarowsky 1992</REF>; <REF>Masterman 1957</REF> correspond broadly to the wider, homograph, distinctions~~~In this paper we attempt to show that the high level of results more typical of systems trained on many instances of a restricted vocabulary can also be obtained by large vocabulary systems, and that the best results are to be obtained from an optimization of a combination of types of lexical knowledge see Section 2~~~11 Lexical Knowledge and WSD Syntactic, semantic, and pragmatic information are all potentially useful for WSD, as can be demonstrated by considering the following sentences: 1 2 3 4 John did not feel well.
Using the definitionbased conceptual co-occurrence data collected from the relatively small Brown corpus, our sense disambiguation system achieves an average accuracy comparable to human performance given the same contextual information~~~Previous corpus-based sense disambiguation methods require substantial amounts of sense-tagged training data <REF>Kelly and Stone, 1975</REF>; <REF>Black, 1988</REF> and <REF>Hearst, 1991</REF> or aligned bilingual corpora <TREF>Brown et al , 1991</TREF>; <REF>Dagan, 1991</REF> and <REF>Gale et al 1992</REF>~~~<REF>Yarowsky 1992</REF> introduces a thesaurus-based approach to statistical sense disambiguation which works on monolingual corpora without the need for sense-tagged training data~~~By collecting statistical data of word occurrences in the context of different thesaurus categories from a relatively large corpus 10 million words, the system can identify salient words for each category.
The results of the experiment are summarized in Table 4~~~52 A data recovery task In the second evaluation, the estimation method had to distinguish between members of two sets of 8It should be emphasized that the TWS method uses only a monolingual target corpus, and not a bilingual corpus as in other methods <TREF>Brown et al , 1991</TREF>; <REF>Gale et al , 1992</REF>~~~The alternative cooccurrence patterns in the target language, which correspond to the alternative translations of the ambiguous source words, are constructed using a bilingual lexicon~~~cooccurrence pairs, one of them containing pairs with relatively high probability and the other pairs with low probability.
Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in learning procedure with the requirement of predefined sense inventory for target words~~~They roughly fall into three categories according to what is used for supervision in learning process: 1 using external resources, eg, thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, <REF>Lesk, 1986</REF>; <REF>Lin, 1997</REF>; <REF>McCarthy et al , 2004</REF>; <REF>Seo et al , 2004</REF>; <REF>Yarowsky, 1992</REF>, 2 exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora eg parallel corpora or untagged monolingual corpora in two languages <TREF>Brown et al , 1991</TREF>; <REF>Dagan and Itai, 1994</REF>; <REF>Diab and Resnik, 2002</REF>; <REF>Li and Li, 2004</REF>; <REF>Ng et al , 2003</REF>, 3 bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data <REF>Hearst, 1991</REF>; <REF>Karov and Edelman, 1998</REF>; <REF>Mihalcea, 2004</REF>; <REF>Park et al , 2000</REF>; <REF>Yarowsky, 1995</REF>~~~As a commonly used semi-supervised learning method for WSD, bootstrapping algorithm works by iteratively classifying unlabeled examples and adding confidently classified examples into labeled dataset using a model learned from augmented labeled dataset in previous iteration~~~It can be found that the affinity information among unlabeled examples is not fully explored in this bootstrapping process.
Later work from the AI community relied heavily upon selectional restrictions for verbs, although primarily in terms of features exhibited by their arguments such as DRINKABLE rather than in terms of individual words or word classes~~~More recent work <TREF>Brown et al 1991</TREF><REF>Hearst 1991</REF> has utilized a set of discrete local questions such as word-to-the-right in the development of statistical decision procedures~~~However, a strong trend in recent years is to treat a reasonably wide context window as an unordered bag of independent evidence points~~~This technique from information retrieval has been used in neural networks, Bayesian discriminators, and dictionary definition matching.
Hence, we use a slightly different framework~~~We view a bilingual corpus as a sequence of sentence beads <TREF>Brown et al , 1991b</TREF>, where a sentence bead corresponds to an irreducible group of sentences that align with each other~~~For example, the correct alignment of the bilingual corpus in Figure 2 consists of the sentence bead El; F1 followed by the sentence bead E2; ;2, F3~~~We can represent an alignment 4 of a corpus as a sequence of sentence beads Epl; Fpl, Ep2; F,, where the E and F can be zero, one, or more sentences long.
In this way, we can ignore the context of the collocations and their translations, and base our decisions only on the patterns of co-occurrence of each collocation and its candidate translations across the entire corpus~~~This approach is quite different from those adopted for the translation of single words <REF>Klavans and Tzoukermann 1990</REF>; <REF>Dorr 1992</REF>; <REF>Klavans and Tzoukermann 1996</REF>, since for single words polysemy cannot be ignored; indeed, the problem of sense disambiguation has been linked to the problem of translating ambiguous words <TREF>Brown et al 1991</TREF>; <REF>Dagan, Itai, and Schwall 1991</REF>; <REF>Dagan and Itai 1994</REF>~~~The assumption of a single meaning per collocation was based on our previous experience with English collocations <REF>Smadja 1993</REF>, is supported for less opaque collocations by the fact that their constituent words tend to have a single sense when they appear in the collocation <REF>Yarowsky 1993</REF>, and was verified during our evaluation of Champollion Section 7~~~We construct a mathematical model of the events we want to correlate, namely, the appearance of any word or group of words in the sentences of our corpus, as follows: To each group of words G, in either the source or the target language, we map a binary random variable Xc that takes the value 1 if G appears in a particular sentence and 0 if not.
If sense labeling is part of the task, an outside source of knowledge is necessary to define the senses~~~Regardless of whether it takes the form of dictionaries <REF>Lesk 1986</REF>; <REF>Guthrie et al 1991</REF>; <REF>Dagan, Itai, and Schwall 1991</REF>; <REF>Karov and Edelman 1996</REF>, thesauri <REF>Yarowsky 1992</REF>; <REF>Walker and Amsler 1986</REF>, bilingual corpora <TREF>Brown et al 1991</TREF>; <REF>Church and Gale 1991</REF>, or hand-labeled training sets <REF>Hearst 1991</REF>; <REF>Leacock, Towell, and Voorhees 1993</REF>; <REF>Niwa and Nitta 1994</REF>; <REF>Bruce and Wiebe 1994</REF>, providing information for sense definitions can be a considerable burden~~~What makes our approach unique is that, since we narrow the problem to sense discrimination, we can dispense of an outside source of knowledge for defining senses~~~ Xerox Palo Alto Research Center, 3333 Coyote Hill Road, Palo Alto, CA 94304 Q 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 1 We therefore call our approach automatic word sense discrimination, since we do not require manually constructed sources of knowledge.
It thrives on raw, unannotated monolingual corpora the more the merrier~~~Although there is some hope from using aligned bilingual corpora as training data for supervised algorithms <TREF>Brown et al , 1991</TREF>, this approach suffers from both the limited availability of such corpora, and the frequent failure of bilingual translation differences to model monolingual sense differences~~~The use of dictionary definitions as an optional seed for the unsupervised algorithm stems from a long history of dictionary-based approaches, including <REF>Lesk 1986</REF>, Guthrie et al~~~1991, <REF>Veronis and Ide 1990</REF>, and <REF>Slator 1991</REF>.
It is also crucial in cross-language text processing including cross-language information retrieval and abstraction~~~Due to the recent availability of large text corpora, various statistical approaches have been tried including using 1 parallel corpora <REF>Brown et al , 1990</REF>, <TREF>Brown et al , 1991</TREF>, <REF>Brown, 1997</REF>, 2 non-parallel bilingual corpora tagged with topic area <REF>Yamabana et al , 1998</REF> and 3 un-tagged mono-language corpora in the target language <REF>Dagan and Itai, 1994</REF>, <REF>Tanaka and Iwasaki, 1996</REF>, <REF>Kikui, 1998</REF>~~~A problem with the first two approaches is that it is not easy to obtain sufficiently large parallel or manually tagged corpora for the pair of languages targeted~~~Although the third approach eases the problem of preparing corpora, it suffers from a lack of useful information in the source language.
Their method thus yields some incorrect noun phrases that will not be proposed by a tagger, but on the other hand does not miss noun phrases that may be missed due to tagging errors~~~3 Bilingual Task: An Application for Word Alignment 31 Sentence and word alignment Bilingual alignment methods <REF>Warwick et al , 1990</REF>; <TREF>Brown et al , 1991a</TREF>; <REF>Brown et al , 1993</REF>; <REF>Gale and Church, 1991b</REF>; <REF>Gale and Church, 1991a</REF>; <REF>Kay and Roscheisen, 1993</REF>; <REF>Simard et al , 1992</REF>; <REF>Church, 1993</REF>; <REF>Kupiec, 1993a</REF>; <REF>Matsumoto et al , 1993</REF>; <REF>Dagan et al , 1993</REF>~~~have been used in statistical machine translation <REF>Brown et al , 1990</REF>, terminology research and translation aids <REF>Isabelle, 1992</REF>; <REF>Ogden and Gonzales, 1993</REF>; van der <REF>Eijk, 1993</REF>, bilingual lexicography <REF>Klavans and Tzoukermann, 1990</REF>; <REF>Smadja, 1992</REF>, word-sense disambiguation <TREF>Brown et al , 1991b</TREF>; <REF>Gale et al , 1992</REF> and information retrieval in a multilingual environment <REF>Landauer and Littman, 1990</REF>~~~Most alignment work was concerned with alignment at the sentence level.
3 Bilingual Task: An Application for Word Alignment 31 Sentence and word alignment Bilingual alignment methods <REF>Warwick et al , 1990</REF>; <TREF>Brown et al , 1991a</TREF>; <REF>Brown et al , 1993</REF>; <REF>Gale and Church, 1991b</REF>; <REF>Gale and Church, 1991a</REF>; <REF>Kay and Roscheisen, 1993</REF>; <REF>Simard et al , 1992</REF>; <REF>Church, 1993</REF>; <REF>Kupiec, 1993a</REF>; <REF>Matsumoto et al , 1993</REF>; <REF>Dagan et al , 1993</REF>~~~have been used in statistical machine translation <REF>Brown et al , 1990</REF>, terminology research and translation aids <REF>Isabelle, 1992</REF>; <REF>Ogden and Gonzales, 1993</REF>; van der <REF>Eijk, 1993</REF>, bilingual lexicography <REF>Klavans and Tzoukermann, 1990</REF>; <REF>Smadja, 1992</REF>, word-sense disambiguation <TREF>Brown et al , 1991b</TREF>; <REF>Gale et al , 1992</REF> and information retrieval in a multilingual environment <REF>Landauer and Littman, 1990</REF>~~~Most alignment work was concerned with alignment at the sentence level~~~Algorithms for the more difficult task of word alignment were proposed in <REF>Gale and Church, 1991a</REF>; <REF>Brown et al , 1993</REF></REF>; <REF>Dagan et al , 1993</REF> and were applied for parameter estimation in the IBM statistical machine translation system <REF>Brown et al , 1993</REF></REF>.
Although this requirement may seem trivial, most corpus-based methods do not, in fact, allow such flexibility~~~For example, defining the senses by the possible translations of the word <REF>Dagan, Itai and Schwall 1991</REF>; <TREF>Brown et al 1991</TREF>; <REF>Gale, Church, and <REF>Yarowsky 1992</REF></REF>, by the Rogets categories <REF>Yarowsky 1992</REF>, or by clustering Schitze 1992, yields a grouping that does not always conform to the desired sense distinctions~~~In comparison to these approaches, our reliance on the MRD for the definition of senses in the initialization of the learning process guarantees the required flexibility in setting the sense distinctions~~~Specifically, the user of our system may choose a certain dictionary definition, a combination of definitions from several dictionaries, or manually listed seed words for every sense that needs to be defined.
Traditionally, the problem of sparse data is approached by estimating the probability of unobserved co-occurrences using the actual co-occurrences in the training set~~~This can be done by smoothing the observed frequencies 7 <REF>Church and Mercer 1993</REF> or by class-based methods <TREF>Brown et al 1991</TREF>; <REF>Pereira and Tishby 1992</REF>; <REF>Pereira, Tishby, and Lee 1993</REF>; <REF>Hirschman 1986</REF>; <REF>Resnik 1992</REF>; <REF>Brill et al 1990</REF>; <REF>Dagan, Marcus, and Markovitch 1993</REF>~~~In comparison to these approaches, we use similarity information throughout training, and not merely for estimating co-occurrence statistics~~~This allows the system to learn successfully from very sparse data.
Traditionally, the problem of sparse data is approached by estimating the probability of unobserved cooccurrences using the actual cooccurrences in the training set~~~This can be done by smoothing the observed frequencies <REF>Church and Mercer, 1993</REF>, or by class-based methods <TREF>Brown et al , 1991</TREF>; <REF>Pereira and Tishby, 1992</REF>; Pereira et ah, 1993; <REF>Hirschman, 1986</REF>; <REF>Resnik, 1992</REF>; Brill et ah, 1990; <REF>Dagan et al , 1993</REF>~~~In comparison to these approaches, we use similarity information throughout training, and not merely for estimating cooccurrence statistics~~~This allows the system to learn successfully from very sparse data.
The aim of this measure is to indicate the relatedness between two elements composing a pair~~~Mutual information has been positively used in many NLP tasks such as collocation analysis <REF>Church and Hanks, 1989</REF>, terminology extraction <REF>Damerau, 1993</REF>, and word sense disambiguation <TREF>Brown et al , 1991</TREF>~~~3 Experimental Evaluation As many other corpus linguistic approaches, our entailment detection model relies partially on some linguistic prior knowledge the expected structure of the searched collocations, ie, the textual entailment patterns and partially on some probability distribution estimation~~~Only a positive combination of both these two ingredients can give good results when applying and evaluating the model.
Brown et al presented an algorithm that  This research was done when the author was at Center for the Study of Language and InformationCSLI, Stanford University~~~1In fact, this is partly shown by the fact that many MT systems have substitutable domain-dependent or user  dictionaries  relies on translation probabilities estimated from large bilingual corpora <REF>Brown et al, 1990</REF><TREF>Brown et al, 1991</TREF>~~~<REF>Dagan and Itai 1994</REF> and <REF>Tanaka and Iwasaki 1996</REF> proposed algorithms for selecting target words by using word co-occurrence statistics in the target language corpora~~~The latter algorithms using mono-lingual corpora are particularly important because, at present, we cannot always get a sufficient amount of bilingual or parallel corpora.
From an application-oriented perspective, there is the further problem of how it is possible to regiment their role and relevance as a function of context variation~~~As a somewhat radical alternative to taxonomical relationships, other ways of measuring semantic similarity based on distributional evidence have been put forward in the literature see, among others, <TREF>Brown et al 1991</TREF>, <REF>Gale et al 1992</REF>, <REF>Pereira and Tishby 1992</REF>, which emphasise the role played by context in this game~~~These approaches compute the semantic similarity between W z and W, on the basis of the extent to which W,/W,s average contexts of use overlap~~~Here, the context is generally defined as an n-word window centred on Wl/W:.
2 Previous Work There are several investigations in literature that explore using parallel corpora to transfer information content from one language most of the time English to another~~~The earliest investigations of the subject have been performed, on word sense disambiguation <REF>Dagan et al, 1991</REF>; PF<TREF>Brown et al, 1991</TREF>; <REF>Gale et al, 1992</REF> perhaps unsurprisingly given its close connection to machine translation  all propose and lightly evaluate methods to use word sense information extracted from the target language to help the sense resolution in the source language and machine translation~~~<REF>Dagan and Itai, 1994</REF> explicitly suggests performing word sense disambiguation in the target language English in the article with the goal of resolving ambiguity in the source language Hebrew, and show moderate 2While applying this method in the case where the source language has absolutely no resources might be an interesting test case, we dont see it as being realistic~~~Resources are build nowadays in a large variety of languages, and not making use of them is rather foolish a certain big bird and sand comes to mind.
In order to utilize models with more complicated interactions among feature variables, <REF>Bruce and Wiebe, 1994b</REF> introduce the use of sequential model selection and decomposable models for word-sense disambiguation~~~ Alternative probabilistic approaches have involved using a single contextual feature to perform disambiguation eg , <TREF>Brown et al , 1991</TREF>, <REF>Dagan et al , 1991</REF>, and <REF>Yarowsky, 1993</REF> present techniques for identifying the optimal feature to use in disambiguation~~~Maximum Entropy models have been used to express the interactions among multiple feature variables eg , <REF>Berger et al , 1996</REF>, but within this framework no systematic study of interactions has been proposed~~~Decision tree induction has been applied to word-sense disambiguation eg.
Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in the learning procedure with the need of predened sense inventories for target words~~~The information for semi-supervised sense disambiguation is usually obtained from bilingual corpora eg parallel corpora or untagged monolingual corpora in two languages <TREF>Brown et al , 1991</TREF>; <REF>Dagan and Itai, 1994</REF>, or sense-tagged seed examples <REF>Yarowsky, 1995</REF>~~~Some observations can be made on the previous supervised and semi-supervised methods~~~They always rely on hand-crafted lexicons eg , WordNet as sense inventories.
2 The Design of PhraseNet Context is one important notion in PhraseNet~~~While the context may mean different things in natural language, many previous work in statistically natural language processing defined context as an n-word window around the target word <REF>Gale et al , 1992</REF>; <TREF>Brown et al , 1991</TREF>; <REF>Roth, 1998</REF>~~~In PhraseNet, context has a more precise definition that depends on the grammatical structure of a sentence rather than simply counting surrounding words~~~We define context to be the syntactic structure of the sentence in which the word of interest occurs.
By repeating the above processes, it can create an accurate classifier for word translation disambiguation~~~For other related work, see, for example, <TREF>Brown et al 1991</TREF>; <REF>Dagan and Itai 1994</REF>; <REF>Pedersen and Bruce 1997</REF>; <REF>Schutze 1998</REF>; <REF>Kikui 1999</REF>; <REF>Mihalcea and Moldovan 1999</REF>~~~3 Bilingual Bootstrapping 31 Overview Instead of using Monolingual Bootstrapping, we propose a new method for word translation disambiguation using Bilingual Bootstrapping~~~In translation from English to Chinese, for instance, BB makes use of not only unclassified data in English, but also unclassified data in Chinese.
To deal with these robustness issues, <REF>Church 1993</REF> developed a character-based alignment method called charalign~~~The method was intended as a replacement for sentence-based methods eg , <TREF>Brown et al , 1991a</TREF>; <REF>Gale and Church, 1991b</REF>; <REF>Kay and Rosenschein, 1993</REF>, which are very sensitive to noise~~~This paper describes a new program, called wordalign, that starts with an initial rough alignment eg , the output of charalign or a sentence-based alignment method, and produces improved alignments by exploiting constraints at the word-level~~~The alignment algorithm consists of two steps: 1 estimate translation probabilities, and 2 use these probabilities to search for most probable alignment path.
More importantly, because wordalign and charalign were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at ATT Language Line Services, a commercial translation service, to help them with difficult terminology~~~Aligning parallel texts has recently received considerable attention <REF>Warwick et al , 1990</REF>; <TREF>Brown et al , 1991a</TREF>; <REF>Gale and Church, 1991b</REF>; <REF>Gale and Church, 1991a</REF>; <REF>Kay and Rosenschein, 1993</REF>; <REF>Simard et al , 1992</REF>; <REF>Church, 1993</REF>; <REF>Kupiec, 1993</REF>; <REF>Matsumoto et al , 1993</REF>~~~These methods have been used in machine translation <REF>Brown et al , 1990</REF>; <REF>Sadler, 1989</REF>, terminology research and translation aids <REF>Isabelle, 1992</REF>; <REF>Ogden and Gonzales, 1993</REF>, bilingual lexicography <REF>Klavans and Tzoukermann, 1990</REF>, collocation studies <REF>Smadja, 1992</REF>, word-sense disambiguation <TREF>Brown et al , 1991b</TREF>; <REF>Gale et al , 1992</REF> and information retrieval in a multilingual environment <REF>Landauer and Littman, 1990</REF>~~~The information retrieval application may be of particular relevance to this audience.
Aligning parallel texts has recently received considerable attention <REF>Warwick et al , 1990</REF>; <TREF>Brown et al , 1991a</TREF>; <REF>Gale and Church, 1991b</REF>; <REF>Gale and Church, 1991a</REF>; <REF>Kay and Rosenschein, 1993</REF>; <REF>Simard et al , 1992</REF>; <REF>Church, 1993</REF>; <REF>Kupiec, 1993</REF>; <REF>Matsumoto et al , 1993</REF>~~~These methods have been used in machine translation <REF>Brown et al , 1990</REF>; <REF>Sadler, 1989</REF>, terminology research and translation aids <REF>Isabelle, 1992</REF>; <REF>Ogden and Gonzales, 1993</REF>, bilingual lexicography <REF>Klavans and Tzoukermann, 1990</REF>, collocation studies <REF>Smadja, 1992</REF>, word-sense disambiguation <TREF>Brown et al , 1991b</TREF>; <REF>Gale et al , 1992</REF> and information retrieval in a multilingual environment <REF>Landauer and Littman, 1990</REF>~~~The information retrieval application may be of particular relevance to this audience~~~It would be highly desirable for users to be able to express queries in whatever language they chose and retrieve documents that may or may not have been written in the same language as the query.
Similar results are reported in <REF>Nakatani, Hirschberg, and Grosz 1995</REF> and <REF>Hirschberg and Nakatani 1996</REF> for spontaneous speech as well~~~<REF>Grosz and Hirschberg 1992</REF> also use the classification and regression tree system CART <REF>Brieman et al 1984</REF> to automatically construct and evaluate decision trees for classifying aspects of discourse structure from intonational feature values~~~The studies of <REF>Swerts 1995</REF> and <REF>Swerts and Ostendorf 1995</REF> also investigate the prosodic structuring of discourse~~~<REF>In Swerts 1995</REF>, paragraph boundaries are empirically obtained as described above.
SEMCOR contains 91,808 words tagged with WordNet synsets, 6,071 of which are proper names, which we ignored, leaving 85,737 words which could potentially be translated~~~The translation contains only 36,869 words tagged with LDOCE senses; however, this is a reasonable size for an evaluation corpus for the task, and it is several orders of magnitude larger than those used by other researchers working in large vocabulary WSD, for example <REF>Cowie, Guthrie, and Guthrie 1992</REF>, <REF>Harley and Glennon 1997</REF>, and Mahesh et al~~~1997~~~This corpus was also constructed without the excessive cost of additional hand-tagging and does not introduce any of the inconsistencies that can occur with a poorly controlled tagging strategy.
A third difference concerns the granularity of WSD attempted, which one can illustrate in terms of the two levels of semantic distinctions found in many dictionaries: homograph and sense see Section 31~~~Like <REF>Cowie, Guthrie, and Guthrie 1992</REF>, we shall give results at both levels, but it is worth pointing out that the targets of, say, work using translation equivalents eg , <REF>Brown et al 1991</REF>; <TREF>Gale, Church, and <REF>Yarowsky 1992</REF>c</TREF>; and see Section 23 and Roget categories <REF>Yarowsky 1992</REF>; <REF>Masterman 1957</REF> correspond broadly to the wider, homograph, distinctions~~~In this paper we attempt to show that the high level of results more typical of systems trained on many instances of a restricted vocabulary can also be obtained by large vocabulary systems, and that the best results are to be obtained from an optimization of a combination of types of lexical knowledge see Section 2~~~11 Lexical Knowledge and WSD Syntactic, semantic, and pragmatic information are all potentially useful for WSD, as can be demonstrated by considering the following sentences: 1 2 3 4 John did not feel well.
Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction <REF>Smadja, 1993</REF>; <REF>Fung and Wu, 1994</REF>, phrasal translation <REF>Smadja et al , 1996</REF>; <REF>Kupiec, 1993</REF>; <REF>Wu, 1995</REF>; <REF>Dagan and Church, 1994</REF>, target word selection <REF>Liu and Li, 1997</REF>; <REF>Tanaka and Iwasaki, 1996</REF>, domain word translation <REF>Fung and Lo, 1998</REF>; <REF>Fung, 1998</REF>, sense disambiguation <REF>Brown et al , 1991</REF>; <REF>Dagan et al , 1991</REF>; <REF>Dagan and Itai, 1994</REF>; <TREF>Gale et al , 1992a</TREF>; <TREF>Gale et al , 1992b</TREF>; <TREF>Gale et al , 1992c</TREF>; <REF>Shiitze, 1992</REF>; <REF>Gale et al , 1993</REF>; <REF>Yarowsky, 1995</REF>, and even recently for query translation in cross-language IR as well <REF>Ballesteros and Croft, 1998</REF>~~~Co-occurrence statistics is collected from either bilingual parallel and 334 non-parallel corpora <REF>Smadja et al , 1996</REF>; <REF>Kupiec, 1993</REF>; <REF>Wu, 1995</REF>; <REF>Tanaka and Iwasaki, 1996</REF>; <REF>Fung and Lo, 1998</REF>, or monolingual corpora <REF>Smadja, 1993</REF>; <REF>Fung and Wu, 1994</REF>; <REF>Liu and Li, 1997</REF>; <REF>Shiitze, 1992</REF>; <REF>Yarowsky, 1995</REF>~~~As we noted in <REF>Fung and Lo, 1998</REF>; <REF>Fung, 1998</REF>, parallel corpora are rare in most domains~~~We want to devise a method that uses only monolingual data in the primary language to train co-occurrence information.
pruning translation alternatives for query translation~~~Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction <REF>Smadja, 1993</REF>; <REF>Fung and Wu, 1994</REF>, phrasal translation <REF>Smadja et al , 1996</REF>; <REF>Kupiec, 1993</REF>; <REF>Wu, 1995</REF>; <REF>Dagan and Church, 1994</REF>, target word selection <REF>Liu and Li, 1997</REF>; <REF>Tanaka and Iwasaki, 1996</REF>, domain word translation <REF>Fung and Lo, 1998</REF>; <REF>Fung, 1998</REF>, sense disambiguation <REF>Brown et al , 1991</REF>; <REF>Dagan et al , 1991</REF>; <REF>Dagan and Itai, 1994</REF>; <TREF>Gale et al , 1992a</TREF>; <TREF>Gale et al , 1992b</TREF>; <TREF>Gale et al , 1992c</TREF>; <REF>Shiitze, 1992</REF>; <REF>Gale et al , 1993</REF>; <REF>Yarowsky, 1995</REF>, and even recently for query translation in cross-language IR as well <REF>Ballesteros and Croft, 1998</REF>~~~Co-occurrence statistics is collected from either bilingual parallel and 334 non-parallel corpora <REF>Smadja et al , 1996</REF>; <REF>Kupiec, 1993</REF>; <REF>Wu, 1995</REF>; <REF>Tanaka and Iwasaki, 1996</REF>; <REF>Fung and Lo, 1998</REF>, or monolingual corpora <REF>Smadja, 1993</REF>; <REF>Fung and Wu, 1994</REF>; <REF>Liu and Li, 1997</REF>; <REF>Shiitze, 1992</REF>; <REF>Yarowsky, 1995</REF>~~~As we noted in <REF>Fung and Lo, 1998</REF>; <REF>Fung, 1998</REF>, parallel corpora are rare in most domains.
In this paper we use many examples from the RSD~~~21 Morphology The morphologic analyzer <REF>Marziali, 1992</REF> derives from the work on a generative approach to the Italian morphology <REF>Russo, 1987</REF>, first used in DANTE, a NLP system for analysis of short narrative texts in the financial domain <REF>Antonacci et al 1989</REF>~~~Tile analyzer includes over 7000 elementary lemmata stems without affixes, eg flex is the elementary lemma for de448 flex, in-flex, re-fiex anti has been experimented since now on economic, financial, commercial and legal domains~~~Elementary lemmata cover much more than 700 words, since many words have an affix.
A second problem with the Fidditch parser is poor performances: tilt recall and precision at detecting word collocations are declared to be as low as 50, I-iowever it is unclear if this value applies only to SVO triples, and how it has been derived~~~The recall is low because tile Fidditch parser, as other partial parsers <REF>Sekine et al, 1992</REF>; Resnik and Hearst, i993, only detect links between adjacent or near-adjacent words~~~Thougll a 50/,, precision and recall might be 447 reasonable for human assisted tasks, like in lexicography, supervised translation, etc , it is not fair enough if collocational analysis must serve a fully automated system~~~In fact, corpus linguistics became a popular research field because of the claim that shallow techniques could overcome the lexical coverage bottleneck of traditional NLP techniques.
The parser The SSA syntactic analysis is a rewriting procedure of a single sentence into a set of 1 meme-yy ijgjin esl~~~The SSA is based on a discontinuous grammar, described more formally in <REF>Basili et al 1992a</REF>~~~In tiffs section we provide a qualitative clescription of the rules by which esls are generated~~~Examples of esls generated by the parser are: NV the subject-verb relation, V N the direct objectverb relation, N P N noun preposition noun, V P N verb preposition noun, NAdj adjective noun, N N conqound etc Overall, we identify over 20 different esls.
In <REF>Zernik 1990</REF>; <REF>Calzolari and Bindi 1990</REF>; <REF>Smadja 1989</REF>; <REF>Church and Hanks 1990</REF> associations are detected in a 5 window~~~A wider window  tO0 words is used in <TREF>Gale et al 1992</TREF>~~~Windowing techniques are also used in <REF>Jelinek et al, 1990</REF>, where it is proposed a trigram model to automatically derive, and refine, context-free rules of the grammar <REF>Fujisaki et al, 1991</REF>~~~Windowing techniques weekly model tile locality of language as well as other lexical information.
In fact, corpus linguistics became a popular research field because of the claim that shallow techniques could overcome the lexical coverage bottleneck of traditional NLP techniques~~~Among the applications of collocational analysis for lexical acquisition are: the derivation of syntactic disambiguation cues <REF>Basili et al 1991, 1993a</REF>; <REF>Hindle and Rooths 1991</REF>,1993; <REF>Sekine 1992</REF> <REF>Bogges et al 1992</REF>, sense preference <REF>Yarowski 1992</REF>, acquisition of selectional restrictions <REF>Basili et al 1992b, 1993b</REF>; <REF>Utsuro et al 1993</REF>, lexical preference in generation <REF>Smadjia 1991</REF>, word clustering <REF>Pereira 1993</REF>; <REF>Hindle 1990</REF>; <REF>Basili et al 1993c</REF>, etc In the majority of these papers, even though the precedent or subsequent statistical processing reduces the number of accidental associations, very large corpora 10,000,000 words are necessary to obtain reliable data on a large enough number of words~~~In addition, most papers produce a performance evaluation of their methods but do not provide a measure of the coverage, ie the percentage of cases for which their method actually provides a right or wrong solution~~~It is quite common that results are discussed only for 10-20 cases.
7 SUBJECTS a, b, c, d, e, f, g 161 I0 Hei falls over, Figure 2: Portion of Segntentation from Narrative 6 Subject Annotation of Narratrs httention Digression to describe suml track No verbal communication ie , speaker describes lack thereof Describes that it is a silent movie with only nature sounds Speaker describes sound techniques used in tnovie Explain that there is no speaking ill nlovie Figure 3: Segment spanning 1,12 through 154 3 Discourse Segment Boundaries In <REF>Passonneau and Litman, 1993</REF>, we show that our subjects agree with one another at levels that are statistically significant, thus demonstrating the reliability of intention as a segmentation criterion~~~Percent agreement is defined in <TREF>Gale et al , 1992</TREF> as the ratio of observed agreements with the majority opinion to possible agreements with the majority opinion~~~We use percent agreement to measure the ability of subjects to agree with one anotlter on whether there is  segment boundary between two adjacent prosodic phrases~~~We find that the average agreement across the 20 narratives on the status of all potential boundary locations is 89 with a range from 82-92.
The precision is the ratio of the number of correct interpretations, to the number of outputs~~~The column of control denotes the precision of a naive WSD technique, in which the system systematicaily chooses the verb sense appearing most frequently in the database <TREF>Gale et al , 1992</TREF>~~~The precision for each similarity calculation method did not differ greatly, and the use of the length of the path in the Bunruigoihyo thesaurus BGH slightly outperformed other method on the whole~~~However, since the overall precision is biased by frequently appeared verbs such as tsukau and ukeru, our word similarity measurement is not necessarily inferior to other methods.
An artificial ambiguous word can be coined with the monosemous words in table 1~~~This process is similar to the use of general pseudowords <TREF>Gale et al , 1992b</TREF>; <REF>Gaustad, 2001</REF>; <REF>Nakov and Hearst, 2003</REF>, but has some essential differences~~~This artificial ambiguous word need to simulate the function of the real ambiguous word, and to acquire semantic knowledge as the real ambiguous word does~~~Thus, we call it an equivalent pseudoword EP for its equivalence with the real ambiguous word.
Surrounding words give lower accuracy, perhaps because in our work, only the current sentence forms the surrounding context, which averages about 20 words~~~Previous work on using the unordered set of surrounding words have used a much larger window, such as the 100-word window of <REF>Yarowsky, 1992</REF>, and the 2-sentence context of <REF>Leacock et al , 1993</REF>~~~Verb-object syntactic relation is the weakest knowledge source~~~Our experimental finding, that local collocations are the most predictive, agrees with past observation that humans need a narrow window of only a few words to perform WSD <REF>Choueka and Lusignan, 1985</REF>.
In contrast, LEXAS uses supervised learning from tagged sentences, which is also the approach taken by most recent work on WSD, including <REF>Bruce and Wiebe, 1994</REF>; <REF>Miller et al , 1994</REF>; <REF>Leacock et al , 1993</REF>; <REF>Yarowsky, 1994</REF>; <REF>Yarowsky, 1993</REF>; <REF>Yarowsky, 1992</REF>~~~The work of <REF>Miller et al , 1994</REF>; <REF>Leacock et al , 1993</REF>; <REF>Yarowsky, 1992</REF> used only the unordered set of surrounding words to perform WSD, and they used statistical classifiers, neural networks, or IR-based techniques~~~The work of <REF>Bruce and Wiebe, 1994</REF> used parts of speech POS and morphological form, in addition to surrounding words~~~However, the POS used are abbreviated POS, and only in a window of -b2 words.
One line of research focuses on the use of the knowledge contained in a machine-readable dictionary to perform WSD, such as <REF>Wilks et al , 1990</REF>; <REF>Luk, 1995</REF>~~~In contrast, LEXAS uses supervised learning from tagged sentences, which is also the approach taken by most recent work on WSD, including <REF>Bruce and Wiebe, 1994</REF>; <REF>Miller et al , 1994</REF>; <REF>Leacock et al , 1993</REF>; <REF>Yarowsky, 1994</REF>; <REF>Yarowsky, 1993</REF>; <REF>Yarowsky, 1992</REF>~~~The work of <REF>Miller et al , 1994</REF>; <REF>Leacock et al , 1993</REF>; <REF>Yarowsky, 1992</REF> used only the unordered set of surrounding words to perform WSD, and they used statistical classifiers, neural networks, or IR-based techniques~~~The work of <REF>Bruce and Wiebe, 1994</REF> used parts of speech POS and morphological form, in addition to surrounding words.
We compared the classification accuracy of LEXAS against the default strategy of picking the most frequent sense~~~This default strategy has been advocated as the baseline performance level for comparison with WSD programs <TREF>Gale et al , 1992</TREF>~~~There are two instantiations of this strategy in our current evaluation~~~Since WORDNET orders its senses such that sense 1 is the most frequent sense, one possibility is to always pick sense 1 as the best sense assignment.
That is, LEXAS is only concerned with disambiguating senses of a word in a given POS~~~Making such an assumption is reasonable since POS taggers that can achieve accuracy of 96 are readily available to assign POS to unrestricted English sentences <REF>Brill, 1992</REF>; <REF>Cutting et al , 1992</REF>~~~In addition, sense definitions are only available for root words in a dictionary~~~These are words that are not morphologically inflected, such as interest as opposed to the plural form interests, fall as opposed to the other inflected forms like fell, fallen, falling, falls, etc The sense of a morphologically inflected content word is the sense of its uninflected form.
9 Related work Using vector space model and similarity measures for ranking is a common approach in IR for query/text and text/text comparisons <REF>Salton and Buckley, 1988</REF>; <REF>Salton and Yang, 1973</REF>; <REF>Croft, 1984</REF>; <REF>Turtle and Croft, 1992</REF>; <REF>Bookstein, 1983</REF>; <REF>Korfhage, 1995</REF>; <REF>Jones, 1979</REF>~~~This approach has also been used by <REF>Dagan and Itai, 1994</REF>; <TREF>Gale et al , 1992</TREF>; <REF>Shiitze, 1992</REF>; <REF>Gale et al , 1993</REF>; <REF>Yarowsky, 1995</REF>; Gale and Church, 1Lunar is not an unknown word in English, Yeltsin finds its translation in the 4-th candidate~~~Table 5: tion out score 0008421 0007895 0007669 0007588 0007283 0006812 0006430 0006218 0005921 0005527 0005335 0005335 0005221 0004731 0004470 0004275 0003878 0003859 0003859 0003784 0003686 0003550 0003519 0003481 0003407 0003407 0003338 0003324 Some Chinese ut English Teng-hui SAR flu Lei poultry SAR hijack poultry Tung Diaoyu PrimeMinister President China Lien poultry China flu PrimeMinister President poultry Kalkanov poultry SAR Zhuhai PrimeMinister President flu apologise unknown word translaChinese  Weng-hui  u Lei j Poultry  Chee-hwa  Teng-hui  SAR  Chee-hwa : Teng-hui  Weng-hui W Weng-hui CLam  Teng-hui - Chee-hwa  Teng-hui Lei  Chee-hwa  Chee-hwa  Leung  Zhuhai I Lei J Yeltsin - Chee-hwa  Lam Lam j Poultry W Teng-hui 0003250 DPP 0003206 Tang 0003202 Tung 0003040 Leung 0003033 China 0002888 Zhuhai 0002886 Tung  Teng-hui Tang Leung Leung  SAR  Lunar Tung 1994 for sense disambiguation between multiple usages of the same word~~~Some of the early statistical terminology translation methods are <REF>Brown et al , 1993</REF>; <REF>Wu and Xia, 1994</REF>; <REF>Dagan and Church, 1994</REF>; <REF>Gale and Church, 1991</REF>; <REF>Kupiec, 1993</REF>; <REF>Smadja et al , 1996</REF>; Kay and R<REF>Sscheisen, 1993</REF>; <REF>Fung and Church, 1994</REF>; <REF>Fung, 1995b</REF>.
It remains to be seen how we can also make use of the multilingual texts as NLP resources~~~In the years since the appearance of the first papers on using statistical models for bilingual lexicon compilation and machine translation<REF>Brown et al , 1993</REF>; <REF>Brown et al , 1991</REF>; <REF>Gale and <REF>Church, 1993</REF></REF>; <REF>Church, 1993</REF>; <REF>Simard et al , 1992</REF>, large amount of human effort and time has been invested in collecting parallel corpora of translated texts~~~Our goal is to alleviate this effort and enlarge the scope of corpus resources by looking into monolingual, comparable texts~~~This type of texts are known as nonparallel corpora.
The same system used in a validation mode, can be used to check and spot alignment errors in multilingually aligned wordnets as BalkaNet and EuroWordNet~~~Word Sense Disambiguation WSD is wellknown as one of the more difficult problems in the field of natural language processing, as noted in <TREF>Gale et al, 1992</TREF>; <REF>Kilgarriff, 1997</REF>; <REF>Ide and Vronis, 1998</REF>, and others~~~The difficulties stem from several sources, including the lack of means to formalize the properties of context that characterize the use of an ambiguous word in a given sense, lack of a standard and possibly exhaustive sense inventory, and the subjectivity of the human evaluation of such algorithms~~~To address the last problem, <TREF>Gale et al, 1992</TREF> argue for upper and lower bounds of precision when comparing automatically assigned sense labels with those assigned by human judges.
The difficulties stem from several sources, including the lack of means to formalize the properties of context that characterize the use of an ambiguous word in a given sense, lack of a standard and possibly exhaustive sense inventory, and the subjectivity of the human evaluation of such algorithms~~~To address the last problem, <TREF>Gale et al, 1992</TREF> argue for upper and lower bounds of precision when comparing automatically assigned sense labels with those assigned by human judges~~~The lower bound should not drop below the baseline usage of the algorithm in which every word that is disambiguated is assigned the most frequent sense whereas the upper bound should not be too restrictive when the word in question is hard to disambiguate even for human judges a measure of this difficulty is the computation of the agreement rates between human annotators~~~Identification and formalization of the determining contextual parameters for a word used in a given sense is the focus of WSD work that treats texts in a monolingual settingthat is, a setting where translations of the texts in other languages either do not exist or are not considered.
The summed deviation for perfect performance is thus 0~~~Finally, to interpret our quantitative results, we use the performance of our human subjects as a target goal for the performance of our algorithms <TREF>Gale et al , 1992</TREF>~~~Table 1 shows the average human performance for both the training and test sets of narratives~~~Note that human performance is basically the same for both sets of narratives.
By asking subjects to segment discourse using a non-linguistic criterion, the correlation of linguistic devices with independently derived segments can then be investigated in a way that avoids circularity~~~Together, <REF>Grosz and Hirschberg, 1992</REF>; <REF>Hirschberg and Grosz, 1992</REF>; <REF>Nakatani et al , 1995</REF> comprise an ongoing study using three corpora: professionally read AP news stories, spontaneous narrative, and read and spontaneous versions of task-oriented monologues~~~Discourse structures are derived from subjects segmentations, then statistical measures are used to characterize these structures in terms of acoustic-prosodic features~~~Grosz and Hirschbergs work also used the classification and regression tree system CART <REF>Breiman et al , 1984</REF> to automatically construct and evaluate decision trees for classifying aspects of discourse structure from intonational feature values.
Researchers have begun to investigate the ability of humans to agree with one another on segmen108 tation, and to propose methodologies for quantifying their findings~~~Several studies have used expert coders to locally and globally structure spoken discourse according to the model of <REF>Grosz and Sidnet 1986</REF>, including <REF>Grosz and Hirschberg, 1992</REF>; <REF>Hirschberg and Grosz, 1992</REF>; <REF>Nakatani et al , 1995</REF>; <REF>Stifleman, 1995</REF>~~~<REF>Hearst 1994</REF> asked subjects to place boundaries between paragraphs of expository texts, to indicate topic changes~~~<REF>Moser and Moore 1995</REF> had an expert coder assign segments and various segment features and relations based on RST.
<REF>Moser and Moore 1995</REF> had an expert coder assign segments and various segment features and relations based on RST~~~To quantify their findings, these studies use notions of agreement <TREF>Gale et al , 1992</TREF>; <REF>Moset and Moore, 1995</REF> and/or reliability <REF>Passonneau and Litman, 1993</REF>; Passonneau and Litman, to appear; <REF>Isard and Carletta, 1995</REF>~~~By asking subjects to segment discourse using a non-linguistic criterion, the correlation of linguistic devices with independently derived segments can then be investigated in a way that avoids circularity~~~Together, <REF>Grosz and Hirschberg, 1992</REF>; <REF>Hirschberg and Grosz, 1992</REF>; <REF>Nakatani et al , 1995</REF> comprise an ongoing study using three corpora: professionally read AP news stories, spontaneous narrative, and read and spontaneous versions of task-oriented monologues.
By using 10-fold cross validation <REF>Kohavi and John, 1995</REF> to automatically pick the best number of nearest neighbors to use, the performance of LSXAS has improved~~~4 Word Sense Disambiguation in the Large In <TREF>Gale et al , 1992</TREF>, it was argued that any wide coverage WSD program must be able to perform significantly better than the most-frequent-sense classifier to be worthy of serious consideration~~~The performance of LEXAS as indicated in Table 1 is significantly better than the most-frequent-sense classifier for the set of 191 words collected in our corpus~~~Figure 1 and 2 also confirm that all the training examples collected in our corpus are effectively utilized by LEXAS to improve its WSD performance.
1~~~Word sense disambiguation has long been one of the major concerns in natural language processing area eg , <REF>Bruce et al , 1994</REF>; <REF>Choueka et al , 1985</REF>; <REF>Gale et al , 1993</REF>; <REF>McRoy, 1992</REF>; <REF>Yarowsky 1992, 1994, 1995</REF>, whose aim is to identify the correct sense of a word in a particular context, among all of its senses defined in a dictionary or a thesaurus~~~Undoubtedly, effective disambiguation techniques are of great use in many natural language processing tasks, eg, machine translation and information retrieving <REF>Allen, 1995</REF>; <REF>Ng and Lee, 1996</REF>; <REF>Resnik, 1995</REF>, etc Previous strategies for word sense disambiguation mainly fall into two categories: statistics-based method and exemplar-based method~~~Statistics-based method often requires large-scale corpora eg , <REF>Hirst, 1987</REF>; <REF>Luk, 1995</REF>, sense-tagging or not, monolingual or aligned bilingual, as training data to specify significant clues for each word sense.
H2s,  max Ps, l ew ,n, E EW where EWi  ew I si  synsetew  1 P  si I ewj  - -nj where si  synsetewj, nj  Isyr et w,l In this formula, n is the number of synsets of the translation et~~~23 Heuristic 3: Sense Ordering <TREF>Gale et al , 1992</TREF> reports that word sense disambiguation would be at least 75 correct if a system assigns the most frequently occurring sense~~~<REF>Miller et al , 1994</REF> found that automatic I We use English WordNet version 16 L 143 assignment of polysemous words in Brown Corpus to senses in WordNet was 58 correct with a heuristic of most frequently occurring sense~~~We adopt these previous results to develop sense ordering heuristic.
In the area of word sense disambiguation, <REF>Black 1988</REF> developed a model based on decision trees using a corpus of 22 million tokens, after manually sense-tagging approximately 2,000 concordance lines for five test words~~~Since then, supervised learning from sense-tagged corpora has since been used by several researchers: Zernik 1990, 1991, <REF>Hearst 1991</REF>, <REF>Leacock, Towell, and Voorhees 1993</REF>, Gale, Church, and Yarowsky 1992d, 1993, <REF>Bruce and Wiebe 1994</REF>, Miller et al~~~1994, <REF>Niwa and Nitta 1994</REF>, <REF>Lehman 1994</REF>, among others~~~However, despite the availability of increasingly large corpora, two major obstacles impede the acquisition of lexical knowledge from corpora: the difficulties of manually sense-tagging a training corpus, and data sparseness.
<REF>Sutcliffe and Slater 1995</REF> replicated this method on full text samples from Orwells Animal Farm and found similar results 72 correct sense assignment, compared with a 33 chance baseline, and 40 using Lesks method~~~Several authors for example, Krovetz and Croft 1989, Guthrie et al 1991, Slator 1992, Cowie, Guthrie, and Guthrie 1992, Janssen 1992, Braden-Harder 1993, Liddy and Paik 1993 have attempted to improve results by using supplementary fields of information in the electronic version of the Longman Dictionary of Contemporary English LDOCE, in particular, the box codes and subject codes provided for each sense~~~Box codes include primitives such as ABSTRACT, ANIMATE, HUMAN, etc , and encode type restrictions on nouns and adjectives and on the arguments of verbs~~~Subject codes use another set of primitives to classify senses of words by subject ECONOMICS, ENGINEERING, etc.
<REF>Atkins 1987</REF> and Kilgarriff forthcoming also implicitly adopt the view of <REF>Harris 1954</REF>, according to which each sense distinction is reflected in a distinct context~~~A similar view underlies the class-based methods cited in Section 243 <REF>Brown et al 1992</REF>; <REF>Pereira and Tishby 1992</REF>; <REF>Pereira, Tishby, and Lee 1993</REF>~~~In this volume, Schiitze continues in this vein and proposes a technique that avoids the problem of sense distinction altogether: he creates sense clusters from a corpus rather than relying on a pre-established sense list~~~324 Enumeration or generation.
Furthermore, our percent agreement figures are comparable with the results of other segmentation studies discussed above~~~While studies of other tasks have achieved stronger results eg , 968 in a word-sense disambiguation study <TREF>Gale et al , 1992</TREF>, the meaning of percent agreement in isolation is unclear~~~For example, a percent agreement figure of less than 90 could still be very meaningful if the probability of obtaining such a figure is low~~~In the next section we demonstrate the significance of our findings.
AGREEMENT AMONG SUBJECTS We measure the ability of subjects to agree with one another, using a figure called percent agreement~~~Percent agreement, defined in <TREF>Gale et al , 1992</TREF>, is the ratio of observed agreements with the majority opinion to possible agreements with the majority opinion~~~Here, agreement among four, five, six, or seven subjects on whether or not there is a segment boundary between two adjacent prosodic phrases constitutes a majority opinion~~~Given a transcript of length n prosodic phrases, there are n-1 possible boundaries.
A farmer is thus the most recent noun phrase that is both consistent with, and 148 in the relevant interpretation context of, the pronoun in question~~~One problem in trying to model such discourse structure effects is that segmentation has been observed to be rather subjective <REF>Mann et al , 1992</REF>; <REF>Johnson, 1985</REF>~~~Several researchers have begun to investigate the ability of humans to agree with one another on segmentation~~~Grosz and Hirschberg <REF>Grosz and Hirschberg, 1992</REF>; <REF>Hirschberg and Grosz, 1992</REF> asked subjects to structure three AP news stories averaging 450 words in length according to the model of <REF>Grosz and Sidner 1986</REF>.
RELIABILITY The correspondence between discourse segments and more abstract units of meaning is poorly understood see <REF>Moore and Pollack, 1992</REF>~~~A number of alternative proposals have been presented which directly or indirectly relate segments to intentions <REF>Grosz and Sidner, 1986</REF>, RST relations <REF>Mann et al , 1992</REF> or other semantic relations <REF>Polanyi, 1988</REF>~~~We present initial results of an investigation of whether naive subjects can reliably segment discourse using speaker intention as a criterion~~~Our corpus consists of 20 narrative monologues about the same movie, taken from <REF>Chafe 1980</REF> N14,000 words.
Answer from five human subjects~~~By this experiment, we try to clarify the upper bound of the performance of the text segmentation task, which can be considered to indicate the degree of the difficulty of the task<REF>Passonneau and Litman, 1993</REF>; <TREF>Gale et al , 1992</TREF>~~~Figure 1,2 and table 1 show the results of the experiments~~~Two figures show the systems mean performance of 14 texts.
Answer from five human subjects~~~By this experiment, we try to clarify the upper bound of the performance of the text segmentation task, which can be considered to indicate the degree of the difficulty of the task<REF>Passonneau and Litman, 1993</REF>; <TREF>Gale et al, 1992</TREF>~~~Figure 1,2 and table 1 show the results of the experiments~~~Two figures show the systems mean performance of 14 texts.
To avoid one zero count of Pvj Ci nullifying the effect of the other non-zero conditional probabilities in the multiplication, we replace zero counts of P vjCi by PCi/N, where N is the total number of training examples~~~Other more complex smoothing procedures such as those used in <TREF>Gale et al , 1992a</TREF> are also possible, although we have not experimented with these other variations~~~For the experimental results reported in this paper, we used the implementation of Naive-Bayes algorithm in the PEBLS program <REF>Rachlin and Salzberg, 1993</REF>, which has an option for training and testing using the Naive-Bayes algorithm~~~We only changed the handling of zero probability counts to the method just described.
587 752 Naive-Bayes 582 745 Table 1: Experimental Results ures are those of <REF>Ng and Lee, 1996</REF>~~~The default strategy of picking the most frequent sense has been advocgted as the baseline performance for evaluating WSD programs <TREF>Gale et al , 1992b</TREF>; <REF>Miller et al , 1994</REF>~~~There are two instantiations of this strategy in our current evaluation~~~Since WORDNET orders its senses such that sense 1 is the most frequent sense, one possibility is to always pick sense 1 as the best sense assignment.
This is in spite of the conditional independence assumption made by the Naive-Bayes algorithm, which may be unjustified in the domains tested~~~Gale, Church and Yarowsky <TREF>Gale et al , 1992a</TREF>; <REF>Gale et al , 1995</REF>; <REF>Yarowsky, 1992</REF> have also successfully used the Naive-Bayes algorithm and several extensions and variations for word sense disambiguation~~~On the other hand, our past work on WSD <REF>Ng and Lee, 1996</REF> used an exemplar-based or nearest neighbor learning approach~~~Our WSD program, LEXAS, extracts a set of features, including part of speech and morphological form, surrounding words, local collocations, and verb-object syntactic relation from a sentence containing the word to be disambiguated.
Much recent research on word sense disambiguation WSD has adopted a corpus-based, learning approach~~~Many different learning approaches have been used, including neural networks <REF>Leacock et al , 1993</REF></REF>, probabilistic algorithms <REF>Bruce and Wiebe, 1994</REF>; <TREF>Gale et al , 1992a</TREF>; <REF>Gale et al , 1995</REF>; <REF>Leacock et al , 1993</REF></REF>; <REF>Yarowsky, 1992</REF>, decision lists <REF>Yarowsky, 1994</REF>, exemplar-based learning algorithms <REF>Cardie, 1993</REF>; <REF>Ng and Lee, 1996</REF>, etc In particular, <REF>Mooney 1996</REF> evaluated seven state-of-the-art machine learning algorithms on a common data set for disambiguating six senses of the word line~~~The seven algorithms that he evaluated are: a Naive-Bayes classifier <REF>Duda and Hart, 1973</REF>, a perceptron <REF>Rosenblatt, 1958</REF>, a decisiontree learner <REF>Quinlan, 1993</REF>, a k nearest-neighbor classifier exemplar-based learner <REF>Cover and Hart, 1967</REF>, logic-based DNF and CNF learners <REF>Mooney, 1995</REF>, and a decision-list learner <REF>Rivest, 1987</REF>~~~His results indicate that the simple Naive-Bayes algorithm gives the highest accuracy on the line corpus tested.
9 Related work Using vector space model and similarity measures for ranking is a common approach in IR for query/text and text/text comparisons <REF>Salton and Buckley, 1988</REF>; <REF>Salton and Yang, 1973</REF>; <REF>Croft, 1984</REF>; <REF>Turtle and Croft, 1992</REF>; <REF>Bookstein, 1983</REF>; <REF>Korflmge, 1995</REF>; <REF>Jones, 1979</REF>~~~This approach has also been used by <REF>Dagan and Itai, 1994</REF>; <TREF>Gale et al, 1992</TREF>; <REF>Shiitze, 1992</REF>; <REF>Gale et al, 1993</REF>; <REF>Yarowsky, 1995</REF>; Gale and Church, 1Lunar is not an unknown word in English, Yeltsin finds its translation in the 4-th candidate~~~Table 5: Some Chinese unknown word translation output score 0008421 0007895 0007669 0007588 0007283 0006812 0006430 0006218 0005921 0005527 0005335 0005335 0005221 0004731 0004470 0004275 0003878 0003859 0003859 0003784 0003686 0003550 0003519 0003481 0003407 0003407 0OO3338 0003324 0003250 0003206 0003202 0003040 0003033 0002888 0002886 English Chinese Teng-hui  Teng-hui SAR , SAR flu N m, Lei  Lei poultry j Poultry SAR  Chee-hwa hijack  Teng-hui poultry  SAR ,ng  Chee-hw Diaoyu  Teng-hui PrimeMinister  Teng-hui President  Teng-hui China  Lava Lien  Teng-hui poultry  Chee-hwa China  Teng-hui flu  Lei PrivaeMinister -I Chee-hwa President 1; Chee-hwa poultry  Leung Kalkanov i Zhuhai poultry I Lei SAR 1 J l Yeltsin Zhuhai -1 l Chee-hwa PrimeMinister  Lain President  Lava flu  Poultry apologise  Teng-hui Dee  Teng-hui Tang J Tang iSlng  Leung Leung : Leung China tN SAR Zhuhai  Lunar Ttulg  Tung 1994 for sense disambiguation between multiple usages of the same word~~~Some of the early statistical terminology translation methods are <REF>Brown et al, 1993</REF>; <REF>Wu and Xia, 1994</REF>; <REF>Dagan and Church, 1994</REF>; <REF>Gale and Church, 1991</REF>; <REF>Kupiec, 1993</REF>; <REF>Smadja et al, 1996</REF>; Kay and R<REF>Sscheisen, 1993</REF>; <REF>Fung and Church, 1994</REF>; <REF>Fhmg, 1995b</REF>.
It remains to be seen how we can also make use of the multilingual texts as NLP resources~~~In the years since the appearance of the first papers on using statistical models for bilingual lexicon compilation and machine translation<REF>Brown et al, 1993</REF>; <REF>Brown et al, 1991</REF>; <REF>Gale and <REF>Church, 1993</REF></REF>; <REF>Church, 1993</REF>; <REF>Simard et al, 1992</REF>, large amount of human effort and time has been invested in collecting parallel corpora of translated texts~~~Our goal is to alleviate this effort and enlarge the scope of corpus resources by looking into monolingual, comparable texts~~~This type of texts are known as nonparallel corpora.
The PARADISE model posits that performance can be correlated with a meaningful external criterion such as usability, and thus that the overall goal of a spoken dialogue agent is to maximize an objective related to usability~~~User satisfaction ratings <REF>Kamm, 1995</REF>; <REF>Shriberg, Wade, and Price, 1992</REF>; <REF>Polifroni et al , 1992</REF> have been frequently used in the literature as an external indicator of the usability of a dialogue agent~~~The model further posits that two types of factors are potential relevant contributors to user satisfaction namely task success and dialogue costs, and that two types of factors are potential relevant contributors to costs <REF>Walker, 1996</REF>~~~In addition to the use of decision theory to create this objective structure, other novel aspects of PARADISE include the use of the Kappa coefficient <REF>Carletta, 1996</REF>; <REF>Siegel and Castellan, 1988</REF> to operationalize task success, and the use of linear regression to quantify the relative contribution of the success and cost factors to user satisfaction.
An agents responses to a query are compared with a predefined key of minimum and maximum reference answers; performance is the proportion of responses that match the key~~~This approach has many widely acknowledged limitations <REF>Hirschman and Pao, 1993</REF>; <REF>Danieli et al , 1992</REF>; <REF>Bates and Ayuso, 1993</REF>, eg, although there may be many potential dialogue strategies for carrying out a task, the key is tied to one particular dialogue strategy~~~In contrast, agents using different dialogue strategies can be compared with measures such as inappropriate utterance ratio, turn correction ratio, concept accuracy, implicit recovery and transaction success Danieli LWe use the term agent to emphasize the fact that we are evaluating a speaking entity that may have a personality~~~Readers who wish to may substitute the word system wherever agent is used.
The problems arise because most sense distinctions are not as clear as the distinction between river bank and money bnk, so it is not always straightforward for a person to say what the correct answer is Thus we do not always know what it would mean to say that a computer program got the right answer~~~The issue is discussed in detail by <TREF>Gale et al , 1992</TREF> who identify the problem as one of identifying the upper bound for the performance of a WSD program~~~If people can only agree on the correct answer x of the time, a claim that a program achieves more than x accuracy is hard to interpret, and x is the upper bound for what the program can meaningfully achieve~~~There have been some discussions as to what this upper bound might be.
Although this methodology could be valid for certain NLP problems, such as English Part-of-Speech tagging, we think that there exists reasonable evidence to say that, in WSD, accuracy results cannot be simply extrapolated to other domains contrary to the opinion of other authors <REF>Ng, 1997b</REF>: On the aSupervised approaches, also known as data-driven or corpus-dmven, are those that learn from a previously semantically annotated corpus~~~172 one hand, WSD is very dependant to the domain of application <TREF>Gale et al , 1992b</TREF> --see also <REF>Ng and Lee, 1996</REF>; <REF>Ng, 1997a</REF>, in which quite different accuracy figures are obtained when testing an exemplar-based WSD classifier on two different corpora~~~Oi1 the other hand, it does not seem reasonable to think that the training material is large and representative enough to cover all potential types of examples~~~To date, a thorough study of the domain dependence of WSD --in the style of other studies devoted to parsing <REF>Sekine, 1997</REF>-has not been carried out.
In order to corroborate the previous hypotheses, this paper explores the portability and tuning of four different ML algorithms previously applied to WSD by training and testing them on different corpora~~~Additionally, supervised methods suffer from the knowledge acquisition bottleneck <TREF>Gale et al , 1992a</TREF>~~~<REF>Ng, 1997b</REF> estimates that the manual annotation effort necessary to build a broad coverage semantically annotated English corpus is about 16 personyears~~~This overhead for supervision could be much greater if a costly tuning procedure is required before applying any existing system to each new domain.
A recent special issue of the Machine Learning journal on Bias Evaluation and Selection introduced by Gordon and des<REF>Jardins 1995</REF> presents current research in this general area~~~Learning to Disambiguate Word Senses Several recent research projects have taken a corpus-based approach to lexical disambiguation Brown, Della-Pietra, Della-<REF>Pietra,  Mercer, 1991</REF>; <TREF>Gale, Church,  Yarowsky, 1992b</TREF>; <REF>Leacock et al , 1993b</REF>; <REF>Lehman, 1994</REF>~~~The goal is to learn 2This explanation was originally presented by Shavlik et al~~~1991.
Word sense disambiguation is a potentially crucial task in many NLP applications, such as machine translation Brown, Della Pietra, and <REF>Della Pietra 1991</REF>, parsing <REF>Lytinen 1986</REF>; <REF>Nagao 1994</REF> and text retrieval <REF>Krovets and Croft 1992</REF>; <REF>Voorhees 1993</REF>~~~Various corpus-based approaches to word sense disambiguation have been proposed <REF>Bruce and Wiebe 1994</REF>; <REF>Charniak 1993</REF>; <REF>Dagan and Itai 1994</REF>; <REF>Fujii et al 1996</REF>; <REF>Hearst 1991</REF>; <REF>Karov and Edelman 1996</REF>; <REF>Kurohashi and Nagao 1994</REF>; <REF>Li, Szpakowicz, and Matwin 1995</REF>; <REF>Ng and Lee 1996</REF>; <REF>Niwa and Nitta 1994</REF>; Schitze 1992; <REF>Uramoto 1994b</REF>; <REF>Yarowsky 1995</REF>~~~The use of corpus-based approaches has grown with the use of machine-readable text, because unlike conventional rule-based approaches relying on hand-crafted selectional rules some of which are reviewed, for example, by Hirst 1987, corpus-based approaches release us from the task of generalizing observed phenomena through a set of rules~~~Our verb sense disambiguation system is based on such an approach, that is, an example-based approach.
Statistically, one would therefore expect the first sense to be the one that is chosen as the most appropriate one in most cases~~~<TREF>Gale et al , 1992</TREF> estimate that automatic sense disambignation would be a, least 75 correct if a system ignored context and assigned the most frequently occurring sense~~~<REF>Miller et al , 1994</REF> found that automatic assignment of polysemous words in the Brown Corpus to senses in WordNet was correct 58 of the time with a guessing heuristic that assumed the most frequently occurring sense to be the correct one~~~The taggers whose work is analyzed here were not aware of the frequency ordering of the senses.
With the growing wdume of text available in electronic lorm, a number of methods have been proposed tor extracting word correspondences from bilingual corpora automatically~~~These methods can be divided into those taking a statistical approach <REF>Gale  Church 1991a</REF>; <TREF>Kupiec 1993</TREF>; <REF>Dagan et al 1993</REF>; <REF>Inoue  Nogaito 1993</REF>; <REF>Fung 1995</REF> and those taking a linguistic approach <REF>Yamamoto  Sakamoto 1993</REF>; Kummo  <REF>Hirakawa 1994</REF>; <REF>Ishimoto  Nagao 1994</REF>~~~The statistical approach utilizes the occurrence frequencies and locations of words in a parallel corpus to calculate the pairwise correlations between the words in the two languages~~~The linguistic approach primarily extracts correspondences between compound words by consulting a bilingual dictionary of simple words.
5 Other applications of this research Other than the constraints described in Section 2 and frequency determination techniques, the proposed methodology is theoretically scalable to any domain where two streams of chunked information require alignment~~~This suggests applications to the extraction of translation pairs from aligned bilingual corpora <REF>Gale and Church, 1991</REF>; <TREF>Kupiec, 1993</TREF>; <REF>Smadja et al , 1996</REF>, where the system input would be made up of aligned strings generally sentences in the two languages~~~Given that we can devise some way of creating an alignment paradigm between the two input segments, it is possible to apply the scoring and learning methods proposed herein in their existing forms~~~Note, however, that in the case of translation pair extraction, there is a real possibility of the alignment mapping being many-to-many, and crossing over of alignment is expected to occur readily.
This paper presents a novel terminology extraction method applied to the French-English part of the database~~~There is a long tradition of research into bilingual terminology extraction <TREF>Kupiec, 1993</TREF>, Gaussier,1998~~~Inmostsystems,candidateterms are first identified in the source language based on predefined PoS patterns  for French, N N, N Prep N, and N Adj are typical patterns~~~In a second step, the translation candidates are extracted from the bilingual corpus based on word alignments.
Alignment at other levels of resolution is obviously useful~~~A section, paragraph, sentence, phrase, collocation, or word can be aligned to its translation <TREF>Kupiec 1993</TREF>; <REF>Smadja, McKeown, and Hatzivassiloglou 1996</REF>~~~Other logical approaches involve aligning parse trees of a sentence and its translation <REF>Matsumoto, Ishimoto, and Utsuro 1993</REF>; <REF>Meyers, Yangarber, and Grishman 1996</REF>, or simultaneously generating parse trees and alignment arrangements <REF>Wu 1995</REF>~~~Department of Computer Science, National Tsing Hua University, Hsinchu, 30043, Taiwan, ROC.
Prec~~~100 97 200 94 Table 2: Multiword notion results As a comparison, <TREF>Kupiec, 1993</TREF> obtained a precision of 90 for the first hundred associations between English and French noun phrases, using the EM algorithm~~~Our experiments with a similar method showed a precision around 92 for the first hundred associations on a set of aligned sentences comprising the one used for the above experiment~~~Art evaluation on single words, showed a precision of 98 for the first hundred and 97 for the first two hundred.
3 Multilingual terminology extraction Several works describe methods to extract terms, or candidate terms, in English and/or French <REF>Justeson and Katz, 1995</REF>; <REF>Daille, 1994</REF>; Nkwenti-<REF>Azeh, 1992</REF>~~~Some more specific works describe methods to align noun phrases within parallel corpora <TREF>Kupiec, 1993</TREF>~~~The underlying assumption beyond these works is that the monolingually extracted units correspond to each other cross-lingually~~~Unfortunately, this is not always the case, and the above methodology suffers flom the weaknesses pointed out by <REF>Wu, 1997</REF> concerning parse-parse-match procedures.
In addition, new collocations are produced one after another and most of them are technical jargons~~~There has been a growing interest in corpus-based approaches which retrieve collocations from large corpora <REF>Nagao and Mori, 1994</REF>, <REF>Ikehara et al , 1996</REF> <TREF>Kupiec, 1993</TREF>, <REF>Fung, 1995</REF>, <REF>Kitamura and Matsumoto, 1996</REF>, <REF>Smadja, 1993</REF>, <REF>Smadja et al , 1996</REF>, <REF>Haruno et al , 1996</REF>~~~Although these approaches achieved good results for the task considered, most of them aim to extract fixed collocations, mainly noun phrases, and require the information which is dependent on each language such as dictionaries and parts of speech~~~From a practical point of view, however, a more robust and flexible approach is desirable.
This highlights the need for finding multi-word translation correspondences~~~Previous works that focus on multi-word translation correspondences from parallel corpora include noun phrase correspondences <TREF>Kupiec, 1993</TREF>, fixed/flexible collocations <REF>Smadja et al , 1996</REF>, n-gram word sequences of arbitrary length <REF>Kitamura and Matsumoto, 1996</REF>, non-compositional compounds <REF>Melamed, 2001</REF>, captoids <REF>Moore, 2001</REF>, and named entities 1~~~In all of these approaches, a common problem seems to be an identification of meaningful multi-word translation units~~~There are a number of factors which make handling of multi-word units more complicated than it appears.
Previous approaches effectively narrow down its search space by some heuristics~~~<TREF>Kupiec 1993</TREF> focuses on noun-phrase translations only, Smadja et al~~~1996 limits to find French translation of English collocation identified by his Xtract system, and <REF>Kitamura and Matsumoto 1996</REF> can exhaustively enumerate only rigid word sequences~~~Many of works mentioned in the last paragraph as well as ours extract non-probabilistic translation lexicons.
Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction <REF>Smadja, 1993</REF>; <REF>Fung and Wu, 1994</REF>, phrasal translation <REF>Smadja et al , 1996</REF>; <TREF>Kupiec, 1993</TREF>; <REF>Wu, 1995</REF>; <REF>Dagan and Church, 1994</REF>, target word selection <REF>Liu and Li, 1997</REF>; <REF>Tanaka and Iwasaki, 1996</REF>, domain word translation <REF>Fung and Lo, 1998</REF>; <REF>Fung, 1998</REF>, sense disambiguation <REF>Brown et al , 1991</REF>; <REF>Dagan et al , 1991</REF>; <REF>Dagan and Itai, 1994</REF>; <REF>Gale et al , 1992a</REF>; <REF>Gale et al , 1992b</REF>; <REF>Gale et al , 1992c</REF>; <REF>Shiitze, 1992</REF>; <REF>Gale et al , 1993</REF>; <REF>Yarowsky, 1995</REF>, and even recently for query translation in cross-language IR as well <REF>Ballesteros and Croft, 1998</REF>~~~Co-occurrence statistics is collected from either bilingual parallel and 334 non-parallel corpora <REF>Smadja et al , 1996</REF>; <TREF>Kupiec, 1993</TREF>; <REF>Wu, 1995</REF>; <REF>Tanaka and Iwasaki, 1996</REF>; <REF>Fung and Lo, 1998</REF>, or monolingual corpora <REF>Smadja, 1993</REF>; <REF>Fung and Wu, 1994</REF>; <REF>Liu and Li, 1997</REF>; <REF>Shiitze, 1992</REF>; <REF>Yarowsky, 1995</REF>~~~As we noted in <REF>Fung and Lo, 1998</REF>; <REF>Fung, 1998</REF>, parallel corpora are rare in most domains~~~We want to devise a method that uses only monolingual data in the primary language to train co-occurrence information.
pruning translation alternatives for query translation~~~Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction <REF>Smadja, 1993</REF>; <REF>Fung and Wu, 1994</REF>, phrasal translation <REF>Smadja et al , 1996</REF>; <TREF>Kupiec, 1993</TREF>; <REF>Wu, 1995</REF>; <REF>Dagan and Church, 1994</REF>, target word selection <REF>Liu and Li, 1997</REF>; <REF>Tanaka and Iwasaki, 1996</REF>, domain word translation <REF>Fung and Lo, 1998</REF>; <REF>Fung, 1998</REF>, sense disambiguation <REF>Brown et al , 1991</REF>; <REF>Dagan et al , 1991</REF>; <REF>Dagan and Itai, 1994</REF>; <REF>Gale et al , 1992a</REF>; <REF>Gale et al , 1992b</REF>; <REF>Gale et al , 1992c</REF>; <REF>Shiitze, 1992</REF>; <REF>Gale et al , 1993</REF>; <REF>Yarowsky, 1995</REF>, and even recently for query translation in cross-language IR as well <REF>Ballesteros and Croft, 1998</REF>~~~Co-occurrence statistics is collected from either bilingual parallel and 334 non-parallel corpora <REF>Smadja et al , 1996</REF>; <TREF>Kupiec, 1993</TREF>; <REF>Wu, 1995</REF>; <REF>Tanaka and Iwasaki, 1996</REF>; <REF>Fung and Lo, 1998</REF>, or monolingual corpora <REF>Smadja, 1993</REF>; <REF>Fung and Wu, 1994</REF>; <REF>Liu and Li, 1997</REF>; <REF>Shiitze, 1992</REF>; <REF>Yarowsky, 1995</REF>~~~As we noted in <REF>Fung and Lo, 1998</REF>; <REF>Fung, 1998</REF>, parallel corpora are rare in most domains.
It can now be assumed that a parallel bilingual corpus may be aligned to the sentence level with reasonable accuracy Kay and Ri3cheisen 1988; <REF>Catizone, Russel, and Warwick 1989</REF>; <REF>Gale and Church 1991</REF>; <REF>Brown, Lai, and Mercer 1991</REF>; <REF>Chen 1993</REF>, even for languages as disparate as Chinese and English <REF>Wu 1994</REF>~~~Algorithms for subsentential alignment have been developed as well as granularities of the character <REF>Church 1993</REF>, word <REF>Dagan, Church, and Gale 1993</REF>; <REF>Fung and Church 1994</REF>; <REF>Fung and McKeown 1994</REF>, collocation <REF>Smadja 1992</REF>, and specially segmented <TREF>Kupiec 1993</TREF> levels~~~However, the identification of subsentential, nested, phrasal translations within the parallel texts remains a nontrivial problem, due to the added complexity of dealing with constituent structure~~~Manual phrasal matching is feasible only for small corpora, either for toy-prototype testing or for narrowly restricted applications.
Table 5: tion out score 0008421 0007895 0007669 0007588 0007283 0006812 0006430 0006218 0005921 0005527 0005335 0005335 0005221 0004731 0004470 0004275 0003878 0003859 0003859 0003784 0003686 0003550 0003519 0003481 0003407 0003407 0003338 0003324 Some Chinese ut English Teng-hui SAR flu Lei poultry SAR hijack poultry Tung Diaoyu PrimeMinister President China Lien poultry China flu PrimeMinister President poultry Kalkanov poultry SAR Zhuhai PrimeMinister President flu apologise unknown word translaChinese  Weng-hui  u Lei j Poultry  Chee-hwa  Teng-hui  SAR  Chee-hwa : Teng-hui  Weng-hui W Weng-hui CLam  Teng-hui - Chee-hwa  Teng-hui Lei  Chee-hwa  Chee-hwa  Leung  Zhuhai I Lei J Yeltsin - Chee-hwa  Lam Lam j Poultry W Teng-hui 0003250 DPP 0003206 Tang 0003202 Tung 0003040 Leung 0003033 China 0002888 Zhuhai 0002886 Tung  Teng-hui Tang Leung Leung  SAR  Lunar Tung 1994 for sense disambiguation between multiple usages of the same word~~~Some of the early statistical terminology translation methods are <REF>Brown et al , 1993</REF>; <REF>Wu and Xia, 1994</REF>; <REF>Dagan and Church, 1994</REF>; <REF>Gale and Church, 1991</REF>; <TREF>Kupiec, 1993</TREF>; <REF>Smadja et al , 1996</REF>; Kay and R<REF>Sscheisen, 1993</REF>; <REF>Fung and Church, 1994</REF>; <REF>Fung, 1995b</REF>~~~These algorithms all require parallel, translated texts as input~~~Attempts at exploring nonparallel corpora for terminology translation are very few <REF>Rapp, 1995</REF>; <REF>Fung, 1995a</REF>; <REF>Fung and McKeown, 1997</REF>.
Other corpus-based methods determine associations between words <REF>Grefenstette 1992</REF>; <REF>Dunning 1993</REF>; <REF>Lin et al 1998</REF>, which yields a basis for computing thesauri, or dictionaries of terminological expressions and multiword lexemes <REF>Gaizauskas, Demetriou, and Humphreys 2000</REF>; <REF>Grefenstette 2001</REF>~~~From multilingual texts, translation lexica can be generated <REF>Gale and Church 1991</REF>; <TREF>Kupiec 1993</TREF>; <REF>Kumano and Hirakawa 1994</REF>; <REF>Boutsis, Piperidis, and Demiros 1999</REF>; <REF>Grefenstette 1999</REF>~~~The analysis of technical texts is used to automatically build dictionaries of acronyms for a given field <REF>Taghva and Gilbreth 1999</REF>; <REF>Yeates, Bainbridge, and Witten 2000</REF>, and related methods help to compute dictionaries that cover the special vocabulary of a given thematic area <REF>Strohmaier et al 2003a</REF>~~~In computer-assisted language learning CALL, mining techniques for corpora are used to create individualized and user-centric exercises for grammar and text understanding <REF>Schwartz, Aikawa, and Pahud 2004</REF>; <REF>Brown and Eskenazi 2004</REF>; <REF>Fletcher 2004a</REF>.
Experimental results show our system outperforms conventional methods for various kinds of Japanese-English texts~~~Corpus-based approaches based on bilingual texts are promising for various applicationsie , lexical knowledge extraction <TREF>Kupiec, 1993</TREF>; <REF>Matsumoto et al , 1993</REF>; <REF>Smadja et al , 1996</REF>; <REF>Dagan and Church, 1994</REF>; <REF>Kumano and Hirakawa, 1994</REF>; <REF>Haruno et al , 1996</REF>, machine translation Brown and others, 1993; <REF>Sato and Nagao, 1990</REF>; <REF>Kaji et al , 1992</REF> and information retrieval <REF>Sato, 1992</REF>~~~Most of these works assume voluminous aligned corpora~~~Many methods have been proposed to align bilingual corpora.
4 GENERATING TRANSLATION CANDIDATES 41 Extraction of Japanese Terms Errors in the extraction of terms and phrases from parallel texts eventually lead to a failure in acquiring the correct term/phrase correspondences~~~<REF>In Kupiec 1993</REF> and <REF>Yamamoto 1993</REF>, term and phrase extraction is applied to both of parallel texts~~~In contrast, we extract from units only Japanese terms, thereby reducing the errors caused by term/phrase recognizer~~~Japanese NPs can be recognized more accurately than English NPs because Japanese has considerably less multi-category words.
Statistics-based processing has proven to be very powerful for aligning sentences and words in parallel corpora <REF>Brown, 1991</REF>; <REF>Gale, 1993</REF>; <REF>Chen, 1993</REF>~~~Kupiec proposes an Mgorithm for finding loun phrases in bilingual corpora <TREF>Kupiec, 1993</TREF>~~~In this algo o rithm, noui-phrase candidates are extracted from tagged and aligned parallel texts using a noun phrase recognizer and tile correspondences of these nonn phrases are calculated based on the EM algorithm~~~Accuracy of around 90 has been attained for the Imndred highest ranking conespondenccs.
It is also interesting to note that the good associations we found are not necessary compositional in nature we must/il Iaut, people of canada/les canadiens, of eourse/6videmment, etc~~~33 Filtering One way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun-phrase contraints <REF>Ganssier, 1995</REF>; <TREF>Kupiec, 1993</TREF>; hua Chen and Chen, 94; <REF>Fung, 1995</REF>; <REF>Evans and Zhai, 1996</REF>~~~It is also possible to focus on non-compositional compounds, a key point in bilingual applications <REF>Su et al , 1994</REF>; <REF>Melamed, 1997</REF>; Lin, 99~~~Another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns <REF>Wu, 1995</REF>; Furuse and Iida, 96.
Unfortunately, these technologies are still not as widely deployed in practical applications as they might be~~~Part-ofspeech taggers are used in a few applications, such as speech synthesis <REF>Sproat et al , 1992</REF> and question answering <TREF>Kupiec, 1993b</TREF>~~~Word alignment is newer, found only in a few places <REF>Gale and Church, 1991a</REF>; <REF>Brown et al , 1993</REF>; <REF>Dagan et al , 1993</REF>~~~It is used at IBM for estimating parameters of their statistical machine translation prototype Brown et Authors current address: Dept of Mathematics and Computer Science, Bar Ilan University, Ramat Gan 52900, Israel.
Automatic bilingual lexicon construction based on bilingual corpora has become an important first step for many studies and applications of natural language processing NLP, such as machine translation MT, crosslanguage information retrieval CLIR, and bilingual text alignment~~~As noted in <REF>Tsuji 2002</REF>, many previous methods <REF>Dagan et al , 1993</REF>; <TREF>Kupiec, 1993</TREF>; <REF>Wu and Xia, 1994</REF>; <REF>Melamed, 1996</REF>; <REF>Smadja et al , 1996</REF> deal with this problem based on frequency of words appearing in the corpora, which can not be effectively applied to lowfrequency words, such as transliterated words~~~These transliterated words are often domain-specific and created frequently~~~Many of them are not found in existing bilingual dictionaries.
There has been some research into matching compositional phrases across bitexts~~~For example, <TREF>Kupiec 1993</TREF> presented a method for finding translations of whole noun phrases~~~<REF>Wu 1995</REF> showed how to use an existing translation lexicon to populate a database of phrasal correspondences for use in example-based MT These compositional translation patterns enable more sophisticated approaches to MT However, they are only useful if they can be discovered reliably and efficiently~~~Their time may come when we have a better understanding of how to model the human translation process.
<REF>Tiedemann, 1993</REF> ; <REF>Boutsis  Piperidis, 1996</REF> ; <REF>Piperidis et al , 1997</REF> combine statistical and linguistic information for the same task~~~Some methods make alignment suggestions at an intermediate level between sentence and word 271 and word <REF>Smadja, 1992</REF> ; <REF>Smadja et al , 1996</REF> ; <TREF>Kupiec, 1993</TREF> ; <REF>Kumano  Hirakawa, 1994</REF> ; <REF>Boutsis  Piperidis, 1998</REF>~~~A common problem is the delimitation and spotting of the units to be matched~~~This is not a real problem for methods aiming at alignments at a high level of granularity paragraphs, sentences where unit delimiters are clear.
We have been studying robust lexicon compilation methods which do not rely on sentence alignment~~~Existing lexicon compilation methods <TREF>Kupiec 1993</TREF>; <REF>Smadja  McKeown 1994</REF>; <REF>Kumano  Hirakawa 1994</REF>; <REF>Dagan et al 1993</REF>; <REF>Wu  Xia 1994</REF> all attempt to extract pairs of words or compounds that are translations of each other from previously sentencealigned, parallel texts~~~However, sentence alignment <REF>Brown et al 1991</REF>; Kay  R<REF>Sscheisen 1993</REF>; <REF>Gale  <REF>Church 1993</REF></REF>; <REF>Church 1993</REF>; <REF>Chen 1993</REF>; <REF>Wu 1994</REF> is not always practical when corpora have unclear sentence boundaries or with noisy text segments present in only one language~~~Our proposed algorithm for bilingual lexicon acquisition bootstraps off of corpus alignment procedures we developed earlier <REF>Fung  Church 1994</REF>; <REF>Fung  McKeown 1994</REF>.
Compilation of translation lexicons is a crucial process for machine translation MT <REF>Brown et al , 1990</REF> and cross-language information retrieval CLIR systems <REF>Nie et al , 1999</REF>~~~A lot of effort has been spent on constructing translation lexicons from domain-specific corpora in an automatic way <REF>Melamed, 2000</REF>; <REF>Smadja et al , 1996</REF>; <TREF>Kupiec, 1993</TREF>~~~However, such methods encounter two fundamental problems: translation of regional variations and the lack of up-to-date and high-lexical-coverage corpus source, which are worthy of further investigation~~~The first problem is resulted from the fact that the translations of a term may have variations in different dialectal regions.
Prec~~~100 97 200 94 Table 2: Multiword notion results As a comparison, <TREF>Kupiec, 1993</TREF> obtained a precision of 90 for the first hundred associations between English and French noun phrases, using the EM algorithm~~~Our experiments with a similar method showed a precision around 92 for the first hundred associations on a set of aligned sentences comprising the one used for the above experiment~~~An evaluation on single words, showed a precision of 9870 for the first hundred and 97 for the first two hundred.
3 Multilingual terminology extraction Several works describe methods to extract terms, or candidate terms, in English and/or French <REF>Justeson and Katz, 1995</REF>; <REF>Daille, 1994</REF>; Nkwenti-<REF>Azeh, 1992</REF>~~~Some more specific works describe methods to align noun phrases within parallel corpora <TREF>Kupiec, 1993</TREF>~~~The underlying assumption beyond these works is that the monolingually extracted units correspond to each other cross-lingually~~~Unfortunately, this is not always the case, and the above methodology suffers from the weaknesses pointed out by <REF>Wu, 1997</REF> concerning parse-parse-match procedures.
<TREF>Kupiec, 1993</TREF>; Kumano and lirakawa, 1194 extracted noun phrme NP correspondences from aligned parallel corpora~~~n <TREF>Kupiec, 1993</TREF>, Nls in English and French t;exts are first extracted by a NP recoguizer~~~Their correspotldence prol> abilities arc then gradually relined by using an EM-like iteration algorithm~~~t,umano and <REF>Ilirakawa, 1994</REF> lirst extractedJapanese N Ps in the SIII.
Although these methods ;re rob/Is HII aSSllllle rio illfOllllttiOll SOltrce their outputs are just word-word correslotMences~~~<TREF>Kupiec, 1993</TREF>; Kumano and lirakawa, 1194 extracted noun phrme NP correspondences from aligned parallel corpora~~~n <TREF>Kupiec, 1993</TREF>, Nls in English and French t;exts are first extracted by a NP recoguizer~~~Their correspotldence prol> abilities arc then gradually relined by using an EM-like iteration algorithm.
After that, the two data are merged to make input for the cluster generation module~~~a Statistical weighting Many methods of extracting lexical translation pairs have been proposed <REF>Daille, Gaussier  Langd 1994</REF>; Eijk t993; <REF>Fung 1995</REF>; Gale  <REF>Church 1991</REF>; <REF>Hiemstra 1996</REF>; ltull 1998; <TREF>Kupiec 1993</TREF>; <REF>Melamed 1996</REF>; <REF>Smadja, McKeown  Hatzivmssiloglou 1996</REF>~~~Though it, is ditficult to evaluate the performance of existing methods as they use ditferent corpora for evaluation 5, the performance does not seem to be radically different~~~We adopted log-likelihood ratio <REF>Danning 1993</REF>, which gave the best pertbrmance among crude non-iterative methods in our test experiments 6 .
We present an algorithm in finding word correlation statistics for automatic bilingual lexicon compilation from a non-parallel corpus in Chinese and English~~~Most previous automatic lexicon compilation techniques require a sentence-aligned clean parallel bilingual corpus <TREF>Kupiec 1993</TREF>; <REF>Smadja  McKeown 1994</REF>; <REF>Kumano  Hirakawa 1994</REF>; <REF>Dagan et al 1993</REF>; <REF>Wu  Xia 1994</REF>~~~We have previously shown an algorithm which extracts a bilingual lexicon from noisy parallel corpus without sentence alignment <REF>Fung  McKeown 1994</REF>; <REF>Fung 1995</REF>~~~Although bilingual parallel corpora have been available in recent years, they are still relatively few in comparison to the large amount of monolingual text.
173 As demonstrated in all the bilingual lexicon compilation algorithms, the foremost task is to identify word features which are similar between a word and its translation, yet different between a word and other words which are not its translations~~~In parallel corpora, this feature could be the positional co-occurrence of a word and its translation in the other language in the same sentences <TREF>Kupiec 1993</TREF>; <REF>Smadja  McKeown 1994</REF>; <REF>Kumano  Hirakawa 1994</REF>; <REF>Dagan et al 1993</REF>; <REF>Wu  Xia 1994</REF> or in the same segments <REF>Fung  Church 1994</REF>; <REF>Fung 1995</REF>~~~In a non-parallel corpus, there is no corresponding sentence or segment pairs, so the co-occurrence feature is not applicable~~~<REF>In Fung  McKeown 1994</REF>; <REF>Fung 1995</REF>, the word feature used was the positional difference vector.
Table 5: Some Chinese unknown word translation output score 0008421 0007895 0007669 0007588 0007283 0006812 0006430 0006218 0005921 0005527 0005335 0005335 0005221 0004731 0004470 0004275 0003878 0003859 0003859 0003784 0003686 0003550 0003519 0003481 0003407 0003407 0OO3338 0003324 0003250 0003206 0003202 0003040 0003033 0002888 0002886 English Chinese Teng-hui  Teng-hui SAR , SAR flu N m, Lei  Lei poultry j Poultry SAR  Chee-hwa hijack  Teng-hui poultry  SAR ,ng  Chee-hw Diaoyu  Teng-hui PrimeMinister  Teng-hui President  Teng-hui China  Lava Lien  Teng-hui poultry  Chee-hwa China  Teng-hui flu  Lei PrivaeMinister -I Chee-hwa President 1; Chee-hwa poultry  Leung Kalkanov i Zhuhai poultry I Lei SAR 1 J l Yeltsin Zhuhai -1 l Chee-hwa PrimeMinister  Lain President  Lava flu  Poultry apologise  Teng-hui Dee  Teng-hui Tang J Tang iSlng  Leung Leung : Leung China tN SAR Zhuhai  Lunar Ttulg  Tung 1994 for sense disambiguation between multiple usages of the same word~~~Some of the early statistical terminology translation methods are <REF>Brown et al, 1993</REF>; <REF>Wu and Xia, 1994</REF>; <REF>Dagan and Church, 1994</REF>; <REF>Gale and Church, 1991</REF>; <TREF>Kupiec, 1993</TREF>; <REF>Smadja et al, 1996</REF>; Kay and R<REF>Sscheisen, 1993</REF>; <REF>Fung and Church, 1994</REF>; <REF>Fhmg, 1995b</REF>~~~These algorithms all require parallel, translated texts as input~~~Attempts at exploring nonparallel corpora for terminology translation are very few <REF>Rapp, 1995</REF>; <REF>Fung, 1995</REF>3; <REF>Fung and McKeown, 1997</REF>.
The technique for acquiring various kinds of rules such as translation rules, grammar rules, dictionary entries and so on from bilingual corpora needs to include several kinds of sub-techniques; identification of aligned sentence pairs which consist of pairs of one language sentence and translation equivalents of the sentence sentence alignment; identification of equivalent words/phrases pairs from aligned sentence pairs word alignment; and extraction of rules such as translation rules, grammar rules, dictionary entries and so on from identified aligned sentence pairs and equivalent word/phrase pairs~~~Several methods have been proposed with regard to aligning sentences <REF>Brown et al , 1991</REF>; <REF>Gale and Church, 1991</REF>; <REF>Haruno and Yamazaki, 1996</REF>; Kay and losche/sen, 1993, alJsnlng words <REF>Church, 1993</REF>; <TREF>Kupiec, 1993</TREF>; <REF>Matsumoto et al , 1993</REF>; <REF>Wu, 1995</REF>; <REF>Yamada et al , 1996</REF> and acquiring rules from bilingual corpora <REF>Dagan et al , 1991</REF>; <REF>Dagan and Church, 1994</REF>; <REF>Fung and Church, 1994</REF>; Tana, 1994; <REF>Yamada et al , 1995</REF>~~~From the point of view of the extraction of resolution rules of zero pronouns, a technique to identify zero pronouns in a sentence in one language and their antecedents in a translation from aligned sentence pairs is needed~~~But there is currently no method to identify zero pronouns and their antecedents automatically within bilingual corpora.
2003 propose a method to extract bilingual collocations using recursive chain-link-type learning~~~In addition to collocation translation, there is also some related work in acquiring phrase or term translations from parallel corpus <TREF>Kupiec, 1993</TREF>; <REF>Yamamoto and Matsumoto 2000</REF>~~~Since large aligned bilingual corpora are hard to obtain, some research has been conducted to exploit translation knowledge from non-parallel corpora~~~Their work is mainly on word level.
<REF>Smadja et al , 1996</REF>; <REF>Gao et al , 2002</REF>; <REF>Wu and Zhou, 2003</REF>~~~Some studies have been done for acquiring collocation translations using parallel corpora <REF>Smadja et al, 1996</REF>; <TREF>Kupiec, 1993</TREF>; Echizen-ya et al , 2003~~~These works implicitly assume that a bilingual corpus on a large scale can be obtained easily~~~However, despite efforts in compiling parallel corpora, sufficient amounts of such corpora are still unavailable.
1~~~Motivation There have been quite a number of recent papers on parallel text: Brown et al 1990, 1991, 1993, <REF>Chen 1993</REF>, <REF>Church 1993</REF>, <REF>Church et al 1993</REF>, <REF>Dagan et al 1993</REF>, Gale and Church 1991, 1993, <REF>Isabelle 1992</REF>, <REF>Kay and Rgsenschein 1993</REF>, <REF>Klavans and Tzoukermann 1990</REF>, <TREF>Kupiec 1993</TREF>, <REF>Matsumoto 1991</REF>, <REF>Ogden and Gonzales 1993</REF>, <REF>Shemtov 1993</REF>, <REF>Simard et al 1992</REF>, <REF>WarwickArmstrong and Russell 1990</REF>, Wu to appear~~~Most of this work has been focused on European language pairs, especially English-French~~~It remains an open question how well these methods might generalize to other language pairs, especially pairs such as English-Japanese and EnglishChinese.
More importantly, because wordalign and charalign were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at ATT Language Line Services, a commercial translation service, to help them with difficult terminology~~~Aligning parallel texts has recently received considerable attention <REF>Warwick et al , 1990</REF>; <REF>Brown et al , 1991a</REF>; <REF>Gale and Church, 1991b</REF>; <REF>Gale and Church, 1991a</REF>; <REF>Kay and Rosenschein, 1993</REF>; <REF>Simard et al , 1992</REF>; <REF>Church, 1993</REF>; <TREF>Kupiec, 1993</TREF>; <REF>Matsumoto et al , 1993</REF>~~~These methods have been used in machine translation <REF>Brown et al , 1990</REF>; <REF>Sadler, 1989</REF>, terminology research and translation aids <REF>Isabelle, 1992</REF>; <REF>Ogden and Gonzales, 1993</REF>, bilingual lexicography <REF>Klavans and Tzoukermann, 1990</REF>, collocation studies <REF>Smadja, 1992</REF>, word-sense disambiguation <REF>Brown et al , 1991b</REF>; <REF>Gale et al , 1992</REF> and information retrieval in a multilingual environment <REF>Landauer and Littman, 1990</REF>~~~The information retrieval application may be of particular relevance to this audience.
The training of the Nave Bayes classifier consists of estimating the prior probabilities for different categories as well as the probabilities of each category for each feature~~~0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 19 6 5 19 6 8 1 9 71 19 7 4 19 7 7 1 9 80 19 8 3 1 98 6 1 9 89 19 9 2 1 99 5 1 9 98 2 00 1 Year N u m b e r of C i t a t i ons Yeast Fly Worm Mouse 0 002 004 006 008 01 012 19 65 19 6 8 19 71 19 74 19 77 19 8 0 19 83 19 86 19 8 9 19 92 19 95 19 98 20 01 Year P r op or t i on Yeast Fly Worm Mouse The Decision List method DLL <TREF>Yarowsky, 1994</TREF> is equivalent to simple case statements in most programming languages~~~In a DLL classifier, a sequence of tests is applied to each feature vector~~~If a test succeeds, then the sense associated with that test is returned.
The minimum score derived from any of the criteria applied is deemed initially to be the score of the constituent~~~That is, an assumption of full statistical dependence <TREF>Yarowsky, 1994</TREF>, rather than the more common full independence, is made3 When llf events El, E2,, E, are fully independent, then the joint probability PE1 A A En is the product of PEIPEn, but if they are maximally dependent, it is the minimum of these values~~~Of course, neither assumption is any more than an approximation to the truth; but assuming dependence has the advantage that the estimate of the joint probability depends much less strongly on n, and so estimates for alternative joint events can be directly compared, without any possibly tricky normalization, even if they are composed of different numbers of atomic events~~~This property is desirable: different sub-paths through a chart may span different numbers of edges, and one can imagine evaluation criteria which are only defined for some kinds of edge, or which often duplicate information supplied by other criteria.
In this paper, we propose a method of detecting Japanese homophone errors in Japanese texts~~~Our method is based on a decision list proposed by Yarowsky <TREF>Yarowsky, 1994</TREF>; <REF>Yarowsky, 1995</REF>~~~We improve the original decision list by using written words in the default evidence~~~The improved decision list can raise the F-measure of error detection.
We also use the special evidence default, frqwl, default is defined as the frequency of wl~~~step5 Pick the highest strength estwh,ej among 5As in this paper, the addition of a small value is an easy and effective way to avoid the unsatisfactory case, as shown in <TREF>Yarowsky, 1994</TREF>~~~estwl, , eaw, e,   , e e, and set the word wk as the answer for the evidence ej~~~In this case, the identifying strength is estwk, ej.
Instead of viewing all positional features as containers of thousands of atomic word features, it treats the positional features as the basic tests, branching on the word values in the tree~~~More generally, as a precursor to the abovementioned work, confusable disambiguation has been investigated in a string of papers discussing the application of various machine learning algorithms to the task <TREF>Yarowsky, 1994</TREF>; <REF>Golding, 1995</REF>; Mangu 31 and <REF>Brill, 1997</REF>; <REF>Huang and Powers, 2001</REF>~~~6 Discussion In this article we explored the scaling abilities of IGTREE, a simple decision-tree algorithm with favorable asymptotic complexities with respect to multi-label classification tasks~~~IGTREE is applied to word prediction, a task for which virtually unlimited amounts of training examples are available, with very large amounts of predictable class labels; and confusable disambiguation, a specialization of word prediction focusing on small sets of confusable words.
Following commonpracticeinfeatureextractioneg~~~<TREF>Yarowsky, 1994</TREF>, and using the mxpost1 part of speech tagger and WordNets lemmatization, the following feature set was used: bag of word lemmas for the context words in the preceding, current and following sentence; unigrams of lemmas and parts of speech in a window of /three words, where each position provides a distinct feature; and bigrams of lemmas in the same window~~~The SVMLight <REF>Joachims, 1999</REF> classifier was used in the supervised settings with its default parameters~~~To obtain a multi-class classifier we used a standard one-vs-all approach of training a binary SVM for each possible sense and then selecting the highest scoring sense for a test example.
1991 present the initial success of applying word trigram conditional probabilities to the problem of context based detection and correction of real-word errors~~~<TREF>Yarowsky 1994</TREF> experiments with the use of decision lists for lexical ambiguity resolution, using context features like local syntactic patterns and collocational information, so that multiple types of evidence are considered in the context of an ambiguous word~~~In addition to word-forms, the patterns involve POS tags and lemmas~~~The algorithm is evaluated in missing accent restoration task for Spanish and French text, against a predefined set of a few words giving an accuracy over 99.
Therefore, a machine learning approach is proposed, which investigates how these features contribute to the task and how they should be combined~~~4 Combining Linguistic Features with Machine Learning Approach Previous efforts in corpus-based NLP have incorporated machine learning methods to coordinate multiple linguistic features, for example, in accent restoration <TREF>Yarowsky, 1994</TREF> and event classification <REF>Siegel, 1998</REF>~~~Temporal relation determination can be modeled as a relation classification task~~~We formulate the thirteen temporal relations see Figure 1 as the classes to be decided by a classifier.
It has been amply demonstrated that a wide assortment of machine learning algorithms are quite effective at extracting linguistic information from manually annotated corpora~~~Among the machine learning algorithms studied, rule based systems have proven effective on many natural language processing tasks, including part-of-speech tagging <REF>Brill, 1995</REF>; <REF>Ramshaw and Marcus, 1994</REF>, spelling correction <REF>Mangu and Brill, 1997</REF>, word-sense disambiguation <REF>Gale et al , 1992</REF>, message understanding <REF>Day et al , 1997</REF>, discourse tagging <REF>Samuel et al , 1998</REF>, accent restoration <TREF>Yarowsky, 1994</TREF>, prepositional-phrase attachment <REF>Brill and Resnik, 1994</REF> and base noun phrase identification Ramshaw and Marcus, In Press; <REF>Cardie and Pierce, 1998</REF>; <REF>Veenstra, 1998</REF>; <REF>Argamon et al , 1998</REF>~~~Many of these rule based systems learn a short list of simple rules typically on the order of 50-300 which are easily understood by humans~~~Since these rule-based systems achieve good performance while learning a small list of simple rules, it raises the question of whether peoand Woman.
Performance ranges from 77-84 correct on the test words, where a lower bound for performance based on always selecting the most frequent sense for the same words ie , the sense with the greatest prior probability would yield 53-80 correct~~~<TREF>Yarowsky 1994</TREF>, building on his earlier work, designed a classifier that looks at words within :kk positions from the target; lemma forms are obtained through morphological analysis; and a coarse part-of-speech assignment is performed by dictionary lookup~~~Context is represented by collocations based on words or parts of speech at specific positions within the window or, less specifically, in any position~~~Also coded are some special classes of words, such as WEEKDAY, that might serve to distinguish among word senses.
Examples containing the string escorted were collected to train on one sense of the pseudoword and examples containing the string abused were collected to train on the other sense~~~In addition, <REF>Yarowsky 1993</REF> used homophones eg , cellarseller and <TREF>Yarowsky 1994</TREF> created homographs by stripping accents from French and Spanish words~~~Although these latter techniques are useful in their own 156 Leacock, Chodorow, and Miller Corpus Statistics and WordNet Relations right eg , spoken language systems or corrupted transmissions, the resulting materials do not generalize to the acquisition of tagged training for real polysemous or even homographic words~~~The results of disambiguation strategies reported for pseudowords and the like are consistently above 95 overall accuracy, far higher than those reported for disambiguating three or more senses of polysemous words <REF>Wilks et al 1993</REF>; <REF>Leacock, Towell, and Voorhees 1993</REF>.
These notions have been used implicitly by researchers and historians when validating the authenticity of documents, but have not been utilised much in automated systems~~~Similar applications have so far been largely confined to authorship identification, such as <REF>Mosteller and Wallace, 1964</REF>; <REF>Fung, 2003</REF> and the identification of association rules <TREF>Yarowsky, 1994</TREF>; <REF>Silverstein et al , 1997</REF>~~~Temporal information is presently underutilised for automated document classification purposes, especially when it comes to guessing at the document creation date automatically~~~This work presents a method of using periodical temporal-frequency information present in documents to create temporal-association rules that can be used for automatic document dating.
WSD is useful in many natural language tasks, such as choosing the correct word in machine translation and coreference resolution~~~In several recent proposals <REF>Hearst, 1991</REF>; <REF>Bruce and Wiebe, 1994</REF>; <REF>Leacock, Towwell, and Voorhees, 1996</REF>; <REF>Ng and Lee, 1996</REF>; <REF>Yarowsky, 1992</REF>; <TREF>Yarowsky, 1994</TREF>, statistical and machine learning techniques were used to extract classifiers from hand-tagged corpus~~~Yarowsky <REF>Yarowsky, 1995</REF> proposed an unsupervised method that used heuristics to obtain seed classifications and expanded the results to the other parts of the corpus, thus avoided the need to hand-annotate any examples~~~Most previous corpus-based WSD algorithms determine the meanings of polysemous words by exploiting their local contexts.
Unsupervised learning holds great promise for breakthroughs in natural language processing~~~In cases like <REF>Yarowsky, 1995</REF>, unsupervised methods offer accuracy results than rival supervised methods <TREF>Yarowsky, 1994</TREF> while requiring only a fraction of the data preparation effort~~~Such methods have also been a key driver of progress in statistical machine translation, which depends heavily on unsupervised word alignments <REF>Brown et al , 1993</REF>~~~There are also interesting problems for which supervised learning is not an option.
In this setting, a Decision List is a list of features extracted from the training examples and sorted by a log-likelihood measure~~~This measure estimates how strong a particular feature is as an indicator of a specific sense <TREF>Yarowsky, 1994</TREF>~~~When testing, the decision list is checked in order and the feature with the highest weight that matches the test example is used to select the winning word sense~~~Thus, only the single most reliable piece of evidence is used to perform disambiguation.
Generally, supervised approaches those that learn from previously semantically annotated corpora have obtained better results than unsupervised methods on small sets of selected ambiguous words, or artificial pseudowords~~~Many standard M L algorithms for supervised learning have been applied, such as: Decision Lists <TREF>Yarowsky, 1994</TREF>; <REF>Agirre and Martinez, 2000</REF>, Neural Networks <REF>Towell and Voorhees, 1998</REF>, Bayesian learning <REF>Bruce and Wiebe, 1999</REF>, Exemplar-based learning <REF>Ng, 1997</REF>, Boosting <REF>Escudero et al , 2000a</REF>, etc Further, in <REF>Mooney, 1996</REF> some of the previous methods are compared jointly with Decision Trees and Rule Induction algorithms, on a very restricted domain~~~Although some published works include the comparison between some alternative algorithms <REF>Mooney, 1996</REF>; <REF>Ng, 1997</REF>; <REF>Escudero et al , 2000a</REF>; <REF>Escudero et al , 2000b</REF>, none of them addresses the issue of the portability of supervised ML algorithms for WSD, ie, testing whether the accuracy of a system trained on a certain corpus can be extrapolated to other corpora or not~~~We think that the study of the domain dependence of WSD --in the style of other studies devoted to parsing <REF>Sekine, 1997</REF>; <REF>Ratnaparkhi, 1999</REF>-is needed to assure the validity of the supervised approach, and to determine to which extent a tuning pre-process is necessary to make real WSD systems portable.
Their investigation was based on a 6word test set with 2 senses for each word~~~Yarowsky 1994 and 1995, <REF>Mihalcea and Moldovan 2000</REF>, and <REF>Mihalcea 2002</REF> have made further research to obtain large corpus of higher quality from an initial seed corpus~~~A semi-supervised method proposed by Niu et al~~~2005 clustered untagged instances with tagged ones starting from a small seed corpus, which assumes that similar instances should have similar tags.
However, machine learning techniques have not previously been applied to aspectual disambiguation~~~Previous efforts have applied machine induction methods to coordinate corpusbased linguistic indicators in particular, for example, to classify adjectives according to markedness <REF>Hatzivassiloglou and McKeown 1995</REF>, to perform accent restoration <TREF>Yarowsky 1994</TREF>, for sense disambiguation problems <REF>Luk 1995</REF>, and for the automatic identification of semantically related groups of words <REF>Pereira, Tishby, and Lee 1993</REF>; <REF>Hatzivassiloglou and McKeown 1993</REF>; Schiitze 1992~~~8~~~Future Work Parallel bilingual corpora are potential sources of supervised examples for training and testing aspectual classification systems.
In contrast, we used exemplar-based learning, where the contributions of all features are summed up and taken into account in coming up with a classification~~~We also include verb-object syntactic relation as a feature, which is not used in <TREF>Yarowsky, 1994</TREF>~~~Although the work of Yarowsky, i994 can be applied to WSD, the results reported in <TREF>Yarowsky, 1994</TREF> only dealt with accent restoration, which is a much simpler problem~~~It is unclear how Yarowskys method will fare on WSD of a common test data set like the one we used, nor has his method been tested on a large data set with highly ambiguous words tagged with the refined senses of WORDNET.
In contrast, LEXAS uses supervised learning from tagged sentences, which is also the approach taken by most recent work on WSD, including <REF>Bruce and Wiebe, 1994</REF>; <REF>Miller et al , 1994</REF>; <REF>Leacock et al , 1993</REF>; <TREF>Yarowsky, 1994</TREF>; <REF>Yarowsky, 1993</REF>; <REF>Yarowsky, 1992</REF>~~~The work of <REF>Miller et al , 1994</REF>; <REF>Leacock et al , 1993</REF>; <REF>Yarowsky, 1992</REF> used only the unordered set of surrounding words to perform WSD, and they used statistical classifiers, neural networks, or IR-based techniques~~~The work of <REF>Bruce and Wiebe, 1994</REF> used parts of speech POS and morphological form, in addition to surrounding words~~~However, the POS used are abbreviated POS, and only in a window of -b2 words.
One line of research focuses on the use of the knowledge contained in a machine-readable dictionary to perform WSD, such as <REF>Wilks et al , 1990</REF>; <REF>Luk, 1995</REF>~~~In contrast, LEXAS uses supervised learning from tagged sentences, which is also the approach taken by most recent work on WSD, including <REF>Bruce and Wiebe, 1994</REF>; <REF>Miller et al , 1994</REF>; <REF>Leacock et al , 1993</REF>; <TREF>Yarowsky, 1994</TREF>; <REF>Yarowsky, 1993</REF>; <REF>Yarowsky, 1992</REF>~~~The work of <REF>Miller et al , 1994</REF>; <REF>Leacock et al , 1993</REF>; <REF>Yarowsky, 1992</REF> used only the unordered set of surrounding words to perform WSD, and they used statistical classifiers, neural networks, or IR-based techniques~~~The work of <REF>Bruce and Wiebe, 1994</REF> used parts of speech POS and morphological form, in addition to surrounding words.
We also include verb-object syntactic relation as a feature, which is not used in <TREF>Yarowsky, 1994</TREF>~~~Although the work of Yarowsky, i994 can be applied to WSD, the results reported in <TREF>Yarowsky, 1994</TREF> only dealt with accent restoration, which is a much simpler problem~~~It is unclear how Yarowskys method will fare on WSD of a common test data set like the one we used, nor has his method been tested on a large data set with highly ambiguous words tagged with the refined senses of WORDNET~~~The work of <REF>Miller et al , 1994</REF> is the only prior work we know of which attempted to evaluate WSD on a large data set and using the refined sense distinction of WORDNET.
That local collocation knowledge provides important clues to WSD is pointed out in <REF>Yarowsky, 1993</REF>, although it was demonstrated only on performing binary or very coarse sense disambiguation~~~The work of <TREF>Yarowsky, 1994</TREF> is perhaps the most similar to our present work~~~However, his work used decision list to perform classification, in which only the single best disambiguating evidence that matched a target context is used~~~In contrast, we used exemplar-based learning, where the contributions of all features are summed up and taken into account in coming up with a classification.
Suppose the initial classifier correctly labels 100 out of 1000 instances, and makes no mistakes~~~Then the initial precision is 1<REF>Yarowsky, 1995</REF>, citing <TREF>Yarowsky, 1994</TREF>, actually uses a superficially different score that is, however, a monotone transform of precision, hence equivalent to precision, since it is used only for sorting~~~1 and recall is 01~~~Suppose further that we add an atomic rule that correctly labels 19 new instances, and incorrectly labels one new instance.
Methods can typically be delineated along two dimensions, corpns-based vs dictionary-based approaches~~~Corpus-based word sense disambignation algorjthm such as <REF>Ng and Lee, 1996</REF>; <REF>Bruce and Wiebe, 1994</REF>; <TREF>Yarowsky, 1994</TREF> relied on supervised learning fzom annotated corpora~~~The main drawback of these approaches is their requirement of a sizable sense-tagged corpus~~~Attempts to alleviate this tagbottleneck ilude tmotstrias Te ot ill,, 1996; <REF>Hearst, 1991</REF> and unsupervised algorith Yarowsky, 199s Dictionary-based approaches rely on linguistic knowledge sources such as mali,e-readable dictionaries <REF>Luk, 1995</REF>; <REF>Veronis and Ide, 1990</REF> and WordNet <REF>Agirre and Rigau, 1996</REF>; <REF>Resnik, 1995</REF> and e0ploit these for word sense disaznbiguation.
In this case, a more effective approach is to learn features that characterize the different contexts in which each word tends to occur~~~A number of feature-based methods have been proposed, including Bayesian classifiers <REF>Gale, Church, and Yarowsky, 1993</REF>, decision lists <TREF>Yarowsky, 1994</TREF>, Bayesian hybrids <REF>Golding, 1995</REF>, and, more recently, a method based on the Winnow multiplicative weight-updating algorithm <REF>Golding and Roth, 1996</REF>~~~We adopt the Bayesian hybrid method, which we will call Bayes, having experimented with each of the methods and found Bayes to be among the best-performing for the task at hand~~~This method has been described elsewhere <REF>Golding, 1995</REF> and so will only be briefly reviewed here; however, the version used here uses an improved smoothing technique, which is mentioned briefly below.
Moreover, word trigrams are ineffective at capturing longdistance properties such as discourse topic and tense~~~Feature-based approaches, such as Bayesian classifters <REF>Gale, Church, and Yarowsky, 1993</REF>, decision lists <TREF>Yarowsky, 1994</TREF>, and Bayesian hybrids <REF>Golding, 1995</REF>, have had varying degrees of success for the problem of context-sensitive spelling correction~~~However, we report experiments that show that these methods are of limited effectiveness for cases such as their, there, theyre and than, then, where the predominant distinction to be made among the words is syntactic~~~71 Confusion set Train Test Most freq.
The essence behind many algorithms for word sense disambiguation is to implicitly or explicitly classify all possible context words into groups relating to one or another sense~~~This can be done in a supervised <TREF>Yarowsky, 1994</TREF>, a semi-supervised <REF>Yarowsky, 1995</REF> or a fully unsupervised way <REF>Pantel  Lin, 2002</REF>~~~However, the classification can only work if the statistical clues are clear enough and if there are not too many exceptions~~~In terms of word co-occurrence statistics, we can say that within the local contexts of an ambiguous word, context words typical of the same sense should have high co-occurrence counts, whereas context words associated with different senses should have cooccurrence counts that are considerably lower.
1995, we formalize the problem of deciding dependency preference of subordinate clauses by utilizing the correlation of scope embedding preference and dependency preference of Japanese subordinate clauses~~~Then, as a statistical learning method, we employ the decision list learning method of <TREF>Yarowsky 1994</TREF>, where optimal combination of those features are selected and sorted in the form of decision rules, according to the strength of correlation between those features and the dependency preference of the two subordinate clauses~~~We evaluate the proposed method through the experiment on learning dependency preference of Japanese subordinate clauses from the EDR bracketed corpus section 4~~~We show that the proposed method outperforms other related methods/models.
The head vp chunk of Clause1 does not modify that of Clause2, but modifies that of another subordinate clause or the matrix clause which follows Clause2~~~Roughly speaking, the first corresponds to the case where Clause2 inherently has a scope of the same or a broader breadth compared with that of Clause1, while the second corresponds to the case where Clause2 inherently has a narrower scope compared with that of Clause17 32 Decision List Learning A decision list <TREF>Yarowsky, 1994</TREF> is a sorted list of the decision rules each of which decides the value of a decision D given some evidence E Each decision rule in a decision list is sorted TOur modeling is slightly different from those of other standard approaches to statistical dependency analysis <REF>Collins, 1996</REF>; <REF>Fujio and Matsumoto, 1998</REF>; <REF>Haruno et al , 1998</REF> which simply distinguish the two cases: the case where dependency relation holds between the given two vp chunks or clauses, and the case where dependency relation does not hold~~~In contrast to those standard approaches, we ignore the case where the head vp chunk of Clause1 modifies that of another subordinate clause which precedes Clause2~~~This is because we assume that this case is more loosely related to the scope embedding preference of subordinate clauses.
We formalize the problem of deciding scope embedding preference as a classification problem, in which various types of linguistic information of each subordinate clause are encoded as features and used for deciding which one of given two subordinate clauses has a broader scope than the other~~~As a statistical learning method, we employ the decision list learning method of <TREF>Yarowsky 1994</TREF>~~~113 Table 2: Features of Japanese Subordinate Clauses Feature Type  of Feat  Each Binary Feature Punctuation 2 with-comma, without-comma Grammatical adverb, adverbial-noun, formal-noun, temporal-noun, some features have distinction 17 quoting-particle, copula, predicate-conjunctive-particle, of chunk-final/middle topic-marking-particle, sentence-final-particle 12 Conjugation form of chunk-final conjugative word Lexical lexicalized forms of Grammatical features, with more than 9 occurrences in EDR corpus 235 stem, base, mizen, renyou, rental, conditional, imperative, ta, tari, re, conjecture, volitional adverb eg , ippou-de, irai, adverbial-noun eg , tame, baai topic-marking-particle eg , ha, mo, quoting-particle to, predicate-conjunctive-particle eg , ga, kara, temporal-noun eg , ima, shunkan, formal-noun eg , koto, copula dearu, sentence-final-particle eg , ka, yo 31 The Task Definition Considering the dependency preference of Japanese subordinate clauses described in section 24, the following gives the definition of our task of deciding the dependency of Japanese subordinate clauses~~~Suppose that a sentence has two subordinate clauses Clause1 and Clause2, where the head vp chunk of Clausel precedes that of Clause2.
For each piece of evidence, calculate the likelihood ratio of the conditional probability of a decision D  xl given the presence of that piece of evidence to the conditional probability of the rest of the decisions D -,xl: PDxl I EI lg2 PDxl EI Then, a decision list is constructed with pieces of evidence sorted in descending order with respect to their likelihood ratios, s 2~~~The final line of a decision list is defined as a default, where the likelihood ratio is calculated as the ratio of the largest marginal probability of the decision D  xl to the marginal proba<REF>Syarowsky 1994</REF> discusses several techniques for avoiding the problems which arise when an observed count is 0~~~Among those techniques, we employ the simplest one, ie, adding a small constant c 01 < c < 025 to the numerator and denominator~~~With this modification, more frequent evidence is preferred when there exist several evidences for each of which the conditional probability PDx  EI equals to 1.
Example confusion sets include: principle, principal, then, than, and weather, whether~~~Until now, many methods have been proposed for this problem including winnow-based algorithms <REF>Golding and Roth, 1999</REF>, differential grammars <REF>Powers, 1998</REF>, transformation based learning <REF>Mangu and Brill, 1997</REF>, decision lists <TREF>Yarowsky, 1994</TREF>~~~Confusion set disambiguation has very similar characteristics to a word sense disambiguation problem in which the system has to identify the meaning of a polysemous word given the surrounding context~~~The merit of using confusion set disambiguation as a test-bed for a learning algorithm is that since one does not need to annotate the examples to make labeled data, one can conduct experiments using an arbitrary amount of labeled data.
<REF>Yarowsky, 1995</REF> also uses wide context, but incorporates the one-senseper-discourse and one-sense-per-collocation constraints, using an unsupervised learning technique~~~The supervised technique in <TREF>Yarowsky, 1994</TREF> has a more specific notion of context, employing not just words that can appear within a window of Ik, but crucially words that abut and fall in the 2 window of the target word~~~More recently, <REF>Lin, 1997</REF> has shown how syntactic context, and dependency structures in particular, can be successfully employed for word sense disambiguation~~~<REF>Stetina and Nagao, 1997</REF> have shown that by employing a fairly simple and somewhat ad-hoc unsupervised method of WSD using a WordNet-based similarity heuristic, they could enhance PP-attachment performance to a significantly higher level than systems that made no use of lexical semantics 881 accuracy.
a7 Decision List DL are lists of weighted classification rules involving the evaluation of one single feature~~~At classification time, the algorithm applies the rule with the highest weight that matches the test example <TREF>Yarowsky, 1994</TREF>~~~The provider is IXA and they also applied smoothing to generate more robust decision lists~~~a7 In the Vector Space Model method cosVSM, each example is treated as a binary-valued feature vector.
3 Discriminant-Based Training Many of the properties extracted from QLFs can be presented to non-expert users in a form they can easily understand~~~Those properties that hold for some analyses of a particular utterance but not for others I will refer to as discriminants Dagan and ltai, 1994; <TREF>Yarowsky, 1994</TREF>~~~Discriminants that fairly consistently hold for correct but not some incorrect analyses, or vice versa, are likely to be useful in distinguishing correct from incorrect analyses at run time~~~Thus for training on an utterance to be effective, we need to provide enough user-friendly discriminants to allow the user to select the correct analyses, and as many as possible system-friendly discriminants that, over the corpus as a whole, distinguish reliably between correct and incorrect analyses.
In this work, we use the words directly~~~<TREF>Yarowsky 1994</TREF> notes that conceptual spelling correction is part of a closely related class of problems which include word sense disambiguation, word choice selection in machine translation, and accent and capitalization restoration~~~This class of problems has been attacked by many others~~~A number of feature-based methods have been tried, including Bayesian classifiers <REF>Gale, Church, and Yarowsky, 1992</REF>; <REF>Golding, 1995</REF>, decision lists <TREF>Yarowsky, 1994</TREF>, and knowledge-based approaches <REF>McRoy, 1992</REF>.
This class of problems has been attacked by many others~~~A number of feature-based methods have been tried, including Bayesian classifiers <REF>Gale, Church, and Yarowsky, 1992</REF>; <REF>Golding, 1995</REF>, decision lists <TREF>Yarowsky, 1994</TREF>, and knowledge-based approaches <REF>McRoy, 1992</REF>~~~<REF>Recently, Golding and Schabes 1996</REF> described a system, Tribayes, that combines a trigram model of the words parts of speech with a Bayesian classifier~~~The trigram component of the system is used to make decisions for those confusion sets that 166 terms documents X T O rxr D  O rxd txd txr Figure 1: Singular value decomposition SVD of matrix X produces matrices T, S and D.
To construct classifiers using supervised methods, we need classified data such as those in Figure 1~~~22 Decision Lists Let us first consider the use of decision lists, as proposed in <TREF>Yarowsky 1994</TREF>~~~Let f  denote a feature of the context of ~~~A feature can be, for example, a words occurrence immediately to the left of .
Many methods for word sense disambiguation based on supervised learning technique have been proposed~~~They include those using naive Bayes <REF>Gale, Church, and Yarowsky 1992a</REF>, decision lists <TREF>Yarowsky 1994</TREF>, nearest neighbor <REF>Ng and Lee 1996</REF>, transformation-based learning <REF>Mangu and Brill 1997</REF>, neural networks <REF>Towell and Voorhees 1998</REF>, Winnow <REF>Golding and Roth 1999</REF>, boosting <REF>Escudero, Marquez, and Rigau 2000</REF>, and naive Bayesian ensemble <REF>Pedersen 2000</REF>~~~The assumption behind these methods is that it is nearly always possible to determine the sense of an ambiguous word by referring to its context, and thus all of the methods build a classifier ie , a classification program using features representing context information eg , surrounding context words~~~For other related work on translation disambiguation, see Brown et al.
1991 present the initial success of applying word trigram conditional probabilities to the problem of context based detection and correction of real-word errors~~~<TREF>Yarowsky 1994</TREF> experiments with the use of decision lists for lexical ambiguity resolution, using context features like local syntactic patterns and collocational information, so that multiple types of evidence are considered in the context of an ambiguous word~~~In addition to word-forms, the patterns involve POS tags and lemmas~~~The algorithm is evaluated in missing accent restoration task for Spanish and French text, against a predefined set of a few words giving an accuracy over 99.
While the basic idea of our model is similar to trigger models, they handle co-occurrences of word pairs independently and do not use a representation of the whole context~~~This omission is also done in applications such as word sense dismnbiguation Yarowsky: 1994; FUNG et al , 1999~~~Our model is the most related to Coccaro mad <REF>Jurafsky 1998</REF>, in that a reduced vector space approach was taken and context is represented by the accumulation of word cooccurrence vectors~~~Their model was reported to decrease the test set perplexity by 12, compared to the bigram nmdel.
Exemplar-based method makes use of typical contexts exemplars of a word sense, eg, verbnoun collocations or adjective-noun collocations, and identifies the correct sense of a word in a particular context by comparing the context with the exemplars <REF>Ng and Lee, 1996</REF>~~~Recently, some kinds of learning techniques have been applied to cumulatively acquire exemplars form large corpora <REF>Yarowsky, 1994, 1995</REF>~~~But ideal resources from which to learn exemplars are not generally available for any languages~~~Moreover, the effectiveness of this method on disambiguating words in large-scale corpora into fine-grained sense distinctions needs to be further investigated <REF>Ng and Lee, 1996</REF>.
1~~~Word sense disambiguation has long been one of the major concerns in natural language processing area eg , <REF>Bruce et al , 1994</REF>; <REF>Choueka et al , 1985</REF>; <REF>Gale et al , 1993</REF>; <REF>McRoy, 1992</REF>; <REF>Yarowsky 1992, 1994, 1995</REF>, whose aim is to identify the correct sense of a word in a particular context, among all of its senses defined in a dictionary or a thesaurus~~~Undoubtedly, effective disambiguation techniques are of great use in many natural language processing tasks, eg, machine translation and information retrieving <REF>Allen, 1995</REF>; <REF>Ng and Lee, 1996</REF>; <REF>Resnik, 1995</REF>, etc Previous strategies for word sense disambiguation mainly fall into two categories: statistics-based method and exemplar-based method~~~Statistics-based method often requires large-scale corpora eg , <REF>Hirst, 1987</REF>; <REF>Luk, 1995</REF>, sense-tagging or not, monolingual or aligned bilingual, as training data to specify significant clues for each word sense.
For another example, tense/aspect can be affected by auxiliary words, trend verbs, etc This shows that classification of temporal indicators based on partof-speech POS information alone cannot determine relative temporal relations~~~3 Machine Learning Approaches for Relative Relation Resolution Previous efforts in corpus-based natural language processing have incorporated machine learning methods to coordinate multiple linguistic features for example in accent restoration <TREF>Yarowsky, 1994</TREF> and event classification <REF>Siegel and McKeown, 1998</REF>, etc Relative relation resolution can be modeled as a relation classification task~~~We model the thirteen relative temporal relations see Figure 1 as the classes to be decided by a classifier~~~The resolution process is to assign an event pair ie the two events under concern 2 to one class according to their linguistic features.
However, despite these findings, the value of N has continued to vary over the course of WSD work more or less arbitrarily~~~Yarowsky 1993, 1994a, 1994b examines different windows of microcontext, including 1-contexts, k-contexts, and words pairs at offsets -1 and -2, -1 and 1, and 1 and 2, and sorts them using a log-likelihood ratio to find the most reliable evidence for disambiguation~~~Yarowsky makes the observation that the optimal value of k varies with the kind of ambiguity: he suggests that local ambiguities need only a window of k  3 or 4, while semantic or topic-based ambiguities require a larger window of 20-50 words see Section 312~~~No single best measure is reported, suggesting that for different ambiguous words, different distance relations are more efficient.
STEP 3a: Train the supervised classification algorithm on the SENSE-A/SENSE-B seed sets~~~The decision-list algorithm used here <TREF>Yarowsky, 1994</TREF> identifies other collocations that reliably partition the seed training data, ranked by the purity of the distribution~~~Below is an abbreviated example of the decision list trained on the plant seed data~~~9 Initial decision list for plant abbreviated LogL 810 758 739 720 627 470 439 430 410 352 348 345 Collocation Sense plant life  A manufacturing plant  B life within 4-2-10 words  A manufacturing in 4-2-10 words  B animal within -I-2-10 words  A equipment within -1-2-10 words , B employee within 4-2-10 words  B assembly plant  B plant closure  B plant species  A automate within 4-2-10 words :: B microscopic plant  A 9Note that a given collocate such as life may appear multiple times in the list in different collocations1 relationships, including left-adjacent, right-adjacent, cooccurrence at other positions in a k-word window and various other syntactic associations.
1 9 Comparison with Previous Work This algorithm exhibits a fundamental advantage over supervised learning algorithms including <REF>Black 1988</REF>, <REF>Hearst 1991</REF>, Gale et al~~~1992, Yarowsky 1993, 1994, Leacock et al~~~1993, <REF>Bruce and Wiebe 1994</REF>, and <REF>Lehman 1994</REF>, as it does not require costly hand-tagged training sets~~~It thrives on raw, unannotated monolingual corpora the more the merrier.
4 In general, the high reliability of this behavior in excess of 97 for adjacent content words, for example makes it an extremely useful property for sense disambiguation~~~A supervised algorithm based on this property is given in <TREF>Yarowsky, 1994</TREF>~~~Using a decisien list control structure based on <REF>Rivest, 1987</REF>, this algorithm integrates a wide diversity of potential evidence sources lemmas, inflected forms, parts of speech and arbitrary word classes in a wide diversity of positional relationships including local and distant collocations, trigram sequences, and predicate-argument association~~~The training procedure computes the word-sense probability distributions for all such collocations, and orders them by r 0 /PrSenseAlColloeationix 5 the log-likelihood ratio  gt prISenseBlColloeationi, with optional steps for interpolation and pruning.
Experimental results show that the new algorithm produces substantially lower error rates and entropy, while simultaneously learning lists that are over an order of magnitude smaller than those produced by the standard algorithm~~~Decision lists <REF>Rivest, 1987</REF> have been used for a variety of natural language tasks, including accent restoration <TREF>Yarowsky, 1994</TREF>, word sense disambiguation <REF>Yarowsky, 2000</REF>, finding the past tense of English verbs <REF>Mooney and Califf, 1995</REF>, and several other problems~~~We show a problem with the standard algorithm for learning probabilistic decision lists, and we introduce an incremental algorithm that consistently works better~~~While the obvious implementation for this algorithm would be very slow, we also show how to efficiently implement it.
Many of the problems that probabilistic decision list algorithms have been used for are very similar: in a given text context, determine which of two choices is most appropriate~~~Accent restoration <TREF>Yarowsky, 1994</TREF>, word sense disambiguation <REF>Yarowsky, 2000</REF>, and other problems all fall into this framework, and typically use similar feature types~~~We thus chose one problem of this type, grammar checking, and believe that our results should carry over at least to these other, closely related problems~~~In particular, we chose to use exactly the same training, test, problems, and feature sets used by Banko and Brill 2001a; 2001b.
Our probabilistic decision lists can thus be thought of as a competitive way to probabilize TBLs, with the advantage of preserving the list-structure and simplicity of TBL, and the possible disadvantage of losing the dependency on the current state~~~<TREF>Yarowsky 1994</TREF> suggests two improvements to the standard algorithm~~~First, he suggests an optional, more complex smoothing algorithm than the one we applied~~~His technique involves estimating both a probability based on the global probability distribution for a question, and a local probability, given that no questions higher in the list were TRUE, and then interpolating between the two probabilities.
If no other rule is used, the last rule always triggers, ensuring that some probability is always returned~~~The standard algorithm for learning decision lists <TREF>Yarowsky, 1994</TREF> is very simple~~~The goal is to minimize the entropy of the decision list, where entropy represents how uncertain we are about a particular decision~~~For each rule, we find the expected entropy using that rule, then sort all rules by their entropy, and output the rules in order, lowest entropy first.
In the system presented here, the classifiers built for each ambiguous word are based on its lemma instead~~~Lemmatization allows for more compact and generalizable data by clustering all inflected forms of an ambiguous word together, an effect already commented on by <TREF>Yarowsky 1994</TREF>~~~The more inflection in a language, the more lemmatization will help to compress and generalize the data~~~In the case of our WSD system this means that less classifiers have to be built therefore adding up the training material available to the algorithm for each ambiguous wordform.
Alternatively, we chose for a model constructing classifiers based on lemmas therefore reducing the number of classifiers that need to be made~~~As has already been noted by <TREF>Yarowsky 1994</TREF>, using lemmas helps to produce more concise and generic evidence than inflected forms~~~Therefore building classifiers based on lemmas increases the data available to each classifier~~~We make use of the advantage of clustering all instances of eg one verb in a single classifier instead of several classifiers one for each inflected form found in the data.
For instance, governing body and governing bodies are different collocations for the sake of this paper~~~4 Adaptation of decision lists to n-way ambiguities Decision lists as defined in <REF>Yarowsky, 1993</REF>; 1994 are simple means to solve ambiguity problems~~~They have been successfully applied to accent restoration, word sense disambiguation 209 and homograph disambiguation <TREF>Yarowsky, 1994</TREF>; 1995; 1996~~~In order to build decision lists the training examples are processed to extract the features each feature corresponds to a kind of collocation, which are weighted with a log-likelihood measure.
4 Adaptation of decision lists to n-way ambiguities Decision lists as defined in <REF>Yarowsky, 1993</REF>; 1994 are simple means to solve ambiguity problems~~~They have been successfully applied to accent restoration, word sense disambiguation 209 and homograph disambiguation <TREF>Yarowsky, 1994</TREF>; 1995; 1996~~~In order to build decision lists the training examples are processed to extract the features each feature corresponds to a kind of collocation, which are weighted with a log-likelihood measure~~~The list of all features ordered by log-likelihood values constitutes the decision list.
The context within which the ambiguous word occurs is typically represented by a set of linguistically motivated features from which a learning algorithm induces a representative model that performs the disambiguation~~~A variety of classifiers have been employed for this task see Mooney 1996 and Ide and Veronis 1998 for overviews, the most popular being decision lists <REF>Yarowsky 1994, 1995</REF> and naive Bayesian classifiers <REF>Pedersen 2000</REF>; <REF>Ng 1997</REF>; <REF>Pedersen and Bruce 1998</REF>; <REF>Mooney 1996</REF>; <REF>Cucerzan and Yarowsky 2002</REF>~~~We employed a naive Bayesian classifier <REF>Duda and Hart 1973</REF> for our experiments, as it is a very convenient framework for incorporating prior knowledge and studying its influence on the classification task~~~In Section 51 we describe a basic naive Bayesian classifier and show how it can be extended with informative priors.
At present we have chosen one algorithm which does not combine features Decision Lists and another which does combine features AdaBoost~~~Despite their simplicity, Decision Lists Dlist for short as defined in <TREF>Yarowsky 1994</TREF> have been shown to be very effective for WSD <REF>Kilgarriff  Palmer, 2000</REF>~~~Features are weighted with a log-likelihood measure, and arranged in an ordered list according to their weight~~~In our case the probabilities have been estimated using the maximum likelihood estimate, smoothed adding a small constant 01 when probabilities are zero.
Previous work~~~<TREF>Yarowsky 1994</TREF> defined a basic set of features that has been widely used with some variations by other WSD systems~~~It consisted on words appearing in a window of k positions around the target and bigrams and trigrams constructed with the target word~~~He used words, lemmas, coarse part-of-speech tags and special classes of words, such as Weekday.
Machine learning methods have become the most popular technique in a variety of classification problems of these sort, and have shown significant success~~~A partial list consists of Bayesian classifiers <REF>Gale et al , 1993</REF>, decision lists <TREF>Yarowsky, 1994</TREF>, Bayesian hybrids <REF>Golding, 1995</REF>, HMMs <REF>Charniak, 1993</REF>, inductive logic methods <REF>Zelle and Mooney, 1996</REF>, memorya3 This research is supported by NSF grants IIS-9801638, IIS0085836 and SBR-987345~~~based methods <REF>Zavrel et al , 1997</REF>, linear classifiers <REF>Roth, 1998</REF>; <REF>Roth, 1999</REF> and transformationbased learning <REF>Brill, 1995</REF>~~~In many of these classification problems a significant source of difficulty is the fact that the number of candidates is very large  all words in words selection problems, all possible tags in tagging problems etc Since general purpose learning algorithms do not handle these multi-class classification problems well see below, most of the studies do not address the whole problem; rather, a small set of candidates typically two is first selected, and the classifier is trained to choose among these.
62 Performance of the Syntagmatic Kernel Table 3 shows the performance of the Syntagmatic Kernel on both data sets~~~As baseline, we report the result of a standard approach consisting on explicit bigrams and trigrams of words and PoS tags around the words to be disambiguated <TREF>Yarowsky, 1994</TREF>~~~The results show that the Syntagmatic Kernel outperforms the baseline in any configuration hard/soft-matching~~~The soft-matching criteria further improve the classification performance.
, 2005a and WordSenseDisambiguation WSD <REF>Strapparava et al , 2004</REF>~~~In general, the strategy adopted to model syntagmatic relations is to provide bigrams and trigrams of collocated words as features to describe local contexts <TREF>Yarowsky, 1994</TREF>, and each word is regarded as a different instance to classify~~~For instance, occurrences of a given class of named entities such as names of persons can be discriminated in texts by recognizing word patterns in their local contexts~~~For example the token Rossi, whenever is preceded by the token Prof , often represents the name of a person.
Obviously, how to measure the confidence of features is a very important issue for the decision list~~~We use the metric described in <TREF>Yarowsky, 1994</TREF>; <REF>Golding, 1995</REF>~~~Provided that 1  0Ps f > for all i :  max    i i confidence f P s f 1 This value measures the extent to which the context is unambiguously correlated with one particular slot i s  24 Slot-value merging and semantic reclassification The slot-value merger is to combine the slots assigned to the concepts in an input sentence~~~Another simultaneous task of the slot-value merger is to check the consistency among the identified slot-values.
Estimating terms of the form Prwlh  is done by assuming some generative probabilistic model, typically using Markov or other independence assumptions, which gives rise to estimating conditional probabilities of n-grams type features in the word or POS space~~~Machine learning based classifiers and maximum entropy models which, in principle, are not restricted to features of these forms have used them nevertheless, perhaps under the influence of probabilistic methods <REF>Brill, 1995</REF>; <TREF>Yarowsky, 1994</TREF>; <REF>Ratnaparkhi et al , 1994</REF>~~~It has been argued that the information available in the local context of each word should be augmented by global sentence information and even information external to the sentence in order to learn 124 better classifiers and language models~~~Efforts in this directions consists of 1 directly adding syntactic information, as in <REF>Chelba and Jelinek, 1998</REF>; <REF>Rosenfeld, 1996</REF>, and 2 indirectly adding syntactic and semantic information, via similarity models; in this case n-gram type features are used whenever possible, and when they cannot be used due to data sparsity, additional information compiled into a similarity measure is used <REF>Dagan et al , 1999</REF>.
Numerous methods have been presented for confusable disambiguation~~~The more recent set of techniques includes mult iplicative weightupdate algorithms <REF>Golding and Roth, 1998</REF>, latent semantic analysis <REF>Jones and Martin, 1997</REF>, transformation-based learning <REF>Mangu and Brill, 1997</REF>, differential grammars <REF>Powers, 1997</REF>, decision lists <TREF>Yarowsky, 1994</TREF>, and a variety of Bayesian classifiers <REF>Gale et al , 1993</REF>, <REF>Golding, 1995</REF>, <REF>Golding and Schabes, 1996</REF>~~~In all of these approaches, the problem is formulated as follows: Given a specific confusion set eg to,two,too, all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker, it must decide which member of the confusion set to choose~~~Confusion set disambiguation is one of a class of natural language problems involving disambiguation from a relatively small set of alternatives based upon the string context in which the ambiguity site appears.
In particular, they may involve performing speech recognition on speech data, parsing on text data, application of hand-coded rules to the results of parsing, or some combination of these~~~Statistics are then compiled to estimate the probability pa j f of each semantic atom a given each separate feature f, using the standard formula pa j f  Naf  1Nf  2 where Nf is the number of occurrences in the training data of utterances with feature f, and N af is the number of occurrences of utterances with both feature f and semantic atom a The decoding process follows <TREF>Yarowsky, 1994</TREF> in assuming complete dependence between the features~~~Note that this is in sharp contrast with the Naive Bayes classifier <REF>Duda et al , 2000</REF>, which assumes complete independence~~~Of course, neither assumption can be true in practice; however, as argued in <REF>Carter, 2000</REF>, there are good reasons for preferring the dependence alternative as the better option in a situation where there are many features extracted in ways that are likely to overlap.
There is typically no corpus data available at the start of a project, but considerable amounts at the end: the intention behind ALTERF is to allow us to shift smoothly from an initial version of the system which is entirely rule-based, to a final version which is largely data-driven~~~ALTERF characterises semantic analysis as a task slightly extending the decision-list classification algorithm <TREF>Yarowsky, 1994</TREF>; <REF>Carter, 2000</REF>~~~We start with a set of semantic atoms, each representing a primitive domain concept, and define a semantic representation to be a non-empty set of semantic atoms~~~For example, in the procedure assistant domain we represent the utterances please speak up show me the sample syringe set an alarm for five minutes from now no i said go to the next step respectively as fincrease volumeg fshow, sample syringeg fset alarm, 5, minutesg fcorrection, next stepg where increase volume, show, sample syringe, set alarm, 5, minutes, correction and next step are semantic atoms.
1~~~For each piece of evidence, we calculate the Iw of likelihood ratio of the largest; conditional probability of the decision D  :rl given the presence of that piece of evidence to the second largest conditional probability of the decision D x2: I EI lg2 PDx2 I EI Then a decision list is constructed with pieces of evidence sorted in descending order with respect to their log of likelihood ratios, where the decision of the rule at each line is D  xl with the largest conditional probability <TREF>Yarowsky 1994</TREF> discusses several techniques for avoiding the problems which arise when an observed count is 0~~~lq-om among those techniques, we employ tlm simplest ram, ie, adding a small constant c 01 < < 025 to the numerator and denominator~~~With this inodification, more frcquent evidence is preferred when several evidence candidates exist with the same 708 2.
Left  Context   ML2MII ll,ight Named Entity  Context  M   Mm<3  1 2 Current Position 4 Supervised Learning for Japanese Named Entity Recognition This section describes how to apply tile decision list learning method to chunking/tagging named entities~~~41 Decision List Learning A decision list <REF>Rivest, 1987</REF>; <TREF>Yarowsky, 1994</TREF> is a sorted list of decision rules, each of which decides the wflue of a decision D given some evidence E Each decision rule in a decision list is sorted in descending order with respect to some preference value, and rules with higher preference values are applied first when applying the decision list to some new test; data~~~First, the random variable D representing a decision w, ries over several possible values, and the random wriable E representing some evidence varies over 1 and 0 where 1 denotes the presence of the corresponding piece of evidence, 0 its absence~~~Then, given some training data in which the correct value of the decision D is annotated to each instance, the conditional probabilities PD  x I E  1 of observing the decision D  x under the condition of the presence of the evidence E E  1 are calculated and the decision list is constructed by the tbllowing procedure.
In general, creating training data tbr supervised learning is somewhat easier than creating pattern matching rules by hand~~~Next, we apply Yarowskys method tbr supervised decision list learning I <TREF>Yarowsky, 1994</TREF> to 1VVe choose tile decision list learning method as the 705 Table 1: Statistics of NE Types of IREX NE Type ORGANIZATION PERSON LOCATION ARTIFACT DATE TIME MONEY PERCENT Total frequency  Training 3676 197 3840 206 5463 292 747 40 3567 191 502 27 390 21 492 26 18677 Test 361 239 338 224 413 274 48 32 260 172 54 35 15 10 21 14 1510 Japanese named entity recognition, into which we incorporate several noun phrase chunking techniques sections 3 and 4 and experimentally evaluate their performance on the IREX, workshops training and test data section 5~~~As one of those noun phrase chunking techniques, we propose a method for incorporating richer contextual information as well as patterns of constituent morphemes within a named entity, compared with those considered in tire previous research <REF>Sekine et al , 1998</REF>; <REF>Borthwick, 1999</REF>, and show that the proposed method outperlbrms these approaches~~~2 Japanese Named Entity Recognition 21 Task of the IREX Workshop The task of named entity recognition of the IREX workshop is to recognize eight named entity types in Table 1 IREX <REF>Conmfittee, 1999</REF>.
This small number has almost no effect on more frequent words, but boosts the score of less common, yet potentially equally informative, words~~~222 Decision List The decision list classifier uses the log-likelihood of correspondence between each context feature and each sense, using additive smoothing <TREF>Yarowsky, 1994</TREF>~~~The decision list was created by ordering the correspondences from strongest to weakest~~~Instances that did not match any rule in the decision list were assigned the most frequent sense, as calculated from the training data.
Much recent research on word sense disambiguation WSD has adopted a corpus-based, learning approach~~~Many different learning approaches have been used, including neural networks <REF>Leacock et al , 1993</REF></REF>, probabilistic algorithms <REF>Bruce and Wiebe, 1994</REF>; <REF>Gale et al , 1992a</REF>; <REF>Gale et al , 1995</REF>; <REF>Leacock et al , 1993</REF></REF>; <REF>Yarowsky, 1992</REF>, decision lists <TREF>Yarowsky, 1994</TREF>, exemplar-based learning algorithms <REF>Cardie, 1993</REF>; <REF>Ng and Lee, 1996</REF>, etc In particular, <REF>Mooney 1996</REF> evaluated seven state-of-the-art machine learning algorithms on a common data set for disambiguating six senses of the word line~~~The seven algorithms that he evaluated are: a Naive-Bayes classifier <REF>Duda and Hart, 1973</REF>, a perceptron <REF>Rosenblatt, 1958</REF>, a decisiontree learner <REF>Quinlan, 1993</REF>, a k nearest-neighbor classifier exemplar-based learner <REF>Cover and Hart, 1967</REF>, logic-based DNF and CNF learners <REF>Mooney, 1995</REF>, and a decision-list learner <REF>Rivest, 1987</REF>~~~His results indicate that the simple Naive-Bayes algorithm gives the highest accuracy on the line corpus tested.
It must be noted however that the LDOCE homograph level is far more rough-grained than the CIDE guideword level, let alone the sub-sense level, and that Wilks and Stevensons approach on its own would, by its very nature, not transfer down to more fine-grained distinctions~~~Other research, such as Yarowskys into accent restoration in <REF>Spanish and French 1994</REF>, which reports accuracy levels of 9099, is again at a more rough-grained level, in this case that of distinguished unaccented and accented word forms~~~While the sense tagging results are fairly encouraging, the part of speech tagging results arc at present relatively poor~~~It thus secrns sensible, especially noting Wilks and Stevensons analysis mentioned above, to first run a sentence through a traditional part of speech tagger before trying to disambiguate the senses.
From this perspective, either accent identification can be extended to truecasing or truecasing can be extended to incorporate accent restoration~~~<TREF>Yarowsky, 1994</TREF> reports good results with statistical methods for Spanish and French accent restoration~~~Truecasing is also a specialized method for spelling correction by relaxing the notion of casing to spelling variations~~~There is a vast literature on spelling correction <REF>Jones and Martin, 1997</REF>; <REF>Golding and Roth, 1996</REF> using both linguistic and statistical approaches.
Finally, we demonstrate the considerable benefits of truecasing through task based evaluations on named entity tagging and automatic content extraction~~~11 Related Work Truecasing can be viewed in a lexical ambiguity resolution framework <TREF>Yarowsky, 1994</TREF> as discriminating among several versions of a word, which happen to have different surface forms casings~~~Wordsense disambiguation is a broad scope problem that has been tackled with fairly good results generally due to the fact that context is a very good predictor when choosing the sense of a word~~~<REF>Gale et al , 1994</REF> mention good results on limited case restoration experiments on toy problems with 100 words.
5 Comparison with Previous Work Several approaches have been proposed for attaching the correct sense from a set of prescribed ones of a word in context~~~Some of them have been fully tested in real size texts eg statistical methods <REF>Yarowsky, 1992</REF>, <TREF>Yarowsky, 1994</TREF>, <REF>Miller and Teibel, 1991</REF>, knowledge based methods <REF>Sussna, 1993</REF>, <REF>Agirre and Rigau, 1996</REF>, or mixed methods <REF>Richardson et al , 1994</REF>, <REF>Resnik, 1995</REF>~~~The performance of WSD is reaching a high stance, although usually only small sets of words with clear sense distinctions are selected for disambiguation eg~~~<REF>Yarowsky, 1995</REF> reports a success rate of 96 disambiguating twelve words with two clear sense distinctions each one.
Syncretism and related morphological ambiguities present a problem for many NL applications where lexical disambiguation is important; cases where the orthographic form is identical but the pronunciations of the various functions differ are particularly important for speech applications, such as text-to-speech, since appropriate word pronunciations must be computed from orthographic forms that underspecify the necessary information~~~Ideally one would like to build models that use contextual information to perform lexical disambiguation <REF>Yarowsky 1992, 1994</REF>, but such models must be trained on specialized tagged corpora either hand-generated or semi-automatically generated and such training corpora are often not available, at least in the early phases of constructing a particular application~~~Lacking good contextual models, one is forced to fall back on estimates of the lexical prior probabilities for the various functions of a form~~~Following standard terminology, a lexical prior can be defined as follows: Imagine that a given form is n-ways ambiguous; the lexical prior probability of sense i of this form is simply the probability of sense i independent of the context in which the particular instantiation of the form occurs.
The Splus statistical package was used for the induction process, with parameters set to their default values~~~Previous efforts in corpus-based natural language processing have incorporated machine learning methods to coordinate multiple linguistic indicators, eg, to classify adjectives according to markedness <REF>Hatzivassiloglou and McKeown, 1995</REF>, to perform accent restoration <TREF>Yarowsky, 1994</TREF></TREF>, for disambiguation problems <TREF>Yarowsky, 1994</TREF></TREF>; <REF>Luk, 1995</REF>, 158 n States Events be 23,409 1000 00 have 7,882 699 301 all other verbs 66,682 162 838 Table 4: Breakdown of verb occurrences~~~and for the automatic identification of semantically related groups of words <REF>Pereira, Tishby, and Lee, 1993</REF>; <REF>Hatzivassiloglou and McKeown, 1993</REF>~~~For more detail on the machine learning experiments described here, see <REF>Siegel 1997</REF>.
A feature expression F of the named entity can be any possible subset of the full feature expression mlength, NEtag,POS , or the set indicating that the system outputs no named entity within the segment~~~F         any subset of braceleftBig mlength, NEtag,POS bracerightBig braceleftBig class sys no outputs bracerightBig In the training and testing phases, within each segment SegEv j of event expression, a class is assigned to each system, where each class class i sys for the i-th system is represented as a list of the classes of the named entities output by the system: class i sys  braceleftbigg /,  , / no output i 1,,n 34 Learning Algorithm We apply a simple decision list learning method to the task of learning a classifier for combining outputs of named entity chunkers 4  A decision list <TREF>Yarowsky, 1994</TREF> is a sorted list of decision rules, each of which decides the value of class given some features f of an event~~~Each decision rule in a decision list is sorted in descending order with respect to some preference value, and rules with higher preference values are applied first when applying the decision list to some new test data~~~In this paper, we simply sort the decision list according to the conditional probability Pclass i  f of the class i of the i-th systems output given a feature f 4 Experimental Evaluation We experimentally evaluate the performance of the proposed system combination method using the IREX workshops training and test data.
32 Nave Bayes The second system used was a nave Bayes classifier where the similarity between an instance, I, and a sense class, Sj, is defined as: SimI,Sj  PI,Sj  PSjPISj We then choose the sense class, Sj, which maximized the similarity function above, making standard independence assumptions~~~33 Decision List The final system was a decision list classifier that found the log-likelihoods of the correspondence beAssociation for Computational Linguistics for the Semantic Analysis of Text, Barcelona, <REF>Spain, July 2004</REF> SENSEVAL-3: Third International Workshop on the Evaluation of Systems tween features and senses, using plus-one smoothing <TREF>Yarowsky, 1994</TREF>~~~The features were ordered from most to least indicative to form the decision list~~~A separate decision list was constructed for each set of lexical samples in the training data.
The ordering of rules employed in a decision list in order to simplify the representation and perform conflict resolution apparently gives it an advantage over other symbolic methods on this task~~~In addition to the results reported by <TREF>Yarowsky 1994</TREF> and <REF>Mooney and Califf 1995</REF>, it provides evidence for the utility of this representation for natural-language problems~~~With respect to training time, the symbolic methods are significantly slower since they are searching for a simple declarative representation of the concept~~~Empirically, the time complexity for most methods are growing somewhat worse than linearly in the number of training examples.
Finally, decision lists <REF>Rivest, 1987</REF> are ordered lists of conjunctive rules, where rules are tested in order and the first one that matches an instance is used to classify it~~~A number of effective concept-learning systems have employed decision lists Clark 84 <REF>Niblett, 1989</REF>; <REF>Quinlan, 1993</REF>; <REF>Mooney  Califf, 1995</REF> and they have already been successfully applied to lexical disambiguation <TREF>Yarowsky, 1994</TREF>~~~All of the logic-based methods are variations of the FOIL algorithm for induction of first-order function-free Horn clauses <REF>Quinlan, 1990</REF>, appropriately simplified for the propositional case~~~They are called PFoIL-DNF, PFOlL-CNF, and PFoIL-DLIsT.
Many methods for word sense disambiguation using a supervised learning technique have been proposed~~~They include those using Nave Bayes <REF>Gale et al 1992a</REF>, Decision List <TREF>Yarowsky 1994</TREF>, Nearest Neighbor <REF>Ng and Lee 1996</REF>, Transformation Based Learning <REF>Mangu and Brill 1997</REF>, Neural Network Towell and 1 In this paper, we take English-Chinese translation as example; it is a relatively easy process, however, to extend the discussions to translations between other language pairs~~~Computational Linguistics ACL, <REF>Philadelphia, July 2002</REF>, pp~~~343-351.
In practice, however, many words have sense ambiguity and a word can belong to several different classes, eg, bird is a member of both BIRD and MEAT~~~Thorough treatment of this problem is beyond the scope of the present paper; we simply note that one can employ an existing word-sense disambiguation technique eg ,<REF>Yarowsky 1992, 1994</REF> in preprocessing, and use the disambiguated word senses as virtual words in the following 220 Li and Abe Generalizing Case Frames ANIMAL BIRD INSECT swallow crow eagle bird bug bee insect Figure 3 An example thesaurus~~~case-pattern acquisition process~~~It is also possible to extend our model so that each word probabilistically belongs to several different classes, which would allow us to resolve both structural and word-sense ambiguities at the time of disambiguation.
32 The Syntagmatic Kernel Syntagmatic aspects are probably the most important evidence for recognizing lexical entailment~~~In general, the strategy adopted to model syntagmatic relations in WSD is to provide bigrams and trigrams ofcollocatedwordsasfeaturestodescribelocalcontexts <TREF>Yarowsky, 1994</TREF>~~~The main drawback of this approach is that non contiguous or shifted collocations cannot be identified, decreasing the generalization power of the learning algorithm~~~For example, suppose that the word job has to be disambiguated into the sentence permanent academic job in, and that the occurrence We offer permanent positions is provided for training.
In this study we used ill the decision-list method the same 152 types of patterns that were used in/;lie maximuln-entropy method~~~To determine the priority order of the rules, we referred to Yarowskys method <TREF>Yarowsky, 1994</TREF> and Nishiokwamas method <REF>Nishiokaymna et al , 1998</REF> and used the probability and frequency of each rule as measures of this priority order~~~When nnlltiple rifles had the same probability, the rules were arranged in order of their frequency~~~Suppose, for example, that Pattern A Noun: Normal Noun; Particle: Case-Particle: none: wo; Verb: Normal Form: 217; Symhol: Punctuatioif occurs 13 times in a learlfing set and that tell of the occurrences include the inserted partition Inal:k Suppose also thai; Pattern B Noun; Particle; Verb; Symbol occurs 12a times in a learning set and that 90 of the occurrences include the mark.
However, such a list may be quite noisy, ie, many of them are not informal phrases at all~~~An alternative approach to extracting the informal phrases is to use a bootstrapping algorithm eg, <TREF>Yarowsky 1995</TREF>~~~Specifically, we first manually collect a small set of example relations~~~Then, using these relations as a seed set, we extract the text patterns eg, the definition pattern showing how the informal and formal phrases co-occur in the data as discussed in Section 31.
The sentiment of a document is calculated as the average semantic orientation of all such phrases~~~<TREF>Yarowsky 1995</TREF> describes a semi-unsupervised approach to the problem of sense disambiguation of words, also using a set of initial seeds, in this case a few high quality sense annotations~~~These annotations are used to start an iterative process of learning information about the contexts in which senses of words appear, in each iteration labeling senses of previously unlabeled word tokens using information from the previous iteration~~~23 Chinese Language Processing A major issue in processing Chinese text is the fact that words are not delimited in the written language.
As we do not use multiple classifiers our approach is quite far from cotraining~~~But it is close to the paradigm described by <TREF>Yarowsky 1995</TREF> and <REF>Turney 2002</REF> as it also employs self-training based on a relatively small seed data set which is incrementally enlarged with unlabelled samples~~~But our approach does not use point-wise mutual information~~~Instead we use relative frequencies of newly found features in a training subcorpus produced by the previous iteration of the classifier.
Finally, I present a simpler and more efcient approach to training dependency parsers by applying a boosting-like procedure to standard training methods~~~Over the past decade, there has been tremendous progress on learning parsing models from treebank data <TREF>Magerman, 1995</TREF>; <REF>Collins, 1999</REF>; <REF>Charniak, 1997</REF>; <REF>Ratnaparkhi, 1999</REF>; <REF>Charniak, 2000</REF>; <REF>Wang et al , 2005</REF>; <REF>McDonald et al , 2005</REF>~~~Most of the early work in this area was based on postulating generative probability models of language that included parse structures <TREF>Magerman, 1995</TREF>; <REF>Collins, 1997</REF>; <REF>Charniak, 1997</REF>~~~Learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation tricks to cope with the sparse data problems <REF>Collins, 1997</REF>; <REF>Bikel, 2004</REF>.
Over the past decade, there has been tremendous progress on learning parsing models from treebank data <TREF>Magerman, 1995</TREF>; <REF>Collins, 1999</REF>; <REF>Charniak, 1997</REF>; <REF>Ratnaparkhi, 1999</REF>; <REF>Charniak, 2000</REF>; <REF>Wang et al , 2005</REF>; <REF>McDonald et al , 2005</REF>~~~Most of the early work in this area was based on postulating generative probability models of language that included parse structures <TREF>Magerman, 1995</TREF>; <REF>Collins, 1997</REF>; <REF>Charniak, 1997</REF>~~~Learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation tricks to cope with the sparse data problems <REF>Collins, 1997</REF>; <REF>Bikel, 2004</REF>~~~Subsequent research began to focus more on conditional models of parse structure given the input sentence, which allowed discriminative training techniques such as maximum conditional likelihood ie maximum entropy  to be applied <REF>Ratnaparkhi, 1999</REF>; <REF>Charniak, 2000</REF>.
The advantage of this approach is that it does not rely on part-ofspeech tags nor grammatical categories~~~Furthermore, I based training on maximizing the conditional probability of a parse tree given a sentence, unlike most previous generative models <TREF>Magerman, 1995</TREF>; <REF>Collins, 1997</REF>; <REF>Charniak, 1997</REF>, which focus on maximizing the joint probability of the parse tree and the sentence~~~An ef cient training algorithm can be achieved by maximizing the conditional probability of each parsing decision, hence minimizing a loss based on each local link decision independently~~~Importantly, inter-dependence between links can still be accommodated by exploiting dynamic features in training features that take into account the labels of some of the surrounding components when predicting the label of a target component.
3 she was a cook in a school cafeteria 1 Figure 3: Dependency graph for the example string~~~tributed to <TREF>Magerman 1995</TREF>, to percolate lexical heads up the tree~~~Figure 3 shows the dependency graph that results from this head percolation approach, where each link in the graph represents a dependency relation from the modifier to the head~~~For example, conventional head percolation rules specify the VP as the head of the S, so was.
The decision tree approach has been used in parsing sentence~~~D <TREF>Magerman, 1995</TREF>Ulf Hermijakob and J<REF>Mooney, 1997</REF> to define the rhetorical of text documents <REF>Daniel Marcu, 1999</REF>~~~Most of the previous methods only produce a short sentence whose word order is the same as that of the original sentence, and in the same language, eg, English~~~When nonnative speaker reduce a long sentence in foreign language, they usually try to link the meaning of words within the original sentence into meanings in their language.
Unlike probabilities in the parsing model, there obviously is not sufficient training data to estimate slot fill probabilities directly~~~Instead, these probabilities are estimated by statistical decision trees similar 58 to those used in the Spatter parser <TREF>Magerman 1995</TREF>~~~Unlike more common decision tree classifiers, which simply classify sets of conditions, statistical decision trees give a probability distribution over all possible outcomes~~~Statistical decision trees are constructed in a two phase process.
A recent trend in natural language processing has been toward a greater emphasis on statistical approaches, beginning with the success of statistical part-of-speech tagging programs <REF>Church 1988</REF>, and continuing with other work using statistical part-of-speech tagging programs, such as BBN PLUM <REF>Weischedel et al 1993</REF> and NYU Proteus <REF>Grishman and Sterling 1993</REF>~~~More recently, statistical methods have been applied to domain-specific semantic parsing <REF>Miller et al 1994</REF>, and to the more difficult problem of wide-coverage syntactic parsing <TREF>Magerman 1995</TREF>~~~Nevertheless, most natural language systems remain primarily rule based, and even systems that do use statistical techniques, such as ATT Chronus <REF>Levin and Pieraccini 1995</REF>, continue to require a significant rule based component~~~Development of a complete end-to-end statistical understanding system has been the focus of several ongoing research efforts, including <REF>Miller et al 1995</REF> and <REF>Koppelman et al 1995</REF>.
In the second phase, these distributions are smoothed by mixing together distributions of various nodes in the decision tree~~~As in <TREF>Magerman 1995</TREF>, mixture weights are determined by deleted interpolation on a separate block of training data~~~43 Searching the Semantic Interpretation Model Searching the interpretation model proceeds in two phases~~~In the first phase, every parse T received from the parsing model is rescored for every possible frame type, computing PT I FT our current model includes only a half dozen different types, so this computation is tractable.
In empirical approaches to parsing, lexical/semantic collocation extracted from corpus has been proved to be quite useful for ranking parses in syntactic analysis~~~For example, <TREF>Magerman 1995</TREF>, <REF>Collins 1996</REF>, and <REF>Charniak 1997</REF> proposed statistical parsing models which incorporated lexical/semantic information~~~In their models, syntactic and lexical/semantic features are dependent on each other and are combined together~~~This paper also proposes a method of utilizing lexical/semantic features for the purpose of applying them to ranking parses in syntactic analysis.
This paper also proposes a method of utilizing lexical/semantic features for the purpose of applying them to ranking parses in syntactic analysis~~~However, unlike the models of <TREF>Magerman 1995</TREF>, <REF>Collins 1996</REF>, and <REF>Charniak 1997</REF>, we assume that syntactic and lexical/semantic features are independent~~~Then, we focus on extracting lexical/semantic collocational knowledge of verbs which is useful in syntactic analysis~~~More specifically, we propose a novel method for learning a probability model of subcategorization preference of verbs.
Deterministic parsing algorithms for building dependency graphs <REF>Kudo and Matsumoto, 2002</REF>; <REF>Yamada and Matsumoto, 2003</REF>; <REF>Nivre, 2003</REF> 2~~~History-based models for predicting the next parser action <REF>Black et al , 1992</REF>; <TREF>Magerman, 1995</TREF>; <REF>Ratnaparkhi, 1997</REF>; <REF>Collins, 1999</REF> 3~~~Discriminative learning to map histories to parser actions <REF>Kudo and Matsumoto, 2002</REF>; <REF>Yamada and Matsumoto, 2003</REF>; <REF>Nivre et al , 2004</REF> In this section we will define dependency graphs, describe the parsing algorithm used in the experiments and finally explain the extraction of features for the history-based models~~~21 Dependency Graphs A dependency graph is a labeled directed graph, the nodes of which are indices corresponding to the tokens of a sentence.
We will discuss the maximum accuracy of 8433~~~Compared to recent stochastic English parsers that yield 86 to 87 accuracy <REF>Collins, 1996</REF>; <TREF>Magerman, 1995</TREF>, 8433 seems unsatisfactory at the first glance~~~The main reason behind this lies in the difference between the two corpora used: Penn Treebank <REF>Marcus et al , 1993</REF> and EDR corpus EDR, 1995~~~Penn Treebank<REF>Marcus et al , 1993</REF> was also used to induce part-of-speech POS taggers because the corpus contains very precise and detailed POS markers as well as bracket, annotations.
A broader range of information, in particular lexical information, was found to be essential in disambiguating the syntactic structures of real-world sentences~~~SPATTER <TREF>Magerman, 1995</TREF> augmented the pure PCFG by introducing a number of lexical attributes~~~The parser controlled applications of each rule by using the lexical constraints induced by decision tree algorithm <REF>Quinlan, 1993</REF>~~~The SPATTER parser attained 87 accuracy and first made stochastic parsers a practical choice.
It is not surprising, then, that the per-sentence parsing acclzracy suffers when parses are predicted from raw text~~~Clearly the present research task is quite considerably harder than the parsing and tagging tasks undertaken in <REF>Jelinek et al , 1994</REF>; <TREF>Magerman, 1995</TREF>; <REF>Black et al , 1993b</REF>, which would seem to be the closest work to ours, and any comparison between this work and ours must be approached with extreme caution~~~Table 3 shows the differences between the treebank utilized in <REF>Jelinek et al , 1994</REF> on the one hand, and in the work reported here, on the other, is Table 4 shows relevant lSFigures for Average Sentence Length lraLuing Corpus and Training Set Size, for the IBM ManuaLs Corpus, are approximate, and cze fzom Black et aL, 1993a~~~24 Length 1-10 11-15 16-23 sentences ltpltp20 447 559 808 436 i47d 66 430 1216 488 cross constits/sent 915 46 8O7 82 565 113 Table 4: Parsing results reported by Jelinek et.
Probabilistic decision trees are utilized as a means of prediction, roughly as in <REF>Jelinek et al , 1994</REF>; <REF>Magermau, 1995</REF>, and as in these references, training is supervised, and in particular is treebank-based~~~In all other respects, our work departs from previous research on broad--coverage 16 I t I I I I I i  I i I I I I I I I I I I I i I 1, I I I I I i I 1 I I I I probabilistic parsing, which either attempts to learn to predict grrarntical structure of test data directly from a training treebank <REF>Brill, 1993</REF>; <REF>Collins, 1996</REF>; <REF>Eisner, 1996</REF>; <REF>Jelinek et al , 1994</REF>; <TREF>Magerman, 1995</TREF>; Skine and <REF>Orishman, 1995</REF>; <REF>Sharman et al , 1990</REF>, or employs a grammar and sometimes a dictionary to capture linguistic expertise directly <REF>Black et al , 1993a</REF>; <REF>GrinBerg et al , 1995</REF>; Schabes; 1992, but arguably at a less detailed and informative level than in the research reported here~~~In what follows, Section 2 explains the contribution to the prediction process of the grammar and of the lexical generalizations created by our grammarian~~~Section 3 shows; from a formal standpoint, how prediction is carried out, and more generally how the parser operates.
 Convert PRT to ADVP~~~This convention was established by <TREF>Magerman 1995</TREF>~~~3~~~Remove quotation marks ie terminal items tagged  or .
Head-lexicalized stochastic grammars have recently become increasingly popular see <REF>Collins 1997, 1999</REF>; <REF>Charniak 1997, 2000</REF>~~~These grammars are based on Magermans headpercolation scheme to determine the headword of each nonterminal <TREF>Magerman 1995</TREF>~~~Unfortunately this means that head-lexicalized stochastic grammars are not able to capture dependency relations between words that according to Magermans head-percolation scheme are nonheadwords -eg between more and than in the WSJ construction carry more people than cargo where neither more nor than are headwords of the NP constituent more people than cargo~~~A frontier-lexicalized DOP model, on the other hand, captures these dependencies since it includes subtrees in which more and than are the only frontier words.
In the first pass, it tags words as either head words or non-head words~~~Training data for this pass is obtained using a head percolation table <TREF>Magerman 1995</TREF> on bracketed Penn Treebank sentences~~~After training, head tagging is performed according to Equation 1, where 15 is the estimated probability and Hi is a characteristic function which is true iff word i is a head word~~~n H  argmaxH HwilHiHilHi-1Hi-2 i1 1 The second pass then takes the words with this head information and supertags them according to Equation 2, where tHio is the supertag of the ePart of speech tagging models have not used heads in this manner to achieve variable length contexts.
There have been two main robust parsing paradigms: Finite State Grammar-based approaches such as <REF>Abney 1990</REF>, <REF>Grishman 1995</REF>, and Hobbs et al~~~1997 and Statistical Parsing such as <REF>Charniak 1996</REF>, <TREF>Magerman 1995</TREF>, and <REF>Collins 1996</REF>~~~<REF>Srinivas 1997a</REF> has presented a different approach called supertagging that integrates linguistically motivated lexical descriptions with the robustness of statistical techniques~~~The idea underlying the approach is that the computation of linguistic structure can be localized if lexical items are associated with rich descriptions Supertags that impose complex constraints in a local context.
<REF>Collins 1996</REF> split the sentence label S into two versions, representing sentences with and without subjects~~~He 1Not to mention earlier non-PCFG lexicalized statistical parsers, notably <TREF>Magerman 1995</TREF> for the Penn Treebank~~~also modified the treebank to contain different labels for standard and for base noun phrases~~~<REF>Klein and Manning 2003</REF> identified nonterminals that could valuably be split into fine-grained ones using hand-written linguistic rules.
Second, one might propagate lexical information upward through the productions~~~Examples of formalisms using this approach include the work of <TREF>Magerman 1995</TREF>, <REF>Charniak 1997</REF>, <REF>Collins 1997</REF>, and <REF>Goodman 1997</REF>~~~A more linguistically motivated approach is to expand the domain of productions downward to incorporate more tree structures~~~The Lexicalized Tree-Adjoining Grammar LTAG formalism <REF>Schabes et al , 1988</REF>, <REF>Schabes, 1990</REF>, although not context-free, is the most well-known instance in this category.
Standard symbolic machine learning techniques have been successfully applied to a number of tasks in natural language processing NLP~~~Examples include the use of decision trees for syntactic analysis <TREF>Magerman, 1995</TREF>, coreference <REF>Aone and Bennett, 1995</REF>; <REF>McCarthy and Lehnert, 1995</REF>, and cue phrase identification <REF>Litman, 1994</REF>; the use of inductive logic programming for learning semantic grammars and building prolog parsers 113 <REF>Zelle and Mooney, 1994</REF>; <REF>Zelle and Mooney, 1993</REF>; the use of conceptual clustering algorithms for relative pronoun resolution <REF>Cardie, 1992a</REF>; <REF>Cardie 1992b</REF>, and the use of case-based learning techniques for lexical tagging tasks <REF>Cardie, 1993a</REF>; Daelemans et al , submitted~~~In theory, both statistical and machine learning techniques can significantly reduce the knowledge-engineering effort for building large-scale NLP systems: they offer an automatic means for acquiring robust heuristics for a host of lexical and structural disambiguation tasks~~~It is well-known in the machine learning community, however, that the success of a learning algorithm depends critically on the representation used to describe the training and test instances <REF>Almuallim and Dietterich, 1991</REF>, Langley and Sage, in press.
1994, and <TREF>Magerman 1995</TREF>~~~A strength of these models is undoubtedly the powerful estimation techniques that they use: maximum-entropy modeling in <REF>Ratnaparkhi 1997</REF> or decision trees in <REF>Jelinek et al 1994</REF> and <TREF>Magerman 1995</TREF>~~~A weakness, we will argue in this section, is the method of associating parameters with transitions taken by bottom-up, shift-reduce-style parsers~~~We give examples in which this method leads to the parameters unnecessarily fragmenting the training data in some cases or ignoring important context in other cases.
3 We find lexical heads in Penn Treebank data using the rules described in Appendix A of <REF>Collins 1999</REF>~~~The rules are a modified version of a head table provided by David Magerman and used in the parser described in <TREF>Magerman 1995</TREF>~~~593 Collins Head-Driven Statistical Models for NL Parsing Internal rules Lexical rules TOP  S JJ  Last S  NP NP VP NN  week NP  JJ NN NNP  IBM NP  NNP VBD  bought VP  VBD NP NNP  Lotus NP  NNP Figure 1 A nonlexicalized parse tree and a list of the rules it contains~~~Internal Rules: TOP  Sbought,VBD Sbought,VBD  NPweek,NN NPIBM,NNP VPbought,VBD NPweek,NN  JJLast,JJ NNweek,NN NPIBM,NNP  NNPIBM,NNP VPbought,VBD  VBDbought,VBD NPLotus,NNP NPLotus,NNP  NNPLotus,NNP Lexical Rules: JJLast,JJ  Last NNweek,NN  week NNPIBM,NNP  IBM VBDbought,VBD  bought NNPLotus,NN  Lotus Figure 2 A lexicalized parse tree and a list of the rules it contains.
Similar observations have been made in the context of tagging problems using maximum-entropy models <REF>Lafferty, McCallum, and Pereira 2001</REF>; <REF>Klein and Manning 2002</REF>~~~We first analyze the model of <TREF>Magerman 1995</TREF> through three common examples of ambiguity: PP attachment, coordination, and appositives~~~In each case a word sequence S has two competing structures, T 1 and T 2, with associated decision sequences d 1,  , d n  and e 1,  , e m , respectively~~~Thus the probability of the two structures can be written as PT 1 S productdisplay i1n Pd i d 1 d i1, S PT 2 S productdisplay i1m Pe i e 1 e i1, S It will be useful to isolate the decision between the two structures to a single probability term.
<REF>Charniak 2000</REF> describes a series of enhancements to the earlier model of <REF>Charniak 1997</REF>~~~The precision and recall of the traces found by Model 3 were 938 and 901, respectively out of 437 cases in section 23 of the treebank, where three criteria must be met for a trace to be correct: 1 It must be an argument to the correct headword; 2 It must be in the correct position in relation to that headword preceding or following; 15 <TREF>Magerman 1995</TREF> collapses ADVP and PRT into the same label; for comparison, we also removed this distinction when calculating scores~~~608 Computational Linguistics Volume 29, Number 4 Table 2 Results on Section 23 of the WSJ Treebank~~~LR/LP  labeled recall/precision.
82 A Comparison to the Models of Jelinek et al~~~1994, <TREF>Magerman 1995</TREF>, and <REF>Ratnaparkhi 1997</REF></REF> We now make a detailed comparison of our models to the history-based models of <REF>Ratnaparkhi 1997</REF></REF>, Jelinek et al~~~1994, and <TREF>Magerman 1995</TREF>~~~A strength of these models is undoubtedly the powerful estimation techniques that they use: maximum-entropy modeling in <REF>Ratnaparkhi 1997</REF> or decision trees in <REF>Jelinek et al 1994</REF> and <TREF>Magerman 1995</TREF>.
Another important differencethe ability of models 1, 2, and 3 to generalize to produce context-free rules not seen in training datawas described in section 74 Section 82 showed that the parsing models of <REF>Ratnaparkhi 1997</REF>, Jelinek et al~~~1994, and <TREF>Magerman 1995</TREF> can suffer from very similar problems to the label bias or observation bias problem observed in tagging models, as described in <REF>Lafferty, McCallum, and Pereira 2001</REF> and <REF>Klein and Manning 2002</REF>~~~635 Collins Head-Driven Statistical Models for NL Parsing Acknowledgments My PhD thesis is the basis of the work in this article; I would like to thank Mitch Marcus for being an excellent PhD thesis adviser, and for contributing in many ways to this research~~~I would like to thank the members of my thesis committeeAravind Joshi, Mark Liberman, Fernando Pereira, and Mark Steedmanfor the remarkable breadth and depth of their feedback.
For discussion of additional related work, chapter 4 of <REF>Collins 1999</REF> attempts to give a comprehensive review of work on statistical parsing up to around 1998~~~Of particular relevance is other work on parsing the Penn WSJ Treebank <REF>Jelinek et al 1994</REF>; <TREF>Magerman 1995</TREF>; <REF>Eisner 1996a, 1996b</REF>; <REF>Collins 1996</REF>; <REF>Charniak 1997</REF>; <REF>Goodman 1997</REF>; <REF>Ratnaparkhi 1997</REF>; <REF>Chelba and Jelinek 1998</REF>; <REF>Roark 2001</REF>~~~Eisner 1996a, 1996b describes several dependency-based models that are also closely related to the models in this article~~~<REF>Collins 1996</REF> also describes a dependency-based model applied to treebank parsing.
<REF>Charniaks 1997</REF> models will most likely perform quite differently with binarybranching trees for example, his current models will learn that rules such as VP  VSGPPare very rare, but with binary-branching structures, this context sensitivity will be lost~~~The models of <TREF>Magerman 1995</TREF> and <REF>Ratnaparkhi 1997</REF> use contextual predicates that would most likely need to be modified given a different annotation style~~~<REF>Goodmans 1997</REF> models are the exception, as he already specifies that the treebank should be transformed into his chosen representation, binary-branching trees~~~731 Representation Affects Structural, not Lexical, Preferences.
22 Statistical Parsers Pioneered by the IBM natural language group <REF>Fujisaki et al 1989</REF> and later pursued by, for example, <REF>Schabes, Roth, and Osborne 1993</REF>, Jelinek et al~~~1994, <TREF>Magerman 1995</TREF>, <REF>Collins 1996</REF>, and <REF>Charniak 1997</REF>, this approach decouples the issue of wellformedness of an input string from the problem of assigning a structure to it~~~These systems attempt to assign some structure to every input string~~~The rules to assign a structure to an input are extracted automatically from hand-annotated parses of large corpora, which are then subjected to smoothing to obtain reasonable coverage of the language.
During the last few years large treebanks have become available to many researchers, which has resulted in researches applying a range of new techniques for parsing systems~~~Most of the methods that are being suggested include some kind of Machine Learning, such as history based grammars and decision tree models <REF>Black et al , 1993</REF>; <TREF>Magerman, 1995</TREF>, training or inducing statistical grammars <REF>Black, Garside and Leech, 1993</REF>; <REF>Pereira and Schabes, 1992</REF>; <REF>Schabes et al , 1993</REF>, or other techniques <REF>Bod, 1993</REF>~~~Consequently, syntactical analysis has become an area with a wide variety of a algorithms and methods for learning and parsing, and b type of information used for learning and parsing sometimes referred to as feature set~~~These methods only could become popular through evaluation methods for parsing systems, such as Bracket Accuracy, Bracket Recall, Sentence Accuracy and Viterbi Score.
Factors contributing to the SCDG parsers performance are analyzed~~~Statistical parsing has been an important focus of recent research <TREF>Magerman, 1995</TREF>; <REF>Eisner, 1996</REF>; <REF>Charniak, 1997</REF>; <REF>Collins, 1999</REF>; <REF>Ratnaparkhi, 1999</REF>; <REF>Charniak, 2000</REF>~~~Several of these parsers generate constituents by conditioning probabilities on non-terminal labels, part-of-speech POS tags, and some headword information <REF>Collins, 1999</REF>; <REF>Ratnaparkhi, 1999</REF>; <REF>Charniak, 2000</REF>~~~They utilize non-terminals that go beyond the level of a single word and do not explicitly use lexical features.
Although we report promising results, parse selection that is sufficiently accurate for many practical applications will require a more lexicalised system~~~Magermans 1995 parser is an extension of the history-based parsing approach developed at IBM <REF>Black et al , 1993</REF> in which rules are conditioned on lexical and other essentially arbitrary information available in the parse history~~~In future work, we intend to explore a more restricted and semantically-driven version of this approach in which, firstly, probabilities are associated with different subcategorisation possibilities, and secondly, alternative predicateargument structures derived from the grammar are ranked probabilistically~~~However, the massively increased coverage obtained here by relaxing subcategorisation constraints underlines the need to acquire accurate and complete subcategorisation frames in a corpus-driven fashion, before such constraints can be exploited robustly and effectively with free text.
Schabes et al~~~1993 and <TREF>Magerman 1995</TREF> report results using the GEIG evaluation scheme which are numerically similar in terms of parse selection to those reported here, but achieve 100 coverage~~~However, their experiments are not strictly comparable because they both utilise more homogeneous and probably simpler corpora~~~The appendix gives an indication of the diversity of the sentences in our corpus.
In our experiments, the window starts at the sentence prior to that containing the token and extends back W the window size sentences~~~The choice to use sentences as the unit of distance is motivated by our intention to incorporate triggers of this form into a probabilistie treebank-based parser and tagger, such as <REF>Black et al , 1998</REF>; <REF>Black et al , 1997</REF>; <REF>Brill, 1994</REF>; <REF>Collins, 1996</REF>; <REF>Jelinek et al , 1994</REF>; <TREF>Magerman, 1995</TREF>; <REF>Ratnaparkhi, 1997</REF>~~~All such parsers and taggers of which we are aware use only intrasentential information in predicting parses or tags, and we wish to remove this information, as far as possible, from our results 7 The window was not allowed to cross a document boundary~~~The perplexity of the task before taking the trigger-pair information into account for tags was 2240 and for rules was 570.
Yet, only the labels are used to score the combination~~~Length  40 LP LR F1 Exact CB 0 CB <TREF>Magerman 1995</TREF> 849 846 126 566 <REF>Collins 1996</REF> 863 858 114 599 this paper 869 857 863 309 110 603 <REF>Charniak 1997</REF> 874 875 100 621 <REF>Collins 1999</REF> 887 886 090 671 Length  100 LP LR F1 Exact CB 0 CB this paper 863 851 857 288 131 572 Figure 8: Results of the final model on the test set section 23~~~capture this, DOMINATES-V marks all nodes which dominate any verbal node V, MD with a -V~~~This brought the cumulative F1 to 8691.
We describe several simple, linguistically motivated annotations which do much to close the gap between a vanilla PCFG and state-of-the-art lexicalized models~~~Specifically, we construct an unlexicalized PCFG which outperforms the lexicalized PCFGs of <TREF>Magerman 1995</TREF> and <REF>Collins 1996</REF> though not more recent models, such as <REF>Charniak 1997</REF> or <REF>Collins 1999</REF>~~~One benefit of this result is a much-strengthened lower bound on the capacity of an unlexicalized PCFG~~~To the extent that no such strong baseline has been provided, the community has tended to greatly overestimate the beneficial effect of lexicalization in probabilistic parsing, rather than looking critically at where lexicalized probabilities are both needed to make the right decision and available in the training data.
This approach was congruent with the great success of word n-gram models in speech recognition, and drew strength from a broader interest in lexicalized grammars, as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such as PP attachments <REF>Ford et al , 1982</REF>; <REF>Hindle and Rooth, 1993</REF>~~~In the following decade, great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized PCFG models <TREF>Magerman, 1995</TREF>; <REF>Charniak, 1997</REF>; <REF>Collins, 1999</REF>; <REF>Charniak, 2000</REF>; <REF>Charniak, 2001</REF>~~~However, several results have brought into question how large a role lexicalization plays in such parsers~~~<REF>Johnson 1998</REF> showed that the performance of an unlexicalized PCFG over the Penn treebank could be improved enormously simply by annotating each node by its parent category.
However, perhaps even more significant has been the lexicalization of the grammar formalisms being probabilistically modeled: crucially, all the recent, successful statistical parsers have in some way made use of bilexical dependencies~~~This includes both the parsers that attach probabilities to parser moves <TREF>Magerman, 1995</TREF>; <REF>Ratnaparkhi, 1997</REF>, but also those of the lexicalized PCFG variety <REF>Collins, 1997</REF>; <REF>Charniak, 1997</REF>~~~155 Even more crucially, the bilexical dependencies involve head-modifier relations hereafter referred to simply as head relations~~~The intuition behind the lexicalization of a grammar formalism is to capture lexical items idiosyncratic parsing preferences.
In those research, extracted lexical/semantic collocation is especially useful in terms of ranking parses in syntactic analysis as well as automatic construction of lexicon for NLP~~~For example, in the context of syntactic disambiguation, <REF>Black 1993</REF> and <TREF>Magerman 1995</TREF> proposed statistical parsing models based-on decisiontree learning techniques, which incorporated not only syntactic but also lexical/semantic information in the decision-trees~~~As lexical/semantic information, <REF>Black 1993</REF> used about 50 semantic categories, while <TREF>Magerman 1995</TREF> used lexicai forms of words~~~<REF>Collins 1996</REF> proposed a statistical parser which is based on probabilities of dependencies between head-words in the parse tree.
For example, in the context of syntactic disambiguation, <REF>Black 1993</REF> and <TREF>Magerman 1995</TREF> proposed statistical parsing models based-on decisiontree learning techniques, which incorporated not only syntactic but also lexical/semantic information in the decision-trees~~~As lexical/semantic information, <REF>Black 1993</REF> used about 50 semantic categories, while <TREF>Magerman 1995</TREF> used lexicai forms of words~~~<REF>Collins 1996</REF> proposed a statistical parser which is based on probabilities of dependencies between head-words in the parse tree~~~In those works, lexical/semantic collocation are used for ranking parses in syntactic analysis.
Other researchers have proposed automatically selecting the conditioning information for various states of the model, thus potentially increasing greatly the space of possible features and selectively choosing the best predictors for each situation~~~Decision trees have been applied for feature selection for statistical parsing models by <TREF>Magerman 1995</TREF> and Haruno et al~~~1998~~~Another example of automatic feature selection for parsing is in the context of a deterministic parsing model that chooses parse actions based on automatically induced decision structures over a very rich feature set <REF>Hermjakob and Mooney, 1997</REF>.
We grew the trees fully and we calculated final expansion probabilities at the leaves by linear interpolation with estimates one level above~~~This is a similar, but more limited, strategy to the one used by <TREF>Magerman 1995</TREF>~~~The features over derivation trees which we made available to the learner are shown in Table 1~~~The node direction features indicate whether a node is a left child, a right child, or a single child.
Figure 1a shows a parse tree in the Penn Treebank style for the English translation in Ex 1~~~Given a parse tree, we use a head percolation table <TREF>Magerman, 1995</TREF> to create the corresponding dependency structure~~~Figure 2a shows the dependency structure derived from the parse tree in Figure 1a~~~32 Word alignment Because most of the 700 languages in ODIN are low-density languages with no on-line bilingual dictionariesorlargeparallelcorpora, aligning the source sentence and its English translation directly would not work well.
Information regarding constituent heads, POS tags, and lexical information is pertinent, as is information on constituent ordering and other grammatical information present in the data~~~Most or all of these factors are considered in some form or another by current state-of-the-art statistical parsers such as those of <REF>Charniak 1997</REF>, <TREF>Magerman 1995</TREF> and <REF>Collins 1996</REF>~~~In the present approach, each feature in the feature set corresponds to a depth-one tree structure in the data, ie a mother node and all of its daughters~~~Within this general structure various schemata may be used to derive actual features, where the information about each node employed in the feature is determined by which schema is used.
Inparticular,weleveragethe increasing availability of off-the-shelf parsers such as <REF>Charniak, 2001</REF>; <REF>Charniak, 2005</REF> to automatically or semi-automatically assign syntactic analyses to a set of suggested output sentences~~~We then draw on lexicalization techniques for statistical language models <TREF>Magerman, 1995</TREF>; <REF>Collins, 1999</REF>; <REF>Chiang, 2000</REF>; <REF>Chiang, 2003</REF> to induce a probabilistic, lexicalized tree-adjoining grammar that supports the derivation of all the suggested output sentences, and many others besides~~~The final step is to use the training examples to learn an effective search policy so that our run-time generation component can find good output sentences in a reasonable time frame~~~In particular, we use variants of existing search optimization <REF>Daum and Marcu, 2005</REF> and ranking algorithms <REF>Collins and Koo, 2005</REF> to train our run-time component to find good outputs within a specified time window; see also <REF>Stent et al, 2004</REF>; <REF>Walker et al, 2001</REF>.
In the first stage, a collection of rules is used to automatically decorate the training syntax with a number of features~~~These include deciding the lexical anchors for each non-terminal constituent and assigning complement/adjunct status for nonterminals which are not on their parents lexicalization path; see <TREF>Magerman, 1995</TREF>; <REF>Chiang, 2003</REF>; <REF>Collins, 1999</REF>~~~In addition, we deterministically add features to improve several grammatical aspects, including 1 enforcing verb inflectional agreement in derived trees, 2 enforcing consistency in the finiteness of VP and S complements, and 3 restricting subject/direct object/indirect object complements to play the same grammatical role in derived trees~~~In the second stage, the complements and adjuncts in the decorated trees are incrementally re80 syntax: cat: SA fin:other,  cat: S cat: NP,  apr: VBP, apn: other pos: PRP we fin:yes,  cat: VP apn: other,  pos: VBP do pos: RB nt fin: yes,  cat: VP, gra: obj1 fin: yes,  cat: VP, gra: obj1 pos: VBP have cat: NP,  gra: obj1 operations: initial tree comp semantics: speech-actaction  assert speech-actcontentpolarity  negative speech-actcontentattribute  resourceAttribute syntax: cat: NP,  apr: VBP, gra: obj1,  apn: other pos: JJ medical pos: NNS supplies cat: ADVP,  gra: adj pos: RB here cat: NP,  apr: VBZ, gra: adj,  apn: 3ps pos: NN captain operations: comp left/right adjunction left/right adjunction semantics: speech-actcontentvalue  medical-supplies speech-actcontentobject-id  market addressee  captain-kirk dialogue-actaddressee  captain-kirk speech-actaddressee  captain-kirk Figure 2: The linguistic resources inferred from the training example in Figure 1.
There is both empirical evidence and linguistic intuition to indicate that semantic features can enhance parse disambiguation performance, however~~~For example, a number of different parsers have been shown to benet from lexicalisation, that is, the conditioning of structural features on the lexical head of the given constituent <TREF>Magerman, 1995</TREF>; <REF>Collins, 1996</REF>; <REF>Charniak, 1997</REF>; <REF>Charniak, 2000</REF>; <REF>Collins, 2003</REF>~~~As an example of lexicalisation, we may observe in our training data that knife often occurs as the manner adjunct of open in prepositional phrases headed by with cf open with a knife, which would provide strong evidence for with a knife attaching to open and not box in open the box with a knife~~~It would not, however, provide any insight into the correct attachment of with scissors in open the box with scissors, as the disambiguation model would not be able to predict that knife and scissors are semantically similar and thus likely to have the same attachment preferences.
Whats more, on top of the large undertaking of designing and implementing a statistical parsing model, the use of heuristics has required a further e ort, forcing the researcher to bring both linguistic intuition and, more often, engineering savvy to bear whenever moving to a new treebank~~~For example, in the rule sets used by the parsers described in <TREF>Magerman, 1995</TREF>; <REF>Ratnaparkhi, 1997</REF>; <REF>Collins, 1999</REF>, the sets of rules for finding the heads of ADJP, ADVP, NAC, PP and WHPP include rules for picking either the rightmost or leftmost FW foreign word~~~The apparently haphazard placement of these rules that pick out FW and the rarity of FW nodes in the data strongly suggest these rules are the result of engineering e ort~~~Furthermore, it is not at all apparent that tree-transforming heuristics that are useful for one parsing model will be useful for another.
In fact, nearly identical head-lexicalizations were used in the disScaughtVBD NPboyNN DET The NN boy ADVPalsoRB RB also VPcaughtVBD VBD caught NPballNN DET the NN ball Figure 2: A simple lexicalized parse tree~~~criminative models described in <TREF>Magerman, 1995</TREF>; <REF>Ratnaparkhi, 1997</REF>, the lexicalized PCFG models in <REF>Collins, 1999</REF>, the generative model in <REF>Charniak, 2000</REF>, the lexicalized TAG extractor in <REF>Xia, 1999</REF> and the stochastic lexicalized TAG models in <REF>Chiang, 2000</REF>; <REF>Sarkar, 2001</REF>; <REF>Chen and VijayShanker, 2000</REF>~~~Inducing a lexicalized structure based on heads has a two-pronged e ect: it not only allows statistical parsers to be sensitive to lexical information by including this information in the probability models dependencies, but it also determines which of all possible dependencies both syntactic and lexicalwill be included in the model itself~~~For example, in Figure 2, the nonterminal NPboyNN is dependent on VPcaughtVBD and not the other way around.
Once enriched, the data can be used as a bootstrap for tools such as taggers~~~311 Enriching IGT In a previous study <REF>Xia and Lewis, 2007</REF>, we proposed a three-step process to enrich IGT data: 1 parse the English translation with an English parser and convert English phrase structures PS into dependency structures DS with a head percolation table <TREF>Magerman, 1995</TREF>, 2 align the target line and the English translation using the gloss line, and 3 project the syntactic structures both PS and DS from English onto the target line~~~For instance, given the IGT example in Ex 1, the enrichment algorithm will produce the word alignment in Figure 1 and the syntactic structures in Figure 2~~~The  teacher  gave  a  book  to    the    boy   yesterday Rhoddodd  yr   athro     lyfr     ir     bachgen  ddoe  Gloss line:  Translation: Target line: gave-3sg  the  teacher book  to-the  boy   yesterday Figure 1: Aligning the target line and the English translation with the help of the gloss line 533 gave a Projecting DS athro bachgen lyfr yr ddoeir  Rhoddodd S NP1 VP NN teacher VBD   gave NP2 DT a NP4PP NN the IN NP3 yesterday NN DT book NN boy DT to S NP NN VBD NP NPPP NN INDT NN NNDT   rhoddodd   gave yrthe    athro teacher lyfr book     ir to-the bachogen boy ddoe yesterday teacher a boy the book the yesterdayto The b Projecting PS Figure 2: Projecting syntactic structure from English to the target language We evaluated the algorithm on a small set of 538 IGT instances for several languages.
I Introduction Research on syntactic parsing has been a focus in natural language processing for a long lime~~~As the developlnent of corpus linguistics, many statistics-based parsers were proposed, such as <TREF>Magerman1995</TREF>s statistical decision tree parser, <REF>Collins1996</REF>s bigram dependency model parser, 1;/atnaparkhi1997s maximum entropy model parser~~~All of lhem fried to get the complete parse trees of the input sentences, based on the statistical data extracted lrom an annotated corpus~~~The besl parsing accuracy of these parsers was about 87.
The extraction procedure determines the structure of each elementary tree by localizing dependencies through the use of heuristics~~~Salient heuristics include the use of a head percolation table <TREF>Magerman, 1995</TREF>, and another table that distinguishes between complements and adjunct nodes in the tree~~~For our current work, we use the head percolation table to determine heads of phrases~~~Also, we treat a PropBank argument ARG0 : : : ARG9 as a complement and a PropBank adjunct ARGMs as an adjunct when such annotation is available1 Otherwise, we basically follow the approach of <REF>Chen, 2001</REF>2 Besides introducing one kind of TAG extraction 1The version of the PropBank we are using is not fully annotated with semantic role information, although the most common predicates are.
unlike MaxEnt, cannot be used as a probabilistic component in a larger model~~~MaxEnt can provide a probability for each tagging decision, which can be used in the probability calculation of any structure that is predicted over the POS tags, such as noun phrases, or entire parse trees, as in <REF>Jelinek et al , 1994</REF>, <TREF>Magerman, 1995</TREF>~~~Thus MaxEnt has at least one advantage over each of the reviewed POS tagging techniques~~~It is better able to use diverse information than Markov Models, requires less supporting techniques than SDT, and unlike TBL, can be used in a probabilistic framework.
Since most realistic natural language applications must process words that were never seen before in training data, all experiments in this paper are conducted on test data that include unknown words~~~Several recent papers<REF>Brill, 1994</REF>, <TREF>Magerman, 1995</TREF> have reported 965 tagging accuracy on the Wall St Journal corpus~~~The experiments in this paper test the hypothesis that better use of context will improve the accuracy~~~A Maximum Entropy model is well-suited for such experiments since it cornbines diverse forms of contextual information in a principled manner, and does not impose any distributional assumptions on the training data.
The total accuracy is higher, implying that the singly-annotated training and test sets are more consistent, and the improvement due to the specialized features is higher than before 1 but still modest, implying that either the features need further improvement or that intra-annotator inconsistencies exist in the corpus~~~Comparison With Previous Work Most of the recent corpus-based POS taggers in the literature are either statistically based, and use Markov Model<REF>Weischedel et al , 1993</REF>, <REF>Merialdo, 1994</REF> or Statistical Decision Tree<REF>Jelinek et al , 1994</REF>, <TREF>Magerman, 1995</TREF>SDT techniques, or are primarily rule based, such as Drills Transformation Based Learner<REF>Drill, 1994</REF>TBL~~~The Maximum Entropy MaxEnt tagger presented in this paper combines the advantages of all these methods~~~It uses a rich feature representation, like TBL and SDT, and generates a tag probability distribution for each word, like Decision Tree and Markov Model techniques.
140 A POS tagger is one component in the SDT based statisticM parsing system described in <REF>Jelinek et al , 1994</REF>, <TREF>Magerman, 1995</TREF>~~~The total word accuracy on Wall St Journal data, 965<TREF>Magerman, 1995</TREF>, is similar to that presented in this paper~~~However, the aforementioned SDT techniques require word classes<REF>Brown et al , 1992</REF> to help prevent data fragmentation, and a sophisticated smoothing algorithm to mitigate the effects of any fragmentation that occurs~~~Unlike SDT, the MaxEnt training procedure does not recursively split the data, and hence does not suffer from unreliable counts due to data fragmentation.
In contrast, the MaxEnt model combines diverse and non-local information sources without making any independence assumptions~~~140 A POS tagger is one component in the SDT based statisticM parsing system described in <REF>Jelinek et al , 1994</REF>, <TREF>Magerman, 1995</TREF>~~~The total word accuracy on Wall St Journal data, 965<TREF>Magerman, 1995</TREF>, is similar to that presented in this paper~~~However, the aforementioned SDT techniques require word classes<REF>Brown et al , 1992</REF> to help prevent data fragmentation, and a sophisticated smoothing algorithm to mitigate the effects of any fragmentation that occurs.
There have been a number of methods proposed to perform structural disambiguation using probability models, many of which have proved to be quite effective <REF>Alshawi and Carter, 1995</REF>; <REF>Black et al , 1992</REF>; Briscoe and Carroll~~~1993; <REF>Chang et al , 1992</REF>; <REF>Collins and Brooks, 1995</REF>; <REF>Fujisaki, 1989</REF>; <REF>Hindle and Rooth, 1991</REF>; <REF>Hindle and Rooth, 1993</REF>; <REF>Jelinek et al , 1990</REF>; <REF>Magerman and Marcus, 1991</REF>; <TREF>Magerman, 1995</TREF>; <REF>Ratnaparkhi et al , 1994</REF>; <REF>Resnik, 1993</REF>; <REF>Su and Chang, 1988</REF>~~~Although each of the disambiguation methods proposed to date has its merits, none resolves the disambiguation problem completely satisfactorily~~~We feel that it is necessary to devise a new method that unifies the above two approaches, ie, to implement psycholinguistic principles of disambiguation on the basis of a probabilistic methodology.
6 Conclusion It is worth noting that while we have presented the use of edge-based best-first chart parsing in the service of a rather pure form of PCFG parsing, there is no particular reason to assume that the technique is so limited in its domain of applicability~~~One can imagine the same techniques coupled with more informative probability distributions, such as lexicalized PCFGs <REF>Charniak, 1997</REF>, or even grammars not based upon literal rules, but probability distributions that describe how rules are built up from smaller components <TREF>Magerman, 1995</TREF>; <REF>Collins, 1997</REF>~~~Clearly further research is warranted~~~Be this as it may, the take-home lesson from this paper is simple: combining an edge-based agenda with the figure of merit from CC  is easy to do by simply binarizing the grammar  provides a factor of 20 or so reduction in the number of edges required to find a first parse, and  improves parsing precision and recall over exhaustive parsing.
Chen and Vijay-<REF>Shanker 2000</REF> explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing~~~The extraction procedure utilizes a head percolation table as introduced by <TREF>Magerman 1995</TREF> in combination with a variation of <REF>Collinss 1997</REF> approach to the differentiation between complement and adjunct~~~This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question~~~The number of frame types extracted ie , an elementary tree without a specific lexical anchor ranged from 2,366 to 8,996.
uk black,eubank,kashiokaatritlcojp GLeechOcentllancsacuk 1 Introduction A treebank is a body of natural language text which has been grammatically annotated by hand, in terms of some previously-established scheme of grammatical analysis~~~Treebanks have been used within the field of natural language processing as a source of training data for statistical part og speech taggers <REF>Black et al , 1992</REF>; <REF>Brill, 1994</REF>; <REF>Merialdo, 1994</REF>; <REF>Weischedel et al , 1993</REF> and for statistical parsers <REF>Black et al , 1993</REF>; <REF>Brill, 1993</REF>; aelinek et al , 1994; <TREF>Magerman, 1995</TREF>; <REF>Magerman and Marcus, 1991</REF>~~~In this article, we present the ATR/Lancaster 7reebauk of American English, a new resource tbr natural-language-, processing research, which has been prepared by Lancaster University UKs Unit for Computer Research on the English Language, according to specifications provided by ATR Japans Statistical Parsing Group~~~First we provide a static description, with a a discussion of the mode of selection and initial processing of text for inclusion in the treebank, and b an explanation of the scheme of grammatical annotation we then apply to the text.
In this framework, a rough grammar is first learned from a bracketed corpus and then the grammar is refined by the combination of rulebased and corpus-based methods~~~Unlike stochastic parsing such as <TREF>Magerman, 1995</TREF><REF>Collins, 1996</REF>, our approach can parse sentences which fall out the current grammar and suggest the plausible hypothesis rules and the best parses~~~The grammar is not acquired from scratch like the approaches shown in 82 Sent~~~Length Comparisons Avg.
Formulating it using Gorns notation and the L; and 2 variables, though, is concise, elegant; and novel~~~Nothing prevents conditioning the random variables on arbitrary portions of Clio 1artial tree generated this far, using, eg, maximum-entroly or decision-tree models to extract relevant tatnres of it; there is no difference 689 in principle between our model and history-based parsing, see Black el; al , 1993; <TREF>Magerman, 1995</TREF>~~~The proposed treatment of string realization through the use of the,5 and A4 variables is also both truly novel and important~~~While phrase-structure grammars overemphasize word order by making the processes generating the S variables deterministic, Tesni6re treats string realization as a secondary issue.
That approach is based on a conversion from constituent structure to dependency structure by recursively defining a head for each constituent~~~The same idea was used by <TREF>Magerman 1995</TREF>, who developed the first head table for the Penn Treebank <REF>Marcus et al , 1994</REF>, and <REF>Collins 1996</REF>, whose constituent parser is internally based on probabilities of bilexical dependencies, ie dependencies between two words~~~<REF>Collins 1997</REF>s parser and its reimplementation and extension by <REF>Bikel 2002</REF> have by now been applied to a variety of languages: English <REF>Collins, 1999</REF>, Czech <REF>Collins et al , 1999</REF>, German <REF>Dubey and Keller, 2003</REF>, Spanish <REF>Cowan and Collins, 2005</REF>, French <REF>Arun and Keller, 2005</REF>, Chinese <REF>Bikel, 2002</REF> and, according to Dan Bikels web page, Arabic~~~<REF>Eisner 1996</REF> introduced a data-driven dependency parser and compared several probability models on English Penn Treebank data.
In the first iteration, the chunker identifies two base phrases, NP Estimated volume and QP 24 million, and replaces each phrase with its nonterminal symbol and head~~~The head word is identified by using the head-percolation table <TREF>Magerman, 1995</TREF>~~~In the second iteration, the chunker identifies NP a light million ounces and converts this phrase into NP~~~This chunking procedure is repeated until the whole sentence is chunked at the fourth iteration, and the full parse tree is easily recovered from the chunking history.
Moreover, the results of a less-than-optimal version of DOP on the Wall Street Journal corpus suggest that the approach can be succesfully extended to larger domains~~~As future research, we will apply the full DOP model on WSJ word strings in order to compare our results with the best known parsers on this domain <TREF>Magerman, 1995</TREF>; <REF>Collins, 1996</REF>~~~Acknowledgements I am grateful to Remko Scha for many useful comments and additions~~~I also thank three anonymous reviewers for their comments.
Stochastic parsing systems either use a closed lexicon, or use a two step approach where first the words are tagged 133 by a stochastic tagger, after which the p-o-s tags with or without the words are parsed by a stochastic parser~~~The latter approach has become increasingly popular eg <REF>Schabes et al , 1993</REF>; <REF>Weischedel et al , 1993</REF>; <REF>Briscoe, 1994</REF>; <TREF>Magerman, 1995</TREF>; <REF>Collins, 1996</REF>~~~Notice, however, that the tagger used in this two step approach often uses Good-Turing or a similar smoothing method to adjust the observed frequencies of n-grams~~~So why not apply Good-Turing directly to the structural units of a stochastic grammar.
Incorporating semantic classes and verb alternation behavior could improve such models performance~~~Automatically derived word clusters are used in the statistical parsers of <REF>Charniak 1997</REF> and <TREF>Magerman 1995</TREF>~~~Incorporating alternation behavior into such models might improve parsing results as well~~~This paper focuses on evaluating probabilistic models of verb-argument structure in terms of how well they model unseen test data, as measured by perplexity.
We calculate the precision, recall, and Fscore; however for brevitys sake we only report the F-score for most experiments in this section~~~In addition to antecedent recovery, we also report parsing accuracy, using the bracketing F-Score, the combined measure of PARSEVAL-style labeled bracketing precision and recall <TREF>Magerman, 1995</TREF>~~~44 Results The results of the experiments are summarized in Table 3~~~UNLEX and LEX refer to the unlexicalized and lexicalized models, respectively.
Given sentence-aligned bi-lingual training data, we first use GIZA <REF>Och and Ney, 2003</REF> to generate word level alignment~~~We use a statistical CFG parser to parse the English side of the training data, and extract dependency trees with Magermans rules 1995~~~Then we use heuristic rules to extract transfer rules recursively based on the GIZA alignment and the target dependency trees~~~The rule extraction procedure is as follows.
As for the learning technique, we used maximum entropy models, speci cally the implementation called MegaM provided by Hal Daume Daume III, 2004~~~For P S, we needed features  40  100 CB 0CB CB 0CB <TREF>Magerman 1995</TREF> 126 566 <REF>Collins 1996</REF> 114 599 Klein/<REF>Manning 2003</REF> 110 603 131 572 this paper 109 582 125 552 <REF>Charniak 1997</REF> 100 621 <REF>Collins 1999</REF> 090 671 Figure 8: Cross-bracketing results for Section 23 of the Penn Treebank~~~that would be relevant to deciding whether a given span i,j should be considered a constituent~~~The basic building blocks we used are depicted in Figure 7.
Thus one trades assurances of polynomial running time for greater modeling exibility~~~There are two canonical parsers that fall into this category: the decision-tree parser of <TREF>Magerman, 1995</TREF>, and the maximum-entropy parser of <REF>Ratnaparkhi, 1997</REF>~~~Both showed decent results on parsing the Penn Treebank, but in the decade since these papers were published, history-based parsers have been largely ignored by the research community in favor of PCFG-based approaches~~~There are several reasons why this may be.
More precisely, the approach is based on four essential components:  A transition-based deterministic algorithm for building labeled projective dependency graphs in linear time <REF>Nivre, 2003</REF>~~~History-based feature models for predicting the next parser action <REF>Black et al, 1992</REF>; <TREF>Magerman, 1995</TREF>; <REF>Ratnaparkhi, 1997</REF>~~~Discriminative classifiers for mapping histories to parser actions <REF>Kudo and Matsumoto, 2002</REF>; <REF>Yamada and Matsumoto, 2003</REF>~~~Pseudo-projectiveparsingforrecoveringnonprojective structures <REF>Nivre and Nilsson, 2005</REF>.
The SPATTER decision trees use predicates on word classes created with a statistical clustering technique, whereas the maximum entropy parser uses predicates that contain merely the words themselves, and thus lacks the need for a typically expensive word clustering procedure~~~Furthermore, the top K BFS search heuristic appears to be much simpler than the stack decoder algorithm outlined in <TREF>Magerman, 1995</TREF>~~~77 Conclusion The maximum entropy parser presented here achieves a parsing accuracy which exceeds the best previously published results, and parses a test sentence in linear observed time, with respect to the sentence length~~~It uses simple and concisely speci fled predicates which can added or modified quickly with little human effort under the maximum entropy framework.
For this reason, research into reranking schemes appears to be a promising step towards the goal of improving parsing accuracy~~~6 Comparison With Previous Work The two parsers which have previously reported the best accuracies on the Penn Treebank Wall St Journal are the bigram parser described in <REF>Collins, 1996</REF> and the SPATTER parser described in <REF>Jelinek et al , 1994</REF>; <TREF>Magerman, 1995</TREF>~~~The parser presented here outperforms both the bigram parser and the SPATTER parser, and uses different modelling technology and different information to drive its decisions~~~The bigram parser is a statistical CKY-style chart parser, which uses cooccurrence statistics of headmodifier pairs to find the best parse.
Figure 9 shows that the perfect scheme would achieve roughly 93 precision and recall, which is a dramatic increase over the top 1 accuracy of 87 precision and 86 recall~~~Figure 10 shows that the Exact Match, which counts the percentage of times 2Results for SPATTER on section 23 are reported in <REF>Collins, 1996</REF></REF> Parser Precision Maximum Entropy  868 Maximum Entropy 875 <REF>Collins, 1996</REF></REF> 857 <TREF>Magerman, 1995</TREF> 843 Recall 856 863 853 840 Table 5: Results on 2416 sentences of section 23 0 to 100 words in length of the WSJ Treebank~~~Evaluations marked with  ignore quotation marks~~~Evaluations marked with  collapse the distinction between ADVP and PRT, and ignore all punctuation.
The PARSEVAL Black and others, 1991 measures compare a proposed parse P with the corresponding correct treebank parse T as follows:  correct constituents in P Recall   constituents in T  correct constituents in P Precision   constituents in P A constituent in P is correct if there exists a constituent in T of the same label that spans the same words~~~Table 5 shows results using the PARSEVAL measures, as well as results using the slightly more forgiving measures of <REF>Collins, 1996</REF> and <TREF>Magerman, 1995</TREF>~~~Table 5 shows that the maximum entropy parser performs better than the parsers presented in <REF>Collins, 1996</REF> and <TREF>Magerman, 1995</TREF> , which have the best previously published parsing accuracies on the Wall St Journal domain~~~It is often advantageous to produce the top N parses instead of just the top 1, since additional information can be used in a secondary model that reorders the top N and hopefully improves the quality of the top ranked parse.
Table 5 shows results using the PARSEVAL measures, as well as results using the slightly more forgiving measures of <REF>Collins, 1996</REF> and <TREF>Magerman, 1995</TREF>~~~Table 5 shows that the maximum entropy parser performs better than the parsers presented in <REF>Collins, 1996</REF> and <TREF>Magerman, 1995</TREF> , which have the best previously published parsing accuracies on the Wall St Journal domain~~~It is often advantageous to produce the top N parses instead of just the top 1, since additional information can be used in a secondary model that reorders the top N and hopefully improves the quality of the top ranked parse~~~Suppose there exists a perfect reranking scheme that, for each sentence, magically picks the best parse from the top N parses produced by the maximum entropy parser, where the best parse has the highest average precision and recall when compared to the treebank parse.
For example, an actual contextual predicate based on the template cons0 might be Does cons0   NP, he  ~~~Constituent head words are found, when necessary, with the algorithm in <TREF>Magerman, 1995</TREF>~~~Contextual predicates which look at head words, or especially pairs of head words, may not be reliable predictors for the procedure actions due to their sparseness in the training sample~~~Therefore, for each lexically based contextual predicate, there also exist one or more corresponding less specific, or backed-off, contextual predicates which look at the same context, but omit one or more words.
The algorithm exploits robust lexical, syntactic, and semantic knowledge sources~~~I Introduction The application of decision-based learning techniques over rich sets of linguistic features has improved significantly the coverage and performance of syntactic and to various degrees semantic parsers <REF>Simmons and Yu, 1992</REF>; <TREF>Magerman, 1995</TREF>; <REF>Hermjakob and Mooney, 1997</REF>~~~In this paper, we apply a similar paradigm to developing a rhetorical parser that derives the discourse structure of unrestricted texts~~~Crucial to our approach is the reliance on a corpus of 90 texts which were manually annotated with discourse trees and the adoption of a shift-reduce parsing model that is well-suited for learning.
The matrix shows that the segmenter has problems mostly with identifying the beginning of parenthetical units and the intra-sentential edu boundaries; for example, it correctly identifies only 133 of the 220 ZLeaming from binary representations of features in the Brown corpus was too computationally expensive to terminate -the Brown data file had about 05GBytes~~~5 The shift-reduce action identifier, 51 Generation of learning examples The learning cases were generated automatically, in the style of <TREF>Magerman 1995</TREF>, by traversing inorder the final rhetorical structures built by annotators and by generating a sequence of discourse parse actions that used only SHIFT and REDUCE operations of the kinds discussed in section 3~~~When a derived sequence is applied as described in the parsing model, it produces a rhetorical tree that is a one-to-one copy of the original tree that was used to generate the sequence~~~For example, the tree at the bottom of figure 1 -the tree found at the top of the stack at step i  4 -can be built if the following sequence of operations is performed: SHIFT 12; SHIFT 13; REDUCE-ATTRIBUTION-NS; SHIFT 14; REDUCE-JOINT-NN; SHIFT 15; REDUCE-CONTRASTSN, SHIFT 16, SHIFT 7; REDUCE-CONDITIONSN; SHIFT 18; SHIFT 19; REDUCE-APPOSITION-NS; REDUCE-ATTRIBUTION-NS; REDUCE-ELABORATIONNS.
If for any tuple Ts, Tt, C> such a sequence of actions can be derived, it is then possible to use a corpus of Ts, Tt, C tuples in order to automatically learn to derive from an unseen tree Ts,, which has the same structural properties as the trees Ts, a tree Ttj, which has structural properties similar to those of the trees Tt~~~In order to solve the problem in definition 31, we extend the shift-reduce parsing paradigm applied by <TREF>Magerman 1995</TREF>, <REF>Hermjakob and Mooney 1997</REF>, and <REF>MarcH 1999</REF>~~~In this extended paradigm, the transfer process starts with an empty Stack and an Input List that contains a sequence of elementary discourse trees edts, one edt for each edu in the tree Ts given as input~~~The status and rhetorical relation associated with each edt is undefined.
If an erroneous string is extracted, its errors will propagate through the rest of the input :trings~~~:3 Our Approach 31 The C45 Learning Algorithm Decision tree induction algorithms have been successfully applied for NLP problems such as sentence boundary dismnbiguation <REF>Pahner et al 1997</REF>, parsing <TREF>Magerman 1995</TREF> and word segmentation <REF>Mekuavin et al 1997</REF>~~~We employ the c45 <REF>Quinhln 1993</REF> decision tree induction program as the learning algorithm for word extraction~~~The induction algorithm proceeds by evaluating content of a series of attributes and iteratively building a tree fiom the attribute values with the leaves of the decision tree being the value of the goal attribute.
By implementing our own version of the publicly available Collins parser <REF>Collins, 1996</REF>, we also learned a dependency model that enables the mapping of parse trees into sets of binary relations between the head-word of each constituent and its sibling-words~~~For example, the parse tree of TREC-9 question Q210: How many dogs pull a sled in the Iditarod  is: JJ S Iditarod VP NP PP NP NNPDTINNN NP DTVBPNNS NP manyHow WRB dogs pull a sled in the For each possible constituent in a parse tree, rules first described in <TREF>Magerman, 1995</TREF> and <REF>Jelinek et al , 1994</REF> identify the head-child and propagate the head-word to its parent~~~For the parse of question Q210 the propagation is: NP sled DT NN DTIN manyHow WRB dogs NNSJJ NP dogs VBP pull a sled in the Iditarod NNP Iditarod NP Iditarod PP Iditarod NP sled VP pull S pull When the propagation is over, head-modifier relations are extracted, generating the following dependency structure, called question semantic form in <REF>Harabagiu et al , 2000</REF>~~~dogs IditarodCOUNT pull sled In the structure above, COUNT represents the expected answer type, replacing the question stem how many.
Two new heavyweight algorithms were developed in the last year~~~One is a full parser of English, using a statistically learned decision procedure; SPATTER has achieved the highest scores yet reported on parsing English text <TREF>Magerman, 1995</TREF>~~~Because the measurable improvement in parsing is so great compared to manually constructed parsers, it appears to offer a qualitatively better parser~~~We are looking ormat De scription Message Message Reader I M orphologieal Analyzer  Lexieai Pattern Matcher  Fast Partial Parser  Semantic Interpreter  entene e-Level Pattern Matcher Discourse  Format  S GML Handling Initial Iden tification of Entities Grouping Words into Meaningful Phrases Establish Relationships within sentences Establish Relationships Overall ----Template/Annotation Generator I Output Entities and Relationships  Output Figure 2: PLUM System Architecture: Rectangles represent domain-independent, language-independent algorithms; ovals represent knowledge bases.
For example, statistical techniques may have suggested the importance of hire, a verb which many groups did not happen to define~~~Second, since there has been a marked improvement in the quality of full parsers, now achieving an F in the high 80s <TREF>Magerman, 1995</TREF>, we believe it is now feasible to consider using full parsers again~~~The rationale is straightforward: for full templates eg , ST scores have been mired with an F in the 50s ever since MUC-3 in 1991~~~Pattern matching has given us very robust, very portable technology, but has not broken the performance barrier all systems have run up against.
137 applicable information extraction functionality such as NE and TE will be a win, maximizing the value of defining reusable knowledge bases for information extraction~~~We developed a full parser of English, using a statistically learned decision procedure; SPATTER has achieved the highest scores yet report on parsing English text <TREF>Magerman, 1995</TREF>~~~The fact that its recall and precision are both in the high 80s represents not just a quantitative improvement in parser performance, but also a qualitative improvement~~~The NLU Shell provides a way for nonprogrammers to build and maintain information extraction systems based on PLUM.
B,~~~I I  As in the case of the models of <REF>Black 1993</REF>, <TREF>Magerman 1995</TREF>, and <REF>Collins 1996</REF>, this paper proposes a method of utilizing lexical/semantic features for the purpose of applying them to ranking parses in syntactic analysis~~~However, unlike the models of <REF>Black 1993</REF>, <TREF>Magerman 1995</TREF>, and <REF>Collins 1996</REF>, we put an assumption that syntactic and lexical/semantie features are independent~~~Then, we focus on extracting lexical/semantic collocational knowledge of verbs which is useful in syntactic analysis.
In those research, extracted lexical/semantic collocation is especially useful in terms of ranking parses in syntactic analysis as well as automatic construction of lexicon for NLP~~~For example, in the context of syntactic disambiguation, <REF>Black 1993</REF> and <TREF>Magerman 1995</TREF> proposed statistical parsing models based-on decision-tree learning techniques, which incorporated not only syntactic but also lexical/semantic information in the decision-trees~~~As lexical/semantic information, <REF>Black 1993</REF> used about 50 semantic categories, while <TREF>Magerman 1995</TREF> used lexical forms of words~~~<REF>Collins 1996</REF> proposed a statistical parser which is based on probabilities of dependencies between head-words in the parse tree.
I I  As in the case of the models of <REF>Black 1993</REF>, <TREF>Magerman 1995</TREF>, and <REF>Collins 1996</REF>, this paper proposes a method of utilizing lexical/semantic features for the purpose of applying them to ranking parses in syntactic analysis~~~However, unlike the models of <REF>Black 1993</REF>, <TREF>Magerman 1995</TREF>, and <REF>Collins 1996</REF>, we put an assumption that syntactic and lexical/semantie features are independent~~~Then, we focus on extracting lexical/semantic collocational knowledge of verbs which is useful in syntactic analysis~~~More specifically, we propose a novel method for learning a probabilistic model of subcategorization preference of verbs.
For example, in the context of syntactic disambiguation, <REF>Black 1993</REF> and <TREF>Magerman 1995</TREF> proposed statistical parsing models based-on decision-tree learning techniques, which incorporated not only syntactic but also lexical/semantic information in the decision-trees~~~As lexical/semantic information, <REF>Black 1993</REF> used about 50 semantic categories, while <TREF>Magerman 1995</TREF> used lexical forms of words~~~<REF>Collins 1996</REF> proposed a statistical parser which is based on probabilities of dependencies between head-words in the parse tree~~~In those works, lexical/semantic collocation are used for ranking parses in syntactic analysis.
A distinctive feature of our work is the fact that we used machine learning techniques to improve an existing rule-based natural language processor from the inside~~~This contrasts with approaches where there are essentially no explicit rules, such as neural networks eg <REF>Buo 1996</REF>, or approaches where the machine learning algorithms attempt to infer--via deduction eg <REF>Samuelsson 1994</REF>, induction eg <REF>Theeramunkong et al 1997</REF>; <REF>Zelle  Mooney 1994</REF> under user cooperation eg <REF>Simmons  Yu 1992</REF>; <REF>Hermjakob  Mooney 1997</REF>, transformation-based error-driven learning eg <REF>Brill 1993</REF>, or even decision trees eg <TREF>Magerman 1995</TREF>--a grammar from raw or preprocessed data~~~In our work, we do not wish to acquire a grammar: we have one and want to devise a mechanism to make some of its parts adaptable to the corpus at hand or, to improve some aspect of its performance~~~Other researchers, such as <REF>Lawrence et al 1996</REF>, have compared neural networks and machine learning methods at the task of sentence classification.
We have extended their work by significantly increasing the expressiveness of the parse action and feature languages, in particular by moving far beyond the few simple features that were limited to syntax only, by adding more background knowledge and by introducing a sophisticated machine learning component~~~<TREF>Magerman, 1995</TREF> uses a decision tree model similar to ours, training his system SPATTER~~~with parse action sequences for 40,000 Wall Street Journal sentences derived from the Penn Treebank <REF>Marcus et al , 1993</REF>~~~Questioning the traditional n-grams, Magerman already advocates a heavier reliance on contextual information.
The key to extraction of the relations is that any phrase can be substituted by the corresponding tree head-word links marked bold in Figure 1~~~To determine the tree head-word we used a set of rules similar to that described by <TREF>Magerman, 1995</TREF><REF>Jelinek et al , 1994</REF> and also used by <REF>Collins, 1996</REF>, which we modified in the following way:  The head of a prepositional phrase PP-IN NP was substituted by a function the name of which corresponds to the preposition, and its sole argument corresponds to the head of the noun phrase NP~~~The head of a subordinate clause was changed to a function named after the head of the first element in the subordinate clause usually that or a NULL element and its sole argument corresponds to the head of its second element usually head of a sentence~~~Because we assumed that the relations within the same phrase are independent, all the relations are between the modifier constituents and the head of a phrase only.
When inspecting manually, the binary word tree representation appears to be the most easy to understand~~~A second application of the binary word tree can be found in decision-tree based systems such as the SPATTER parser <TREF>Magerman, 1995</TREF> or the ATR Decision-Tree Part-Of-Speech Tagger, as described by Ushioda <REF>Ushioda, 1996</REF>~~~In this case it is necessary to use a hard-clustering method, such that a binary word tree can be constructed by the clustering process, as we did in the example in the previous sections~~~A decision tree classifies data according to its properties by asking successive often binary questions.
The mentioned studies use word-clusters for interpolated n-gram language models~~~Another application of hard clustering methods in particular bottom-up variants is that they can also produce a binary tree, which can be used for decision-tree based systems such as the SPATTER parser <TREF>Magerman, 1995</TREF> or the ATR Decision-Tree Part-OfSpeech Tagger <REF>Black et al , 1992</REF>, <REF>Ushioda, 1996</REF>~~~Hogenhout  Matsumoto 18 Word Clustering from Syntactic Behavior In this case a decision tree contains binary questions to decide the properties of a word~~~We present a hard clustering algorithm, in the sense that every word belongs to exactly one cluster or is one leaf in the binary word-tree of a particular part of speech.
These results have important implications for crosslinguistic parsing research, as they allow us to tease apart language-specific and annotationspecific effects~~~Previous work for English eg , <TREF>Magerman, 1995</TREF>; <REF>Collins, 1997</REF> has shown that lexicalization leads to a sizable improvement in parsing performance~~~English is a language with nonflexible word order and with a treebank with a nonflat annotation scheme see Table 2~~~Research on German <REF>Dubey and Keller, 2003</REF> showed that lexicalization leads to no sizable improvement in parsing performance for this language.
A second recent strand in parsing research has dealt with the role of lexicalization~~~The conventional wisdom since <TREF>Magerman 1995</TREF> has been that lexicalization substantially improves performance compared to an unlexicalized baseline model eg , a probabilistic context-free grammar, PCFG~~~However, this has been challenged by <REF>Klein and Manning 2003</REF>, who demonstrate that an unlexicalized model can achieve a performance close to the state of the art for lexicalized models~~~<REF>Furthermore, Bikel 2004</REF> provides evidence that lexical information in the form of bi-lexical dependencies only makes a small contribution to the performance of parsing models such as <REF>Collinss 1997</REF>.
342 Decision Tree~~~Algorithms for decision tree induction <REF>Quinlan 1986</REF>; <REF>Bahl et al 1989</REF> have been successfully applied to NLP problems such as parsing <REF>Resnik 1993</REF>; <TREF>Magerman 1995</TREF> and discourse analysis <REF>Siegel and McKeown 1994</REF>; <REF>Soderland and Lehnert 1994</REF>~~~We tested the Satz system using the c45 <REF>Quinlan 1993</REF> decision tree induction program as the learning algorithm and compared the results to those obtained previously with the neural network~~~These results are discussed in Section 410.
rile detfil of our statistic framework is described ill <REF>Uehimoto et al , 1999</REF>~~~There have been a lot of prolOSfls for statistical analysis, in ninny languages, in particular in English and Japanese <TREF>Magerman, 1995</TREF> <REF>Sekine and Grishman, 1995</REF> <REF>Collins, 1997</REF> I/atnalarkhi, 1997 KShirai etal, 1998 <REF>Fujio and Matsnlnoto, 1998</REF> Itaruno ctal, 1997<REF>Ehara, 1998</REF>~~~One of the most advancet systems in English is lroposed 1y Iatnaparkhi~~~It, uses the Maximum Entropy ME model and both of the accuracy and the speed of the system arc among the best retortcd to date.
In order to construct the etrees, which make such distinction, LexTract requires its user to provide additional information in the form of three tables: a Head Percolation Table, an Argument Table, and a Tagset Table~~~A Head Percolation Table has previously been used in several statistical parsers <TREF>Magerman, 1995</TREF>; <REF>Collins, 1997</REF> to find heads of phrases~~~Our strategy for choosing heads is similar to the one in <REF>Collins, 1997</REF>~~~An Argument Table informs LexTract what types of arguments a head can take.
To start with performance values, Table 3 displays previous results on parsing Section 23 of the WSJ section of the Penn tree-bank~~~Comparison indicates that our best model is already better than the early lexicalized model of <TREF>Magerman 1995</TREF>~~~It is a bit worse than the unlexicalized PCFGs of <REF>Klein and Manning 2003</REF> and Matsuzaki et al~~~2005, and of course, it is also worse than state-of-the-art lexicalized parsers experience shows that evaluation results on sections 22 and 23 do not differ much.
The method of transforming the tree-bank is of major influence on the accuracy and coverage of the statistical parser~~~The most important tree-bank transformation in the literature is lexicalization: Each node in a tree is labeled with its head word, the most important word of the constituent under the node <TREF>Magerman 1995</TREF>, <REF>Collins 1996</REF>, <REF>Charniak 1997</REF>, <REF>Collins 1997</REF>, <REF>Carroll and Rooth 1998</REF>, etc~~~It turns out, however, that lexicalization is not unproblematic: First, there is evidence that full lexicalization does not carry over across different tree-banks for other languages, annotations or domains <REF>Dubey and Keller, 2003</REF>~~~Second, full lexicalization leads to a serious sparse-data problem, which can only be solved by sophisticated smoothing and pruning techniques.
We emphasize that our task is to automatically induce a more refined grammar based on a few linguistic principles~~~With automatic refinement it is harder to guarantee improved performance than with manual refinements <REF>Klein and Manning, 2003</REF> or with refinements based on direct lexicalization <TREF>Magerman 1995</TREF>, <REF>Collins 1996</REF>, <REF>Charniak 1997</REF>, etc~~~If, however, our refinement provides improved performance then it has a clear advantage: it is automatically induced, which suggests that it is applicable across different domains, languages and tree-bank annotations~~~Applying our method to the benchmark Penn treebank Wall-Street Journal, we obtain a refined probabilistic grammar that significantly improves over the original tree-bank grammar and that shows performance that is on par with early work on lexicalized probabilistic grammars.
Second, estimation from head distributions consistently outperforms estimation from most probable heads for both models~~~Although coarse-grained models clearly benefit from POS information in the heads L  1,2,5, it is surprising that the best models with completely latent heads are on a par with or almost as good as the best ones using POS 122 LP LR F1 Exact CB Model 1 this paper 848 844 846 264 137 <TREF>Magerman 1995</TREF> 849 846 126 Model 2 this paper 857 857 857 293 129 <REF>Collins 1996</REF> 863 858 114 Matsuzaki etal~~~2005 866 867 119 <REF>Klein and Manning 2003</REF> 869 857 863 309 110 <REF>Charniak 1997</REF> 874 875 100 <REF>Collins 1997</REF> 886 881 091 Table 3: Comparison with other parsers sentences of length  40 as head information~~~Finally, our absolutely best model F1857 combines POS tags with latent extra-information L  10 and is estimated from latent-head distributions.
Rather, our approach induces head classes for the words h and d from the tree-bank and aims at a exact calculation of rule probabilities prC,classh and dependency probabilities pclassdD,C,classh~~~This is in sharp contrast to the smoothed fixed-word statistics in most lexicalized parsing models derived from sparse data <TREF>Magerman 1995</TREF>, <REF>Collins 1996</REF>, <REF>Charniak 1997</REF>, etc~~~Particularly, class-based dependency probabilities pclassdD,C,classh induced from the tree-bank are not exploited by most of these parsers~~~Second, our method results in an automatic linguistic mark-up of tree-bank grammars.
In empirical approaches to parsing, lexical/semantic collocation extracted from corpus has been proved to be quite useful for ranking parses in syntactic analysis~~~For example, <TREF>Magerman 1995</TREF>, <REF>Collins 1996</REF>, and <REF>Charniak 1997</REF> proposed statistical parsing models which incorporated lexical/semantic information~~~In their models, syntactic and lexical/semantic features are dependent on each other and are combined together~~~This paper also proposes a method of utilizing lexical/semantic features for the purpose of applying them to ranking parses in syntactic analysis.
This paper also proposes a method of utilizing lexical/semantic features for the purpose of applying them to ranking parses in syntactic analysis~~~However, unlike the models of <TREF>Magerman 1995</TREF>, <REF>Collins 1996</REF>, and <REF>Charniak 1997</REF>, we assume that syntactic and lexical/semantic features are independent~~~Then, we focus on extracting lcxical/semantic collocational knowledge of verbs which is useful in syntactic analysis~~~More specifically, we propose a novel method for learning a probability model of subcategorization preference of verbs.
A distinctive feature of our work is the fact that we used machine learning techniques to improve an existing rule-based natural language processor from the inside~~~This contrasts with approaches where there are essentially no explicit rules, such as neural networks eg <REF>Buo 1996</REF>, or approaches where the machine learning algorithms attempt to infer--via deduction eg <REF>Samuelsson 1994</REF>, induction eg <REF>Theeramunkong et al 1997</REF>; <REF>Zelle  Mooney 1994</REF> under user cooperation eg <REF>Simmons  Yu 1992</REF>; <REF>Hermjakob  Mooney 1997</REF>, transformation-based error-driven learning eg <REF>Brill 1993</REF>, or even decision trees eg <TREF>Magerman 1995</TREF>--a grammar from raw or preprocessed data~~~In our work, we do not wish to acquire a grammar: we have one and want to devise a mechanism to make some of its parts adaptable to the corpus at hand or, to improve some aspect of its performance~~~Other researchers, such as <REF>Lawrence et al 1996</REF>, have compared neural networks and machine learning methods at the task of sentence classification.
2 Notation for context-free grammars The reader is assumed to be familiar with context-free grammars~~~Our notation fol1Other relevant parsers simultaneously consider two or more words that are not necessarily in a dependency relationship <REF>Lafferty et al , 1992</REF>; <TREF>Magerman, 1995</TREF>; <REF>Collins and Brooks, 1995</REF>; <REF>Chelba and Jelinek, 1998</REF>~~~457 lows <REF>Harrison, 1978</REF>; <REF>Hopcroft and Ullman, 1979</REF>~~~A context-free grammar CFG is a tuple G  VN, VT, P, S, where VN and VT are finite, disjoint sets of nonterminal and terminal symbols, respectively, and S E VN is the start symbol.
They consider systematically a number of alternative probao bilistic formulations, including those of <REF>Resnik 1992</REF> and <REF>Schabes 1992</REF> and implemented systems based on other underlying grammatical frameworks, evaluating their adequacy from both a theoretical and empirical perspective in terms of their ability to model particular distributions of data that occur in existing treebanks~~~<TREF>Magerman 1995</TREF>, <REF>Collins 1996</REF>, <REF>Ratnaparkhi 1997</REF>, <REF>Charniak 1997</REF> and others describe implemented systems with impressive accuracy on parsing unseen data from the Penn Treebank <REF>Marcus, Santorini  Marcinkiewicz, 1993</REF>~~~These parsers model probabilistically the strengths of association between heads of phrases, and the configurations in which these lexical associations occur~~~The accuracies reported for these systems are substantially better than their non-lexicalised probabilistic context-free grammar analogues, demonstrating clearly the value of lexico-statistical information.
In our experiments, the window starts al the sentence prior to that containing the token and extends back W the window size sentences~~~The choice to use sentences as the unit of distance is motivated by our intention to incorporale triggers of this form into a probabilistie treebank based parser and tagger, sneh as <REF>Black et al, 1998</REF>; <REF>Black et al, 1997</REF>; <REF>Brill, 1994</REF>; <REF>Collins, 1996</REF>: aelinek et al, 1994; <TREF>Magerman, 1995</TREF>; blatnaparkhi, 1997~~~All su<h parsers and taggers of which we are aware use only intrasentential information in predicting parses or tags, and we wish to remove this information, as far as possible, from our results ; The window was not allowed to cross a document bmndary~~~The perplexity of lhe task before taking the trigger-pair information into account for tags was 2240 and for rules was 570.
In both cases, we report PARSEVAL labeled No suffix With suffix F-score F-score GF Baseline 694 691 Coord GF 702 715 NP case 711 724 PP case 710 727 SBAR 709 726 S GF 713 731 Table 2: Effect of re-annotation and suffix analysis with Markov rules~~~bracket scores <TREF>Magerman, 1995</TREF>, with the brackets labeled by syntactic categories but not grammatical functions~~~Rather than reporting precision and recall of labelled brackets, we report only the F-score, ie the harmonic mean of precision and recall~~~34 Results Table 1 shows the effect of rule type choice, and Table 2 lists the effect of the GF re-annotations.
In addition to the standard formulation in Equation 1, we consider two alternative variants of Pr~~~The first is a Markov context-free rule <TREF>Magerman, 1995</TREF>; <REF>Charniak, 2000</REF>~~~A rule may be turned into a Markov rule by first binarizing it, then making independence assumptions on the new binarized rules~~~Binarizing the rule A a0 B1 a8a9a8a9a8 Bn results in a number of smaller rules A a0 B1AB1, AB1 a0 B2AB1B2, a8a9a8a9a8, AB1 a10a10a10 Bn a11 1 a0 Bn.
To construct deterministic parsers based on this system, we use classifiers trained on treebank data in order to predict the next transition and dependency type given the current configuration of the parser~~~In this way, our approach can be seen as a form of history-based parsing <REF>Black et al , 1992</REF>; <TREF>Magerman, 1995</TREF>~~~In the experiments reported here, we use memory-based learning to train our classifiers~~~3 Memory-Based Learning Memory-based learning and problem solving is based on two fundamental principles: learning is the simple storage of experiences in memory, and solving a new problem is achieved by reusing solutions from similar previously solved problems <REF>Daelemans, 1999</REF>.
used for training and section 23 for testing <REF>Collins, 1999</REF>; <REF>Charniak, 2000</REF>~~~The data has been converted to dependency trees using head rules <TREF>Magerman, 1995</TREF>; <REF>Collins, 1996</REF>~~~We are grateful to Yamada and Matsumoto for letting us use their rule set, which is a slight modification of the rules used by <REF>Collins 1999</REF>~~~This permits us to make exact comparisons with the parser of <REF>Yamada and Matsumoto 2003</REF></REF>, but also the parsers of <REF>Collins 1997</REF> and <REF>Charniak 2000</REF>, which are evaluated on the same data set in <REF>Yamada and Matsumoto 2003</REF></REF>.
Second, one might propagate lexical information upward through the productions~~~Examples of formalisms using this approach include the work of <TREF>Magerman 1995</TREF>, <REF>Charniak 1997</REF>, <REF>Collins 1997</REF>, and <REF>Goodman 1997</REF>~~~A more linguistically motivated approach is to expand the domain of productions downward to incorporate more tree structures~~~The Lexicalized Tree-Adjoining Grammar LTAG formalism <REF>Schabes et al, 1988</REF>, <REF>Schabes, 1990</REF> , although not context-free, is the most well-known instance in this category.
Our model uses both lexical and syntactic features for determining the probability of inserting discourse boundaries~~~We apply canonical lexical head projection rules <TREF>Magerman, 1995</TREF> in order to lexicalize syntactic trees~~~For each word a45, the upper-most node with lexical head a45 which has a right sibling node determines the features on the basis of which we decide whether to insert a discourse boundary~~~We denote such node a68a70a69, and the features we use are node a68a71a69, its parent a68a73a72, and the siblings of a68a74a69  In the example in Figure 2, we determine whether to insert a discourse boundary after the word says using as features node a68a75a72a65a43a77a76a6a78a79a21a10a80a33a81a33a82a34a80a33a24 and its children a68 a69 a43a83a76a22a84a86a85a34a21a14a80a33a81a26a82a34a80a32a24 and a68a74a87a88a43a90a89a33a84a92a91a6a93a79a21a95a94a8a96a32a97a6a97a22a24  We use our corpus to estimate the likelihood of inserting a discourse boundary between word a45 and the next word using formula 1, a57a58a21a35a59 a60a45a75a37a40a7a55a24a99a98a101a100 a5a8a7a29a21a35a68a73a72a75a102a103a50a52a50a52a50a40a68a104a69a74a42a27a68 a87 a50a52a50a52a50a38a24 a100 a5a8a7a29a21a35a68 a72 a102a105a50a52a50a4a50a55a68 a69 a68a104a87a62a50a4a50a52a50a38a24 1 where the numerator represents all the counts of the rule a68 a72 a102a105a50a52a50a4a50a40a68 a69 a68a104a87a51a50a52a50a52a50 for which a discourse boundary has been inserted after word a45, and the denominator represents all the counts of the rule.
Coming to this problem from the standpoint of tree transformation, we naturally view our work as a descendent of <REF>Johnson 1998</REF> and <REF>Klein and Manning 2003</REF>~~~In retrospect, however, there are perhaps even greater similarities to that of <TREF>Magerman, 1995</TREF>; <REF>Henderson, 2003</REF>; <REF>Matsuzaki et al , 2005</REF>~~~Consider the approach of Matsuzaki et al~~~2005.
We would like to infer the number of annotations for each nonterminal automatically~~~However, again in retrospect, it is in the work of <TREF>Magerman 1995</TREF> that we see the greatest similarity~~~Rather than talking about clustering nodes, as we do, Magerman creates a decision tree, but the differences between clustering and decision trees are small~~~Perhaps a more substantial difference is that by not casting his problem as one of learning phrasal categories Magerman loses all of the free PCFG technology that we can leverage.
<REF>Charniak, 1996</REF>~~~Instead researchers condition parsing decisions on many other features, such as parent phrase-marker, and, famously, the lexical-head of the phrase <TREF>Magerman, 1995</TREF>; <REF>Collins, 1996</REF>; <REF>Collins, 1997</REF>; <REF>Johnson, 1998</REF>; <REF>Charniak, 2000</REF>; <REF>Henderson, 2003</REF>; <REF>Klein and Manning, 2003</REF>; <REF>Matsuzaki et al , 2005</REF> and others~~~One particularly perspicuous way to view the use of extra conditioning information is that of tree-transformation <REF>Johnson, 1998</REF>; <REF>Klein and Manning, 2003</REF>~~~Rather than imagining the parser roaming around the tree for picking up the information it needs, we rather relabel the nodes to directly encode this information.
Our experiment quantitatively analyzes several feature types power for syntactic structure prediction and draws a series of interesting conclusions~~~In the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types <REF>Black, 1992</REF> <REF>Briscoe, 1993</REF> <REF>Brown, 1991</REF> <REF>Charniak, 1997</REF> <REF>Collins, 1996</REF> <REF>Collins, 1997</REF> <REF>Magerman, 1991</REF> <REF>Magerman, 1992</REF> <TREF>Magerman, 1995</TREF> <REF>Eisner, 1996</REF>~~~How to evaluate the different feature types effects for syntactic parsing~~~The paper proposes an information-theory-based feature types analysis model, which uses the measures of predictive information quantity, predictive information gain, predictive information redundancy and predictive information summation to quantitatively analyse the different contextual feature types or feature types combinations predictive power for syntactic structure.
Unfortunately, the state of knowledge in this regard is very limited~~~Many probabilistic evaluation models have been published inspired by one or more of these feature types <REF>Black, 1992</REF> <REF>Briscoe, 1993</REF> <REF>Charniak, 1997</REF> <REF>Collins, 1996</REF> <REF>Collins, 1997</REF> <TREF>Magerman, 1995</TREF> <REF>Eisner, 1996</REF>, but discrepancies between training sets, algorithms, and hardware environments make it difficult, if not impossible, to compare the models objectively~~~In the paper, we propose an information-theory-based feature type analysis model by which we can quantitatively analyse the predictive power of different feature types or feature type combinations for syntactic structure in a systematic way~~~The conclusion is expected to provide reliable reference for feature type selection in the probabilistic evaluation modelling for statistical syntactic parsing.
PF model describes the probability of each feature in feature set FS taking on specific values when a CFG rule A ->  is given~~~To make the model more practical in parameter estimation, we assume the features in feature set FS are independent from each other, thus:    FSFi AFiPAFSP ,,  5 Under this PCFGPF model, the goal of a parser is to choose a parse that maximizes the following score: ,maxarg 1 AFS i i i n i T PSTScore    6 Our model is thus a simplification of more sophisticated models which integrate PCFGs with features, such as those in <TREF>Magerman1995</TREF>, <REF>Collins1997</REF> and <REF>Goodman1997</REF>~~~Compared with these models, our model is more practical when only small training data is available, since we assume the independence between features~~~For example, in Goodmans probabilistic feature grammar PFG, each symbol in a PCFG is replaced by a set of features, so it can describe specific constraints on the rule.
Thematic segmentation also relates to several notions such as speakers intention, topic flow and cohesion~~~Since it is elusive what mental representations humans use in order to distinguish a coherent text, different surface markers <TREF>Hirschberg and Nakatani, 1996</TREF>; <REF>Passonneau and Litman, 1997</REF> and external knowledge sources <REF>Kozima and Furugori, 1994</REF> have been exploited for the purpose of automatic thematic segmentation~~~<REF>Halliday and Hasan 1976</REF> claim that the text meaning is realised through certain language resources and they refer to these resources by the term of cohesion~~~The major classes of such text-forming resources identified in <REF>Halliday and Hasan, 1976</REF> are: substitution, ellipsis, conjunction, reiteration and collocation.
A large literature in linguistics and related fields has shown that topic boundaries as well as similar entities such as paragraph boundaries in read speech, or discourselevel boundaries in spontaneous speech are indicated prosodically in a manner that is similar to sentence or utterance boundaries--only stronger~~~Major shifts in topic typically show longer pauses, an extra-high F0 onset or reset, a higher maximum accent peak, greater range in F0 and intensity <REF>Brown, Currie, and Kenworthy 1980</REF>; <REF>Grosz and Hirschberg 1992</REF>; <REF>Nakajima and Allen 1993</REF>; <REF>Geluykens and Swerts 1993</REF>; <REF>Ayers 1994</REF>; <TREF>Hirschberg and Nakatani 1996</TREF></TREF>; <REF>Nakajima and Tsukada 1997</REF>; <REF>Swerts 1997</REF> and shifts in speaking rate <REF>Brubaker 1972</REF>; Koopmans-van geinum and van <REF>Donzel 1996</REF>; <TREF>Hirschberg and Nakatani 1996</TREF></TREF>~~~Such cues are known to be salient for human listeners; in fact, subjects can perceive major discourse boundaries even if the speech itself is made unintelligible via spectral filtering <REF>Swerts, Geluykens, and Terken 1992</REF>~~~Work in automatic extraction and computational modeling of these characteristics has been more limited, with most of the work in computational prosody modeling dealing with boundaries at the sentence level or below.
<REF>In Grosz and Hirschberg 1992</REF>, percent agreement see Section 32 among 7 coders on 3 texts under two conditions--text plus speech or text alone--is reported at levels ranging from 743 to 951~~~<REF>In Hirschberg and Nakatani 1996</REF>, average reliability measured using the kappa coefficient discussed in Carletta 1996 of segmentinitial labels among 3 coders on 9 monologues produced by the same speaker, labeled using text and speech, is8 or above for both read and spontaneous speech; values of at least 8 are typically viewed as representing high reliability see Section 32~~~Reliability labeling from text alone is 56 for read and 63 for spontaneous speech~~~Other notions of segment have also been used in evaluating naive or trained coders.
Finally, Table 11 shows the results from a set of additional machine learning experiments, in which more conservative definitions of boundary are used~~~For example, using a threshold of seven subjects yields the set of consensus boundaries, as defined in <TREF>Hirschberg and Nakatani 1996</TREF>~~~Comparison with Table 9 shows that for T  5, Learning 1 rather than Learning 2 is the better performer~~~However, the more interesting result is that for T  6 and T  7, the learning approach has an important limitation with respect to the boundary classification task.
Many of the studies discussed in the preceding subsection take this approach~~~The types of linguistic features investigated indude prosody <REF>Grosz and Hirschberg 1992</REF>; <REF>Nakatani, Hirschberg, and Grosz 1995</REF>; <TREF>Hirschberg and Nakatani 1996</TREF>; <REF>Swerts 1995</REF>; <REF>Swerts and Ostendorf 1995</REF>, term repetition <REF>Hearst 1994</REF>, cue words <REF>Moser and Moore 1995</REF>; <REF>Whittaker and Stenton 1988</REF>, and discourse anaphora <REF>Walker and Whittaker 1990</REF>~~~<REF>Grosz and Hirschberg 1992</REF> investigate the prosodic structuring of discourse~~~The correlation of various prosodic features with their independently obtained consensus codings of segmental structure codings on which all labelers agreed is analyzed using t-tests; the results support the hypothesis that discourse structure is marked intonationally in read speech.
14 Note that the humans did not have access to pause information~~~Other studies have shown that when both speech and text are available to labelers, segmentation is clearer <REF>Swerts 1995</REF> and reliability improves <TREF>Hirschberg and Nakatani 1996</TREF>~~~123 Computational Linguistics Volume 23, Number 1 Table 4 Evaluation for Tj > 4~~~Recall Precision Fallout Error Summed Deviation PAUSE 92 18 54 49 193 CUE 72 15 53 50 216 NP 50 31 15 19 153 Humans 74 55 09 11 91 if cue1  true then boundary else nonboundary Figure 11 Cue word algorithm.
For example, pauses tended to precede phrases that initiated segments independent of hierarchical structure and to follow phrases that ended segments~~~Similar results are reported in <REF>Nakatani, Hirschberg, and Grosz 1995</REF> and <TREF>Hirschberg and Nakatani 1996</TREF> for spontaneous speech as well~~~<REF>Grosz and Hirschberg 1992</REF> also use the classification and regression tree system CART <REF>Brieman et al 1984</REF> to automatically construct and evaluate decision trees for classifying aspects of discourse structure from intonational feature values~~~The studies of <REF>Swerts 1995</REF> and <REF>Swerts and Ostendorf 1995</REF> also investigate the prosodic structuring of discourse.
The types of discourse units being coded and the relations among them vary~~~Several studies have used trained coders to locally and globally structure spontaneous or read speech using the model of <REF>Grosz and Sidner 1986</REF>, including <REF>Grosz and Hirschberg 1992</REF>; <REF>Nakatani, Hirschberg, and Grosz 1995</REF>; <REF>Stifleman 1995</REF>; <TREF>Hirschberg and Nakatani 1996</TREF>~~~<REF>In Grosz and Hirschberg 1992</REF>, percent agreement see Section 32 among 7 coders on 3 texts under two conditions--text plus speech or text alone--is reported at levels ranging from 743 to 951~~~<REF>In Hirschberg and Nakatani 1996</REF>, average reliability measured using the kappa coefficient discussed in Carletta 1996 of segmentinitial labels among 3 coders on 9 monologues produced by the same speaker, labeled using text and speech, is8 or above for both read and spontaneous speech; values of at least 8 are typically viewed as representing high reliability see Section 32.
In 7This tagging can be hand generated, or system generated and hand corrected~~~Preliminary studies indicate that reliability for human tagging is higher for AVM attribute tagging than for other types of discourse segment tagging <REF>Passonneau and Litman, 1997</REF>; <TREF>Hirschberg and Nakatani, 1996</TREF>~~~8Previous work has shown that this can be done with high reliability <REF>Hirschman and Pao, 1993</REF>~~~general, if an utterance U contributes to the information goals of N different attributes, each attribute accounts for 1/N of any costs derivable from U Thus, c2D2 is5.
<REF>Wilson and Wiebe 2005</REF> extend this basic annotation scheme to include different types of subjectivity, including Positive Sentiment, Negative Sentiment, Positive Arguing, and Negative Arguing~~~Speech was found to improve inter-annotator agreement in discourse segmentation of monologs <TREF>Hirschberg and Nakatani 1996</TREF>~~~Acoustic clues have been successfully employed for the reliable detection of the speakers emotions, including frustration, annoyance, anger, happiness, sadness, and boredom <REF>Liscombe et al 2003</REF>~~~Devillers et al.
Although lexical information is important in English segmentation <REF>Stoleke and Shriberg, 1996</REF>, what other information can help improve such segmentation~~~<TREF>Hirschberg and Nakatani 1996</TREF> showed that prosodic information helps human discourse segmentation~~~<REF>Litman and Passonneau 1995</REF> addressed the usefulness of a multiple knowledge source in human and automatic discourse segmentation~~~<REF>Vendittiand Swerts 1996</REF> stated that the intonational features for many Indo-European languages help cue the structure of spoken discourse.
61 Reader Judgments There is a growing concern surrounding issues of intercoder reliability when using human judgments to evaluate discourse-processing algorithms <REF>Carletta 1996</REF>; <REF>Condon and Cech 1995</REF>~~~Proposals have recently been made for protocols for the collection of human discourse segmentation data <REF>Nakatani et al 1995</REF> and for how to evaluate the validity of judgments so obtained <REF>Carletta 1996</REF>; <REF>Isard and Carletta 1995</REF>; Ros6 1995; <REF>Passonneau and Litman 1993</REF>; <REF>Litman and Passonneau 1995</REF>~~~Recently, Hirschberg 52 <REF>Hearst TextTiling and Nakatani 1996</REF> have reported promising results for obtaining higher interjudge agreement using their collection protocols~~~For the evaluation of the TextTiling algorithms, judgments were obtained from seven readers for each of 12 magazine articles that satisfied the length criteria between 1,800 and 2,500 words 9 and that contained little structural demarcation.
Proposals have recently been made for protocols for the collection of human discourse segmentation data <REF>Nakatani et al 1995</REF> and for how to evaluate the validity of judgments so obtained <REF>Carletta 1996</REF>; <REF>Isard and Carletta 1995</REF>; Ros6 1995; <REF>Passonneau and Litman 1993</REF>; <REF>Litman and Passonneau 1995</REF>~~~Recently, Hirschberg 52 <REF>Hearst TextTiling and Nakatani 1996</REF> have reported promising results for obtaining higher interjudge agreement using their collection protocols~~~For the evaluation of the TextTiling algorithms, judgments were obtained from seven readers for each of 12 magazine articles that satisfied the length criteria between 1,800 and 2,500 words 9 and that contained little structural demarcation~~~The judges were asked simply to mark the paragraph boundaries at which the topic changed; they were not given more explicit instructions about the granularity of the segmentation.
We should expect to see, in grouping together paragraph-sized units instead of utterances, a decrease in the complexity of the feature set and algorithm needed~~~The work described here makes use only of lexical distribution information, in lieu of prosodic cues such as intonational pitch, pause, and duration <TREF>Hirschberg and Nakatani 1996</TREF>, discourse markers such as oh, well, ok, however <REF>Schiffrin 1987</REF>; <REF>Litman and Passonneau 1995</REF>, pronoun reference resolution <REF>Passonneau and Litman 1993</REF>; <REF>Webber 1988</REF> and tense and aspect <REF>Webber 1987</REF>; <REF>Hwang and Schubert 1992</REF>~~~From a computational viewpoint, deducing textual topic structure from lexical occurrence information alone is appealing, both because it is easy to compute, and because discourse cues are sometimes misleading with respect to the topic structure <REF>Brown and Yule 1983</REF>, Section 3~~~4.
2 Collecting a database of texts annotated with coherence relations This section describes 1 how we define discourse segments, 2 which coherence relations we used to connect the discourse segments, and 3 how the annotation procedure worked~~~21 Discourse segments Discourse segments can be defined as nonoverlapping spans of prosodic units <TREF>Hirschberg  Nakatani, 1996</TREF>, intentional units <REF>Grosz  Sidner, 1986</REF>, phrasal units <REF>Lascarides  Asher, 1993</REF>, or sentences <REF>Hobbs, 1985</REF>~~~We adopted a sentence unit-based definition of discourse segments~~~However, we also assume that contentful coordinating and subordinating conjunctions cf.
The goal in developing the annotation instructions is that they can be used reliably by non-experts after a reasonable amount of training cf~~~<REF>Passonneau  Litman 1993</REF>, <REF>Condon  Cech 1995</REF>, and <TREF>Hirschberg  Nakatani 1996</TREF>, where reliability is measured in terms of the amount of agreement among annotators~~~High reliability indicates that the encoding scheme is reproducible given multiple labelers~~~In addition, the instructions serve to document the annotations.
As discussed in <REF>Hays 1988</REF>, J will be 00 when the agreement is what one would expect under independence, and it will be 10 when the agreement is exact~~~A  value of 08 or greater indicates a high level of reliability among raters, with values between 067 and 08 indicating only moderate agreement Hirschberg  <REF>Nakatani 1996</REF>; <REF>Carletta 1996</REF>~~~In addition to measuring intercoder reliability, we compared each coders annotations to the evaluation Temporal Units used to assess the systems performance~~~These evaluation Temporal Units were assigned by an expert working on the project.
That is, the systems answers are mapped from its more complex internal representation an ILT, see section 41 into this simpler vector representation before evaluation is performed~~~As in much recent empirical work in discourse processing eg , <REF>Arhenberg et al 1995</REF>; <REF>Isard  Carletta 1995</REF>; <REF>Litman  Passonneau 1995</REF>; <REF>Moser  Moore 1995</REF>; <TREF>Hirschberg  Nakatani 1996</TREF>, we performed an intercoder reliability study investigating agreement in annotating the times~~~The goal in developing the annotation instructions is that they can be used reliably by non-experts after a reasonable amount of training cf~~~<REF>Passonneau  Litman 1993</REF>, <REF>Condon  Cech 1995</REF>, and <TREF>Hirschberg  Nakatani 1996</TREF>, where reliability is measured in terms of the amount of agreement among annotators.
Metric F NM S noNM p  user turns 218 53 228 65 065  correct turns 72 18 67 22 059 AsrMis 37 27 46 28 046 SemMis 5 6 12 14 009 Table 2~~~Average standard deviation for objective metrics in the first problem 6 Related work Discourse structure has been successfully used in non-interactive settings eg understanding specific lexical and prosodic phenomena <TREF>Hirschberg and Nakatani, 1996</TREF>, natural language generation <REF>Hovy, 1993</REF>, essay scoring <REF>Higgins et al , 2004</REF> as well as in interactive settings eg predictive/generative models of postural shifts <REF>Cassell et al , 2001</REF>, generation/interpretation of anaphoric expressions <REF>Allen et al , 2001</REF>, performance modeling <REF>Rotaru and Litman, 2006</REF>~~~In this paper, we study the utility of the discourse structure on the user side of a dialogue system~~~One related study is that of <REF>Rich and Sidner, 1998</REF>.
Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases <REF>Passonneau and Litman, 1997</REF>~~~In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries <REF>Grosz and Hirschberg, 1992</REF>; <REF>Nakatani et al , 1995</REF>; <TREF>Hirschberg and Nakatani, 1996</TREF>; <REF>Passonneau and Litman, 1997</REF>; <REF>Hirschberg and Nakatani, 1998</REF>; <REF>Beeferman et al , 1999</REF>; <REF>Tur et al , 2001</REF>~~~Such indicators include long pauses, shifts in speaking rate, great range in F0 and intensity, and higher maximum accent peak~~~These approaches use different learning mechanisms to combine features, including decision trees <REF>Grosz and Hirschberg, 1992</REF>; <REF>Passonneau and Litman, 1997</REF>; <REF>Tur et al , 2001</REF> exponential models <REF>Beeferman et al , 1999</REF> or other probabilistic models <REF>Hajime et al , 1998</REF>; <REF>Reynar, 1999</REF>.
Such observations inform the increasing trend towards analysis of homogeneous corpora to identify linguistic constraints for use in systems intended to understand or generate coherent discourse~~~Recent work in this vein includes identification of lexical constraints from textual tutorial dialogue <REF>Moser and Moore, 1995</REF>, constraints on illocutionary act type from spoken task-oriented dialogue <REF>Allen et al , 1995</REF>, prosodic constraints from spoken information-seeking monologues <TREF>Hirschberg and Nakatani, 1996</TREF>, and constraints on referring expressions from spoken narrative monologue <REF>Passonneau, 1996</REF>~~~Related work suggests that constraints of different types are interdependent <REF>Biber, 1993</REF>; Passonneau and Litman, forthcoming, hence should be investigated together~~~Our ultimate goal is to develop methods to tag lexical semantic features in discourse corpora in order to enhance extraction of constraints of the sort just listed.
<REF>Passonneau and Litman 1993</REF> identified that topic shifts often occur after a pause of relatively long duration~~~Other prosodic cues eg , pitch contour, energy have been studied for their correlation with story segments in read speech <REF>Tur et al , 2001</REF>; <REF>Levow, 2004</REF>; <REF>Christensen et al , 2005</REF> and with theory-based discourse segments in spontaneous speech eg , directiongiven monologue <TREF>Hirschberg and Nakatani, 1996</TREF>~~~In addition, head and hand/forearm movements are used to detect group-action based segments <REF>McCowan et al , 2005</REF>; Al-<REF>Hames et al , 2005</REF>~~~However, many other features that we expect to signal segment boundaries have not been studied systematically.
In this paper, we describe our framework for building an automatic prosody labeler for English~~~We report results on the Boston University BU Radio Speech Corpus <REF>Ostendorf et al , 1995</REF> and Boston Directions Corpus BDC <TREF>Hirschberg and Nakatani, 1996</TREF>, two publicly available speech corpora with manual ToBI annotations intended for experiments in automatic prosody labeling~~~We condition prosody not only on word strings and their parts-of-speech but also on richer syntactic information encapsulated in the form of Supertags <REF>Bangalore and Joshi, 1999</REF>~~~We propose a maximum entropy modeling framework for the syntactic features.
The main drawback of this corpus is that it comprises only read speech~~~Prosody labeling on spontaneous speech corpora like Boston Directions corpus BDC, Switchboard SWBD has garnered attention in <TREF>Hirschberg and Nakatani, 1996</TREF>; <REF>Gregory and Altun, 2004</REF>~~~Automatic prosody labeling has been achieved through various machine learning techniques, such as decision trees <REF>Hirschberg, 1993</REF>; <REF>Wightman and Ostendorf, 1994</REF>; <REF>Ma et al , 2003</REF>, rule-based systems <REF>Shimei and McKeown, 1999</REF>, bagging and boosting on CART <REF>Sun, 2002</REF>, hidden markov models <REF>Conkie et al , 1999</REF>, neural networks Hasegawa-<REF>Johnson et al , 2005</REF>,maximum-entropy models <REF>Brenier et al , 2005</REF> and conditional random fields <REF>Gregory and Altun, 2004</REF>~~~Prosody labeling of the BU corpus has been reported in many studies <REF>Hirschberg, 1993</REF>; <REF>HasegawaJohnson et al , 2005</REF>; <REF>Ananthakrishnan and Narayanan, 2005</REF>.
In this paper we study the utility of discourse structure as an information source for SDS performance analysis~~~The discourse structure hierarchy has been shown to be useful for other tasks: understanding specific lexical and prosodic phenomena <TREF>Hirschberg and Nakatani, 1996</TREF>; <REF>Levow, 2004</REF>, natural language generation <REF>Hovy, 1993</REF>, predictive/generative models of postural shifts <REF>Cassell et al , 2001</REF>, and essay scoring <REF>Higgins et al , 2004</REF>~~~We perform our analysis on a corpus of speech-based tutoring dialogues~~~A tutoring SDS <REF>Litman and Silliman, 2004</REF>; Pon-<REF>Barry et al , 2004</REF> has to discuss concepts, laws and relationships and to engage in complex subdialogues to correct student misconceptions.
For example, in Figure 1, if the student would have answered Tutor 2 correctly, the next tutor turn would have had the same content as Tutor 5 but the Advance label~~~Also, while a human annotation of the discourse structure will be more complex but more time consuming <TREF>Hirschberg and Nakatani, 1996</TREF>; <REF>Levow, 2004</REF>, its advantages are outweighed by the automatic nature of our discourse structure annotation~~~We would like to highlight that our transition annotation is domain independent and automatic~~~Our transition labels capture behavior like starting a new dialogue NewTopLevel, crossing discourse segment boundaries Push, PopUp, PopUpAdv and local phenomena inside a discourse segment Advance, SameGoal.
They develop and evaluate three algorithms for producing the target behavior from these features, two hand-developed and one automatically induced~~~Generalizations of this work arise from other research on different speech genres In different environments that also found that coreferential relationships, pausing, and intonation are correlated with discourse structure <REF>Cahn 1992</REF>; <REF>Fox 1987</REF>; <TREF>Hirschberg and Nakatani 1996</TREF>~~~Future work can further test the generalizability of the results reported here: the features used could be examined in other types of spoken monologues, in texts, and in dialogue~~~35 Smith and Gordon Smith and Gordon examine the effect of initiative on dialogue structure in dialogues in which human subjects interact with the computer to diagnose and repair problems with simple circuits.
Discourse tagging classifies discourse units in naturally occurring texts or dialogues into one of a set of categories~~~Discourse units range from referring expressions and syntactic constructions <REF>Fox 1987</REF>; <REF>Kroch and Hindle 1982</REF>; <REF>Prince 1985</REF>, to words or phrases <REF>Heeman and Allen 1994</REF>; <REF>Hirschberg and Litman 1993</REF>; <REF>Novick and Sutton 1994</REF>, to utterances and relationships among them <REF>Dahlback 1991</REF>; <REF>Reithinger and Maier 1995</REF>; <REF>Moser and Moore 1995</REF>; <REF>Nagata 1992</REF>; <REF>Rose et al 1995</REF>, to multiutterance units identified by a range of criteria such as speaker intention or initiative <REF>Flammia and Zue 1995a</REF>; <TREF>Hirschberg and Nakatani 1996</TREF>; <REF>Whittaker and Stenton 1988</REF>~~~The article by Carletta et al~~~this volume presents a tagging scheme for three levels of discourse structure.
Hearsts algorithm performs comparably to other algorithms on the new task, showing that term repetition may be a more general indicator of subtopic boundaries~~~Future work could test further generalizations; term repetition may also indicate subtopics in other discourse or dialogue environments, and may interact with other features that correlate with topic boundaries, such as pauses, intonation, or cue words <REF>Cahn 1992</REF>; <TREF>Hirschberg and Nakatani 1996</TREF>~~~33 Lester and Porter The target behavior that concerns Lester and Porter is generating paragraph-length explanations in the biology domain~~~Given a goal to explain a biology concept or process, their KNIGHT system selects relevant information from a large knowledge base, organizes it, and then generates it.
<REF>Ayers 1992</REF> found that pitch range appears to correlate more closely with hierarchical topic structure in read speech than in spontaneous speech~~~In spontaneous monologue, <REF>Butterworth 1972</REF> found that the beginning of a discourse segment exhibited slower speaking rate; <REF>Swerts 1995</REF>, and <REF>Passonneau and Litman 1997</REF> found that pause length correlates with discourse segment boundaries; <TREF>Hirschberg and Nakatani 1996</TREF> found that the beginning of a discourse segment correlates with higher pitch~~~In humanhuman dialogue, similar behavior was observed: the pitch value tends to be higher for starting a new discourse segment <REF>Nakajima and Allen, 1993</REF>~~~In human-computer dialogue, <REF>Swerts and Ostendorf 1995</REF> found that the first utterance of a discourse segment correlates with slower speaking rate and longer preceding pause.
Form of expression feature values areadverbial noun, cardinal, definite NP, demonstrative NP, indefinite NP, pronoun, proper name, quantifier NP, verbal noun, etc 334 Global focus feature The global focusing status of baseNPs is computed using two sets of analyses: discourse segmentations and coreference coding~~~Expert discourse structure analyses are used to derive CONSENSUS SEGMENTATIONS, consisting of discourse boundaries whose coding all three labelers agreed upon <TREF>Hirschberg and Nakatani, 1996</TREF>~~~The consensus labels for segment-initial boundaries provide a linear segmentation of a discourse into discourse segments~~~Coreferential relations are coded by two labelers using DTT Discourse Tagging Tool <REF>Aone and Bennett, 1995</REF>.
Information status has generated large interest among researchers because of its complex interaction with other linguistic phenomena, thus affecting several Natural Language Processing tasks~~~Since it correlates with word order and pitch accent <REF>Lambrecht, 1994</REF>; <TREF>Hirschberg and Nakatani, 1996</TREF>, for instance, incorporating knowledge on information status would be helpful for natural language generation, and in particular text-tospeech systems~~~Stober and colleagues, for example, ascribe to the lack of such information the lower performance of text-to-speech compared to concept-to-speech generation, where such knowledge could be made directly available to the system <REF>Stober et al , 2000</REF>~~~Another area where information status can play an important role is anaphora resolution.
The segmentation algorithm presented in this paper focuses on one source of linguistic information for discourse analysis  lexical cohesion~~~Multiple studies of discourse structure, however, have shown that prosodic cues are highly predictive of changes in topic structure <TREF>Hirschberg and Nakatani, 1996</TREF>; <REF>Shriberg et al , 2000</REF>~~~In a supervised framework, we can further enhance audio-based segmentation by combining features derived from pattern analysis with prosodic information~~~We can also explore an unsupervised fusion of these two sources of information; for instance, we can induce informative prosodic cues by using distributional evidence.
Others are specifically adapted for processing speech input by adding relevant acoustic features such as pause length and speaker change <REF>Galley et al , 2003</REF>; <REF>Dielmann and Renals, 2005</REF>~~~In parallel, researchers extensively study the relationship between discourse structure and intonational variation <TREF>Hirschberg and Nakatani, 1996</TREF>; <REF>Shriberg et al , 2000</REF>~~~However, all of the existing segmentation methods require as input a speech transcript of reasonable quality~~~In contrast, the method presented in this paper does not assume the availability of transcripts, which prevents us from using segmentation algorithms developed for written text.
9This tagging can be hand generated, or system generated and hand corrected~~~Preliminary studies indicate that reliability for human tagging is higher for AVM attribute tagging than for other types of discourse segment tagging <REF>Passonneau and Litman, 1997</REF>; <TREF>Hirschberg and Nakatani, 1996</TREF>~~~:EAC, DR, D :AIA9 SEGcr: S3 SMlCr: S4 G0: I GOALS: AC orrcES: A3u5 0TI/ES: A6U6 Figure 4: Task-defined discourse structure of Agent A dialogue interaction utterances that contribute to the success of the whole dialogue, such as greetings, are tagged with all the attributes~~~Since the structure of a dialogue reflects the structure of the task <REF>Carberry, 1989</REF>; <REF>Grosz and Sidner, 1986</REF>; <REF>Litman and Allen, 1990</REF>, the tagging of a dialogue by the AVM attributes can be used to generate a hierarchical discourse structure such as that shown in Figure 4 for Dialogue 1 Figure 2.
Form of expression feature values are adverbial noun, cardinal, definite NP, demonstrative NP, indefinite NP,, pronoun, proper name, quantifier NP,, verbal noun, etc 334 Global focus feature The global focusing status of baseNPs is computed using two sets of analyses: discourse segmentations and coreference coding~~~Expert discourse structure analyses are used to derive CONSENSUS SEGMENTATIONS, consisting of discourse boundaries whose coding all three labelers agreed upon <TREF>Hirschberg and Nakatani, 1996</TREF>~~~The consensus labels for segment-initial boundaries provide a linear segmentation of a discourse into discourse segments~~~Coreferential relations are coded by two labelers using DTT Discourse Tagging Tool <REF>Aone and Bennett, 1995</REF>.
This talk will begin with a summary of pilot studies that demonstrated reliable correlations of discourse structure and intonational features <REF>Grosz and Hirschberg, 1992</REF>; <REF>Hirschberg and Grosz, 1992</REF>; <REF>Hirschberg and Grosz, 1994</REF>~~~It will then tbcus on a new corpus of directiongiving monologues, the Boston Directions Corpus <REF>Nakatani et al , 1995a</REF>; <TREF>Hirschberg and Nakatani, 1996</TREF>~~~I will describe the methodology we developed to elicit fluent spontaneous direction-giving monologues ranging over a spectrum of planning complexity~~~Next I will describe the development of annotation instructions used to train labelers to segment spoken discourses <REF>Nakatani et al , 1995b</REF> and will discuss agreement among segmentations on the Boston Directions Corpus obtained using these instructions.
More recently, a great deal of interest has arisen in using automatic segmentation for the detection of topic and story boundaries in news feeds <REF>Mani et al 1997</REF>; <REF>Merlino, Morey, and Maybury 1997</REF>; <REF>Ponte and Croft 1997</REF>; <REF>Hauptmann and Witbrock 1998</REF>; <REF>Allan et al 1998</REF>; <REF>Beeferman, Berger, and Lafferty 1997, 1999</REF>~~~Sometimes segmentation is done at the clause level, for the purposes of detecting nuances of dialogue structure or for more sophisticated discourse-processing purposes <REF>Morris and Hirst 1991</REF>; <REF>Passonneau and Litman 1993</REF>; <REF>Litman and Passonneau 1995</REF>; <TREF>Hirschberg and Nakatani 1996</TREF>; <REF>Marcu 2000</REF>~~~Some of these algorithms produce hierarchical dialogue segmentations whose evaluation is outside the scope of this discussion~~~11 Evaluating Segmentation Algorithms There are two major difficulties associated with evaluating algorithms for text segmentation.
Nine coders provided IU trees starting from identical CGUs~~~Following the methodology in ttirschberg and <REF>Nakatani, 1996</REF>, we measured the reliability of coding for a linearized version of the IU tree, by calculating the reliability of coding of IU beginnings using the kappa metric~~~We calculated the observed pairwise agreement of CGUs marked as the beginnings of IUs, and factored out the expected agreement estimated from the actual data, giving the pairwise kappa score~~~Table 3 gives the raw data on coders marking of IU beginnings.
Next, we present several automatic methods to combine multiple human-annotated discourse segmentations into one gold standard~~~21 Flat vs Hierarchical Approaches Most previous work that has combined multiple annotations has used linear segmentations, ie discourse segmentations without hierarchies <TREF>Hirschberg and Nakatani, 1996</TREF>~~~In general, the hierarchical nature of discourse structure has not been considered when computing labeler inter-reliability and in evaluations of agreement with automatic methods~~~Since computational discourse theory relies on the hierarchy of its segments, we will consider it in this paper.
<REF>Rotondo 1984</REF> reported that hierarchical segmentation is impractical for naive subjects in discourses longer than 200 words <REF>Passonneau and Litman 1993</REF> conducted a pilot study in which subjects found it difficult and time-consuming to identify hierarchical relations in discourse~~~Other attempts have had more success using improved annotation tools and more precise instructions <REF>Grosz and Hirschberg, 1992</REF>; <TREF>Hirschberg and Nakatani, 1996</TREF>~~~Second, hierarchical segmentation of discourse is subjective~~~While agreement among annotators regarding linear segmentation has been found to be higher than 80 <REF>Hearst, 1997</REF>, with respect to hierarchical segmentation it has been observed to be as low as 60 <REF>Flammia and Zue, 1995</REF>.
However, annotation unification approaches have not been formally evaluated, and although manual unification might be the best approach, it can be time-consuming~~~Indeed, much of the work on automatic recognition of discourse structure has focused on linear, rather than hierarchical segmentation <REF>Hearst, 1997</REF>; <TREF>Hirschberg and Nakatani, 1996</TREF>, because of the difficulties of obtaining consistent hierarchical annotations~~~In addition, those approaches that do handle hierarchical segmentation do not address automatic unification methods <REF>Carlson et al , 2001</REF>; <REF>Marcu, 2000</REF>~~~There are several reasons for the prevailing emphasis on linear annotation and the lack of work on automatic methods for unifying hierarchical discourse annotations.
Let a28a30a40a30a11a14a28a21a40a30a28a16a31a36a3a41a34 be the set of senses of a3  For each sense of a3 a3a42a28a44a43a46a45a47a28a30a40a30a11a14a28a21a40a30a28a16a31a36a3a41a34  we obtain a ranking score by summing over the a27a48a28a21a28a16a31a32a3a6a15a33a11a50a49a30a34 of each neighbour a11a50a49a51a45a52a4a6a5  multiplied by a weight~~~This weight is the WordNet similarity score a3a42a11a14a28a30a28  between the target sense a3a53a28a35a43  and the sense of a11a54a49 a11a14a28a35a55a56a45a57a28a30a40a30a11a14a28a21a40a30a28a16a31a36a11a54a49a16a34  that maximises this score, divided by the sum of all such WordNet similarity scores for a28a30a40a21a11a19a28a21a40a30a28a26a31a32a3a41a34 and a11a50a49  Thus we rank each sense a3a42a28 a43 a45a10a28a30a40a21a11a19a28a21a40a30a28a26a31a32a3a41a34 using:a58a41a59 a11a14a2a54a60a61a11a50a62a64a63a66a65a35a67a16a68a12a40a26a31a32a3a53a28 a43 a34a69a7 a70 a71a44a72a33a73a16a74a76a75 a27a48a28a21a28a16a31a32a3a6a15a33a11a50a49a30a34a30a77 a3a42a11a14a28a21a28a16a31a32a3a53a28a35a43a36a15a33a11a50a49a30a34 a78 a5a19a79a81a80a39a82 a73 a79a61a83 a71 a79a81a83a61a79a36a84a85a5a19a86 a3a53a11a14a28a30a28a26a31a32a3a42a28 a43 a82 a15a17a11 a49 a34 1 where: a3a42a11a14a28a21a28a16a31a32a3a53a28a35a43a87a15a17a11a54a49a21a34a66a7 a88a46a89a21a90 a71 a79a92a91 a73 a79a81a83 a71 a79a81a83a61a79a93a84 a71a44a72 a86 a31a36a3a42a11a14a28a30a28a26a31a32a3a53a28a35a43a36a15a33a11a14a28a35a55a29a34a87a34 22 Acquiring the Automatic Thesaurus There are many alternative distributional similarity measures proposed in the literature, for this work we used the measure and thesaurus construction method described by <TREF>Lin 1998</TREF>~~~For input we used grammatical relation data extracted using an automatic parser <REF>Briscoe and Carroll, 2002</REF>~~~For each noun we considered the co-occurring verbs in the direct object and subject relation, the modifying nouns in noun-noun relations and the modifying adjectives in adjective-noun relations.
Importantly, inter-dependence between links can still be accommodated by exploiting dynamic features in training features that take into account the labels of some of the surrounding components when predicting the label of a target component~~~To cope with the sparse data problem, I use distributional word similarity <REF>Pereira et al , 1993</REF>; <REF>Grefenstette, 1994</REF>; <TREF>Lin, 1998</TREF> to generalize the observed frequency counts in the training corpus~~~The experimental results on the Chinese Treebank 40 show that the accuracy of the conditional model is 136 higher than corresponding joint models, while similarity smoothing also allows the strictly lexicalised approach to outperform corresponding models based on part-of-speech tags~~~4 Extensions to Large Margin Parsing The approach presented above has a limitation: it uses a local scoring function instead of a global scoring function to compute the score for a candidate tree.
Realword spelling correction is also referred to as context sensitive spelling correction, which tries to detect incorrect usage of valid words in certain contexts <REF>Golding and Roth, 1996</REF>; <REF>Mangu and Brill, 1997</REF>~~~Distributional similarity between words has been investigated and successfully applied in many natural language tasks such as automatic semantic knowledge acquisition <TREF>Dekang Lin, 1998</TREF> and language model smoothing <REF>Essen and Steinbiss, 1992</REF>; <REF>Dagan et al , 1997</REF>~~~An investigation on distributional similarity functions can be found in <REF>Lillian Lee, 1999</REF>~~~3 Distributional Similarity-Based Models for Query Spelling Correction 31 Motivation Most of the previous work on spelling correction concentrates on the problem of designing better error models based on properties of character strings.
The calculation of word semantic similarity scores is also a problem that has attracted a lot of interest~~~The numerous notable approaches can usually be divided into those which utilize the hierarchical information from an ontology, such as <REF>Resnik 1995</REF> and <REF>Agirre and Martinez 2002</REF>; and those which simply use word distribution information from a large corpus, such as <TREF>Lin 1998</TREF> and <REF>Lee 1999</REF>~~~9 Conclusion This paper represents a first step towards a corpusbased approach for cross-lingual identification of word concepts and alignment of ontologies~~~The method borrows from techniques used in machine translation and information retrieval, and does not make any assumptions about the structure of the ontology, or use any but the most basic structural information.
The formula is described in Equation 12~~~,,, , 212 2 1 2 2 1 1 1 21 relrelsimeesimeesim eesim colcol  12 where ,, 21 iiiicol erelee   We assume that the relation type keeps the same, so 1, 21 relrelsim  The similarity of the words is calculated with the same method as described in <TREF>Lin, 1998</TREF>, which is rewritten in Equation 13~~~The similarity of the words is calculated through the surrounding context words which have dependency relationships with the investigated words~~~,,,, ,,,, , 2 2, 1 1, 21 21, 21 erelewerelew erelewerelew eeSim eTereleTerel eTeTerel  .
Up to now, there have been few researches which directly address the problem of extracting synonymous collocations~~~However, a number of studies investigate the extraction of synonymous words from monolingual corpora <REF>Carolyn et al , 1992</REF>; <REF>Grefenstatte, 1994</REF>; <TREF>Lin, 1998</TREF>; <REF>Gasperin et al , 2001</REF>~~~The methods used the contexts around the investigated words to discover synonyms~~~The problem of the methods is that the precision of the extracted synonymous words is low because it extracts many word pairs such as cat and dog, which are similar but not synonymous.
3~~~Semantic Correlations Although there exists many methods to derive the semantic correlations between words <REF>Lee, 1999</REF>; <TREF>Lin, 1998</TREF>; <REF>Karov  Edelman, 1998</REF>; <REF>Resnik, 1995</REF>; <REF>Dagan et al, 1995</REF>, we adopt a relatively simple and yet practical and effective approach to derive three topic -oriented semantic correlations: thesaurus-based, co-occurrence-based and contextbased correlation~~~31 Thesaurus based correlation WordNet is an electronic thesaurus popularly used in many researches on lexical semantic acquisition, and word sense disambiguation <REF>Green, 1999</REF>; <REF>Leacock et al, 1998</REF>~~~In WordNet, the sense of a word is represented by a list of synonyms synset, and the lexical information is represented in the form of a semantic network.
In this light, the contributions of this paper are fourfold~~~First, instead of separately addressing the tasks of collecting unlabeled sets of instances <TREF>Lin, 1998</TREF>, assigning appropriate class labels to a given set of instances <REF>Pantel and Ravichandran, 2004</REF>, and identifying relevant attributes for a given set of classes <REF>Pasca, 2007</REF>, our integrated method from Section 2 enables the simultaneous extraction of class instances, associated labels and attributes~~~Second, by exploiting the contents of query logs during the extraction of labeled classes of instances from Web documents, we acquire thousands 4,583, to be exact of open-domain classes covering a wide range of topics and domains~~~The accuracy reported in Section 32 exceeds 80 for both instance sets and class labels, although the extraction of classes requires a remarkably small amount of supervision, in the form of only a few commonly-used Is-A extraction patterns.
A first major algorithmic approach is to represent word contexts as vectors in some space and use similarity measures and automatic clustering in that space <REF>Curran and Moens, 2002</REF>~~~<REF>Pereira 1993</REF> and <TREF>Lin 1998</TREF> use syntactic features in the vector definition~~~<REF>Pantel and Lin, 2002</REF> improves on the latter by clustering by committee~~~<REF>Caraballo 1999</REF> uses conjunction and appositive annotations in the vector representation.
One is automatic thesaurus acquisition, that is, to identify synonyms or topically related words from corpora based on various measures of similarity eg <REF>Riloff and Shepherd, 1997</REF>; <REF>Thelen and Riloff, 2002</REF>~~~For instance, <TREF>Lin 1998</TREF> used dependency relation as word features to compute word similarities from large corpora, and compared the thesaurus created in such a way with WordNet and Roget classes~~~<REF>Caraballo 1999</REF> selected head nouns from conjunctions and appositives in noun phrases, and used the cosine similarity measure with a bottomup clustering technique to construct a noun hierarchy from text~~~<REF>Curran and Moens 2002</REF> explored a new similarity measure for automatic thesaurus extraction which better compromises with the speed/performance tradeoff.
There have been many approachs to automatic detection of similar words from text~~~Our method is similar to <REF>Hindle, 1990</REF>, <TREF>Lin, 1998</TREF>, and <REF>Gasperin, 2001</REF> in the use of dependency relationships as the word features~~~Another approach used the words distribution to cluster the words <REF>Pereira, 1993</REF>, and Inoue <REF>Inoue, 1991</REF> also used the word distributional information in the Japanese-English word pairs to resolve the polysemous word problem~~~Wu <REF>Wu, 2003</REF> shows one approach to collect synonymous collocation by using translation information.
The WMTS is well suited to LRA, because the WMTS scales well to large corpora one terabyte, in our case, it gives exact frequency counts unlike most Web search engines, it is designed for passage retrieval rather than document retrieval, and it has a powerful query syntax~~~53 Thesaurus As a source of synonyms, we use <TREF>Lins 1998a</TREF> automatically generated thesaurus~~~This thesaurus is available through an on-line interactive demonstration or it can be downloaded~~~5 We used the on-line demonstration, since the downloadable version seems to contain fewer words.
Analogy is a high degree of relational similarity~~~382 Turney Similarity of Semantic Relations 22 Measuring Attributional Similarity Algorithms for measuring attributional similarity can be lexicon-based <REF>Lesk 1986</REF>; <REF>Budanitsky and Hirst 2001</REF>; <REF>Banerjee and Pedersen 2003</REF>, corpus-based <REF>Lesk 1969</REF>; <REF>Landauer and Dumais 1997</REF>; <TREF>Lin 1998a</TREF>; <REF>Turney 2001</REF>, or a hybrid of the two <REF>Resnik 1995</REF>; <REF>Jiang and Conrath 1997</REF>; <REF>Turney et al 2003</REF>~~~Intuitively, we might expect that lexicon-based algorithms would be better at capturing synonymy than corpusbased algorithms, since lexicons, such as WordNet, explicitly provide synonymy information that is only implicit in a corpus~~~However, experiments do not support this intuition.
The LRA algorithm consists of the following 12 steps: 1~~~Find alternates: For each word pair A:B in the input set, look in <TREF>Lins 1998a</TREF> thesaurus for the top num sim words in the following experiments, num sim is 10 that are most similar to A For each A prime that is similar to A, make a new word pair A prime :B Likewise, look for the top num sim words that are most similar to B, and for each B prime , make a new word pair A:B prime  A:B is called the original pair and each A prime :B or A:B prime is an alternate pair~~~The intent is that alternates should have almost the same semantic relations as the original~~~For each input pair, there will now be 2  num sim alternate pairs.
For each input pair, there will now be 2  num sim alternate pairs~~~When looking for similar words in <TREF>Lins 1998a</TREF> thesaurus, avoid words that seem unusual eg, hyphenated words, words with three characters or less, words with non-alphabetical characters, multiword phrases, and capitalized words~~~The first column in Table 7 shows the alternate pairs that are generated for the original pair quart:volume~~~2.
If the number of hits for a query is x, then the corresponding element in the vector r is logx  1~~~Several authors report that the logarithmic transformation of frequencies improves cosine-based similarity measures <REF>Salton and Buckley 1988</REF>; <REF>Ruge 1992</REF>; <TREF>Lin 1998b</TREF>~~~<REF>Turney and Littman 2005</REF> evaluated the VSM approach by its performance on 374 SAT analogy questions, achieving a score of 47~~~Since there are five choices for each question, the expected score for random guessing is 20.
They are included for comparison~~~Algorithm Type Precision Recall F Hirst and St-<REF>Onge 1998</REF> Lexicon-based 349 321 334 <REF>Jiang and Conrath 1997</REF> Hybrid 298 273 285 <REF>Leacock and Chodorow 1998</REF> Lexicon-based 328 313 320 <TREF>Lin 1998b</TREF> Hybrid 312 273 291 <REF>Resnik 1995</REF> Hybrid 357 332 344 <REF>Turney 2001</REF> Corpus-based 350 350 350 <REF>Turney and Littman 2005</REF> Relational VSM 477 471 474 Random Random 200 200 200 385 Computational Linguistics Volume 32, Number 3 structured analogies~~~SME takes representations of a source domain and a target domain and produces an analogical mapping between the source and target~~~The domains are given structured propositional representations, using predicate logic.
As a courtesy to other users of Lins on-line system, we insert a 20-second delay between each two queries~~~Lins thesaurus was generated by parsing a corpus of about 5  10 7 English words, consisting of text from the Wall Street Journal, San Jose Mercury,andAP Newswire <TREF>Lin 1998a</TREF>~~~The parser was used to extract pairs of words and their grammatical relations~~~Words were then clustered into synonym sets, based on the similarity of their grammatical relations.
Street and riverbed are only moderately attributionally similar~~~Many algorithms have been proposed for measuring the attributional similarity between two words <REF>Lesk 1969</REF>; <REF>Resnik 1995</REF>; <REF>Landauer and Dumais 1997</REF>; <REF>Jiang and Conrath 1997</REF>; <TREF>Lin 1998b</TREF>; <REF>Turney 2001</REF>; <REF>Budanitsky and Hirst 2001</REF>; <REF>Banerjee and Pedersen 2003</REF>~~~Measures of attributional similarity have been studied extensively, due to their applications in problems such as recognizing synonyms <REF>Landauer and Dumais 1997</REF>, information retrieval <REF>Deerwester et al 1990</REF>, determining semantic orientation <REF>Turney 2002</REF>, grading student essays <REF>Rehder et al 1998</REF>, measuring textual cohesion <REF>Morris and Hirst 1991</REF>, and word sense disambiguation <REF>Lesk 1986</REF>~~~On the other hand, since measures of relational similarity are not as well developed as measures of attributional similarity, the potential applications of relational similarity are not as well known.
Although there is no univocally accepted definition for the OP task, a useful approximation has been suggested by <REF>Bontcheva and Cunningham, 2005</REF> as Ontology Driven Information Extraction with the goal of extracting and classifying instances of concepts and relations defined in a Ontology, in place of filling a template~~~A similar task has been approached in a variety of perspectives, including term clustering <TREF>Lin, 1998</TREF> and <REF>Almuhareb and Poesio, 2004</REF> and term categorization <REF>Avancini et al 2003</REF>~~~A rather different task is Ontology Learning, where new concepts and relations are supposed to be acquired, with the consequence of changing the definition of the Ontology itself <REF>Velardi et al 2005</REF>~~~However, since mentions have been introduced as an evolution of the traditional Named Entity Recognition task see <REF>Tanev and Magnini, 2006</REF>, they guarantee a reasonable level of difficulty, which makes OPTM challenging both for the Computational Linguistic side and the Knowledge Representation community.
All digits in both patterns and sentences are replaced with a common marker, such 810 that any two numerical values with the same number of digits will overlap during matching~~~Many methods have been proposed to compute distributional similarity between words, eg, <REF>Hindle, 1990</REF>, <REF>Pereira et al , 1993</REF>, <REF>Grefenstette, 1994</REF> and <TREF>Lin, 1998</TREF>~~~Almost all of the methods represent a word by a feature vector, where each feature corresponds to a type of context in which the word appeared~~~They differ in how the feature vectors are constructed and how the similarity between two feature vectors is computed.
The first word class in the sequence, CL1, consists of words such as was, is, could, whereas the second class includes February, April, June, Aug , November and other similar words~~~The classes of words are computed on the fly over all sequences of terms in the extracted patterns, on top of a large set of pairwise similarities among words <TREF>Lin, 1998</TREF> extracted in advance from around 50 million news articles indexed by the Google search engine over three years~~~All digits in both patterns and sentences are replaced with a common marker, such 810 that any two numerical values with the same number of digits will overlap during matching~~~Many methods have been proposed to compute distributional similarity between words, eg, <REF>Hindle, 1990</REF>, <REF>Pereira et al , 1993</REF>, <REF>Grefenstette, 1994</REF> and <TREF>Lin, 1998</TREF>.
The most commonly used resource for lexical substitution is the manually constructed WordNet <REF>Fellbaum, 1998</REF>~~~Another option is to use statistical word similarities, such asin thedatabaseconstructed byDekang Lin<TREF>Lin, 1998</TREF>~~~We generically refer to such resources as substitution lexicons~~~When using a substitution lexicon it is assumed that there are some contexts in which the given synonymous words share the same meaning.
524 German test word Baby Brot Frau gelb Hiuschen Kind Kohl Krankheit Midchen Musik Ofen pfeifen Religion Schaf Soldat StraBe siiB Tabak weiB Whisky expected translation and rank baby 1 bread 1 woman 2 yellow 1 cottage 2 child 1 cabbage 17074 sickness 86 baby bread man yellow bungalow child Major disease top five translations as automatically generated child mother daughter father cheese meat food butter woman boy friend wife blue red pink green cottage house hut village daughter son father mother Kohl Thatcher Gorbachev Bush illness Aids patient doctor girl 1 girl music 1 music dance stove 3 heat oven stove house whistle 3 linesman referee whistle blow offside religion 1 sheep 1 soldier 1 street 2 boy man brother lady theatre musical song burn religion culture faith religious belief sheep cattle cow pig goat soldier army troop force civilian road street city town walk sweet smell delicious taste love sweet 1 tobacco 1 white 46 whiskey 11 tobacco cigarette consumption nicotine drink know say thought see think whisky beer Scotch bottle wine Table 1: Results for 20 of the 100 test words for full list see http://wwwfaskuni-mainzde/user/rappl 5 Discussion and Conclusion The method described can be seen as a simple case of the gradient descent method proposed by <REF>Rapp 1995</REF>, which does not need an initial lexicon but is computationally prohibitively expensive~~~It can also be considered as an extension from the monolingual to the bilingual case of the well-established methods for semantic or syntactic word clustering as proposed by <REF>Schtitze 1993</REF>, <REF>Grefenstette 1994</REF>, <REF>Ruge 1995</REF>, <REF>Rapp 1996</REF>, <TREF>Lin 1998</TREF>, and others~~~Some of these authors perform a shallow or full syntactical analysis before constructing the cooccurrence vectors~~~Others reduce the size of the co-occurrence matrices by performing a singular value decomposition.
Theaccuracyachievedinthisexperimentissometimesashighas78andisthereforecomparabletotheresultsreportedinthis paper~~~Anotherwaytoobtainword-sensesdirectly from corpora is to use clustering algorithms onfeature-vectorsLin,1998; Sch utze, 1998~~~Clusteringtechniquescanalsobeusedtodiscriminatebetweendi erentsensesofanambiguousword~~~Ageneralproblemforsuchclusteringtechniquesliesinthequestionofhowmany clustersoneshouldhave,ie howmanysenses areappropriateforaparticularwordinagiven domainManningandSch utze,1999,Ch14.
Ageneralproblemforsuchclusteringtechniquesliesinthequestionofhowmany clustersoneshouldhave,ie howmanysenses areappropriateforaparticularwordinagiven domainManningandSch utze,1999,Ch14~~~LinsapproachtothisproblemLin,1998is tobuildasimilaritytreeusingwhatisineffectahierarchicalclusteringmethodofwords relatedtoatargetwordinthiscasetheword dutyDi erentsensesofdutycanbediscerned asdi erentsub-treesofthissimilaritytreeWe presentanewmethodforword-sensediscriminationinSection6~~~3 Building a Graph from a PoS-tagged Corpus Inthissectionwedescribehowagrapha collection of nodesand links  was built to representtherelationshipsbetweennounsThe modelwasbuiltusingtheBritishNationalCorpuswhichisautomaticallytaggedforpartsof speech~~~Initially,grammaticalrelationsbetweenpairs ofwordswereextracted.
This is known as the Distributional Hypothesis in linguistics <REF>Harris, 1968</REF>~~~For example, the words test and exam are similar because both of them follow verbs such as administer, cancel, cheat on, conduct,  and both of them can be preceded by adjectives such as academic, comprehensive, diagnostic, difficult,  Many methods have been proposed to compute distributional similarity between words <REF>Hindle, 1990</REF>; <REF>Pereira et al , 1993</REF>; <REF>Grefenstette, 1994</REF>; <TREF>Lin, 1998</TREF>~~~Almost all of the methods represent a word by a feature vector where each feature corresponds to a type of context in which the word appeared~~~They differ in how the feature vectors are constructed and how the similarity between two feature vectors is computed.
538 ture for NPi whose value is the most likely NE type~~~7 NEIGHBOR: Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs see <TREF>Lin 1998a</TREF>~~~Motivated by this observation, we create for each of NPis ten most semantically similar NPs a NEIGHBOR feature whose value is the surface string of the NP~~~To determine the ten nearest neighbors, we use the semantic similarity values provided by Lins dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic de nition of similarity.
An example extraction would be <Eastern Airlines, the carrier>, where the rst entry is a proper noun labeled with either one of the seven MUC-style NE types4 or OTHERS5 and the second entry is a common noun~~~We then infer the SC of a common noun as follows: 1 we compute the probability that the common noun co-occurs with each of the eight NE types6 based on the extracted appositive relations, and 2 if the most likely NE type has a co-occurrence probability above a certain threshold we set it to 07, we create a INDUCED CLASS fea1This is motivated by <TREF>Lins 1998c</TREF> observation that a coreference resolver that employs only the rst WordNet sense performs slightly better than one that employs more than one sense~~~2The keywords are obtained via our experimentation with WordNet and the ACE SCs of the NPs in the ACE training data~~~3We used 1 the BLLIP corpus 30M words, which consists of WSJ articles from 1987 to 1989, and 2 the Reuters Corpus 37GB data, which has 806,791 Reuters articles.
2 SUBJ VERB: If NPi is involved in a subjectverb relation, we create a SUBJ VERB feature whose value is the verb participating in the relation~~~We use <TREF>Lins 1998b</TREF> MINIPAR dependency parser to extract grammatical relations~~~Our motivation here is to coarsely model subcategorization~~~3 VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NPi participates in a verb-object relation.
1993 and Rooth et al~~~1999 referring to a direct object noun for describing verbs, or used any syntactic relationship detected by a chunker or a parser such as <TREF>Lin 1998</TREF> and McCarthy et al~~~2003~~~We used a statistical grammar Schulte im <REF>Walde, 2003</REF> to lter all verb-noun pairs where the nouns represent nominal heads in NPs or PPs in syntactic relation to the verb subject, object, adverbial function, etc, and to lter all verb-adverb pairs where the adverbs modify the verbs.
 6615 5779 913 172 3927 1551 Table 3: Coverage of verb association features by grammar/window resources~~~were considered as verb features, such as <TREF>Lin 1998</TREF> and McCarthy et al~~~2003~~~Of the adverb associations, we nd only a small proportion among the parsed adverbs.
This is known as the Distributional Hypothesis in linguistics <REF>Harris, 1968</REF>~~~For example, the words test and exam are similar because both of them can follow verbs such as administer, cancel, cheat on, conduct, etc Many methods have been proposed to compute distributional similarity between words, eg, <REF>Hindle, 1990</REF>; <REF>Pereira et al , 1993</REF>; <REF>Grefenstette, 1994</REF>; <TREF>Lin, 1998</TREF>~~~Almost all of the methods represent a word by a feature vector where each feature corresponds to a type of context in which the word appeared~~~They differ in how the feature vectors are constructed and how the similarity between two feature vectors is computed.
pointwise mutual information <REF>Church and Hanks, 1990</REF>, 3~~~least mutual information difference with similar collocations, based on <REF>Lin, 1999</REF> and using Lins thesaurus <TREF>Lin, 1998</TREF> for obtaining the similar collocations~~~4~~~The distributed frequency of an object, which takes an average of the frequency of occurrence with an object over all verbs occurring with the object above a threshold.
Most previous work using distributional approaches to compositionality either contrasts distributional information of candidate phrases with constituent words <REF>Schone and Jurafsky, 2001</REF>; <REF>Bannard et al , 2003</REF>; <REF>Baldwin et al , 2003</REF>; <REF>McCarthy et al , 2003</REF> or uses distributionally similar words to detect nonproductive phrases <REF>Lin, 1999</REF>~~~<REF>Lin 1999</REF> used his method <TREF>Lin, 1998</TREF> for automatic thesaurus construction~~~He identi ed candidate phrases involving several open-class words output from his parser and ltered these by the loglikelihood statistic~~~Lin proposed that if there is a phrase obtained by substitution of either the head or modi er in the phrase with a nearest neighbour from the thesaurus then the mutual information of this and the original phrase must be signi cantly different for the original phrase to be considered noncompositional.
We used three different types of probabilistic models, which vary in the classes selected for representation over which the probability distribution of the argument heads 2 is estimated~~~Two use WordNet and the other uses the entries in a thesaurus of distributionally similar words acquired automatically following <TREF>Lin, 1998</TREF>~~~The rst method is due to <REF>Li and Abe 1998</REF>~~~The classes over which the probability distribution is calculated are selected according to the minimum description length principle MDL which uses the argument head tokens for nding the best classes for representation.
We cannot show the full WNPROTO due to lack of space, but we show some of the classes with higher probability which cover some typical nouns that occur as objects of park~~~373 Algorithm 1 WNPROTO algorithm C  classes in WNPROTO D   disambiguated ty  TY  fD  0 frequency of disambiguated items TY  argument head types nouns occurring as objects of verb, with associated frequencies C1  WordNet where ty  TY occurring in c  C1 > 1 for all ty  TY do nd c  classesty  C1 where c  argmaxc typeratioc if c  c / C then add c to C add ty  c to D Disambiguated ty with c end if end for for all c  C do if ty  c  D > 1 then fD  fD  frequencytysum frequencies of types under classes to be used in model else remove c from C classes with less than two disambiguated nouns are removed end if end for for all c  C do pc  frequency-of-all-tys-disambiguated-to-classc,DfD calculating class probabilities end for Algorithm 2 DSPROTO algorithm C  classes in DSPROTO D   disambiguated ty  TY  fD  0 frequency of disambiguated items TY  argument head types nouns occurring as objects of verb, with associated frequencies C1  cty  TY where num-types-in-thesauruscty,TY  > 1 order C1 by num-types-in-thesauruscty,TY  classes ordered by coverage of argument head types for all cty  ordered C1 do Dcty   disambiguated for this class for all ty  TY where in-thesaurus-entrycty,ty do if ty / D then add ty to Dcty types disambiguated to this class only if not disambiguated by a class used already end if end for if Dcty > 1 then add cty to C for all ty  Dcty do add ty  cty to D Disambiguated ty with cty fD  fD  frequencyty end for end if end for for all cty  C do pcty  frequency-of-all-tys-disambiguated-to-classcty,DfD calculating class probabilities end for 374 33 DSPROTOs We use a thesaurus acquired using the method proposed by <TREF>Lin 1998</TREF>~~~For input we used the grammatical relation data from automatic parses of the BNC~~~For each noun we considered the cooccurring verbs in the object and subject relation, the modifying nouns in noun-noun relations and the modifying adjectives in adjective-noun relations.
One way to generalise the query is by using similarity classes, ie groups of words with lexically similar behaviour~~~In his work on distributional similarity <TREF>Lin, 1998</TREF> designed a parser to identify grammatical relationships between words~~~However, broad-coverage parsers suitable for processing BNC-like corpora are not available for many languages~~~Another, resource-light approach treats the context as a bag of words BoW and detects the similarity of contexts on the basis of collocations in a window of a certain size, typically 3-4 words, eg.
Similarity and association measures can provide greater coverage for these near-synonym relations~~~The measures both of <TREF>Lin 1998</TREF> and of Pado and Lapata 2003, 2007 are distributional methods; for each word, they create a distribution of the contexts they occur in, and similarity between two words is calculated as the similarity of these distributions2 The difference in these two methods is the representation of the contexts~~~While Lin uses contexts that are expected to determine semantic preferences like being in the direct object position of one verb, Pado and Lapata only use the co-occuring words, weighted by syntax-based distance~~~For example, in 3 Peter subj likes dobj ice-cream.
Similarity and association measures can help for the cases of near-synonymy~~~However, while similarity measures such as WordNet distance or Lins similarity metric only detect cases of semantic similarity, association measures such as the ones used by Poesio et al , or by Garera and Yarowsky also find cases of associative bridg497 Lin98 RFF TheY TheY:G2 PL03 Land country/state/land Staat Staat Kemalismus Regierung Kontinent state state Kemalism government continent Stadt Stadt Bauernfamilie Prasident Region city city agricultural family president region Region Landesregierung Bankgesellschaft Dollar Stadt region country government banking corporation dollar city Bundesrepublik Bundesregierung Baht Albanien Staat federal republic federal government Baht Albania state Republik Gewerkschaft Gasag Hauptstadt Bundesland republic trade union a gas company capital state Medikament medical drug Arzneimittel Pille RU Patient Arzneimittel pharmaceutical pill a drug patient pharmaceutical Praparat Droge Abtreibungspille Arzt Lebensmittel preparation drug non-medical abortion pill doctor foodstuff Pille Praparat Viagra Pille Praparat pill preparation Viagra pill preparation Hormon Pestizid Pharmakonzern Behandlung Behandlung hormone pesticide pharmaceutical company treatment treatment Lebensmittel Lebensmittel Praparat Abtreibungspille Arznei foodstuff foodstuff preparation abortion pill drug highest ranked words, with very rare words removed : RU 486, an abortifacient drug Lin98: Lins distributional similarity measure <TREF>Lin, 1998</TREF> RFF: Geffet and Dagans Relative Feature Focus measure <REF>Geffet and Dagan, 2004</REF> TheY: association measure introduced by <REF>Garera and Yarowsky 2006</REF> TheY:G2: similar method using a log-likelihood-based statistic see <REF>Dunning 1993</REF> this statistic has a preference for higher-frequency terms PL03: semantic space association measure proposed by <REF>Pado and Lapata 2003</REF> Table 1: Similarity and association measures: most similar items ing like 1a,b; the result of this can be seen in table 2: while the similarity measures Lin98, RFF list substitutable terms which behave like synonyms in many contexts, the association measures Garera and Yarowskys TheY measure, Pado and Lapatas association measure also find non-compatible associations such as countrycapital or drugtreatment, which is why they are commonly called relationfree~~~For the purpose of coreference resolution, however we do not want to resolve the door to the antecedent the house as the two descriptions do not corefer, and it may be useful to filter out non-similar associations~~~12 Information Sources Different resources may be differently suited for the recognition of the various relations.
While none of the information sources can match the precision of the hypernymy information encoded in GermaNet, or that of using a combination of high-precision patterns with the World Wide Web as a very large corpus, it is possible to achieve a considerable improvement in terms of recall without sacrificing too much precision by combining these methods~~~Very interestingly, the distributional methods based on intra-sentence relations <TREF>Lin, 1998</TREF>; <REF>Pado and Lapata, 2003</REF> outperformed <REF>Garera and Yarowskys 2006</REF> association measure when used for ranking, which may due to sparse data problems or simply too much noise for the latter~~~For the association measures, the fact that they are relation-free also means that they can profit from added semantic filtering~~~The novel distance-bounded semantic similarity method where we use the most similar words in the previous discourse together with a semantic classbased filter and a distance limit comes near the precision of using surface patterns, and offers better accuracy than Gasperin and Vieiras method of using the globally most similar words.
Other approaches use large corpora to get an indication for bridging relations: Poesio et al~~~1998 use a general word association metric based on common terms occuring in a fixed-width window, <REF>Gasperin and Vieira 2004</REF> use syntactic contexts of words in a large corpus to induce a semantic similarity measure similar to the one introduced by <TREF>Lin, 1998</TREF>, and then use lists of the n nouns that are globally most similar to a given noun~~~<REF>Markert and Nissim 2005</REF> mine the World Wide Web for shallow patterns like China and other countries, indicating an is-a relationship~~~<REF>Finally, Garera and Yarowsky 2006</REF> propose an association-based approach using nouns that occur in a 2-sentence window before a definite description that has no same-head antecedent.
32 Contexts The context in which a word appears often imposesconstraintsonthesemantictypeoftheword~~~This basic idea has been exploited by many proposals for distributional similarity and clustering, eg, <REF>Church and Hanks, 1989</REF>; <TREF>Lin, 1998</TREF>; <REF>Pereira et al , 1993</REF>~~~Similar to <REF>Lin and Pantel 2001</REF>, we define the contexts of a word to be the undirected paths in dependency trees involving that word at either the beginning or the end~~~The following diagram shows an example dependency tree: Which city hosted the 1988 Winter Olympics.
Note, however, that <REF>McCarthy et al , 2004</REF> used the information about distributionally similar words to approximate corpus frequencies for word senses, whereas we target the estimation of a property of a given word sense the subjectivity~~~Starting with a given ambiguous word w, we first find the distributionally similar words using the method of <TREF>Lin, 1998</TREF> applied to the automatically parsed texts of the British National Corpus~~~Let DSW  dsw1, dsw2,  , dswn be the list of top-ranked distributionally similar words, sorted in decreasing order of their similarity~~~Next, for each sense wsi of the word w, we determine the similarity with each of the words in the list DSW, using a WordNet-based measure of semantic similarity wnss.
41 Weight Tuning There are several motivations for learning the graph weights  in this domain~~~First, some dependency relations  foremost, subject and object  are in general more salient than others <TREF>Lin, 1998</TREF>; <REF>Pado and Lapata, 2007</REF>~~~In addition, dependency relations may have varying importance per different notions of word similarity eg, noun vs verb similarity <REF>Resnik and Diab, 2000</REF>~~~Weight tuning allows the adaption of edge weights to each task ie, distribution of queries.
The learning methods described in this paper can be readily applied to 911 other directed and labelled entity-relation graphs7 The graph representation described in this paper is perhaps most related to syntax-based vector space models, which derive a notion of semantic similarity from statistics associated with a parsed corpus <REF>Grefenstette, 1994</REF>; <TREF>Lin, 1998</TREF>; <REF>Pado and Lapata, 2007</REF>~~~In most cases, these models construct vectors to represent each word wi, where each element in the vector for wi corresponds to particular context c, and represents a count or an indication of whether wi occurred in context c A context can refer to simple co-occurrence with another word wj, to a particular syntactic relation to another word eg, a relation of direct object to wj, etc Given these word vectors, inter-word similarity is evaluated using some appropriate similarity measure for the vector space, such as cosine vector similarity, or Lins similarity <TREF>Lin, 1998</TREF>~~~Recently, Pado and Lapata <REF>Pado and Lapata, 2007</REF> have suggested an extended syntactic vector space model called dependency vectors, in which rather than simple counts, the components of a word vector of contexts consist of weighted scores, which combine both co-occurrence frequency and the importance of a context, based on properties of the connecting dependency paths~~~They considered two different weighting schemes: a length weighting scheme, assigning lower weight to longer connecting paths; and an obliqueness weighting hierarchy <REF>Keenan and Comrie, 1977</REF>, assigning higher weight to paths that include grammatically salient relations.
Instead, we include learning techniques to optimize the graphwalk based similarity measure~~~The learning methods described in this paper can be readily applied to 911 other directed and labelled entity-relation graphs7 The graph representation described in this paper is perhaps most related to syntax-based vector space models, which derive a notion of semantic similarity from statistics associated with a parsed corpus <REF>Grefenstette, 1994</REF>; <TREF>Lin, 1998</TREF>; <REF>Pado and Lapata, 2007</REF>~~~In most cases, these models construct vectors to represent each word wi, where each element in the vector for wi corresponds to particular context c, and represents a count or an indication of whether wi occurred in context c A context can refer to simple co-occurrence with another word wj, to a particular syntactic relation to another word eg, a relation of direct object to wj, etc Given these word vectors, inter-word similarity is evaluated using some appropriate similarity measure for the vector space, such as cosine vector similarity, or Lins similarity <TREF>Lin, 1998</TREF>~~~Recently, Pado and Lapata <REF>Pado and Lapata, 2007</REF> have suggested an extended syntactic vector space model called dependency vectors, in which rather than simple counts, the components of a word vector of contexts consist of weighted scores, which combine both co-occurrence frequency and the importance of a context, based on properties of the connecting dependency paths.
To further enhance the quality of co-occurrence data, we search on the specific phrase a16 is measured in in which a16 is one of the related concepts of a14  This allows for the simultaneous discovery of unknown units and the retrieval of their co-occurrence counts~~~Sentences in which the pattern occurs are parsed using Minipar <TREF>Lin, 1998b</TREF> so that we can obtain the word related to measured via the prepositional in relation~~~This allows us to handle sentential constructions that may intervene between measured and a meaningful unit~~~For each unit a17 that is related to measured via in, we increment the co-occurrence count a18a20a19a21a17a23a22a24a16a26a25, thereby collecting frequency counts for each a17 with a16  The patterns precision prevents incidental cooccurrence between a related concept and some unit that may occur simply because of the general topic of the document.
Thus, there is strong motivation to expand the list of units obtained from Google by automatically considering similar units~~~519 We gather similar units from an automaticallyconstructed thesaurus of distributionally similar words <TREF>Lin, 1998a</TREF>~~~The similar word expansion can add a term like gigs as a unit for size by virtue of its association with gigabytes, which is on the original list~~~Unit similarity can be thought of as a mapping a18 a1 a6 a4 a0 a8 in which a6 is a set of units and a0 a8 is sets of related units.
CLUSTER CLUSTERED SIMILAR WORDS OF DUTY WITH SIMILARITY SCORE responsibility 016, obligation 0109, task 0101, function 0098, role 0091, post 0087, position 0086, job 0084, chore 008, mission 008, assignment 0079, liability 0077  tariff0091, restriction 0089, tax 0086, regulation 0085, requirement 0081, procedure 0079, penalty 0079, quota 0074, rule 007, levy 0061  fee 0085, salary 0081, pay 0064, fine 0058 personnel 0073, staff0073 training 0072, work 0064, exercise 0061 privilege 0069, right 0057, license 0056 22~~~Corpus-based thesaurus Using the collocation database, Lin used an unsupervised method to construct a corpusbased thesaurus <TREF>Lin, 1998a</TREF> consisting of 11839 nouns, 3639 verbs and 5658 adjectives/adverbs~~~Given a word w, the thesaurus returns a clustered list of similar words of w along with their similarity to w For example, the clustered similar words of duty are shown in Table 1~~~23.
2~~~Resources The input to our algorithm includes a collocation database <TREF>Lin, 1998b</TREF> and a corpus-based thesaurus <TREF>Lin, 1998a</TREF>, which are both available on the Interne0~~~In addition, we require a bilingual thesaurus~~~Below, we briefly describe these resources.
78 Table 1~~~Clustered similar words of duty as given by <TREF>Lin, 1998a</TREF>~~~CLUSTER CLUSTERED SIMILAR WORDS OF DUTY WITH SIMILARITY SCORE responsibility 016, obligation 0109, task 0101, function 0098, role 0091, post 0087, position 0086, job 0084, chore 008, mission 008, assignment 0079, liability 0077  tariff0091, restriction 0089, tax 0086, regulation 0085, requirement 0081, procedure 0079, penalty 0079, quota 0074, rule 007, levy 0061  fee 0085, salary 0081, pay 0064, fine 0058 personnel 0073, staff0073 training 0072, work 0064, exercise 0061 privilege 0069, right 0057, license 0056 22~~~Corpus-based thesaurus Using the collocation database, Lin used an unsupervised method to construct a corpusbased thesaurus <TREF>Lin, 1998a</TREF> consisting of 11839 nouns, 3639 verbs and 5658 adjectives/adverbs.
It then assigns a score of 1 if the text contains a synonym, hyponym or derived form of the target word and a score of 0 otherwise~~~42 Similarity As a second measure we used the distributional similarity measure of <TREF>Lin, 1998</TREF>~~~For a text t and a word u we assign the max similarity score as follows: similarityt,u  maxvt simu,v 1 where simu,v is the similarity score for u and v4~~~43 Alignment model <REF>Glickman et al , 2006</REF> was among the top scoring systems on the RTE-1 challenge and supplies a probabilistically motivated lexical measure based on word co-occurrence statistics.
<REF>Bos and Markert, 2005</REF>; <REF>Corley and Mihalcea, 2005</REF>; Jijkoun and de <REF>Rijke, 2005</REF>; <REF>Glickman et al , 2006</REF> applied or utilized lexical based word overlap measures~~~Various word-to-word similarity measures where applied, including distributional similarity such as <TREF>Lin, 1998</TREF>, web-based co-occurrence statistics and WordNet based similarity measures such as <REF>Leacock et al , 1998</REF>~~~23 Paraphrase Acquisition A substantial body of work has been dedicated to learning patterns of semantic equivalency between different language expressions, typically considered as paraphrases~~~Recently, several works addressed the task of acquiring paraphrases semi- automatically from corpora.
Lexical similarity measures eg~~~<TREF>Lin, 1998</TREF> have also been suggested to measure semantic similarity~~~They are based on the distributional hypothesis, suggestingthatwordsthatoccurwithinsimilar contexts are semantically similar~~~22 Textual Entailment TheRecognisingTextualEntailmentRTE-1challenge <REF>Dagan et al , 2006</REF> is an attempt to promote anabstractgenerictaskthatcapturesmajorsemantic inference needs across applications.
We are going to extend the set of content bearing words and to include verbs~~~We will take advantage of the flexibility provided by our framework and use syntax based measure of similarity in the computation of the verb vectors, following <TREF>Lin, 1998</TREF>~~~Currently we are using string matching to compute the named entity based measure of similarity~~~We are planning to integrate more sophisticated techniques in our framework.
Distributional similarity represents the relatedness of two words by the commonality of contexts the words share, based on the distributional hypothesis <REF>Harris, 1985</REF>, which states that semantically similar words share similar contexts~~~A number of researches which utilized distributional similarity have been conducted, including <REF>Hindle, 1990</REF>; <TREF>Lin, 1998</TREF>; <REF>Geffet and Dagan, 2004</REF> and many others~~~Although they have been successful in acquiring related words, various parameters such as similarity measures and weighting are involved~~~As Weeds et al.
2 Distributional Features In this section, we firstly describe how we extract contexts from corpora and then how distributional features are constructed for word pairs~~~21 Context Extraction We adopted dependency structure as the context of words since it is the most widely used and wellperforming contextual information in the past studies <REF>Ruge, 1997</REF>; <TREF>Lin, 1998</TREF>~~~In this paper the sophisticated parser RASP Toolkit 2 <REF>Briscoe et al, 2006</REF> was utilized to extract this kind of word relations~~~We use the following example for illustration purposes: The library has a large collection of classic books by such authors as Herrick and Shakespeare.
Before explaining the following process Clustering 2, let us describe the measure used to calculate the similarity between syntactic positions~~~We use a particular weighted version of the <TREF>Lin 1998</TREF> coefficient~~~Our version, however, does not use pointwise mutual information to characterize the weight on position-word pairs~~~As Manning and Schu tze 1999 argued, this does not seem to be a good measure of the strength of association between a word and a local position.
Inspired by <REF>Lin1999</REF>, weexamine the strength of association between the verb and noun constituents of the target combination and its variants, as an indirect cue to their idiomaticity~~~We use the automatically-built thesaurus of <TREF>Lin 1998</TREF> to find similar words to the noun of the target expression, in order to automatically generate variants~~~Only the noun constituent is varied, since replacing the verb constituent of a VNIC with a semantically related verb is more likely to yield another VNIC, as in keep/lose ones cool <REF>Nunberg et al , 1994</REF>~~~Let a0a2a1a4a3a6a5a8a7a10a9a12a11a14a13a16a15a17a9a18a5a20a19a22a21a24a23a26a25a27a23a29a28a31a30 be the set of the a28 most similar nouns to the noun a9 of the target pair a32a34a33a36a35 a9a38a37.
The measurement is used to rank candidates relative to their compositionality~~~Building on <TREF>Lin 1998</TREF>, McCarthy et al~~~2003 measure the semantic similarity between expressions verb particles as a whole and their component words verb~~~They exploit contextual features and frequency information in order to assess meaning overlap.
<REF>Fazly and Stevenson 2006</REF> use lexical and syntactic fixedness as partial indicators of noncompositionality~~~Their method uses Lins 1998 automatically generated thesaurus to compute a metric of lexical fixedness~~~Lexical fixedness measures the deviation between the pointwise mutual information of a verb-object phrase and the average pointwise mutual information of the expressions resulting from substituting the noun by its synonyms in the original phrase~~~This measure is similar to Lins 1999 proposal for finding noncompositional phrases.
Comparative evaluation in <REF>Cimiano and Volker, 2005</REF> shows that syntactic features lead to better performance~~~Feature weights can be calculated either by Machine Learning algorithms <REF>Fleischman and Hovy, 2002</REF> or by statistical measures, like Point Wise Mutual Information or the Jaccard coefficient <TREF>Lin, 1998a</TREF>~~~A hybrid approach using both pattern-based, term structure, and contextual feature methods is presented in <REF>Cimiano et al , 2005</REF>~~~State-of-the-art approaches may be divided in two classes, according to different use of training data: Unsupervised approaches see <REF>Cimiano et al , 2005</REF> for details and supervised approaches which use manually tagged training data, eg.
4 Representing Syntactic Information Since both the Class-Word and the Class-Example methods work with syntactic features, the main source of information is a syntactically parsed corpus~~~We parsed about half a gigabyte of a news corpus with MiniPar <TREF>Lin, 1998b</TREF>~~~It is a statistically based dependency parser which is reported to reach 89 precision and 82 recall on press reportage texts~~~MiniPar generates syntactic dependency structures directed labeled graphs whose 20 g1 g2 SyntNetg1,g2 loves1 s d15d15 o d37d37d74d74d74 d74d74d74d74 d74d74d74d74 loves4 o d47d47 s d15d15 Jane6 loves1,4 1,24,5 d15d15 4,6 o d47d47 1,3 o d42d42d84d84d84d84d84d84d84d84d84 d84d84d84d84d84d84 d84d84 Jane6 John2 Mary3 John5 John2,5 Mary3 Figure 2: Two syntactic graphs and their Syntactic Network.
Moreover, we used virtually all the words connected syntactically to a term, not only the modifiers~~~A syntactic feature is a pair: word, syntactic relation <TREF>Lin, 1998a</TREF>~~~We use two feature types: First order features, which are directly connected to the training or test examples in the dependency parse trees of Corpus; second order features, which are connected to the training or test instances indirectly byskipping one wordthe verbin the dependency tree~~~As an example, lets consider two sentences: Edison invented the phonograph and Edison created the phonograph.
17 According with the demand for weakly supervised approaches to OP, we propose a method, called Class  Example, which learns a classification model from a set of classified terms, exploiting lexico-syntactic features~~~Unlike most of theapproacheswhichconsiderpairwisesimilarity between terms <REF>Cimiano and Volker, 2005</REF>; <TREF>Lin, 1998a</TREF>, the Class-Example method considers the similarity between a term ti and a set of training examples which represent a certain class~~~This results in a great number of class features and opens the possibility to exploit more statistical data, such asthefrequencyofappearance ofaclassfeature in different training terms~~~In order to show the effectiveness of the ClassExample approach, it has been compared against twodifferentapproaches: iaClass-Patternunsupervised approach, in the style of <REF>Hearst, 1998</REF>; ii an unsupervised approach that considers the word of the class as a pivot word for acquiring relevant contexts for the class we refer to this methodasClassWord.
Context feature approaches use a corpus to extract features from the context in which a semantic class tends to appear~~~Contextual features may be superficial <REF>Fleischman and Hovy, 2002</REF> or syntactic <TREF>Lin, 1998a</TREF>, <REF>Almuhareb and Poesio, 2004</REF>~~~Comparative evaluation in <REF>Cimiano and Volker, 2005</REF> shows that syntactic features lead to better performance~~~Feature weights can be calculated either by Machine Learning algorithms <REF>Fleischman and Hovy, 2002</REF> or by statistical measures, like Point Wise Mutual Information or the Jaccard coefficient <TREF>Lin, 1998a</TREF>.
The task has been approached in a variety of similar perspectives, including term clustering eg~~~<TREF>Lin, 1998a</TREF> and <REF>Almuhareb and Poesio, 2004</REF> and term categorization eg~~~<REF>Avancini et al , 2003</REF>~~~A rather different task is Ontology Learning OL, where new concepts and relations are supposed to be acquired, with the consequence of changing the definition of the Ontology itself see, for instance, <REF>Velardi et al , 2005</REF>.
However, this is not always the case~~~Several researchers <REF>Curran and Moens 2002</REF>, <TREF>Lin 1998</TREF>, van der <REF>Plas and Bouma 2005</REF> have used large monolingual corpora to extract distributionally similar words~~~They use grammatical relations1 to determine the context of a target word~~~We will refer to such systems as monolingual syntax-based systems.
In this paper we use both monolingual syntaxbased approaches and multilingual alignmentbased approaches and compare their performance when using the same similarity measures and evaluation set~~~Monolingual syntax-based distributional similarity is used in many proposals to nd semantically related words <REF>Curran and Moens 2002</REF>, <TREF>Lin 1998</TREF>, van der <REF>Plas and Bouma 2005</REF>~~~Several authors have used a monolingual parallel corpus to nd paraphrases Ibrahim et al~~~2003, <REF>Barzilay and McKeown 2001</REF>.
As a baseline, we implemented the non-referential it detector of <REF>Lappin and Leass 1994</REF>, labelled as LL in the results~~~This is a syntactic detector, a point missed by <REF>Evans 2001</REF> in his criticism: the patterns are robust to intervening words and modi ers eg  it was never thought by the committee that  provided the sentence is parsed correctly7 We automatically parse sentences with Minipar, a broad-coverage dependency parser <TREF>Lin, 1998b</TREF>~~~We also use a separate, extended version of the LL detector, implemented for large-scale nonreferential detection by <REF>Cherry and Bergsma 2005</REF>~~~This system, also for Minipar, additionally detects instances of it labelled with Minipars pleonastic category Subj.
Our approach avoids hand-crafting a set of spe11 ci c indicator features; we simply use the distribution of the pronouns context~~~Our method is thus related to previous work based on <REF>Harris 1985</REF>s distributional hypothesis2 It has been used to determine both word and syntactic path similarity <REF>Hindle, 1990</REF>; <TREF>Lin, 1998a</TREF>; <REF>Lin and Pantel, 2001</REF>~~~Our work is part of a trend of extracting other important information from statistical distributions~~~<REF>Dagan and Itai 1990</REF> use the distribution of a pronouns context to determine which candidate antecedents can  t the context.
These definitions are valid in the context of particular applications; however, in general, the correspondence between paraphrasing and types of lexical relations is not clear~~~The same question arises with automatically constructed thesauri <REF>Pereira et al , 1993</REF>; <TREF>Lin, 1998</TREF>~~~While the extracted pairs are indeed similar, they are not paraphrases~~~For example, while dog and cat are recognized as the most similar concepts by the method described in <TREF>Lin, 1998</TREF>, it is hard to imagine a context in which these words would be interchangeable.
While the extracted pairs are indeed similar, they are not paraphrases~~~For example, while dog and cat are recognized as the most similar concepts by the method described in <TREF>Lin, 1998</TREF>, it is hard to imagine a context in which these words would be interchangeable~~~The first attempt to derive paraphrasing rules from corpora was undertaken by <REF>Jacquemin et al , 1997</REF>, who investigated morphological and syntactic variants of technical terms~~~While these rules achieve high accuracy in identifying term paraphrases, the techniques used have not been extended to other types of paraphrasing yet.
For this reason, knowledge-poor approaches such as the distributional approach are particularly suited for this task~~~Its previous applications eg , <REF>Grefenstette 1993</REF>, <REF>Hearst and Schuetze 1993</REF>, <REF>Takunaga et al 1997</REF>, <TREF>Lin 1998</TREF>, <REF>Caraballo 1999</REF> demonstrated that cooccurrence statistics on a target word is often sufficient for its automatical classification into one of numerous classes such as synsets of WordNet~~~Distributional techniques, however, are poorly applicable to rare words, ie, those words for which a corpus does not contain enough cooccurrence data to judge about their meaning~~~Such words are the primary concern of many practical NLP applications: as a rule, they are semantically focused words and carry a lot of important information.
We also extensively investigated other corpusbased features, such as the number of times the phrase occurred hyphenated or capitalized, and the 4We exclude counts from the training, development, and testing queries discussed in Section 41~~~822 corpus-based distributional similarity <TREF>Lin, 1998</TREF> between a pair of tokens~~~These features are not available from search-engine statistics because search engines disregard punctuation and capitalization, and collecting page-count-based distributional similarity statistics is computationally infeasible~~~Unfortunately, none of the corpus-based features improved performance on the development set and are thus excluded from further consideration.
1 Thesaurus creation Over the last ten years, interest has been growing in distributional thesauruses hereafter simply thesauruses~~~Following initial work by <REF>Sparck Jones, 1964</REF> and <REF>Grefenstette, 1994</REF>, an early, online distributional thesaurus presented in <TREF>Lin, 1998</TREF> has been widely used and cited, and numerous authors since have explored thesaurus properties and parameters: see survey component of <REF>Weeds and Weir, 2005</REF>~~~A thesaurus is created by  taking a corpus  identifying contexts for each word  identifying which words share contexts~~~For each word, the words that share most contexts according to some statistic which also takes account of their frequency are its nearest neighbours.
22 Compound Similarity As a critical technique, word similarity is generally used in the example-based models of semantic classification~~~The measure of word similarity can be divided into two major approaches: taxonomy-based lexical approach <REF>Resnik 1995</REF>, <TREF>Lin 1998a</TREF>, <REF>Chen and Chen 1998</REF> and context-based syntactic approach <TREF>Lin 1998b</TREF>,<REF>Chen and You 2002</REF>, which is not the concern in this context-free model~~~However, two problems arise here for the taxonomy-based lexical approach~~~First, such similarity measures risk the failure to capture the similarity among some semantically highly related words, if they happen to be put under classes distant from each other according to a specific ontology4.
We then calculate normalized tuple similarity scores over the tuple pairs using a metric that accounts for similarities in both syntactic structure and content of each tuple~~~A thesaurus constructed from corpus statistics <TREF>Lin, 1998</TREF> is utilized for the content similarity~~~We utilize this metric to greedily pair together the most similar predicate argument tuples across Figure 2: System architecture sentences~~~Any remaining unpaired tuples represent extra information and are passed to a dissimilarity classi er to decide whether such information is signi cant.
Although the 24 MSR corpus used strict means of resolving interrater disagreements during its construction, the annotators agreed with the MSR corpus labels only 935 187/200 of the time~~~One weakness of our system is that we rely on a thesaurus <TREF>Lin, 1998</TREF> for word similarity information for predicate argument tuple pairing~~~However, it is designed to provide similarity scores between pairs of individual words rather than phrases~~~If a predicate argument tuples target or one argument is realized as a phrase borrow  check out, for instance, the thesaurus is unable to provide an accurate similarity score.
We collected the statistics on the grammatical relations contexts output by Minipar and used these as the feature vectors~~~<REF>Following Lin 1998</REF>, we measure each feature f for a word e not by its frequency but by its pointwise mutual information, mi ef : 126    fPeP feP mi ef  , log 4 Inducing ontological features The resource described in the previous section yields lexical feature vectors for each word in a corpus~~~We term these vectors lexical because they are collected by looking only at the lexicals in the text ie no sense information is used~~~We use the term ontological feature vector to refer to a feature vector whose features are for a particular sense of the word.
We believe that this framework will be useful for a variety of applications, including adding additional semantic information to existing semantic term banks by disambiguating lexical-semantic resources~~~Ontologizing semantic resources Recently, researchers have applied textand web-mining algorithms for automatically creating lexical semantic resources like similarity lists <TREF>Lin 1998</TREF>, semantic lexicons <REF>Riloff and Shepherd 1997</REF>, hyponymy lists <REF>Shinzato and Torisawa 2004</REF>; <REF>Pantel and Ravichandran 2004</REF>, partwhole lists <REF>Girgu et al 2003</REF>, and verb relation graphs <REF>Chklovski and Pantel 2004</REF>~~~However, none of these resources have been directly linked into an ontological framework~~~For example, in VERBOCEAN <REF>Chklovski and Pantel 2004</REF>, we find the verb relation to surpass is-stronger-than to hit, but it is not specified that it is the achieving sense of hit where this relation applies.
The hypothesis states that words that occur in the same contexts tend to have similar meaning~~~Researchers have mostly looked at representing words by their surrounding words <REF>Lund and Burgess 1996</REF> and by their syntactical contexts <REF>Hindle 1990</REF>; <TREF>Lin 1998</TREF>~~~However, these representations do not distinguish between the different senses of words~~~Our framework utilizes these principles and representations to induce disambiguated feature vectors.
ii Sentence splitting, using mxterminator <REF>Reynar and Ratnaparkhi, 1997</REF>~~~iii Dependency parsing, using Minipar <TREF>Lin, 1998b</TREF>~~~The proof search is implemented as a depth-first search, with maximal depth ie proof length of 4~~~If the text contains more than one sentence, the prover aims to prove h from each of the parsed sentences, and entailment is determined based on the minimal cost.
summationtext lh Scorel OpenClassWordsh 2 2We set the threshold to 001 3The active verbal form with direct modifiers where Scorel is 1 if it appears in p, or if it is a derivation of a word in p according to WordNet~~~Otherwise, Scorel is the maximal Lin dependency-basedsimilarityscorebetweenlandthe lemmas of p <TREF>Lin, 1998a</TREF> synonyms and hypernyms/hyponyms are handled by the lexical rules~~~7 System Implementation Deriving the initial propositions t and h from the input text fragments consists of the following steps: i Anaphora resolution, using the MARS system <REF>Mitkov et al , 2002</REF>~~~Each anaphor was replaced by its antecedent.
Figure 1: Application of inference rules~~~POS and relation labels are based on Minipar <TREF>Lin, 1998b</TREF> If a complete proof is found h was generated, the prover concludes that entailment holds~~~Otherwise, entailment is determined by comparing the minimal cost found during the proof search to some threshold ~~~3 Proof System Like logic-based systems, our proof system consists of propositions t, h, and intermediate premises, and inference entailment rules, which derive new propositions from previously established ones.
As a striking example, the 14 most syntactically similar verbs to believe in order are think, guess, hope, feel, wonder, theorize, fear, reckon, contend, suppose, understand, know, doubt, and suggest  all mental action verbs~~~This observation further supports the distributional hypothesis of word similarity and corresponding technologies for identifying synonyms by similarity of lexical-syntactic context <TREF>Lin, 1998</TREF>~~~Verb pairs instances Cosine bind 83 bound 95 0950 plunge 94 tumble 87 0888 dive 36 plunge 94 0867 dive 36 tumble 87 0866 jump 79 tumble 87 0865 fall 84 fell 102 0859 intersperse 99 perch 81 0859 assail 100 chide 98 0859 dip 81 fell 102 0858 buffet 72 embroil 100 0856 embroil 100 lock 73 0856 embroil 100 superimpose 100 0856 fell 102 jump 79 0855 fell 102 tumble 87 0855 embroil 100 whipsaw 63 0850 pluck 100 whisk 99 0849 acquit 100 hospitalize 99 0849 disincline 70 obligate 94 0848 jump 79 plunge 94 0848 dive 36 jump 79 0847 assail 100 lambaste 100 0847 festoon 98 strew 100 0846 mar 78 whipsaw 63 0846 pluck 100 whipsaw 63 0846 ensconce 101 whipsaw 63 0845 Table 2~~~Top 25 most syntactically similar pairs of the 3257 verbs in PropBank.
Our approach is strictly empirical; the similarity of verbs is determined by examining the syntactic contexts in which they appear in a large text corpus~~~Our approach is analogous to previous work in extracting collocations from large text corpora using syntactic information <TREF>Lin, 1998</TREF>~~~In our work, we utilized the GigaWord corpus of English newswire text Linguistic <REF>Data Consortium, 2003</REF>, consisting of nearly 12 gigabytes of textual data~~~To prepare this corpus for analysis, we extracted the body text from each of the 41 million entries in the corpus and applied a maximum-entropy algorithm to identify sentence boundaries <REF>Reynar and Ratnaparkhi, 1997</REF>.
There have been several attempts to group WordNet senses using various different types of information sources~~~This paper describes work to automatically relate WordNet word senses using automatically acquired thesauruses <TREF>Lin, 1998</TREF> and WordNet similarity measures <REF>Patwardhan and Pedersen, 2003</REF>~~~This work proposes using graded word sense relationships rather than fixed groupings clusters~~~Previous research has focused on clustering WordNet senses into groups.
This is transformed from a distance measure in the WN-Similarity package by taking the reciprocal: jcns1,s2  1/Djcns1,s2 We use raw BNC data for calculating IC values~~~DIST We use a distributional similarity measure <TREF>Lin, 1998</TREF> to obtain a fixed number 50 of the top ranked nearest neighbours for the target nouns~~~For input we used grammatical relation data extracted using an automatic parser <REF>Briscoe and Carroll, 2002</REF>~~~We used the 90 million words of written English from the British National Corpus BNC <REF>Leech, 1992</REF>.
To do this, many approaches to lexical acquisition employ the distributional model of word meaning induced from the distribution of the word across various lexical contexts of its occurrence found in the corpus~~~The approach is now being actively explored for a wide range of semantics-related tasks including automatic construction of thesauri <TREF>Lin, 1998</TREF>; <REF>Caraballo, 1999</REF>, their enrichment <REF>Alfonseca and Manandhar, 2002</REF>; <REF>Pekar and Staab, 2002</REF>, acquisition of bilingual lexica from nonaligned <REF>Kay and Rscheisen, 1993</REF> and nonparallel corpora <REF>Fung and Yee, 1998</REF>, learning of information extraction patterns from un-annotated text <REF>Riloff and Schmelzenbach, 1998</REF>~~~However, because of irregularities in corpus data, corpus statistics cannot guarantee optimal performance, notably for rare lexical items~~~In order to improve robustness, recent research has attempted a variety of ways to incorporate external knowledge into the distributional model.
Also, the patterns are learned with the specific goal of scaling to the terascale see Table 2~~~22 Co-occurrence-based approaches The second class of algorithms uses cooccurrence statistics <REF>Hindle 1990</REF>, <TREF>Lin 1998</TREF>~~~These systems mostly employ clustering algorithms to group words according to their meanings in text~~~Assuming the distributional hypothesis <REF>Harris 1985</REF>, words that occur in similar grammatical contexts are similar in meaning.
Other researchers have also clustered words to create semantic lexicons~~~<TREF>Lin 1998</TREF> created a thesaurus using syntactic relationships with other words~~~Rooth et al~~~1999 used clustering to create clusters similar to Levin verb classes <REF>Levin, 1993</REF>.
The number of unique collocations in the resulting database 2 is about 11 million~~~Using the similarity measure proposed in <TREF>Lin, 1998</TREF>, we constructed a corpus-based thesaurus 3 consisting of 11839 nouns, 3639 verbs and 5658 adjective/adverbs which occurred in the corpus at least 100 times~~~3 Mutual Information of a Collocation We define the probability space to consist of all possible collocation triples~~~We use LH R M L to denote the 1 available at http://wwwcsumanitobaca/-lindek/miniparhtm/ 2available at http://wwwcsumanitobca/-lindek/nlldemohtm/ 3available at http://wwwcsumanitobaca/-lindek/nlldemohtm/ 317 frequency count of all the collocations that match the pattern H R M, where H and M are either words or the wild card  and R is either a dependency type or the wild card.
Mutual information has often been used to separate systematic associations from accidental ones~~~It was also used to compute the distributional similarity between words CHin dle, 1990; <TREF>Lin, 1998</TREF>~~~A method to determine the compositionality of verb-object pairs is proposed in <REF>Tapanainen et al , 1998</REF>~~~The basic idea in there is that if an object appears only with one verb of few verbs in a large corpus we expect that it has an idiomatic nature <REF>Tapanainen et al , 1998</REF>, p1290.
We briefly describe the process of obtaining this input~~~More details about the construction of the collocation database and the thesaurus can be found in <TREF>Lin, 1998</TREF>~~~We parsed a 125-million word newspaper corpus with Minipar, 1 a descendent of Principar <REF>Lin, 1993</REF>; <REF>Lin, 1994</REF>, and extracted dependency relationships from the parsed corpus~~~A dependency relationship is a triple: head type modifier, where head and modifier are words in the input sentence and type is the type of the dependency relation.
One possibility is to compare the automatically identified relationships with relationships listed in a manually compiled dictionary~~~For example, <TREF>Lin, 1998</TREF> compared automatically created thesaurus with the WordNet <REF>Miller et al , 1990</REF> and Rogets Thesaurus~~~However, since the lexicon used in our parser is based on the WordNet, the phrasal words in WordNet are treated as a single word~~~For example, take advantage of is treated as a transitive verb by the parser.
From this tree structure, the similarity is obtained: simhill,coast  2355  06~~~4 The similarity between word w with senses w1,,wn and word v with senses v1,,vm is defined as the maximum similarity between all the pairs of word senses: simw,v  maxi,j simwi,vj, 5 whose idea came from Lins method <TREF>Lin, 1998</TREF>~~~42 Discrimination Rate The following two sections describe two evaluation measures based on the reference similarity~~~The first one is discrimination rate DR.
For example, <REF>Hindle 1990</REF> used cooccurrences between verbs and their subjects and objects, and proposed a similarity metric based on mutual information, but no exploration concerning the effectiveness of other kinds of word relationship is provided, although it is extendable to any kinds of contextual information~~~<TREF>Lin 1998</TREF> also proposed an information theorybased similarity metric, using a broad-coverage parser and extracting wider range of grammatical relationship including modifications, but he didnt further investigate what kind of relationships actually had important contributions to acquisition, either~~~The selection of useful contextual information is considered to have a critical impact on the performance of synonym acquisition~~~This is an independent problem from the choice of language model or acquisition method, and should therefore be examined by itself.
Among many kinds of lexical relations, synonyms are especially useful ones, having broad range of applications such as query expansion technique in information retrieval and automatic thesaurus construction~~~Various methods <REF>Hindle, 1990</REF>; <TREF>Lin, 1998</TREF>; <REF>Hagiwara et al , 2005</REF> have been proposed for synonym acquisition~~~Most of the acquisition methods are based on distributional hypothesis <REF>Harris, 1985</REF>, which states that semantically similar words share similar contexts, and it has been experimentally shown considerably plausible~~~However, whereas many methods which adopt the hypothesis are based on contextual clues concerning words, and there has been much consideration on the language models such as Latent Semantic Indexing <REF>Deerwester et al , 1990</REF> and Probabilistic LSI <REF>Hofmann, 1999</REF> and synonym acquisition method, almost no attention has been paid to what kind of categories of contextual information, or their combinations, are useful for word featuring in terms of synonym acquisition.
The method is described in <REF>McCarthy et al , 2004</REF>, which we summarise here~~~We acquire thesauruses for nouns, verbs, adjectives and adverbs based on the method proposed by <TREF>Lin 1998</TREF> using grammatical relations output from the RASP parser <REF>Briscoe and Carroll, 2002</REF>~~~The grammatical contexts used are listed in table 3, but there is scope for extending or restricting the contexts for a given PoS~~~We use the thesauruses for ranking the senses of the target words.
To measure the compositionality, semantically similar words are more suitable than synomys~~~Hence, we choose to use Lins thesaurus <TREF>Lin, 1998</TREF> instead of Wordnet <REF>Miller et al , 1990</REF>~~~902 614 Distributed Frequency of Object a0  The distributed frequency of object is based on the idea that if an object appears only with one verb or few verbs in a large corpus, the collocation is expected to have idiomatic nature <REF>Tapanainen et al , 1998</REF>~~~For example, sure in make sure occurs with very few verbs.
The higher the value of a38, the more is the likelihood of the collocation to be a MWE~~~2obtained from Lins <TREF>Lin, 1998</TREF> automatically generated thesaurus http://wwwcsualbertaca/a66 lindek/downloadshtm~~~We obtained the best results section 8 when we substituted top-5 similar words for both the verb and the object~~~To measure the compositionality, semantically similar words are more suitable than synomys.
As a representative of this approach we use Lins dependency-baseddistributionalsimilaritydatabase~~~Lins database was created using the particular distributionalsimilaritymeasurein<TREF>Lin, 1998</TREF>, applied to a large corpus of news data 64 million words 4~~~Two words obtain a high similarity score if they occur often in the same contexts, as captured by syntactic dependency relations~~~For example, two verbs willbeconsideredsimilariftheyhavelargecommon sets of modifying subjects, objects, adverbs etc Distributional similarity does not capture directly meaning equivalence and entailment but rather a looser notion of meaning similarity <REF>Geffet and Dagan, 2005</REF>.
The setting allowed us to analyze different types of state of the art models and their behavior with respect to characteristic sub-cases of the problem~~~The major conclusion that seems to arise from our experiments is the effectiveness of combining a knowledge based thesaurus such as WordNet with distributional statistical information such as <TREF>Lin, 1998</TREF>, overcoming the known deficiencies of each method alone~~~Furthermore, modeling the a priori substitution likelihood captures the majority of cases in the evaluated setting, mostly because WordNet provides a rather noisy set of substitution candidates~~~On the other hand, successfully incorporating local and global contextual information, as similar to WSD methods, remains a challenging task for future research.
Gurevych 10 JSD and ASD calculate the difference in distributions of words that co-occur with the targets~~~Lin dist distributional measure and Lin GN GermaNet measure follow from <TREF>Lins 1998b</TREF> information-theoretic definition of similarity~~~11 Information content measures rely on finding the lowest common subsumer lcs of the target synsets in a hypernym hierarchy and using corpus counts to determine how specific or general this concept is In general, the more specific the lcs is and the smaller the difference of its specificity with that of the target concepts, the closer the target concepts are~~~12 As GermaNet does not have glosses for synsets, <REF>Gurevych 2005</REF> proposed a way of creating a bag-of-words-type pseudogloss for a synset by including the words in the synset and in synsets close to it in the network.
They used these DPCs to implement an unsupervised nave Bayes word sense classifier that placed first among all unsupervised systems taking part in the Multilingual Chinese English Lexical Sample Task task 5 of SemEval07 <REF>Jin et al , 2007</REF>~~~4 Evaluation We evaluated the newly proposed cross-lingual distributional measures of concept-distance on the tasks of 1 measuring semantic distance between German words and ranking German word pairs according to semantic distance, and 2 solving German Word Power questions from Readers DigestInorder to compare results with state-of-the-art monolingual approaches we conducted experiments using Ger575 Cross-lingual Distributional Measures Monolingual GermaNet Measures Information Contentbased Lesk-like -skew divergence <REF>Lee, 2001</REF> ASD <REF>Jiang and Conrath 1997</REF> JC hypernym pseudo-gloss HPG cosine <REF>Schutze and Pedersen, 1997</REF> Cos <TREF>Lin 1998b</TREF> Lin GN  radial pseudo-gloss RPG Jensen-Shannon divergence JSD <REF>Resnik 1995</REF> Res Lins measure 1998a Lin dist  Table 1: Distance measures used in our experiments~~~Dataset Year Language  pairs PoS Scores  subjects Correlation Gur65 2005 German 65 N discrete 0,1,2,3,4 24 810 Gur350 2006 German 350 N, V, A discrete 0,1,2,3,4 8 690 Table 2: Comparison of datasets used for evaluating semantic distance in German~~~maNet measures as well.
The method we use to predict the rst sense is that of McCarthy et al~~~2004, which was obtained using a thesaurus automatically created from the British National Corpus BNC applying the method of <TREF>Lin 1998</TREF>, coupled with WordNetbased similarity measures~~~This method is fully unsupervised and completely unreliant on any annotations from our dataset~~~In the case of SFs, we perform full synset WSD based on one of the above options, and then map the prediction onto the corresponding unique SF.
2 Related Research The vocabulary mis-match between user queries and indexed documents is often addressed through query expansion~~~Two common techniques for query expansion are blind relevance feedback <REF>Buckley et al , 1995</REF>; <REF>Mitra et al , 1998</REF> and word sense disambiguation WSD <REF>Mihalcea and Moldovan, 1999</REF>; <REF>Lytinen et al , 2000</REF>; <REF>Schutze and Pedersen, 1995</REF>; <TREF>Lin, 1998</TREF>~~~Blind relevance feedback consists of retrieving a small number of documents using a query given by a user, and then constructing an expanded query that includes content words that appear frequently in these documents~~~This expanded query is used to retrieve a new set of documents.
<REF>Mihalcea and Moldovan 1999</REF> and Lytinen et al~~~2000 used a machine readable thesaurus, specifically WordNet <REF>Miller et al , 1990</REF>, to obtain the sense of a word, while <REF>Schutze and Pedersen 1995</REF> and <TREF>Lin 1998</TREF> used automatically constructed thesauri~~~The improvements in retrieval performance reported in <REF>Mitra et al , 1998</REF> are comparable to those reported here note that these researchers consider precision, while we consider recall~~~The results obtained by <REF>Schutze and Pedersen 1995</REF> and by Lytinen et al.
A noun, a4, is thus described by a set of co-occurrence triples a94 a4a7a14 a55 a14a32a95a97a96 and associated frequencies, where a55 is a grammatical relation and a95 is a possible cooccurrence with a4 in that relation~~~For every pair of nouns, where each noun had a total frequency in the triple data of 10 or more, we computed their distributional similarity using the measure given by <TREF>Lin 1998</TREF>~~~If a98a56a30a31a4 a33 is the set of co-occurrence types a30 a55 a14a16a95 a33 such that a99a100a30a42a4a43a14 a55 a14a32a95 a33 is positive then the similarity between two nouns, a4 and a10, can be computed as: a26a41a28a15a28a27a30a42a4a43a14a16a10 a33 a8 a75 a83a102a101a42a103a50 a85 a70a11a104 a83a102a6a13a85a106a105 a104 a83 a67 a85 a30a78a99a100a30a31a4a7a14 a55 a14a16a95 a33a41a107 a99a108a30a31a10a109a14 a55 a14a16a95 a33a86a33 a75 a83a102a101a42a103a50 a85 a70a11a104 a83a102a6a13a85 a99a108a30a31a4a7a14 a55 a14a32a95 a33a45a107 a75 a83a84a101a42a103a50 a85 a70a11a104 a83 a67 a85 a99a108a30a31a10a109a14 a55 a14a16a95 a33 where: a99a108a30a31a4a7a14 a55 a14a32a95 a33 a8a111a110a21a112a114a113 a54 a30a31a95a73a115a116a4a118a117 a55 a33 a54 a30a42a95a73a115 a55 a33 A thesaurus entry of size a3 for a target noun a4 is then defined as the a3 most similar nouns to a4  22 The WordNet Similarity Package We use the WordNet Similarity Package 005 and WordNet version 16~~~2 The WordNet Similarity package supports a range of WordNet similarity scores.
We describe some related work in section 6 and conclude in section 7~~~In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of <TREF>Lin 1998</TREF>~~~This provides the a3 nearest neighbours to each target word, along with the distributional similarity score between the target word and its neighbour~~~We then use the WordNet similarity package <REF>Patwardhan and Pedersen, 2003</REF> to give us a semantic similarity measure hereafter referred to as the WordNet similarity measure to weight the contribution that each neighbour makes to the various senses of the target word.
Let a28a15a34a20a10a18a28a20a34a15a28a25a30a31a4 a33 be the set of senses of a4  For each sense of a4 a4a35a28a37a36a39a38a40a28a20a34a15a10a13a28a15a34a20a28a27a30a31a4 a33  we obtain a ranking score by summing over the a26a41a28a15a28a27a30a42a4a43a14a16a10a45a44 a33 of each neighbour a10a46a44a47a38a48a5 a6  multiplied by a weight~~~This weight is the WordNet similarity score a4a49a10a13a28a15a28  between the target sense a4a35a28a37a36  and the sense of a10a45a44 a10a13a28a37a50a51a38a52a28a15a34a15a10a13a28a20a34a15a28a27a30a42a10a45a44 a33  that maximises this score, divided by the sum of all such WordNet similarity scores for a28a20a34a15a10a13a28a15a34a20a28a27a30a31a4 a33 and a10a46a44  Thus we rank each sense a4a49a28 a36 a38a53a28a15a34a20a10a18a28a20a34a15a28a25a30a31a4 a33 using: a54a56a55 a34a15a57a41a58a41a59a60a34a15a10a13a61a37a34a63a62a64a61a37a65 a55 a34a27a30a31a4a35a28a37a36 a33 a8 a66 a67a69a68a32a70a27a71a73a72 a26a29a28a20a28a27a30a31a4a7a14a32a10 a44 a33a15a74 a4a49a10a13a28a20a28a27a30a31a4a35a28a37a36a42a14a32a10a46a44 a33 a75 a6a18a76a78a77a80a79 a70 a76a82a81 a67 a76a78a81a82a76a42a83a84a6a18a85 a4a35a10a13a28a15a28a25a30a31a4a49a28 a36 a79 a14a16a10a45a44 a33 1 where: a4a49a10a13a28a20a28a27a30a31a4a35a28a37a36a86a14a16a10a45a44 a33 a8 a87a89a88a20a90 a67 a76a92a91 a70 a76a78a81 a67 a76a78a81a82a76a93a83 a67a69a68 a85 a30a42a4a49a10a13a28a15a28a25a30a31a4a35a28a37a36a42a14a32a10a13a28a37a50 a33a86a33 21 Acquiring the Automatic Thesaurus The thesaurus was acquired using the method described by <TREF>Lin 1998</TREF>~~~For input we used grammatical relation data extracted using an automatic parser <REF>Briscoe and Carroll, 2002</REF>~~~For the experiments in sections 3 and 4 we used the 90 million words of written English from the BNC.
In this paper, we will discuss data which reveals that purely statistics-based measures exhibit virtually no difference compared with frequency of occurrence counts, while linguistically more informed measures do reveal such a marked difference  for the problem of term and collocation mining at least~~~Although there has been a fair amount of work employing linguistically sophisticated analysis of candidate items eg , on CE by <TREF>Lin 1998</TREF> and <REF>Lin 1999</REF> as well as on ATR by <REF>Daille 1996</REF>, <REF>Jacquemin 1999</REF>, and <REF>Jacquemin 2001</REF>, these approaches are limited by the difficulty to port grammatical specifications to other domains in the case of ATR or by the error-proneness of full general-language parsers in the case of CE~~~Therefore, most recent approaches in both areas have backed off to more shallow linguistic filtering techniques, such as POS tagging and phrase chunking eg , Frantzi et al~~~2000, <REF>Krenn and Evert 2001</REF>, Nenadic et al.
by comparing its strength of association measured by PMI with those of its lexical variants~~~<REF>Like Lin 1999</REF>, we generate lexical variants of the target automatically by replacing either the verb or the noun constituent by a semantically similar word from the automatically-built thesaurus of <TREF>Lin 1998</TREF>~~~We then use a standard statistic, the z-score, to calculate Fixednesslex: Fixednesslexv, n  PMIv, nPMIstd 2 where PMI is the mean and std the standard deviation over the PMI of the target and all its variants~~~Fixednesssyn quantifies the degree of syntactic fixedness of the target combination, by comparing its behaviour in text with the behaviour of a typical verbobject, both defined as probability distributions over a predefined set of patterns.
In particular, by associating each word with a distribution over the words observed in its context, we can distinguish synonyms from non-synonyms with fair reliability~~~This capability may be exploited to generate corpus-based thesauri automatically <TREF>Lin, 1998</TREF>, or used in any other application of text that might benefit from a measure of lexical semantic similarity~~~And synonymy is a logical first step in a broader research program that seeks to account for natural language semantics through distributional means~~~Previous research into corpus-analytic approaches to synonymy has used the Test of English as a Foreign Language TOEFL.
We seek to overcome these difficulties by generating TOEFL-like tests automatically from WordNet <REF>Fellbaum, 1998</REF>~~~While WordNet has been used before to evaluate corpus-analytic approaches to lexical similarity <TREF>Lin, 1998</TREF>, the metric proposed in that study, while useful for comparative purposes, lacks an intuitive interpretation~~~In contrast, we emulate the TOEFL using WordNet and inherit the TOEFLs easy interpretability~~~Given a corpus, we first derive a list of words occurring with sufficient marginal frequency to support a distributional comparison.
et al~~~2004 we use a3a5a4a7a6a9a8 and obtain our thesaurus using the distributional similarity metric described by <TREF>Lin 1998</TREF>~~~We use WordNet WN as our sense inventory~~~The senses of a worda2 are each assigned a ranking score which sums over the distributional similarity scores of the neighbours and weights each neighbours score by a WN Similarity score <REF>Patwardhan and Pedersen, 2003</REF> between the sense of a2 and the sense of the neighbour that maximises the WN Similarity score.
The distributional hypothesis <REF>Harris, 1968</REF> says the following: The meaning of entities, and the meaning of grammatical relations among them, is related to the restriction of combinations of these entities relative to other entities~~~Over recent years, many applications <TREF>Lin, 1998</TREF>, <REF>Lee, 1999</REF>, <REF>Lee, 2001</REF>, <REF>Weeds et al , 2004</REF>, and <REF>Weeds and Weir, 2006</REF> have been investigating the distributional similarity of words~~~Similarity means that words with similar meaning tend to appear in similar contexts~~~In NLG, the considerationofsemanticsimilarityisusuallypreferred to just distributional similarity.
A word w possesses the feature f if f and w belong to the same Roget category~~~The similarity between two words is then defined as the Dice coefficient of the two feature vectors <TREF>Lin, 1998</TREF>~~~simwl,w2  21Rwl n Rwl tnw,l  Inw l where Rw is the set of words that belong to the same Roget category as w 23 Corpus-based Thesaurus 231 Co-occurrence-based Thesaurus This method is based on the assumption that a pair of words that frequently occur together in the same document are related to the same subject~~~Therefore word co-occurrence information can be used to identify semantic relationships between words <REF>Schutze and Pederson, 1997</REF>; <REF>Schutze and Pederson, 1994</REF>.
232 Syntactically-based Thesaurus In contrast to the previous section, this method attempts to gather term relations on the basis of linguistic relations and not document cooccurrence statistics~~~Words appearing in similax grammatical contexts are assumed to be similar, and therefore classified into the same class <TREF>Lin, 1998</TREF>; <REF>Grefenstette, 1994</REF>; <REF>Grefenstette, 1992</REF>; <REF>Ruge, 1992</REF>; <REF>Hindle, 1990</REF>~~~First, all the documents are parsed using the Apple Pie Parser~~~The Apple Pie Parser is a natural language syntactic analyzer developed by Satoshi Sekine at New York University <REF>Sekine and Grishman, 1995</REF>.
When these are used with weighted attributes, if the weight is greater than zero, then it is considered in the set~~~Other measures, such as LIN and JACCARD have previously been used for thesaurus extraction <TREF>Lin, 1998a</TREF>; <REF>Grefenstette, 1994</REF>~~~Finally, we have generalised some set measures using similar reasoning to <REF>Grefenstette 1994</REF>~~~Alternative generalisations are marked with a dagger.
Section 5 reports on the trade-off between the minimum cutoff and execution time~~~Early experiments in thesaurus extraction <REF>Grefenstette, 1994</REF> suffered from the limited size of available corpora, but more recent experiments have used much larger corpora with greater success <TREF>Lin, 1998a</TREF>~~~For these experiments we ran our relation extractor over the British National Corpus BNC consisting of 114 million words in 62 million sentences~~~The POS tagging and chunking took 159 minutes, and the relation extraction took an addiSETCOSINE jwm; ; wn; ; jpjw m; ; j jwn; ; j COSINE P r;w0 wgtwm; r; w0 wgtwn; r; w0pP wgtwm; ; 2 Pwgtwn; ; 2 SETDICE 2jwm; ; wn; ; jjwm; ; jjwn; ; j DICE P r;w0 wgtwm; r; w0 wgtwn; r; w0P r;w0 wgtwm; r; w0wgtwn; r; w0 DICEy 2 P r;w0 minwgtwm; r; w0;wgtwn; r; w0P r;w0 wgtwm; r; w0wgtwn; r; w0 SETJACCARD jwm; ; wn; ; jjwm; ; wn; ; j JACCARD P r;w0 minwgtwm; r; w0;wgtwn; r; w0P r;w0 maxwgtwm; r; w0;wgtwn; r; w0 JACCARDy P r;w0 wgtwm; r; w0 wgtwn; r; w0P r;w0 wgtwm; r; w0wgtwn; r; w0 LIN P r;w0 wgtwm; r; w0wgtwn; r; w0P wgtwm; ; Pwgtwn; ;  Table 1: Measure functions evaluated tional 7:5 minutes.
The list of measure and weight functions we compared against is not complete, and we hope to add other functions to provide a general framework for thesaurus extraction experimentation~~~We would also like to expand our evaluation to include direct methods used by others <TREF>Lin, 1998a</TREF> and using the extracted thesaurus in NLP tasks~~~We have also investigated the speed/performance trade-off using frequency cutoffs~~~This has lead to the proposal of a new approximate comparison algorithm based on canonical attributes and a process of coarseand ne-grained comparisons.
These experiments also cover a range of weight functions as de ned in Table 2~~~The weight functions LIN98A, LIN98B, and GREF94 are taken from existing systems <TREF>Lin, 1998a</TREF>; <TREF>Lin, 1998b</TREF>; <REF>Grefenstette, 1994</REF>~~~Our proposed weight functions are motivated by our intuition that highly predictive attributes are strong collocations with their terms~~~Thus, we have implemented many of the statistics described in the Collocations chapter of Manning and Schcurrency1utze 1999, including the T-Test, 2-Test, Likelihood Ratio, and Mutual Information.
The resultant representation contained a total of 28 million relation occurrences over 10 million different relations~~~We describe the functions evaluated in these experiments using an extension of the asterisk notation used by <TREF>Lin 1998a</TREF>, where an asterisk indicates a set ranging over all existing values of that variable~~~For example, the set of attributes of the term w is: w; ;  fr;w0j9w;r;w0g For convenience, we further extend the notation for weighted attribute vectors~~~A subscripted asterisk indicates that the variables are bound together: X r;w0 wgtwm; r; w0 wgtwn; r; w0 which is a notational abbreviation of: X r;w02wm; ; wn; ;  wgtwm;r;w0 wgtwn;r;w0 For weight functions we use similar notation: f w; ;  X r;w02w; ;  f w;r;w0 nw; ;  jw; ; j Nw jfwj9w; ; ,;gj Table 1 de nes the measure functions evaluated in these experiments.
Some systems de ne the context as a window of words surrounding each thesaurus term <REF>McDonald, 2000</REF>~~~Many systems extract grammatical relations using either a broad coverage parser <TREF>Lin, 1998a</TREF> or shallow statistical tools <REF>Grefenstette, 1994</REF>; <REF>Curran and Moens, 2002</REF>~~~Our experiments use a shallow relation extractor based on <REF>Grefenstette, 1994</REF>~~~We de ne a context relation instance as a tuple w;r;w0 where w is the thesaurus term, which occurs in some grammatical relation r with another word w0 in the sentence.
These systems differ primarily in their de nition of context and the way they calculate similarity from the contexts each term appears in~~~Most systems extract co-occurrence and syntactic information from the words surrounding the target term, which is then converted into a vector-space representation of the contexts that each target term appears in <REF>Pereira et al , 1993</REF>; <REF>Ruge, 1997</REF>; <TREF>Lin, 1998b</TREF>~~~Other systems take the whole document as the context and consider term co-occurrence at the document level <REF>Crouch, 1988</REF>; <REF>Sanderson and Croft, 1999</REF>~~~Once these contexts have been dened, these systems then use clustering or nearest neighbour methods to nd similar terms.
431 Corpus-based Lexical Similarity Lexical similarity was computed using the Word Sketch Engine WSE <REF>Killgarrif et al , 2004</REF> similarity metric applied over British National Corpus~~~The WSE similarity metric implements the word similarity measure based on grammatical relations as defined in <TREF>Lin, 1998</TREF> with minor modifications~~~432 The Brandeis Semantic Ontology As a second source of lexical coherence, we used the Brandeis Semantic Ontology or BSO <REF>Pustejovsky et al , 2006</REF>~~~The BSO is a lexicallybased ontology in the Generative Lexicon tradition <REF>Pustejovsky, 2001</REF>; <REF>Pustejovsky, 1995</REF>.
<REF>Mihalcea and Moldovan, 2001</REF> implements six semantic rules, using twin and autohyponym features, in addition to other WordNet-structure-based rules such as whether two synsets share a pertainym, antonym, or are clustered together in the same verb group~~~A large body of work has attempted to capture corpus-based estimates of word similarity <REF>Pereira et al , 1993</REF>; <TREF>Lin, 1998</TREF>; however, the lack of large sense-tagged corpora prevent most such techniques from being used effectively to compare different senses of the same word~~~Some corpus-based attempts that are capable of estimating similarity between word senses include the topic signatures method; here, <REF>Agirre and Lopez, 2003</REF> collect contexts for a polysemous word based either on sensetagged corpora or by using a weighted agglomeration of contexts of a polysemous words monosemous relatives ie , single-sense synsets related by hypernym, hyponym, or other relations from some large untagged corpus~~~Other corpus-based techniques developed specifically for sense clustering include <REF>McCarthy, 2006</REF>, which uses a combination of word-to-word distributional similarity combined with the JCN WordNet-based similarity measure, and work by <REF>Chugur et al , 2002</REF> in finding co-occurrences of senses within documents in sense-tagged corpora.
Much work has gone into methods for measuring synset similarity; early work in this direction includes <REF>Dolan, 1994</REF>, which attempted to discover sense similarities between dictionary senses~~~A variety of synset similarity measures based on properties of WordNet itself have been proposed; nine such measures are discussed in <REF>Pedersen et al , 2004</REF>, including gloss-based heuristics <REF>Lesk, 1986</REF>; <REF>Banerjee and Pedersen, 2003</REF>, information-content based measures <REF>Resnik, 1995</REF>; <TREF>Lin, 1998</TREF>; <REF>Jiang and Conrath, 1997</REF>, and others~~~Other approaches have used specific cues from WordNet structure to inform the construction of semantic rules; for example, <REF>Peters et al , 1998</REF> suggest clustering two senses based on a wide variety of structural cues from WordNet, including if they are twins if two synsets share more than one word in their synonym list or if they represent an example of autohyponymy if one sense is the direct descendant of the other~~~<REF>Mihalcea and Moldovan, 2001</REF> implements six semantic rules, using twin and autohyponym features, in addition to other WordNet-structure-based rules such as whether two synsets share a pertainym, antonym, or are clustered together in the same verb group.
3 Learning to merge word senses 31 WordNet-based features Here we describe the feature space we construct for classifying whether or not a pair of synsets should be merged; first, we employ a wide variety of linguistic features based on information derived from WordNet~~~We use eight similarity measures implemented within the WordNet::Similarity package5, described in <REF>Pedersen et al , 2004</REF>; these include three measures derived from the paths between the synsets in WordNet: HSO Hirst and St-<REF>Onge, 1998</REF>, LCH <REF>Leacock and Chodorow, 1998</REF>, and WUP <REF>Wu and Palmer, 1994</REF>; three measures based on information content: RES <REF>Resnik, 1995</REF>, LIN <TREF>Lin, 1998</TREF>, and JCN <REF>Jiang and Conrath, 1997</REF>; the gloss-based Extended Lesk Measure LESK, <REF>Banerjee and Pedersen, 2003</REF>, and finally the gloss vector similarity measure VECTOR <REF>Patwardan, 2003</REF>~~~We implement the TWIN feature <REF>Peters et al , 1998</REF>, which counts the number of shared synonyms between the two synsets~~~Additionally we produce pairwise features indicating whether two senses share an ANTONYM, PERTAINYM, or derivationally-related forms DERIV.
Our thesaurus brings up only alternatives that have the same part-of-speech with the target word~~~The choices could come from various inventories of near-synonyms or similar words, for example the Roget thesaurus Roget, 1852, dictionaries of synonyms <REF>Hayakawa, 1994</REF>, or clusters acquired from corpora <TREF>Lin, 1998</TREF>~~~In this paper we focus on the task of automatically selecting the best near-synonym that should be used in a particular context~~~The natural way to validate an algorithm for this task would be to ask human readers to evaluate the quality of the algorithms output, but this kind of evaluation would be very laborious.
For more details on hypernym collocations, see OHara, forthcoming~~~Word-similarity classes <TREF>Lin, 1998</TREF> derived from clustering are also used to expand the pool of potential collocations; this type of semantic relatedness among words is expressed in the SimilarColl feature~~~For the DictColl features, definition analysis OHara, forthcoming is used to determine the semantic relatedness of the defining words~~~Dierences between these two sources of word relations are illustrated by looking at the information they provide for ballerina: word-clusters: dancer:0115 baryshnikov:0072 pianist:0056 choreographer:0049  18 other words nicole:0041 wrestler:0040 tibetans:0040 clown:0040 definition words: dancer:00013 female:00013 ballet:00004 This shows that word clusters capture a wider range of relatedness than the dictionary definitions at the expense of incidental associations eg , nicole.
Again, because context words are not disambiguated, the relations for all senses of a context word are conflated~~~For details on the extraction of word clusters, see <TREF>Lin, 1998</TREF>; and, for details on the definition analysis, see OHara, forthcoming~~~When formulating the features SimilarColl and DictColl, the words related to each context word are considered as potential collocations <REF>Wiebe et al , 1998</REF>~~~Co-occurrence freSense Distinctions Precision Recall Fine-grained 566 565 Course-grained 660 658 Table 1: Results for Senseval-3 test data.
A set of seed words begins the process~~~For each seed s i, the precision of the set s i C i,n in the training data is calculated, where C i,n is the set of n words most similar to s i, according to <TREF>Lins 1998</TREF> method~~~If the precision of s i C i,n is greater than a threshold T, then the words in this set are retained as PSEs~~~If it is not, neither s i nor the words in C i,n are retained.
Many variants of distributional similarity have been used in NLP <REF>Lee 1999</REF>; <REF>Lee and Pereira 1999</REF>~~~<TREF>Dekang Lins 1998</TREF> method is used here~~~In contrast to many implementations, which focus exclusively on verb-noun relationships, Lins method incorporates a variety of syntactic relations~~~This is important for subjectivity recognition, because PSEs are not limited to verb-noun relationships.
We are not aware of other work that uses such collocations as we do~~~Features identified using distributional similarity have previously been used for syntactic and semantic disambiguation <REF>Hindle 1990</REF>; <REF>Dagan, Pereira, and Lee 1994</REF> and to develop lexical resources from corpora <TREF>Lin 1998</TREF>; <REF>Riloff and Jones 1999</REF>~~~We are not aware of other work identifying and using density parameters as described in this article~~~Since our experiments, other related work in NLP has been performed.
The method is then used to identify an unusual form of collocation: One or more positions in the collocation may be filled by any word of an appropriate part of speech that is unique in the test data~~~The third type of subjectivity clue we examine here are adjective and verb features identified using the results of a method for clustering words according to distributional similarity <TREF>Lin 1998</TREF> Section 34~~~We hypothesized that two words may be distributionally similar because they are both potentially subjective eg , tragic, sad, and poignant are identified from bizarre~~~In addition, we use distributional similarity to improve estimates of unseen events: A word is selected or discarded based on the precision of it together with its n most similar neighbors.
We also presented a procedure for automatically identifying potentially subjective collocations, including fixed collocations and collocations with placeholders for unique words~~~In addition, we used the results of a method for clustering words according to distributional similarity <TREF>Lin 1998</TREF> to identify adjectival and verbal clues of subjectivity~~~Table 9 summarizes the results of testing all of the above types of PSEs~~~All show increased precision in the evaluations.
The verbs were clustered into 64 classes using the probabilistic co-occurrence model of <REF>Hofmann and Puzicha 1998</REF>~~~The clustering algorithm uses a database of verb-direct-object relations extracted by <TREF>Lin 1998</TREF>~~~We then use the verb class of the current predicate as a feature~~~4.
Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries~~~For these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity <REF>Pereira et al , 1993</REF>; <TREF>Lin, 1998</TREF>~~~techniques~~~For intercategorial synonymy involving a derivational morphology link, some resources are available which however are only partial in that they only store morphological families that is, sets of items that are morphologically related.
The experiment was conducted using an 18 million tokens subset of the Reuters RCV1 corpus,2 parsed by Lins Minipar dependency parser <REF>Lin, 1993</REF>~~~We considered first an evaluation based on WordNet data as a gold standard, as in <TREF>Lin, 1998</TREF>; <REF>Weeds and Weir, 2003</REF>~~~However, we found that many word pairs from the Reuters Corpus that are clearly substitutable are not linked appropriately in WordNet~~~We therefore conducted a manual evaluation based on the judgments of two human subjects.
Typical feature weighting functions include the logarithm of the frequency of word-feature cooccurrence <REF>Ruge, 1992</REF>, and the conditional probability of the feature given the word within probabilistic-based measures <REF>Pereira et al , 1993</REF>, <REF>Lee, 1997</REF>, <REF>Dagan et al , 1999</REF>~~~Probably the most widely used association weight function is point-wise Mutual Information MI <REF>Church et al , 1990</REF>, <REF>Hindle, 1990</REF>, <TREF>Lin, 1998</TREF>, <REF>Dagan, 2000</REF>, defined by:  ,log, 2 fPwP fwPfwMI  A known weakness of MI is its tendency to assign high weights for rare features~~~Yet, similarity measures that utilize MI showed good performance~~~In particular, a common practice is to filter out features by minimal frequency and weight thresholds.
Finally, a novel feature weighting and selection function is presented, which yields superior feature vectors and better word similarity performance~~~Distributional Similarity has been an active research area for more than a decade <REF>Hindle, 1990</REF>, <REF>Ruge, 1992</REF>, <REF>Grefenstette, 1994</REF>, <REF>Lee, 1997</REF>, <TREF>Lin, 1998</TREF>, <REF>Dagan et al , 1999</REF>, <REF>Weeds and Weir, 2003</REF>~~~Inspired by Harris distributional hypothesis <REF>Harris, 1968</REF>, similarity measures compare a pair of weighted feature vectors that characterize two words~~~Features typically correspond to other words that co-occur with the characterized word in the same context.
We picked the widely cited and competitive eg~~~<REF>Weeds and Weir, 2003</REF> measure of <TREF>Lin 1998</TREF> as a representative case, and utilized it for our analysis and as a starting point for improvement~~~21 Lins 98 Similarity Measure Lins similarity measure between two words, w and v, is defined as follows:,,, ,, ,           fvweightfwweight fvweightfwweight vwsim vFfwFf vFwFf where Fw and Fv are the active features of the two words and the weight function is defined as MI~~~A feature is defined as a pair <term, syntacCountryState Ranks CountryEconomy Ranks Broadcast Goods Civilservant Bloc Nonaligned Neighboring Statistic Border Northwest 24 140 64 30 55 15 165 10 41 50 16 54 77 60 165 43 247 174 Devastate Developed Dependent Industrialized Shattered Club Black Million Electricity 81 36 101 49 16 155 122 31 130 8 78 26 85 141 38 109 245 154 Table 3: The top-10 common features for the word pairs country-state and country-economy, along with their corresponding ranks in the sorted feature lists of the two words.
These categories represent the coarse senses of bark~~~Note that published thesauri are structurally quite different from the thesaurus automatically generated by <TREF>Lin 1998</TREF>, wherein a word has exactly one entry, and its neighbors may be semantically related to it in any of its senses~~~All future mentions of thesaurus will refer to a published thesaurus~~~While other sense inventories such as WordNet exist, use of a published thesaurus has three distinct advantages: i coarse sensesit is widely believed that the sense distinctions of WordNet are far too fine-grained Agirre and Lopez de <REF>Lacalle Lekuona 2003</REF> and citations therein; ii computational easewith just around a thousand categories, the wordcategory matrix has a manageable size; iii widespread availabilitythesauri are available or can be created with relatively less effort in numerous languages, while WordNet is available only for English and a few romance languages.
2004 automatically determine domainspecific predominant senses of words, where the domain may be specified in the form of an untagged target text or simply by name for example, financial domain~~~The system Figure 1 automatically generates a thesaurus <TREF>Lin, 1998</TREF> using a measure of distributional similarity and an untagged corpus~~~The target text is used for this purpose, provided it is large enough to learn a thesaurus from~~~Otherwise a large corpus with sense distribution similar to the target text text pertaining to the specified domain must be used.
This requires large amounts of partof-speech-tagged and chunked data from that domain~~~Further, the target text must be large enough to learn a thesaurus from <TREF>Lin 1998</TREF> used a 64million-word corpus, or a large auxiliary text with a sense distribution similar to the target text must be provided McCarthy et al~~~2004 separately used 90-, 325-, and 91-million-word corpora~~~By contrast, in this paper we present a method that accurately determines sense dominance even in relatively small amounts of target text a few hundred sentences; although it does use a corpus, it does not require a similarly-sense-distributed corpus.
Furthermore, whereas we expect to yield different trees modeling the connotations of different texts, MSTs ignore this aspect dependency since they focus on a unique spanning tree of the underlying feature space~~~Another candidate is given by dependency trees <REF>Rieger, 1984</REF> which are equal to similarity trees <TREF>Lin, 1998</TREF>: for a given root x, the nodes are inserted into its similarity tree ST in descending order of their similarity to x, where the predecessor of any node z is chosen to be the node y already inserted, to which z is most similar~~~Although STs already capture the aspect dependency induced by their varying roots, the path criterion is still not met~~~Thus, we generalize the concept of a ST to that of a cohesion tree as follows: First, we observe that the construction of STs uses two types of order relations: the first, let it call 1x, determines the order of the nodes inserted dependent on root x; the second, let it call 2y, varies with node y to be inserted and determines its predecessor.
2003~~~Typical relation-constrained DPs are those of <TREF>Lin 1998</TREF> and <REF>Lee 2001</REF>~~~Below are contrived, but plausible, examples of each for the word pulse; the numbers are conditional probabilities~~~relation-free DP pulse: beat 28, racing 2, grow 13, beans 09, heart 04,   .
Co-occurrence counts less than 5 were reset to 0, and words that co-occurred with more than 2000 other words were stoplisted 543 in all~~~We used ASD cp  BP 0BM99, Cos cp, JSD cp,andLin pmi 4 to populate corresponding conceptconcept distance matrices and 4 <REF>Whereas Lin 1998</REF> used relation-constrained DPs, in our experiments all DPs are relation-free~~~Table 2: Correlation of distributional measures with human ranking~~~Best results for each measure-type are shown in boldface.
The distance between two words, given their DPs, is calculated using a measure of DP distance, such as cosine~~~While any of the measures of DP distance may be used with any of the measures of strength of association see Table 1, in practice -skew divergence ASD, cosine, and JensenShannon divergence JSD are used with conditional probability CP, whereas Lin is used with PMI, resulting in the distributional measures ASD cp <REF>Lee, 2001</REF>, Cos cp <REF>Schutze and Pedersen, 1997</REF>, JSD cp,andLin pmi <TREF>Lin, 1998</TREF>, respectively~~~ASD cp is a modification of Kullback-Leibler divergence that overcomes the latters problem of division by zero, which can be caused by data sparseness~~~JSD cp is another relative entropybased measure like ASD cp  but it is symmetric.
Given this assumption, identifying the types of information provided by speakers and distinguishing and quantifying the relationships between stimulus and response can serve a number of purposes for NLP applications~~~First, the notion of semantic verb relations is crucial for many NLP tasks and applications such as verb clustering <REF>Pereira et al , 1993</REF>; <REF>Merlo and Stevenson, 2001</REF>; <TREF>Lin, 1998</TREF>; Schulte im <REF>Walde, 2003</REF>, thesaurus extraction <REF>Lin, 1999</REF>; <REF>McCarthy et al , 2003</REF>, word sense discrimination <REF>Schutze, 1998</REF>, text indexing <REF>Deerwester et al , 1990</REF>, and summarisation <REF>Barzilay et al , 2002</REF>~~~Different applications incorporate different semantic verb relations, varying with respect to their demands~~~To date, limited effort has been spent on specifying the range of verb-verb relations.
As mentioned before, most previous work on distributional similarity has focused either on a specific word-word relation such as Pereira et al~~~1993 referring to a direct object noun for describing verbs, or used any syntactic relationship detected by the chunker or parser such as Lin 1999; 1998 and McCarthy et al~~~2003~~~Naturally, the contribution of distributional features depends on the distributional objects and the application, but our results suggest that it is worth determining a task-specific set of prominent features.
The features in the distributional descriptions can be varied in nature: words co-occurring in a document, in a context window, or with respect to a word-word relationship, such as syntactic structure, syntactic and semantic valency, etc Most previous work on distributional similarity has either focused on a specific word-word relation such as Pereira et al~~~1993 referring to a direct object noun for describing verbs, or used any dependency relation detected by the chunker or parser such as Lin 1999; 1998, and McCarthy et al~~~2003~~~Little effort has been spent on varying the mostly nominal types of verb features.
We therefore decide to break it into two individual dependency relations: PPwith, PP-fork~~~Although dependency relations have been widelyusedinautomaticacquisitionoflexicalinformation, such as detection of polysemy <TREF>Lin, 1998</TREF> and WSD <REF>McCarthy et al, 2004</REF>, their utility in AVC still remains untested~~~Co-occurrence CO: CO features mostly convey lexical information only and are generally considered not particularly sensitive to argument structures <REF>Rohde et al, 2004</REF>~~~Nevertheless, it is worthwhile testing whether the meaning components that are brought out by syntactic alternations are also correlated to the neighboring words.
The related Dice Coe cient Frakes and Baeza-<REF>Yates, 1992</REF> is omitted here since it has been shown van <REF>Rijsbergen, 1979</REF> that Dice and Jaccards Coe cients are monotonic in each other~~~Lins Measure <TREF>Lin, 1998</TREF> is based on his information-theoretic similarity theorem, which states, the similarity between A and B is measured by the ratio between the amount of information needed to state the commonality of A and B and the information needed to fully describe what A and B are~~~The nal three measures are settings in the additive MI-based Co-occurrence Retrieval Model AMCRM <REF>Weeds and Weir, 2003</REF>; <REF>Weeds, 2003</REF>~~~We can measure the precision and the recall of a potential neighbours retrieval of the co-occurrences of the target word, where the sets of required and retrieved cooccurrences Fw1 and Fw2 respectively are those co-occurrences for which MI is positive.
We then generated ranked sets of nearest neighbours of size k  200 and where a word is excluded from being a neighbour of itself for each word and each measure~~~For a given word, we compute the overlap between neighbour sets using a comparison technique adapted from <TREF>Lin 1998</TREF>~~~Given a word w, each word w0 in WScomp is assigned a rank score of k rank if it is one of the k nearest neighbours of w using measure m and zero otherwise~~~If NSw;m is the vector of such scores for word w and measure m, then the overlap, CNSw;m1;NSw;m2, of two neighbour sets is the cosine between the two vectors: CNSw;m1;NSw;m2  P w0rm1w0;w rm2w0;wP k i1i2 The overlap score indicates the extent to which sets share members and the extent to which they are in the same order.
In the simplest case, the features of a word are de ned as the contexts in which it has been seen to occur~~~simjami is a variant <TREF>Lin, 1998</TREF> in which the features of a word are those contexts for which the pointwise mutual information MI between the word and the context is positive, where MI can be calculated using Ic;w  log PcjwPc~~~The related Dice Coe cient Frakes and Baeza-<REF>Yates, 1992</REF> is omitted here since it has been shown van <REF>Rijsbergen, 1979</REF> that Dice and Jaccards Coe cients are monotonic in each other~~~Lins Measure <TREF>Lin, 1998</TREF> is based on his information-theoretic similarity theorem, which states, the similarity between A and B is measured by the ratio between the amount of information needed to state the commonality of A and B and the information needed to fully describe what A and B are.
Other potential applications apply the hypothesised relationship <REF>Harris, 1968</REF> between distributional similarity and semantic similarity; ie, similarity in the meaning of words can be predicted from their distributional similarity~~~One advantage of automatically generated thesauruses <REF>Grefenstette, 1994</REF>; <TREF>Lin, 1998</TREF>; <REF>Curran and Moens, 2002</REF> over large-scale manually created thesauruses such as WordNet <REF>Fellbaum, 1998</REF> is that they might be tailored to a particular genre or domain~~~However, due to the lack of a tight de nition for the concept of distributional similarity and the broad range of potential applications, a large number of measures of distributional similarity have been proposed or adopted see Section 2~~~Previous work on the evaluation of distributional similarity methods tends to either compare sets of distributionally similar words to a manually created semantic resource <TREF>Lin, 1998</TREF>; <REF>Curran and Moens, 2002</REF> or be oriented towards a particular task such as language modelling <REF>Dagan et al , 1999</REF>; <REF>Lee, 1999</REF>.
However, due to the lack of a tight de nition for the concept of distributional similarity and the broad range of potential applications, a large number of measures of distributional similarity have been proposed or adopted see Section 2~~~Previous work on the evaluation of distributional similarity methods tends to either compare sets of distributionally similar words to a manually created semantic resource <TREF>Lin, 1998</TREF>; <REF>Curran and Moens, 2002</REF> or be oriented towards a particular task such as language modelling <REF>Dagan et al , 1999</REF>; <REF>Lee, 1999</REF>~~~The rst approach is not ideal since it assumes that the goal of distributional similarity methods is to predict semantic similarity and that the semantic resource used is a valid gold standard~~~Further, the second approach is clearly advantageous when one wishes to apply distributional similarity methods in a particular application area.
Let Seenrp be the set of seen headwords for an argument rp of a predicate p Then we model the selectional preference S of rp for a possible headword w0 as a weighted sum of the similarities between w0 and the seen headwords: Srpw0  summationdisplay wSeenrp simw0,wwtrpw simw0,w is the similarity between the seen and the potential headword, and wtrpw is the weight of seen headword w Similarity simw0,w will be computed on the generalization corpus, again on the basis of extracted tuples p,rp,w~~~We will be using the similarity metrics shown in Table 1: Cosine, the Dice and Jaccard coefficients, and <REF>Hindles 1990</REF> and <TREF>Lins 1998</TREF> mutual information-based metrics~~~We write f for frequency, I for mutual information, and Rw for the set of arguments rp for which w occurs as a headword~~~In this paper we only study corpus-based metrics.
Some approaches have used WordNet for the generalization step <REF>Resnik, 1996</REF>; <REF>Clark and Weir, 2001</REF>; <REF>Abe and Li, 1993</REF>, others EM-based clustering <REF>Rooth et al , 1999</REF>~~~In this paper we propose a new, simple model for selectional preference induction that uses corpus-based semantic similarity metrics, such as Cosine or <TREF>Lins 1998</TREF> mutual informationbased metric, for the generalization step~~~This model does not require any manually created lexical resources~~~In addition, the corpus for computing the similarity metrics can be freely chosen, allowing greater variation in the domain of generalization than a fixed lexical resource.
The use of synonyms is another way of increasing the coverage of question terminology;; while semantic features try to achieve it by generalization, synonyms do it by lexical expansion~~~Our plan is to use the synonyms obtained from very large corpora reported in <TREF>Lin, 1998</TREF>~~~We are also planning to compare the lexical and semantic features we derived automatically in this work with manually selected features~~~In our previous work, manually selected lexical featuresshowedslightlybetterperformanceforthe training data but no signi cant dierence for the test data.
In addition, a lot of methods have been proposed to automatically construct thesauri of synonyms~~~For example, <TREF>Lin 1998</TREF> clustered words with similar meanings by calculating the dependency similarity~~~<REF>Barzilay and McKeown 2001</REF> extracted paraphrases using multiple translations of literature works~~~<REF>Wu and Zhou 2003</REF> extracted synonyms with multiple resources, including a monolingual dictionary, a bilingual corpus, and a monolingual corpus.
The boundaries for determining cooccurrence will affect the estimates and as a consequence the word similarity measures~~~Statistical word similarity measures play an important role in information retrieval and in many other natural language applications, such as the automatic creation of thesauri <REF>Grefenstette, 1993</REF>; <REF>Li and Abe, 1998</REF></REF>; <TREF>Lin, 1998</TREF> and word sense disambiguation <REF>Yarowsky, 1992</REF>; <REF>Li and Abe, 1998</REF></REF>~~~<REF>Pantel and Lin 2002</REF> use word similarity to create groups of related words, in order to discover word senses directly from text~~~Recently, Tan et al.
In the next section, we proceed to apply this technique for generating noun similarity lists~~~4 Building Noun Similarity Lists A lot of work has been done in the NLP community on clustering words according to their meaning in text <REF>Hindle, 1990</REF>; <TREF>Lin, 1998</TREF>~~~The basic intuition is that words that are similar to each other tend to occur in similar contexts, thus linking the semantics of words with their lexical usage in text~~~One may ask why is clustering of words necessary in the first place.
Other approaches usually consider either given sets of synonyms among which one is to be chosen for a translation for instance <REF>Edmonds and Hirst, 2002</REF> or must choose a synonym word against unrelated terms in the context of a synonymy test <REF>Freitag et al , 2005</REF>, a seemingly easier task than actually proposing synonyms~~~<TREF>Lin, 1998</TREF> proposes a different methodology for evaluation of candidate synonyms, by comparing similarity measures of the terms he provides with the similarity measures between them in Wordnet, using various semantic distances~~~This makes for very complex evaluation procedures without an intuitive interpretation, and there is no assessment of the quality of the automated thesaurus~~~6 Conclusion We have developed a general method to extract nearsynonyms from a dictionary, improving on the two baselines.
A measure of similarity is almost always used to rank possible candidates~~~In the case of distributional approaches, similarity if determined from the appearance in similar contexts <TREF>Lin, 1998</TREF>; in the case of dictionary-based methods, lexical relations are deduced from the links between words expressed in definitions of entries~~~Approaches that rely on distributional data have two major drawbacks: they need a lot of data, generally syntactically parsed sentences, that is not always available for a given language English is an exception, and they do not discriminate well among lexical relations mainly hyponyms, antonyms, hypernyms <REF>Weeds et al , 2004</REF>  Dictionary-based 70 approaches address the first problem since dictionaries are readily available for a lot of language, even electronically, and this is the raison dtre of our effort~~~As we have seen here, it is not an obvious task to sort related terms with respect to synonymy, hypernymy, etc, just as with distribution approaches.
5 Related work Among the methods proposed to collect synonymy information, two families can be distinguished according to the input they consider~~~Either a general dictionary is used or more than one <REF>Wu and Zhou, 2003</REF>, or a corpus of unconstrained texts from which lexical distributions are computed simple collocations or syntactic dependencies <TREF>Lin, 1998</TREF>; <REF>Freitag et al , 2005</REF>  The approach of <REF>Barzilay and McKeown, 2001</REF> uses a related kind of resource: multiple translations of the same text, with additional constraints on availability, and problems of text alignment, for only a third of the results being synonyms when compared to Wordnet~~~A measure of similarity is almost always used to rank possible candidates~~~In the case of distributional approaches, similarity if determined from the appearance in similar contexts <TREF>Lin, 1998</TREF>; in the case of dictionary-based methods, lexical relations are deduced from the links between words expressed in definitions of entries.
<REF>Mihalcea, 2003</REF>; <REF>Riloff and Wiebe, 2003</REF>~~~Severalapproachesmakeuseofdependency triples <TREF>Lin, 1998</TREF>; <REF>Gorman and Curran, 2005</REF>~~~Our vector representation of the behavior of a word type across all its instances in a corpus is based on <TREF>Lin 1998</TREF>s DESCRIPTION OF A WORD~~~<REF>Yarowsky 1995</REF> uses a conceptually similar technique for WSD that learns from a small set of seed examples and then increases recall by bootstrapping, evaluated on 12 idiosyncratically polysemous words.
Severalapproachesmakeuseofdependency triples <TREF>Lin, 1998</TREF>; <REF>Gorman and Curran, 2005</REF>~~~Our vector representation of the behavior of a word type across all its instances in a corpus is based on <TREF>Lin 1998</TREF>s DESCRIPTION OF A WORD~~~<REF>Yarowsky 1995</REF> uses a conceptually similar technique for WSD that learns from a small set of seed examples and then increases recall by bootstrapping, evaluated on 12 idiosyncratically polysemous words~~~In that task, often a single disambiguating feature can be found in the context of a polysemous word instance, motivating his use of the decision list algorithm.
It allows us to identify triple instances~~~Each triple have the form w1Rw2 where w1 and w2 are lexical units and R is a syntactic relation <TREF>Lin, 1998</TREF>; Kilgarriff  al 2004~~~Our approach can be distinguished from classical distributional approach by different points~~~First, we use triple occurrences to build a distributional space one triple implies two contexts and two lexical units, but we use the transpose of the classical space: each point x i of this space is a syntactical context with the form Rw, each dimension j is a lexical units, and each value x i j is the frequency of corresponding triple occurrences.
2 Related work Previous work on automated paraphrasing has considered different levels of paraphrase granularity~~~Learning synonyms via distributional similarity has been well-studied <REF>Pereira et al , 1993</REF>; <REF>Grefenstette, 1994</REF>; <TREF>Lin, 1998</TREF>~~~<REF>Jacquemin 1999</REF> and <REF>Barzilay and McKeown 2001</REF> identify phraselevel paraphrases, while <REF>Lin and Pantel 2001</REF> and Shinyama et al~~~2002 acquire structural paraphrases encoded as templates.
The model is highly general and can be optimised for different tasks~~~It extends prior work on syntax-based models <REF>Grefenstette, 1994</REF>; <TREF>Lin, 1998</TREF>, by providing a general framework for defining context so that a large number of syntactic relations can be used in the construction of the semantic space~~~Our approach differs from <TREF>Lin 1998</TREF> in three important ways: a by introducing dependency paths we can capture non-immediate relationships between words ie , between subjects and objects, whereas Lin considers only local context dependency edges in our terminology; the semantic space is therefore constructed solely from isolated head/modifier pairs and their inter-dependencies are not taken into account; b Lin creates the semantic space from the set of dependency edges that are relevant for a given word; by introducing dependency labels and the path value function we can selectively weight the importance of different labels eg , subject, object, modifier and parametrize the space accordingly for different tasks; c considerable flexibility is allowed in our formulation for selecting the dimensions of the semantic space; the latter can be words see the leaves in Figure 1, parts of speech or dependency edges; in Lins approach, it is only dependency edges features in his terminology that form the dimensions of the semantic space~~~Experiment 1 revealed that the dependency-based model adequately simulates semantic priming.
Write A for the lexical association function which computes the value of a cell of the matrix from a co-occurrence frequency: Ki j  A f bi;t j 3 Evaluation 31 Parameter Settings All our experiments were conducted on the British National Corpus BNC, a 100 million word collection of samples of written and spoken language <REF>Burnard, 1995</REF>~~~We used <TREF>Lins 1998</TREF> broad coverage dependency parser MINIPAR to obtain a parsed version of the corpus~~~MINIPAR employs a manually constructed grammar and a lexicon derived from WordNet with the addition of proper names 130,000 entries in total~~~Lexicon entries contain part-of-speech and subcategorization information.
This makes semantic spaces more flexible, different types of contexts can be selected and words do not have to physically co-occur to be considered contextually relevant~~~However, existing models either concentrate on specific relations for constructing the semantic space such as objects eg , <REF>Lee, 1999</REF> or collapse all types of syntactic relations available for a given target word <REF>Grefenstette, 1994</REF>; <TREF>Lin, 1998</TREF>~~~Although syntactic information is now used to select a words appropriate contexts, this information is not explicitly captured in the contexts themselves which are still represented by words and is therefore not amenable to further processing~~~A commonly raised criticism for both types of semantic space models ie , word-based and syntaxbased concerns the notion of semantic similarity.
It extends prior work on syntax-based models <REF>Grefenstette, 1994</REF>; <TREF>Lin, 1998</TREF>, by providing a general framework for defining context so that a large number of syntactic relations can be used in the construction of the semantic space~~~Our approach differs from <TREF>Lin 1998</TREF> in three important ways: a by introducing dependency paths we can capture non-immediate relationships between words ie , between subjects and objects, whereas Lin considers only local context dependency edges in our terminology; the semantic space is therefore constructed solely from isolated head/modifier pairs and their inter-dependencies are not taken into account; b Lin creates the semantic space from the set of dependency edges that are relevant for a given word; by introducing dependency labels and the path value function we can selectively weight the importance of different labels eg , subject, object, modifier and parametrize the space accordingly for different tasks; c considerable flexibility is allowed in our formulation for selecting the dimensions of the semantic space; the latter can be words see the leaves in Figure 1, parts of speech or dependency edges; in Lins approach, it is only dependency edges features in his terminology that form the dimensions of the semantic space~~~Experiment 1 revealed that the dependency-based model adequately simulates semantic priming~~~Experiment 2 showed that a model that relies on rich context specifications can reliably distinguish between different types of lexical relations.
Contexts are defined as a small number of words surrounding the target word <REF>Lund and Burgess, 1996</REF>; <REF>Lowe and McDonald, 2000</REF> or as entire paragraphs, even documents <REF>Landauer and Dumais, 1997</REF>~~~Context is typically treated as a set of unordered words, although in some cases syntactic information is taken into account <TREF>Lin, 1998</TREF>; <REF>Grefenstette, 1994</REF>; <REF>Lee, 1999</REF>~~~A word can be thus viewed as a point in an n-dimensional semantic space~~~The semantic similarity between words can be then mathematically computed by measuring the distance between points in the semantic space using a metric such as cosine or Euclidean distance.
However, at structural level, the concept-based seeds share the same or similar linguistic patterns eg Subject-Verb-Object patterns with the corresponding types of proper names~~~The rationale behind using concept-based seeds in NE bootstrapping is similar to that for parsingbased word clustering <TREF>Lin 1998</TREF>: conceptually similar words occur in structurally similar context~~~In fact, the anaphoric function of pronouns and common nouns to represent antecedent NEs indicates the substitutability of proper names by the corresponding common nouns or pronouns~~~For example, this man can be substituted for the proper name John Smith in almost all structural patterns.
Distributional Similarity Score: The GD04 similarity score of the pair was used as a feature~~~We 583 also attempted adding Lins 1998 similarity scores but they appeared to be redundant~~~Intersection Feature: A binary feature indicating candidate pairs acquired by both methods, which was found to indicate higher entailment likelihood~~~In summary, the above feature types utilize mutually complementary pattern-based and distributional information.
For the distributional similarity component we employ the similarity scheme of <REF>Geffet and Dagan, 2004</REF>, which was shown to yield improved predictions of non-directional lexical entailment pairs~~~This scheme utilizes the symmetric similarity measure of <TREF>Lin, 1998</TREF> to induce improved feature weights via bootstrapping~~~These weights identify the most characteristic features of each word, yielding cleaner feature vector representations and better similarity assessments~~~22 Pattern-based <REF>Approaches Hearst 1992</REF> pioneered the use of lexicalsyntactic patterns for automatic extraction of lexical semantic relationships.
The degree of similarity between two target words is then determined by a vector comparison function~~~Amongst the many proposals for distributional similarity measures, <TREF>Lin, 1998</TREF> is maybe the most widely used one, while <REF>Weeds et al , 2004</REF> provides a typical example for recent research~~~Distributional similarity measures are typically computed through exhaustive processing of a corpus, and are therefore applicable to corpora of bounded size~~~It was noted recently by Geffet and Dagan 2004, 2005 that distributional similarity captures a quite loose notion of semantic similarity, as exemplified by the pair country  party identified by Lins similarity measure.
The method uses a thesaurus obtained from the text by parsing, extracting grammatical relations and then listing each word w with its top k nearest neighbours, where k is a constant~~~Like <REF>McCarthy et al , 2004</REF> we use k  50 and obtain our thesaurus using the distributional similarity metric described by <TREF>Lin, 1998</TREF> and we use WordNet WN as our sense inventory~~~The senses of a word w are each assigned a ranking score which sums over the distributional similarity scores of the neighbours and weights each neighbours score by a WN Similarity score <REF>Patwardhan and Pedersen, 2003</REF> between the sense of w and the sense of the neighbour that maximises the WN Similarity score~~~This weight is normalised by the sum of such WN similarity scores between all senses of w and the senses of the neighbour that maximises this score.
Besides, the automatically constructed thesauri can also be used~~~<TREF>Lin 1998</TREF> constructed a thesaurus by automatically clustering words based on context similarity~~~<REF>BarzilayandMcKeown 2001</REF>usedmonolingual parallel corpora for identifying paraphrases~~~They exploited a corpus of multiple English translations ofthesamesourcetextwritteninaforeignlanguage, from which phrases in aligned sentences that appear in similar contexts were extracted as paraphrases.
Therefore, we keep at most 20 paraphrases for a phrase when extracting phrasal paraphrases using each resource~~~1 Thesaurus: The thesaurus4 used in this work was automatically constructed by <TREF>Lin 1998</TREF>~~~The similarity of two words e1 and e2 was calculated through the surrounding context words that have dependency relations with the investigated words: Sime1,e2  P r,eTre1Tre2Ie1,r,eIe2,r,eP r,eTre1 Ie1,r,e P r,eTre2 Ie2,r,e 5 where Trei denotes the set of words that have dependency relation r with word ei~~~Iei,r,e is the mutual information between ei, r and e For each word, we keep 20 most similar words as paraphrases.
Those words that obtain the best values are considered to be most similar~~~Practical implementations of algorithms based on this principle have led to excellent results as documented in papers by <REF>Ruge 1992</REF>, <REF>Grefenstette 1994</REF>, <REF>Agarwal 1995</REF>, <REF>Landauer  Dumais 1997</REF>, <REF>Schtze 1997</REF>, and <TREF>Lin 1998</TREF>~~~21 Human Data In this section we relate the results of our version of such an algorithm to similarity estimates obtained by human subjects~~~Fortunately, we did not need to conduct our own experiment to obtain the humans similarity estimates.
Given the great deal of similar work in information extraction and ontology learning, we focus here only on techniques for weakly supervised or unsupervised semantic class ie, supertype-based learning, since that is most related to the work in this paper~~~Fully unsupervised semantic clustering eg, <TREF>Lin, 1998</TREF>; <REF>Lin and Pantel, 2002</REF>; <REF>Davidov and Rappoport, 2006</REF> has the disadvantage that it may or may not produce the types and granularities of semantic classes desired by a user~~~Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes eg, <REF>Caraballo, 1999</REF>; <REF>Cimiano and Volker, 2005</REF>; <REF>Mann, 2002</REF>, and learning semantic relations such as meronymy <REF>Berland and Charniak, 1999</REF>; <REF>Girju et al, 2003</REF>~~~Our research focuses on semantic lexicon induction, which aims to generate lists of words that belong to a given semantic class eg, lists of FISH or VEHICLE words.
This is evident for word-pairs where at least one of the words is a polysemous word eg , pairs that include cock, brother~~~Page counts-based measures do not consider the context in which the words appear in a page, thus cannot disambiguate Table 4: Comparison with taxonomy based methods Method correlation Human replication 0901 <REF>Resnik 1995</REF> 0745 <TREF>Lin 1998</TREF> 0822 <REF>Li et al 2003</REF> 0891 Edge-counting 0664 Information content 0745 <REF>Jiang  Conrath 1998</REF> 0848 proposed SVM 0834 the multiple senses~~~As summarized in Table 43, proposed method is comparable with the WordNet based methods~~~In fact, the proposed method outperforms simple WordNet based approaches such as Edge-Counting and Information Content measures.
The study of semantic similarity between words has been an integral part of natural language processing and information retrieval for many years~~~Semantic similarity measures are vital for various applications in natural language processing such as word sense disambiguation <REF>Resnik, 1999</REF>, language modeling <REF>Rosenfield, 1996</REF>, synonym extraction <TREF>Lin, 1998a</TREF> and automatic thesaurus extraction <REF>Curran, 2002</REF>~~~Pre-compiled taxonomies such as WordNet 1 and text corpora have been used in previous work on semantic similarity <TREF>Lin, 1998a</TREF>; <REF>Resnik, 1995</REF>; <REF>Jiang and Conrath, 1998</REF>; <TREF>Lin, 1998b</TREF>~~~However, semantic similarity between words change over time as new senses and associations of words are constantly created.
Semantic similarity measures are vital for various applications in natural language processing such as word sense disambiguation <REF>Resnik, 1999</REF>, language modeling <REF>Rosenfield, 1996</REF>, synonym extraction <TREF>Lin, 1998a</TREF> and automatic thesaurus extraction <REF>Curran, 2002</REF>~~~Pre-compiled taxonomies such as WordNet 1 and text corpora have been used in previous work on semantic similarity <TREF>Lin, 1998a</TREF>; <REF>Resnik, 1995</REF>; <REF>Jiang and Conrath, 1998</REF>; <TREF>Lin, 1998b</TREF>~~~However, semantic similarity between words change over time as new senses and associations of words are constantly created~~~One major issue behind taxonomies and corpora oriented approaches is that they might not necessarily capture similarity between proper names such as named entities eg , personal names, location names, product names and the new uses of existing words.
Five part-of-speech features, two lexical features, and a paragraph feature were used~~~Toidentify richer features, <REF>Wiebe, 2000</REF> used Lins 1998 method for clustering words according to distributional similarity,seededby a small amount of detailed manual annotation, to automatically identify adjective PSEs~~~There are two parameters of this process, neither of whichwas varied in <REF>Wiebe, 2000</REF>: C, the cluster size considered, andFT, a lteringthreshold, such that, if the seed word and the words in its cluster have, as a set, lower precision than the ltering threshold on the training data, the entire cluster, including the seed word, is ltered out~~~This process is adapted for use in the current paper, as described in section 7.
Secondly, our kind of syntactic preprocessing which is standard nowadays allows collocation extraction algorithms to better control the structural types of collocations~~~<TREF>Lin 1998</TREF> acquires a lexical dependency database by assembling dependency relationships from a parsed corpus~~~An entry in this database is classified as collocation if its log-likelihood value is greater than some threshold~~~Using an automatically constructed similarity thesaurus, <REF>Lin 1999</REF> then separates compositional from non-compositional collocations by taking into account the second linguistic property described in Section 1, viz.
,, iwcount : frequency of the triples including word iw  N: number of triples in the corpus~~~We use it instead of point-wise mutual information in <TREF>Lin 1998</TREF> because the latter tends to overestimate the association between two parts with low frequencies~~~Weighted mutual information meliorates this effect by adding , ji attwp  24 Combining the Three Extractors In terms of combining the outputs of the different methods, the ensemble method is a good candidate~~~Originally, the ensemble method is a machine learning technique of combining the outputs of several classifiers to improve the classification performance <REF>Dietterich, 2000</REF>.
However, many studies investigate synonym extraction from only one resource~~~The most frequently used resource for synonym extraction is large monolingual corpora <REF>Hindle, 1990</REF>; <REF>Crouch and Yang, 1992</REF>; <REF>Grefenstatte, 1994</REF>; <REF>Park and Choi, 1997</REF>; <REF>Gasperin et al , 2001</REF> and <TREF>Lin, 1998</TREF>~~~The methods used the contexts around the investigated words to discover synonyms~~~The problem of the methods is that the precision of the extracted synonymous words is low because it extracts many word pairs such as cat and dog, which are similar but not synonymous.
The similarity is the sum of the WordNet similarities between all attribute keywords in the two exhibits K1, K2, normalised over the length of both keyword sets: summationtext k1K1 summationtext k2K2 WNsimk1,k2 K1K2 For the purposes of this experiment we have chosen to use three WordNet similarity/relatedness measures to simulate the conceptual connections that visitors make between exhibits~~~The Lin <TREF>Lin, 1998</TREF> and Leacock-Chodorow <REF>Leacock et al , 1998</REF> similarity measures and the BanerjeePedersen <REF>Patwardhan and Pedersen, 2003</REF> relatedness measures were used~~~The similarities were normalised and transformed into probability matrices such that summationtextj PWNsimecj  1 for each next exhibit ci~~~The use of WordNet measures is intended to simulate the mental connections that visitors make between exhibit content, given that each visit can interpret content in a number of different ways.
Using Language Weavers 1 English-to-Spanish machine translation system, English marginal notes can be translated into Spanish~~~22 Vocabulary Support Synonyms for lower frequency more difficult words are output using a statistically-generated word similarity matrix <TREF>Lin, 1998</TREF>~~~ATA v10 generates antonyms for vocabulary in the text using WordNet ~~~2 Cognates are words which have the same spelling and meaning in two languages eg , animal in English and Spanish.
While WordNet has its advantages, we aimed to create a knowledge-light 304 system~~~A more knowledge-free system would have used a machine readable dictionary or a large natural language sample to retrieve its synonyms see, for example, <TREF>Lin 1998</TREF>, but our system falls short of this, relying on Rogets New Millennium Thesaurus1 henceforth RT as a source of synonyms~~~Though this thesaurus is similar to WordNet in some ways, it does not contain semantic relationships beyond synonyms and antonyms~~~One important advantage of a thesaurus over WordNet is that it is easier to obtain for languages other than English.
The parameters K and T are usually considered to be small numbers~~~3 Word Similarity Following <TREF>Lin 1998</TREF>, we represent each word by a feature vector~~~Each feature corresponds to a context in which the word occurs~~~For example, threaten with  is a context.
Variations of the value difference metric <REF>Stanfill and Waltz, 1986</REF> have been employed for supervised disambiguation Ng and HB <REF>Lee, 1996</REF>; <REF>Ng, 1997</REF>; but it is not reasonable in language modeling to expect training data tagged with correct probabilities~~~The Dice coejcient <REF>Smadja et al , 1996</REF>; D <REF>Lin, 1998a, 1998b</REF> is monotonic in Jaccards coefficient van <REF>Rijsbergen, 1979</REF>, so its inclusion in our experiments would be redundant~~~Finally, we did not use the KL divergence because it requires a smoothed base language model~~~SZero would also be a reasonable choice, since it indicates zero correlation between q and r However, it would then not be clear how to average in the estimates of negatively correlated words in equation 1.
Furthermore, by using a restricted version of model 1 that stripped incomparable parameters, we were able to empirically demonstrate that the confusion probability is fundamentally worse at selecting useful similar words~~~D Lin also found that the choice of similarity function can affect the quality of automatically-constructed thesauri to a statistically significant degree 1998a and the ability to determine common morphological roots by as much as 49 in precision 1998b~~~1The term similarity-based, which we have used previously, has been applied to describe other models as well L <REF>Lee, 1997</REF>; <REF>Karov and Edelman, 1998</REF>~~~These empirical results indicate that investigating different similarity measures can lead to improved natural language processing.
In essence, Smadja et al argue that information from the union of supports, rather than the just the intersection, is important~~~D Lin 1997; 1998a takes an axiomatic approach to determining the characteristics of a good similarity measure~~~Starting with a formalization based on certain assumptions of the intuition that the similarity between two events depends on both their commonality and their differences, he derives a unique similarity function schema~~~The 04 038 I 036  034 032 03 028 026 100 Error rates averages and ranges L1 JS 0 300 0 0 600 700 800 0 1000 k Figure 4: Performance of the skew divergence with respect to the best functions from Figure 2.
3 It is worth noting at this point that there are several well-known measures from the NLP literature that we have omitted from our experiments~~~Arguably the most widely used is the mutual information <REF>Hindle, 1990</REF>; <REF>Church and Hanks, 1990</REF>; <REF>Dagan et al , 1995</REF>; <REF>Luk, 1995</REF>; D <TREF>Lin, 1998a</TREF>~~~It does not apply in the present setting because it does not measure the similarity between two arbitrary probability distributions in our case, PVIn  and PVIm, but rather the similarity between a joint distribution PX1,X2 and the corresponding product distribution PX1PX2~~~Hamming-type metrics <REF>Cardie, 1993</REF>; <REF>Zavrel and Daelemans, 1997</REF> are intended for data with symbolic features, since they count feature label mismatches, whereas we are dealing feature Values that are probabilities.
Words cannot be substituted between the two phrases because they are composed in different ways~~~3 Proposal Recently, there has been much interest in finding words which are distributionally similar eg, <TREF>Lin 1998</TREF>, <REF>Lee 1999</REF>, <REF>Curran and Moens 2002</REF>, <REF>Weeds 2003</REF> and <REF>Geffet and Dagan 2004</REF>~~~Two words are said to be distributionally similar if they appear in similar contexts~~~For example, the two words apple and pear are likely to be seen as the objects of the verbs eat and peel, and this adds to their distributional similarity.
The reason for choosing this measure is that it can be used to compute the distance between any two co-occurrence vectors independent of any information about other words~~~This is in contrast to many other measures, eg, <TREF>Lin 1998</TREF>, which use the co-occurrences of features with other words to compute a weighting function such as mutual information MI <REF>Church and Hanks, 1989</REF>~~~Since we only have corpus data for the target phrases, it is not possible for us to use such a measure~~~However, the -skew divergence measure has been shown <REF>Weeds, 2003</REF> to perform comparably with measures which use MI, particularly for lower frequency target words.
Table 1 shows the number of feature types and tokens extracted for each phrase~~~This shows that we have extracted a reasonable number of features for each phrase, since distributional similarity techniques have been shown to work well for words which occur more than 100 times in a given corpus <TREF>Lin, 1998</TREF>; <REF>Weeds and Weir, 2003</REF>~~~We then computed the distributional similarity between each co-occurrence vector using the -skew divergence measure <REF>Lee, 1999</REF>~~~The -skew divergence measure is an approximation to the KullbackLeibler KL divergence meassure between two distributions p and q: Dpq  summationdisplay x pxlogpxqx 5We currently retain all of the distinctions between grammatical relations output by RASP.
In their work on QA, Lin and Pantel restrict the grammatical relations considered to two slots at either end of the path where the word occupying the slot is a noun~~~Co-occurrence vectors for paths are then built up using evidence from multiple occurrences of the paths in corpus data, for which similarity can then be calculated using a standard metric eg , <TREF>Lin 1998</TREF>~~~In our work, we extend the notion of distributional similarity from linear paths to trees~~~This allows us to compute distributional similarity for any part of an expression, of arbitrary length and complexity although, in practice, we are still limited by data sparseness.
W ORD S IMILAR W ORDS  WITH SIMILARITY SCORE  EAT cook 0127, drink 0108, consume 0101, feed 0094, taste 0093, like 0092, serve 0089, bake 0087, sleep 0086, pick 0085, fry 0084, freeze 0081, enjoy 0079, smoke 0078, harvest 0076, love 0076, chop 0074, sprinkle 0072, Toss 0072, chew 0072 SALAD soup 0172, sandwich 0169, sauce 0152, pasta 0149, dish 0135, vegetable 0135, cheese 0132, dessert 013, entree 0121, bread 0116, meat 0116, chicken 0115, pizza 0114, rice 0112, seafood 011, dressing 0109, cake 0107, steak 0105, noodle 0105, bean 0102 the collocation database for the words eat and salad  The database contains a total of 11 million unique dependency relationships~~~22 Corpus-based thesaurus Using the collocation database, <TREF>Lin 1998b</TREF> used an unsupervised method to construct a corpusbased thesaurus consisting of 11839 nouns, 3639 verbs and 5658 adjectives/adverbs~~~Given a word w, the thesaurus returns a set of similar words of w along with their similarity to w  For example, the 20 most similar words of eat and salad are shown in Table 1~~~3 Training Data Extraction We parsed a 125-million word newspaper corpus with Minipar 3, a descendent of Principar <REF>Lin, 1994</REF>.
Below, we briefly describe these resources~~~21 Collocation database Given a word w in a dependency relationship such as subject or object , the collocation database is used to retrieve the words that occurred in that relationship with w, in a large corpus, along with their frequencies <TREF>Lin, 1998a</TREF>~~~Figure 1 shows excerpts of the entries in 2 Available at wwwcsualbertaca/lindek/demoshtm~~~eat : object: almond 1, a pple 25, bean 5, beam 1, binge 1, bread 13, cake 17, cheese 8, dish 14, disorder 20, egg 31, grape 12, grub 2, hay 3, junk 1, meat 70, poultry 3, rabbit 4, soup 5, sandwich 18, pasta 7, vegetable 35,  subject: adult 3, animal 8, beetle 1, cat 3, child 41, decrease 1, dog 24, family 29, guest 7, kid 22, patient 7, refugee 2, rider 1, Russian 1, shark 2, something 19, We 239, wolf 5,  salad : adj-modifier: assorted 1, crisp 4, fresh 13, good 3, grilled 5, leftover 3, mixed 4, olive 3, prepared 3, side 4, small 6, special 5, vegetable 3,  object-of: add 3, consume 1, dress 1, grow 1, harvest 2, have 20, like 5, love 1, mix 1, pick 1, place 3, prepare 4, return 3, rinse 1, season 1, serve 8, sprinkle 1, taste 1, test 1, Toss 8, try 3,  Figure 1.
eat : object: almond 1, a pple 25, bean 5, beam 1, binge 1, bread 13, cake 17, cheese 8, dish 14, disorder 20, egg 31, grape 12, grub 2, hay 3, junk 1, meat 70, poultry 3, rabbit 4, soup 5, sandwich 18, pasta 7, vegetable 35,  subject: adult 3, animal 8, beetle 1, cat 3, child 41, decrease 1, dog 24, family 29, guest 7, kid 22, patient 7, refugee 2, rider 1, Russian 1, shark 2, something 19, We 239, wolf 5,  salad : adj-modifier: assorted 1, crisp 4, fresh 13, good 3, grilled 5, leftover 3, mixed 4, olive 3, prepared 3, side 4, small 6, special 5, vegetable 3,  object-of: add 3, consume 1, dress 1, grow 1, harvest 2, have 20, like 5, love 1, mix 1, pick 1, place 3, prepare 4, return 3, rinse 1, season 1, serve 8, sprinkle 1, taste 1, test 1, Toss 8, try 3,  Figure 1~~~Excepts of entries in the collocation database for eat and salad  Table 1  The top 20 most similar words of eat and salad as given by <TREF>Lin, 1998b</TREF>~~~W ORD S IMILAR W ORDS  WITH SIMILARITY SCORE  EAT cook 0127, drink 0108, consume 0101, feed 0094, taste 0093, like 0092, serve 0089, bake 0087, sleep 0086, pick 0085, fry 0084, freeze 0081, enjoy 0079, smoke 0078, harvest 0076, love 0076, chop 0074, sprinkle 0072, Toss 0072, chew 0072 SALAD soup 0172, sandwich 0169, sauce 0152, pasta 0149, dish 0135, vegetable 0135, cheese 0132, dessert 013, entree 0121, bread 0116, meat 0116, chicken 0115, pizza 0114, rice 0112, seafood 011, dressing 0109, cake 0107, steak 0105, noodle 0105, bean 0102 the collocation database for the words eat and salad  The database contains a total of 11 million unique dependency relationships~~~22 Corpus-based thesaurus Using the collocation database, <TREF>Lin 1998b</TREF> used an unsupervised method to construct a corpusbased thesaurus consisting of 11839 nouns, 3639 verbs and 5658 adjectives/adverbs.
5 We also attempted to implement Sussnas 1993, 1997 measure Section 251, but ran into problems because a key element depended closely on the particulars of an earlier version of WordNet; see <REF>Budanitsky 1999</REF> for details~~~We did not include Wu and Palmers measure Section 252 because <TREF>Lin 1998b</TREF> has shown it to be a special case of his measure in which all childparent probabilities are equal~~~6 In their original experiments, Lin and Jiang and Conrath used SemCor, a sense-tagged subset of the Brown Corpus, as their empirical data; but we decided to follow Resnik in using the full and untagged corpus~~~While this means trading accuracy for size, we believe that using a non-disambiguated corpus constitutes a more-general approach, as the availability and size of disambiguated texts such as SemCor is highly limited.
Expanding the sum in the right-hand side of equation 14, plugging in the expression for parentchild distance from equation 13, and performing necessary eliminations results in the following final formula for the semantic distance between concepts c 1 and c 2 : dist JC c 1 , c 2   ICc 1   ICc 2   2  IClsoc 1 , c 2  15  2logplsoc 1 , c 2  log pc 1   log pc 2  16 263 Lins Universal Similarity Measure~~~Noticing that all of the similarity measures known to him were tied to a particular application, domain, or resource, <TREF>Lin 1998b</TREF> attempted to define a measure of similarity that would be both universal applicable to arbitrary objects and not presuming any form of knowledge representation and theoretically justified derived from a set of assumptions, instead of directly by a formula, so that if the assumptions are deemed reasonable, the similarity measure necessarily follows~~~He used the following three intuitions as a basis: 1~~~The similarity between arbitrary objects A and B is related to their commonality; the more commonality they share, the more similar they are.
22 Words that are distributionally similar do indeed often represent semantically related concepts, and vice versa, as the following examples demonstrate~~~<REF>Weeds 2003</REF>, in her study of 15 distributional-similarity measures, found that words distributionally similar to hope noun included confidence, dream, feeling, and desire; <TREF>Lin 1998b</TREF> found pairs such as earningsprofit, biggestlargest, nylonsilk, and pilltablet~~~It is intuitively clear why these results occur: if two concepts are similar or related, it is likely that their role in the world will be similar, so similar things will be said about them, and so the contexts of occurrence of the corresponding words will be similar~~~And conversely albeit with less certainty, if the contexts of occurrence of two words are similar, then similar things are being said about each, so they are playing similar roles in the world and hence are semantically similar  at least to the extent of these roles.
Imbalance in the corpus and data sparseness is an additional source of anomalous results even for good measures~~~For example, <TREF>Lin 1998b</TREF> found peculiar similarities that were reasonable for his corpus of news articles, such as captivewesterner because in the news articles, more than half of the westerners mentioned were being held captive and auditionrite because both were infrequent and were modified by uninhibited~~~We now turn to the hypothesis that distributional similarity can usefully stand in for semantic relatedness in NLP applications such as malapropism detection~~~<REF>Weeds 2003</REF> considered the hypothesis in detail.
For example, Weeds 2003; <REF>Weeds and Weir, 2005</REF> see below took verbs as contexts for nouns in object position: so they regarded two nouns to be similar to the extent that they occur as direct objects of the same set of verbs~~~Lin 1998b, 1998a considered other syntactic relationships as well, such as subjectverb and modifier noun, and looked at both roles in the relationship~~~Given this framework, many different methods of measuring distributional similarity have been proposed; see <REF>Dagan 2000</REF>, <REF>Weeds 2003</REF>, or <REF>Mohammad and Hirst 2005</REF> for a review~~~For example, the set of words that co-occur with w 1 and those that co-occur with w 2 may be regarded as a feature vector of each and their similarity measured as the cosine between the vectors; or a measure may be based on the KullbackLeibler divergence between the probability distributions Pww 1 andPww 2 , as, for example, <REF>Lees 1999</REF> -skew divergence.
For example, the set of words that co-occur with w 1 and those that co-occur with w 2 may be regarded as a feature vector of each and their similarity measured as the cosine between the vectors; or a measure may be based on the KullbackLeibler divergence between the probability distributions Pww 1 andPww 2 , as, for example, <REF>Lees 1999</REF> -skew divergence~~~<TREF>Lin 1998b</TREF> uses his similarity theorem equation 19 above to derive a measure based on the degree of overlap of the sets of words with which w 1 and w 2 , respectively, have positive mutual information~~~22 Words that are distributionally similar do indeed often represent semantically related concepts, and vice versa, as the following examples demonstrate~~~<REF>Weeds 2003</REF>, in her study of 15 distributional-similarity measures, found that words distributionally similar to hope noun included confidence, dream, feeling, and desire; <TREF>Lin 1998b</TREF> found pairs such as earningsprofit, biggestlargest, nylonsilk, and pilltablet.
Three kinds of approaches are prevalent in the literature~~~The first kind <REF>Wei 1993</REF>; <TREF>Lin 1998b</TREF> is a chiefly theoretical examination of a proposed measure for those mathematical properties thought desirable, such as whether it is a metric or the inverse of a metric, whether it has singularities, whether its parameter-projections are smooth functions, and so on~~~In our opinion, such analyses act at best as a coarse filter in the comparison of a set of measures and an even coarser one in the assessment of a single measure~~~The second kind of evaluation is comparison with human judgments.
Notice that because dist JC measures distance, the JiangConrath plot has a slope opposite to the rest of each group~~~9 <REF>Resnik 1995</REF>, <REF>Jiang and Conrath 1997</REF>, and <TREF>Lin 1998b</TREF> report the coefficients of correlation between their measures and the MillerCharles ratings to be 07911, 08282, and 08339, respectively, which differ slightly from the corresponding figures in Table 3~~~These discrepancies can be explained by possible minor differences in implementation eg, the compound-word recognition mechanism used in collecting the frequency data, differences between the versions of WordNet used in the experiments Resnik, and differences in the corpora used to obtain the frequency data Jiang and Conrath, Lin~~~Also, the coefficients reported by Resnik and Lin are actually based on only 28 out of the 30 MillerCharles pairs because of a noun missing from an earlier version of WordNet.
The Distributional Hypothesis <REF>Harris 1985</REF> states that words that occur in the same contexts tend to be similar~~~There have been many approaches to compute the similarity between words based on their distribution in a corpus <REF>Hindle 1990</REF>; <REF>Landauer and Dumais 1997</REF>; <TREF>Lin 1998</TREF>~~~The output of these programs is a ranked list of similar words to each word~~~For example, Lins approach outputs the following similar words for wine and suit: wine: beer, white wine, red wine, Chardonnay, champagne, fruit, food, coffee, juice, Cabernet, cognac, vinegar, Pinot noir, milk, vodka, suit: lawsuit, jacket, shirt, pant, dress, case, sweater, coat, trouser, claim, business suit, blouse, skirt, litigation,  The similar words of wine represent the meaning of wine.
Each cluster corresponds to a sense of the headword~~~2 Feature Representation Following <TREF>Lin 1998</TREF>, we represent each word by a feature vector~~~Each feature corresponds to a context in which the word occurs~~~For example, sip  is a verbobject context.
All words in the vocabulary sharing the same root form are grouped together~~~Then we do corpus analysis to filter out the words which are clustered incorrectly, according to word distributional similarity, following <REF>Xu and Croft, 1998</REF>; <TREF>Lin 1998</TREF>~~~The rationale behind this is that words sharing the same meaning tend to occur in the same contexts~~~The context of each word in the vocabulary is represented by a vector containing the frequencies of the context words which co-occur with the word within a predefined window in a training corpus.
<REF>Tanev et al , 2004</REF> used an algorithm for partial matching of syntactic structures~~~For lexical variations they used a dependency based thesaurus of similar words <TREF>Lin, 1998</TREF>~~~Hang et al~~~<REF>Cui et al , 2004</REF> used an algorithm to compute the similarity between dependency relation paths from a parse tree to rank the candidate answers.
Thus, correct match of an argument corresponds to correct role identification~~~The templates were represented as Minipar <TREF>Lin, 1998b</TREF> dependency parse-trees~~~The Contextual Preferences for h were constructed manually: the named-entity types for cpv:nh were set by adapting the entity types given in the guidelines to the types supported by the Lingpipe NER described in Section 32~~~cpgh was generated from a short list of nouns and verbs that were extracted from the verbal event definition in the ACE guidelines.
As a more natural ranking method, we also utilize SCBC directly, denoted rankedCBC, having mv:er,t  SCBCr,t~~~In addition, we tried a simpler method that directly compares the terms in two cpv:e lists, utilizing the commonly-used term similarity metric of <TREF>Lin, 1998a</TREF>~~~This method, denoted LIN, uses the same raw distributional data as CBC but computes only pair-wise similarities, without any clustering phase~~~We calculated the scores of the 1000 most similar terms for every term in the Reuters RVC1 corpus3.
A comparison of the upper half BOW with the lower half SYN shows that the dependency-based space generally shows better correlation with human judgements~~~This corresponds to a beneficial effect of syntactic information found for other applications of semantic spaces <TREF>Lin, 1998</TREF>; <REF>Pado and Lapata, 2007</REF>~~~All instances of the SELPREF model show highly significant correlations~~~SELPREF and SELPREF-CUT show very similar performance.
3 Distributional Word Similarity Words that tend to appear in the same contexts tend to have similar meanings <REF>Harris 1968</REF>~~~For example, the words corruption and abuse are similar because both of them can be subjects of verbs like arouse, become, betray, cause, continue, cost, exist, force, go on, grow, have, increase, lead to, and persist, etc, and both of them can modify nouns like accusation, act, allegation, appearance, and case, etc Many methods have been proposed to compute distributional similarity between words, eg, <REF>Hindle, 1990</REF>, <REF>Pereira et al 1993</REF>, <REF>Grefenstette 1994</REF> and <TREF>Lin 1998</TREF>~~~Almost all of the methods represent a word by a feature vector where each feature corresponds to a type of context in which the word appeared~~~84 31 Proximity-based Similarity It is natural to use dependency relationship Meluk, 1987 as features, but a parser has to be available.
First of all, it allows us to provide both positive and negative examples, avoiding the use of one-class classification algorithms that in practice perform poorly <REF>Dagan et al , 2006</REF>~~~Second, the large availability of manually constructed substitution lexica, such as WordNet <REF>Fellbaum, 1998</REF>, or the use of repositories based on statistical word similarities, such as the database constructed by <TREF>Lin 1998</TREF>, allows us to find an adequate substitution lexicon for each target word in most of the cases~~~For example, as shown in Table 1, the word job has different senses depending on its context, some of them entailing its direct hyponym position eg , looking for permanent job, others entailing the word task eg , the job of repairing~~~The problem of deciding whether a particular instance of job can be replaced by position, and not by the word place, can be solved by looking for the most similar contexts where either position or place occur in the training data, and then selecting the class ie , the entailed word characterized by the most similar ones, in an instance based style.
<REF>Cimiano and Volker 2005</REF> assign a particular entity to the fine-grained class suchthatthecontextualsimilarityismaximalamong the set of fine-grained subclasses of a coarse-grained category~~~Contextual similarity has been measured by adopting lexico-syntactic features provided by a dependency parser, as proposed in <TREF>Lin, 1998</TREF>~~~3 Instance Based Lexical Entailment Dagan et al~~~2006 adapted the classical supervised WSD setting to approach the sense matching problem ie , the binary lexical entailment problem of deciding whether a word, such as position, entails a different word, such as job, in a given context by defining a one-class learning algorithm based on support vector machines SVM.
It is worthwhile to remark that, due to the ambiguity of the entailed words eg , position could also entail either perspective or place, not every occurrence of them should be taken into account, in order to avoid misleading predictions caused by the irrelevant senses~~~Therefore, approaches based on a more classical contextual similarity technique <TREF>Lin, 1998</TREF>; <REF>Dagan, 2000</REF>, where words are described globally by context vectors, are doomed to fail~~~We will provide empirical evidence of this in the evaluation section~~~Choosing an appropriate similarity function for the contexts of the words to be substituted is a primary issue.
Hence, neighbor sets derived using sim hind are identical to those obtained using recall  0,  0 in the difference-weighted MI-based CRM~~~46 Lins <REF>Measure Lin 1998a</REF> proposed a measure of lexical distributional similarity based on his information-theoretic similarity theorem <REF>Lin 1997, 1998b</REF>: The similarity between A and B is measured by the ratio between the amount of information needed to state the commonality of A and B and the information needed to fully describe what A and B are~~~459 Computational Linguistics Volume 31, Number 4 If the features of a word are grammatical relation contexts, the similarity between two words w 1 and w 2 can be written according to Lins measure as: sim lin w 1, w 2   summationtext Tw 1 Tw 2  Iw 1, c  Iw 2, c summationtext Tw 1  Iw 1, c  summationtext Tw 2  Iw 2, c 41 where Tw c : Iw, c > 0~~~There are parallels between sim lin and sim dice in that both measures compute a ratio between what is shared by the descriptions of both nouns and the sum of the descriptions of each noun.
There are a number of ways to measure the distance between two nouns in the WordNet noun hierarchy see Budanitsky 1999 for a review~~~In previous work <REF>Weeds and Weir 2003b</REF>, we used the WordNet-based similarity measure first proposed in <REF>Lin 1997</REF> and used in <TREF>Lin 1998a</TREF>: wn sim lin w 1, w 2   max c 1 Sw 1 c 2 Sw 2  parenleftbigg max csupc 1 supc 2  2logPc log Pc 1   log Pc 2  parenrightbigg 49 where Sw is the set of senses of the word w in WordNet, supc is the set of possibly indirect super-classes of concept c in WordNet, and Pc is the probability that a randomly selected word refers to an instance of concept c estimated over some corpus such as SemCor <REF>Miller et al 1994</REF>~~~However, in other research <REF>Budanitsky and Hirst 2001</REF>; <REF>Patwardhan, Banerjee, and Pedersen 2003</REF>; <REF>McCarthy, Koeling, and Weeds 2004</REF>, it has been shown that the distance measure of <REF>Jiang and Conrath 1997</REF> referred to herein as the JC measure is a superior WordNet-based semantic similarity measure: wn dist JC w 1, w 2   max c 1 Sw 1 c 2 Sw 2  parenleftbigg max csupc 1 supc 2  2logc  log Pc 1   log Pc 2  parenrightbigg 50 In our work, we make an empirical comparison of neighbors derived using a WordNet-based measure and each of the distributional similarity measures using the technique discussed in Section 3~~~We have carried out the same experiments using both the Lin measure and the JC measure.
The underlying idea is based largely on the central claim of the distributional hypothesis <REF>Harris 1968</REF>, that is: The meaning of entities, and the meaning of grammatical relations among them, is related to the restriction of combinations of these entities relative to other entities~~~This hypothesized relationship between distributional similarity and semantic similarity has given rise to a large body of work on automatic thesaurus generation <REF>Hindle 1990</REF>; <REF>Grefenstette 1994</REF>; <TREF>Lin 1998a</TREF>; <REF>Curran and Moens 2002</REF>; <REF>Kilgarriff 2003</REF>~~~There are inherent problems in evaluating automatic thesaurus extraction techniques, and much research assumes a gold standard that does not exist see Kilgarriff 2003 and Weeds 2003 for more discussion of this~~~A further problem for distributional similarity methods for automatic thesaurus generation is that they do not offer any obvious way to distinguish between linguistic relations such as synonymy, antonymy, and hyponymy see Caraballo 1999 and Lin et al 2003 for work on this.
45 Hindles <REF>Measure Hindle 1990</REF> proposed an MI-based measure, which he used to show that nouns could be reliably clustered based on their verb co-occurrences~~~We consider the variant of 458 Weeds and Weir Co-occurrence Retrieval Figure 1 Variation with parameters  and  in development set mean similarity between neighbor sets of the additive t-test based CRM and of dist   Hindles Measure proposed by <TREF>Lin 1998a</TREF>, which overcomes the problem associated with calculating MI for word-feature combinations that do not occur: sim hind w 1, w 2   summationdisplay Tw 1 Tw 2  minIc, w 1 , Ic, w 2  38 where Tw 1  c : Ic, n > 0~~~This expression is the same as the numerator in the expressions for precision and recall in the difference-weighted MI-based CRM: P dw mi w 1, w 2   summationtext TP Iw 1, c  minIw 1, c,Iw 2, c Iw 1, c summationtext Fw 1  Iw 1, c  summationtext TP minIw 1, c, Iw 2, c summationtext Fw 1  Iw 1, c 39 R dw mi w 1, w 2   summationtext TP Iw 2, c  minIw 2, c,Iw 1, c Iw 2, c summationtext Fw 2  Iw 2, c  summationtext TP minIw 2, c, Iw 1, c summationtext Fw 2  Iw 2, c 40 since TP  Tw 1   Tw 2 ~~~However, we also note that the denominator in the expression for recall depends only on w 2, and therefore, for a given w 2, is a constant.
Further, the noun hyponymy hierarchy in WordNet, which will be used as a pseudo-gold standard for comparison, is widely recognized in this area of research~~~Some previous work on distributional similarity between nouns has used only a single grammatical relation eg , <REF>Lee 1999</REF>, whereas other work has considered multiple grammatical relations eg , <TREF>Lin 1998a</TREF>~~~We consider only a single grammatical relation because we believe that it is important to evaluate the usefulness of each grammatical relation in calculating similarity before deciding how to combine information from 5 This results in a single 80:20 split of the complete data set, in which we are guaranteed that the original relative frequencies of the target nouns are maintained~~~6 The use of grammatical relations to model context precludes finding similarities between words of different parts of speech.
Second, as we noted in Section 22, standard dictionary definitions are not usually fine-grained enough they define the core meaning but not all the nuances of a word and can even be circular, defining each of several nearsynonyms in terms of the other near-synonyms~~~And third, although corpus-based methods eg , Lins 1998 do compute different similarity values for different pairs of near-synonyms of the same cluster, Church et al~~~1994 and <REF>Edmonds 1997</REF> show that such methods are not yet capable of uncovering the more subtle differences in the use of near-synonyms for lexical choice~~~But one benefit of the clustered model of lexical knowledge is that it naturally lends itself to the computation of explicit differences or degrees of similarity between near-synonyms.
Recent research in computational linguistics has focused more on developing methods to compute the degree of semantic similarity between any two words, or, more precisely, between the simple or primitive concepts 15 denoted by any two words~~~There are many different similarity measures, which variously use taxonomic lexical hierarchies or lexical-semantic networks, large text corpora, word definitions in machine-readable dictionaries or other semantic formalisms, or a combination of these <REF>Dagan, Marcus, and Markovitch 1993</REF>; <REF>Kozima and Furugori 1993</REF>; <REF>Pereira, Tishby, and Lee 1993</REF>; <REF>Church et al 1994</REF>; <REF>Grefenstette 1994</REF>; <REF>Resnik 1995</REF>; <REF>McMahon and Smith 1996</REF>; <REF>Jiang and Conrath 1997</REF>; Sch utze 1998; <TREF>Lin 1998</TREF>; <REF>Resnik and Diab 2000</REF>; <REF>Budanitsky 1999</REF>; <REF>Budanitsky and Hirst 2001, 2002</REF>~~~Unfortunately, these methods are generally unhelpful in computing the similarity of near-synonyms because the measures lack the required precision~~~First, taxonomic hierarchies and semantic networks inherently treat near-synonyms as absolute synonyms in grouping near-synonyms into single nodes eg , in WordNet.
This is calculated over a corpus of text~~~<TREF>Lin 1998c</TREF> also employs the idea of corpusderived information content, and defines the similarity between two concepts in the following way: simC1; C2  2 log PC0log PC 1  log PC2 1 where C0 is the lowest class in the hierarchy that subsumes both classes~~~2http://wwwdumnedu/tpederse/ distancehtml Hirst and St-<REF>Onge 1998</REF> use a system of relations of different strength to determine the similarity of word senses, conditioned on the type, direction and relative distance of edges separating them~~~The Patwardhan et al.
One interesting exception is <REF>Lin 1999</REF>, whose approach is explained as follows: The intuitive idea behind the method is that the metaphorical usage of a noncompositional expression causes it to have a different distributional characteristic than expressions that are similar to its literal meaning~~~The expressions he uses are taken from a collocation database <TREF>Lin, 1998b</TREF>~~~These expressions that are similar to their literal meaning are found by substituting each of the words in the expression with the 10 most similar words according to a corpus derived thesaurus <TREF>Lin, 1998a</TREF>~~~Lin models the distributional difference as a significant difference in mutual information.
The expressions he uses are taken from a collocation database <TREF>Lin, 1998b</TREF>~~~These expressions that are similar to their literal meaning are found by substituting each of the words in the expression with the 10 most similar words according to a corpus derived thesaurus <TREF>Lin, 1998a</TREF>~~~Lin models the distributional difference as a significant difference in mutual information~~~Significance here is defined as the absence of overlap between the 95 confidence interval of the mutual information scores.
2 Previous Work There have been several approaches to automatically discovering lexico-semantic information from text <REF>Hearst 1992</REF>; <REF>Riloff and Shepherd 1997</REF>; <REF>Riloff and Jones 1999</REF>; <REF>Berland and Charniak 1999</REF>; <REF>Pantel and Lin 2002</REF>; <REF>Fleischman et al 2003</REF>; <REF>Girju et al 2003</REF>~~~One approach constructs automatic thesauri by computing the similarity between words based on their distribution in a corpus <REF>Hindle 1990</REF>; <TREF>Lin 1998</TREF>~~~The output of these programs is a ranked list of similar words to each word~~~For example, Lins approach outputs the following top-20 similar words of orange: D peach, grapefruit, yellow, lemon, pink, avocado, tangerine, banana, purple, Santa Ana, strawberry, tomato, red, pineapple, pear, Apricot, apple, green, citrus, mango A common problem of such lists is that they do not discriminate between the senses of polysemous words.
Distributional similarity between words has been investigated and successfully applied in many natural language tasks such as automatic semantic knowledge acquisition <REF>Dekang Lin, 1998</REF> and language model smoothing <REF>Essen and Steinbiss, 1992</REF>; <REF>Dagan et al , 1997</REF>~~~An investigation on distributional similarity functions can be found in <TREF>Lillian Lee, 1999</TREF>~~~3 Distributional Similarity-Based Models for Query Spelling Correction 31 Motivation Most of the previous work on spelling correction concentrates on the problem of designing better error models based on properties of character strings~~~This direction ever evolves from simple Damerau-Levenshtein distance <REF>Damerau, 1964</REF>; <REF>Levenshtein, 1966</REF> to probabilistic models that estimate string edit probabilities from corpus <REF>Church and Gale, 1991</REF>; <REF>Mayes et al, 1991</REF>; <REF>Ristad and Yianilos, 1997</REF>; <REF>Brill and Moore, 2000</REF>; and <REF>Ahmad and Kondrak, 2005</REF>.
The calculation of word semantic similarity scores is also a problem that has attracted a lot of interest~~~The numerous notable approaches can usually be divided into those which utilize the hierarchical information from an ontology, such as <REF>Resnik 1995</REF> and <REF>Agirre and Martinez 2002</REF>; and those which simply use word distribution information from a large corpus, such as <REF>Lin 1998</REF> and <TREF>Lee 1999</TREF>~~~9 Conclusion This paper represents a first step towards a corpusbased approach for cross-lingual identification of word concepts and alignment of ontologies~~~The method borrows from techniques used in machine translation and information retrieval, and does not make any assumptions about the structure of the ontology, or use any but the most basic structural information.
A number of simple linguistic techniques has been developed to alleviate such problems, ranging from the use of stemming, lexical chain and thesaurus <REF>Jing  Tzoukermann, 1999</REF></REF>; <REF>Green, 1999</REF>, to word-sense disambiguation <REF>Chen  Chang, 1998</REF>; <REF>Leacock et al, 1998</REF>; <REF>Ide  Veronis, 1998</REF> and context <REF>Cohen  Singer, 1999</REF>; <REF>Jing  Tzoukermann, 1999</REF></REF>~~~The connectionist approach has been widely used to extract knowledge in a wide range of information processing tasks including natural language processing, information retrieval and image understanding <REF>Anderson, 1983</REF>; <REF>Lee  Dubin, 1999</REF>; <REF>Sarkas  Boyer, 1995</REF>; <REF>Wang  Terman, 1995</REF>~~~Because the connectionist approach closely resembling human cognition process in text processing, it seems natural to adopt this approach, in conjunction with linguistic analysis, to perform topic spotting~~~However, there have been few attempts in this direction.
33 Evaluation of Class Attributes Extraction Parameters: Given a target class specified as a set of instances and a set of five seed attributes for a class eg, quality, speed, number of users, market share, reliability for SearchEngine, the method described in Section 22 extracts ranked lists of class attributes from the input query logs~~~Internally, the ranking uses Jensen-Shannon <TREF>Lee, 1999</TREF> to compute similarity scores between internal representations of seed attributes, on one hand, and each of the candidate attributes, on the other hand~~~Evaluation Procedure: To remove any possible bias towards higher-ranked attributes during the assessment of class attributes, the ranked lists of attributes to be evaluated are sorted alphabetically into a merged list~~~Each attribute of the merged list is  0  02  04  06  08  1  0  10  20  30  40  50 Precision Rank Class: Holiday manually assembled instances automatically extracted instances  0  02  04  06  08  1  0  10  20  30  40  50 Precision Rank Class: Average-Class manually assembled instances automatically extracted instances  0  02  04  06  08  1  0  10  20  30  40  50 Precision Rank Class: Mountain manually assembled instances automatically extracted instances  0  02  04  06  08  1  0  10  20  30  40  50 Precision Rank Class: Average-Class manually assembled instances automatically extracted instances Figure 3: Accuracy of attributes extracted based on manually assembled, gold standard M vs automatically extracted E instance sets, for a few target classes leftmost graphs and as an average over all 37 target classes rightmost graphs.
The first and second row shows the number of distinct words and word pairs used for the distributional and pattern-based word clustering respectively~~~Three K-means algorithms using different distributional similarity or dissimilarity measures: cosine, -skew divergence <TREF>Lee, 1999</TREF> 4 , and Lins similarity <REF>Lin, 1998</REF>~~~The CBC algorithm <REF>Lin and Pantel, 2002</REF>; <REF>Pantel and Lin, 2002</REF>~~~53 Evaluation procedure All the nouns in the data set were clustered by the proposed and baseline systems.
et al, 1999; <REF>Torisawa, 2002</REF>~~~Others proposed distributional similarity measures between words <REF>Hindle, 1990</REF>; <REF>Lin, 1998</REF>; <TREF>Lee, 1999</TREF>; <REF>Weeds et al, 2004</REF>~~~Once such similarity is defined, it is trivial to perform clustering~~~On the other hand, some researchers utilized co-occurrence for word clustering.
Moreover, it may not work well for dealing with general predicate phrases because it is hard to enumerate all phrases to determine the weights of features w,fWe thus simply adopted the co-occurrence frequency of the phrase and the feature as in <REF>Fujita and Sato, 2008</REF>~~~Skew divergence The skew divergence, a variant of KL divergence, was proposed in <TREF>Lee, 1999</TREF> based on an insight: the substitutability of one word for another need not be symmetrical~~~The divergence is given by the following formula: d skew t,sD P s bardblP t 1 P s , where P s and P t are the probability distributions of features for the given original and substituted words s and t, respectively~~~0    1 is a parameter for approximating KL divergence DThe score can be recast into a similarity score via, for example, the following function <REF>Fujita and Sato, 2008</REF>: Par skew stexpd skew t,s.
We preferred taxonomic class-based methods over distributional clustering mainly because we wanted to compare directly methods that use distributional information inherent in the corpus without making external assumptions with regard to how concepts and their similarity are represented with methods that quantify similarity relationships based on information present in a hand-crafted taxonomy~~~Furthermore, as <REF>Lee and Pereiras 1999</REF> results indicate that distributional clustering 365 Lapata The Disambiguation of Nominalizations and distance-weighted averaging obtain similar levels of performance, we restricted ourselves to the latter~~~We evaluated the contribution of the different smoothing methods on the nominalization task by exploring how each method and their combination influences disambiguation performance~~~Sections 3133 review discounting, class-based smoothing, and distance-weighted averaging.
The problem arises when the probability of word combinations that do not occur in the training data needs to be estimated~~~The smoothing methods proposed in the literature overviews are provided by <REF>Dagan, Lee, and Pereira 1999</REF> and <TREF>Lee 1999</TREF> can be generally divided into three types: discounting <REF>Katz 1987</REF>, class-based smoothing <REF>Resnik 1993</REF>; <REF>Brown et al 1992</REF>; 364 Computational Linguistics Volume 28, Number 3 <REF>Pereira, Tishby, and Lee 1993</REF>, and distance-weighted averaging <REF>Grishman and Sterling 1994</REF>; <REF>Dagan, Lee, and Pereira 1999</REF>~~~Discounting methods decrease the probability of previously seen events so that the total probability of observed word co-occurrences is less than one, leaving some probability mass to be redistributed among unseen co-occurrences~~~Class-based smoothing and distance-weighted averaging both rely on an intuitively simple idea: interword dependencies are modeled by relying on the corpus evidence available for words that are similar to the words of interest.
Furthermore, some nominalizations are conventionalized eg , business administration, health organization and are therefore attested more frequently than their verb-subject or verb-object counterparts~~~We re-created the frequencies of unseen verb-argument pairs by experimenting with three types of smoothing techniques proposed in the literature: back-off smoothing <REF>Katz 1987</REF>, class-based smoothing <REF>Resnik 1993</REF>; <REF>Lauer 1995</REF>, and distanceweighted averaging <REF>Grishman and Sterling 1994</REF>; <REF>Dagan, Lee, and Pereira 1999</REF>~~~We present these three smoothing variants and their underlying assumptions in the following section~~~3.
A key feature of this type of smoothing is the function that measures distributional similarity from co-occurrence frequencies~~~Several measures of distributional similarity have been proposed in the literature <REF>Dagan, Lee, and Pereira 1999</REF>; <TREF>Lee 1999</TREF>~~~We used two measures, the Jensen-Shannon divergence and the confusion probability~~~The choice of these two measures was motivated by work described in <REF>Dagan, Lee, and Pereira 1999</REF>, in which the JensenShannon divergence outperforms related similarity measures such as the confusion probability or the L 1 norm on a pseudodisambiguation task that uses verb-object pairs.
<REF>Grishman and Sterling 1994</REF> in particular employ the confusion probability to re-create the frequencies of verb-noun co-occurrences in which the noun is the object or the subject of the verb in question~~~In the following we describe these two similarity measures and show how they can be used to re-create the frequencies for unseen verb-argument tuples for a more detailed description see <REF>Dagan, Lee, and Pereira 1999</REF>~~~331 Confusion Probability~~~The confusion probability P C is an estimate of the probability that a word w 1 can be substituted for a word w prime 1, in the sense of being found in the same contexts.
In class-based smoothing, classes are used as the basis according to which the co-occurrence probability of unseen word combinations is estimated~~~Classes can be induced directly from the corpus using distributional clustering <REF>Pereira, Tishby, and Lee 1993</REF>; <REF>Brown et al 1992</REF>; <REF>Lee and Pereira 1999</REF> or taken from a manually crafted taxonomy <REF>Resnik 1993</REF>~~~In the latter case the taxonomy is used to provide a mapping from words to conceptual classes~~~Distance-weighted averaging differs from distributional clustering in that it does not explicitly cluster words.
33 Distance-Weighted Averaging Distance-weighted averaging induces classes of similar words from word co-occurrences without making reference to a taxonomy~~~Instead, it is based on the assumption that if a word w prime 1 is similar to word w 1, then w prime 1 can provide information about the frequency of unseen word pairs involving w 1 <REF>Dagan, Lee, and Pereira 1999</REF>~~~A key feature of this type of smoothing is the function that measures distributional similarity from co-occurrence frequencies~~~Several measures of distributional similarity have been proposed in the literature <REF>Dagan, Lee, and Pereira 1999</REF>; <TREF>Lee 1999</TREF>.
In language modeling, smoothing techniques are typically evaluated by showing that a language model that uses smoothed estimates incurs a reduction in perplexity on test data over a model that does not employ smoothed estimates <REF>Katz 1987</REF>~~~<REF>Dagan, Lee, and Pereira 1999</REF> use perplexity to compare back-off smoothing against distance-weighted averaging methods within the context of language modeling for speech recognition and show that the latter outperform the former~~~They also compare different distance-weighted averaging methods on a pseudoword disambiguation task in which the language model decides which of two verbs v 1 and v 2 is more likely to take a noun n as its object~~~The method being tested must reconstruct which of the unseen v 1, n and v 2, n is a valid verb-object combination.
The method being tested must reconstruct which of the unseen v 1, n and v 2, n is a valid verb-object combination~~~The same task is used by <REF>Lee and Pereira 1999</REF> in a detailed comparison between distributional clustering and distance-weighted averaging that demonstrates that the two methods yield comparable results~~~In our experiments we re-created co-occurrence frequencies for unseen verb-subject and verb-object pairs using three maximally different approaches: back-off smoothing, class-based smoothing using a predefined taxonomy, and distance-weighted averaging~~~We preferred taxonomic class-based methods over distributional clustering mainly because we wanted to compare directly methods that use distributional information inherent in the corpus without making external assumptions with regard to how concepts and their similarity are represented with methods that quantify similarity relationships based on information present in a hand-crafted taxonomy.
We used two measures, the Jensen-Shannon divergence and the confusion probability~~~The choice of these two measures was motivated by work described in <REF>Dagan, Lee, and Pereira 1999</REF>, in which the JensenShannon divergence outperforms related similarity measures such as the confusion probability or the L 1 norm on a pseudodisambiguation task that uses verb-object pairs~~~The confusion probability has been used by several authors to smooth word co367 Lapata The Disambiguation of Nominalizations occurrence probabilities <REF>Essen and Steinbiss 1992</REF>; <REF>Grishman and Sterling 1994</REF> and shown to give promising performance~~~<REF>Grishman and Sterling 1994</REF> in particular employ the confusion probability to re-create the frequencies of verb-noun co-occurrences in which the noun is the object or the subject of the verb in question.
McCarthy determines the sense profile of a verb/slot pair using a minimum description length tree cut model over the frequency-populated hierarchy <REF>Li and Abe, 1998</REF>~~~The two profiles for a verb are aligned to permit comparison using skew divergence as a probability distance measure <TREF>Lee 1999</TREF>~~~This step is explained in more detail in the next section, with an example~~~The value of the distance measure is compared to a threshold, which determines classification of a verb as causative the two profiles are similar or non-causative the two profiles are dissimilar, leading to best performance of 73 accuracy, on a set of hand-selected verbs.
<REF>Briefly, Clark and Weir 2002</REF> populate the WordNet hierarchy based on corpus frequencies of all nouns for a verb/slot pair, and then determine the appropriate probability estimate at each node in the hierarchy by using a24 a102 to determine whether to generalize an estimate to a parent node in the hierarchy~~~We compare SPD to other measures applied directly to the unpropagated probability profiles given by the Clark-Weir method: the probability distribution distance given by skew divergence skew <TREF>Lee, 1999</TREF>, as well as the general vector distance given by cosine cos~~~These are the measures aside from SPD that performed best in our pilot experiments~~~It is worth noting that the method of <REF>Clark and Weir 2002</REF> does not yield a tree cut, but instead generally populates the WordNet hierarchy with non-zero probabilities.
A drawback of this approach for generalizing to other sense profile comparisons is the assumption in relative entropy of an asymmetry between the two probability distributions~~~<REF>Similarly, McCarthy 2000</REF> uses skew divergence a variant of KL divergence proposed by <TREF>Lee, 1999</TREF> to compare the sense profile of one argument of a verb eg , the subject position of the intransitive to another argument of the same verb eg , the object position of the transitive, to determine if the verb participates in an argument alternation involving the two positions~~~For example, the causative alternation in sentences 1 and 2 illustrates how the subject of the intransitive is the same underlying semantic argument ie , the Themethe argument undergoing the action as the object of the transitive: 1 The snow melted~~~2 The sun melted the snow.
Due to the original KL distance is asymmetric and is not defined when zero frequency occurs~~~Some enhanced KL models were developed to prevent these problems such as Jensen-Shannon <REF>Jianhua, 1991</REF>, which introducing a probabilistic variable m, or  -Skew Divergence <TREF>Lee, 1999</TREF>, by adopting adjustable variable ~~~Research shows that Skew Divergence achieves better performance than other measures~~~<REF>Lee, 2001</REF> 1yxS rgenceDSkewDive yxxKL aaa  2/,2/yx,JS Shannon-DJensen yxm myKLmxKL   To convert distance to similarity value, we adopt the formula inspired by <REF>Mochihashi, and Matsumoto 2002</REF>.
Figure 2 exemplifies this process for two TOMs TCM1 and TCM2 in an imaginary hierarchy~~~The UBC is at the classes B, c and D To quantify the similarity between the probability distributions for the target slots we use the a-skew divergence aSD proposed by <TREF>Lee 1999</TREF>~~~1 This measure, defined in equation 2, is a smoothed version of the Kulback-Liebler divergence, plx and p2x are the two probability distributions which are being compared~~~The  constant is a value between 0 and 1 We also experimented with euclidian distance, the L1 norm, and cosine measures.
synonyms from the hypernyms verbs and nouns or closely related classes adjectives of all synsets of the target, ranked with the BNC frequency data~~~We also produced best and oot baselines using the distributional similarity measures l1, jaccard, cosine, lin <REF>Lin, 1998</REF> and SD <TREF>Lee, 1999</TREF> 4~~~We took the word with the largest similarity or smallest distance for SD and l1 for best and the top 10 for oot~~~For mw detection and identification we used WordNet to detect if a multiword in WordNet which includes the target word occurs within a window of 2 words before and 2 words after the target word.
All counts are log-likelihood transformed~~~We experiment with two distance measures to compute vector similarity, namely the Jaccard Coefficient and Cosine Distance, both of which have been shown to yield good performance in NLP tasks <TREF>Lee, 1999</TREF>; <REF>McDonald and Lowe, 1998</REF>~~~Evaluation Procedure~~~We evaluate our models by correlating the predicted plausibility values with the human judgements, which range between 1 and 7.
The distributional similarity was measured by means of three different similarity measures: the Jaccards coefficient, L1 distance, and the skew divergence~~~This choice of similarity measures was motivated by results of studies by <REF>Levy et al 1998</REF> and <TREF>Lee 1999</TREF> which compared several well known measures on similar tasks and found these three to be superior to many others~~~Another reason for this choice is that there are different ideas underlying these measures: while the Jaccards coefficient is a binary measure, L1 and the skew divergence are probabilistic, the former being geometrically motivated and the latter being a version of the information theoretic Kullback Leibler divergence cf~~~, <TREF>Lee 1999</TREF>.
The same procedure is applied for symmetric KL divergence and JS divergence~~~The second approach is from <TREF>Lee 1999</TREF>~~~Here similarity for KL is defined as Simp,q  C KLpq, where C is a free parameter to be tuned~~~4 Experimental Setup 41 Materials Following Chen et al.
Comparing the different divergence measures for LDA, we found that KL and JS perform significantly better than symmetrised KL divergence~~~Interestingly, the performance of the asymmetric KL divergence and the symmetric JS divergence is very close, which makes it difficult to conclude whether the relation discovery domain is a symmetric domain or an asymmetric domain like <TREF>Lees 1999</TREF> task of improving probability estimates for unseen word co-occurrences~~~A shortcoming of all the models we will describe here is that they are derived from the basic bag-of-words models and as such do not account for word order or other notions of syntax~~~Related work on relation discovery by Zhang et al.
The optimal configuration varies by the divergence measure with D  50 and C  14 for KL divergence, D  200 and C  4 for symmetrised KL, and D  150 and C  2 for JS divergence~~~For all divergence measures, <TREF>Lees 1999</TREF> method outperformed Dagan et als 1997 method~~~Also for all divergence measures, the model hyper-parameter  was found to be optimal at 00001~~~The  hyper-parameter was always set to 50/T following <REF>Griffiths and Steyvers 2004</REF>.
wx,f stands for the weight frequency in our experiment of f in F x  While Par Lin is symmetric, it has been argued that itisimportant todetermine thedirection ofparaphrase~~~As an asymmetric measure, we examine skew divergence defined by the following equation <TREF>Lee, 1999</TREF>: d skew t,sD P s bardblP t 1 P s , where P x denotes a probability distribution estimated 6 from a feature set F x HowwellP t approximates P s is calculated based on the KL divergence, D The parameter  is set to 099, following tradition, because the optimization of  is difficult~~~To take consistent measurements, we define the paraphrasability score Par skew as follows: Par skew stexpd skew t,s~~~6 We estimate them simply using maximum likelihood estimation, ie, P x fwx,f/ P f prime F x wx,f prime .
Note that if any a56 a8 a64a80a50, then a57 a11a81a1a59a55a60a52a61a56a4a5 is infinite; in general, the KLdivergence is very sensitive to small probabilities, and careful attention must be paid to smoothing if it is to be used with text co-occurrence data~~~The Jensen-Shannon divergencean average of the divergences of a55 and a56 from their mean distribution does not share this sensitivity and has previously been used in tests of lexical similarity <TREF>Lee, 1999</TREF>~~~Furthermore, unlike the KL-divergence, it is symmetric, presumably a desirable property in this setting, since synonymy is a symmetric relation, and our test design exploits this symmetry~~~However, a57 a11a83a82a45a17 a1a59a55a60a52a61a56a4a5, the Hellinger distance 3, is also symmetric and robust to small or zero estimates.
For a48 a49 a1a51a50a23a52a54a53a19a5 and word-conditional context distributions a55 and a56, we have the so-called a48 -divergences <REF>Zhu and Rohwer, 1998</REF>: a57 a58 a1a59a55a60a52a61a56a4a5a63a62a59a64 a53a65a14 a7 a55 a58 a56 a11a38a66 a58 a48a67a1a45a53a18a14a16a48a42a5 1 Divergences a57 a68 and a57 a11 are defined as limits as a48a6a69 a50 and a48a6a69a70a53 :a57 a11 a1a59a55a60a52a61a56a4a5a71a64 a57 a68 a1a51a56a67a52a51a55a72a5a71a64a74a73 a55a76a75a78a77a47a79 a55 a56 In other words, a57 a11a19a1a59a55a60a52a61a56a4a5 is the KL-divergence of a55 from a56  Members of this divergence family are in some sense preferred by theory to alternative measures~~~It can be shown that the a48 -divergences or divergences defined by combinations of them, such as the Jensen-Shannon or skew divergences <TREF>Lee, 1999</TREF> are the only ones that are robust to redundant contexts ie , only divergences in this family are invariant <REF>Csiszar, 1975</REF>~~~Several notions of lexical similarity have been based on the KL-divergence~~~Note that if any a56 a8 a64a80a50, then a57 a11a81a1a59a55a60a52a61a56a4a5 is infinite; in general, the KLdivergence is very sensitive to small probabilities, and careful attention must be paid to smoothing if it is to be used with text co-occurrence data.
By default, we bracket a token sequence with pseudo-tokens <bos> and <eos>2 Contextual tokens in the window may be either observed or disregarded, and the policy governing which to admit is one of the dimensions we explore here~~~The decision whether or not to observe a particular contextual token is made before counting commences, and is not sensitive to the circumstances of a particular occurrence eg , its participation in some syntactic relation <REF>Lin, 1997</REF>; <TREF>Lee, 1999</TREF>~~~When a contextual token is observed, it is always counted as a single occurrence~~~Thus, in contrast with earlier approaches <REF>Sahlgren, 2001</REF>; <REF>Ehlert, 2003</REF>, we do not use a weighting scheme that is a function of distance from the reference token.
We do not know whether or to what extent this particular parameter setting is universally best, best only for English, best for newswire English, or best only for the specific test we have devised~~~We have restricted our attention to a relatively small space of similarity measures, excluding many previously proposed measures of lexical affinity but see Weeds, et al 2004, and <TREF>Lee 1999</TREF> for some empirical comparisons~~~Lee observed that measures from the space of invariant divergences particularly the JS and skew divergences perform at least as well as any of a wide variety of alternatives~~~As noted, we experimented with the JS divergence and observed accuracies that tracked those of the Hellinger closely.
One could look at differences in the ranking over all words, using a meaTraining Testing FINANCE SPORTS Finance 355 Sports 409 SemCor 142 153 100 Table 4: WSD accuracy for words with a different first sense to the BNC~~~sure such as pairwise agreement of rankings or a ranking correlation coefficient, such as Spearmans One could also use the rankings to estimate probability distributions and compare the distributions with measures such as alpha-skew divergence <TREF>Lee, 1999</TREF>~~~A simple definition would be where the rankings assign different predominant senses to a word~~~Taking this simple definition of deviation, we demonstrate how this might be done for our corpora.
The distributional hypothesis <REF>Harris, 1968</REF> says the following: The meaning of entities, and the meaning of grammatical relations among them, is related to the restriction of combinations of these entities relative to other entities~~~Over recent years, many applications <REF>Lin, 1998</REF>, <TREF>Lee, 1999</TREF>, <REF>Lee, 2001</REF>, <REF>Weeds et al , 2004</REF>, and <REF>Weeds and Weir, 2006</REF> have been investigating the distributional similarity of words~~~Similarity means that words with similar meaning tend to appear in similar contexts~~~In NLG, the considerationofsemanticsimilarityisusuallypreferred to just distributional similarity.
The pairs are generally either related in one type of relationship, or completely unrelated~~~In general we may be able to identify related phrases for example with distributional similarity <TREF>Lee, 1999</TREF>, but would like to be able to automatically classify the related phrases by the type of the relationship~~~For this task we identify a larger set of candidate-related phrases~~~32 Query Log Data To find phrases that are similar or substitutable for web searchers, we turn to logs of user search sessions.
We draw a distinction between the task of identifying terms which are topically related and identifying the specific semantic class~~~For example, the terms dog, puppy, canine, schnauzer, cat and pet are highly related terms, which can be identified using techniques that include distributional similarity <TREF>Lee, 1999</TREF> and withindocument cooccurrence measures such as pointwise mutual information <REF>Turney et al , 2003</REF>~~~These techniques, however, do not allow us to distinguish the more specific relationships:  hypernymdog,puppy This work was carried out while these authors were at Yahoo~~~Research.
In all studies done so far, however, the first classifier  the confusion sets  were constructed manually by the researchers~~~Other word predictions tasks have also constructed manually the list of confusion sets <REF>Lee and Pereira, 1999</REF>; <REF>Dagan et al , 1999</REF>; <TREF>Lee, 1999</TREF> and justifications where given as to why this is a reasonable way to construct it~~~Even-<REF>Zohar and Roth, 2000</REF> present a similar task in which the confusion sets generation was automated~~~Their study also quantified experimentally the advantage in using early classifiers to restrict the size of the confusion set.
Therefore, the  term in skew divergence implicitly defines a parameter stating how many orders of magnitude smaller than pj to count qj if qj  0~~~We define the Zero-KL divergence with respect to 2<REF>In Lees 1999</REF> original presentation, skew divergence is defined not as sp,q but rather as sq,p~~~We reverse the argument order for consistency with the other measures discussed here~~~586 gamma: ZKLp,q  summationdisplay i pi braceleftbigg logpi qi qi negationslash 0  qi  0 Note that this is exactly KL-divergence when KLdivergence is defined and, like skew divergence, approximates KL divergence in the limit as   .
One is Jensen-Shannon divergence <REF>Lin, 1991</REF>, a symmetric measure based on KL-divergence defined as the average of the KL divergences of each distribution to their average distribution~~~Jensen-Shannon is well defined for all distributions becausetheaverageofpi andqi isnon-zerowhenevereither number is These measures and others are surveyed in <REF>Lee, 2001</REF>, who finds that Jensen-Shannon is outperformed by the Skew divergence measure introduced by Lee in 1999~~~The skew divergence2 accounts for zeros in q by mixing in a small amount of p sp,q  Dp bardbl q  1p  summationtexti pi log piqi1pi Lee found that as   1, the performance of skew divergence on natural language tasks improves~~~In particular, it outperforms most other models and even beats pure KL divergence modified to avoid zeros with sophisticated smoothing models.
22 Nearest-neighbors averaging As noted earlier, the nearest-neighbors averaging method is an alternative to clustering for estimating the probabilities of unseen cooccurfences~~~Given an unseen pair n, v, we calculate an estimate 15vln  as an appropriate average of pvln I where n I is distributionally similar to n Many distributional similarity measures can be considered <TREF>Lee, 1999</TREF>~~~In this paper, we focus on the one that gave the best results in our earlier work <REF>Dagan et al , 1999</REF>, the Jensen-Shannon divergence <REF>Rao, 1982</REF>; <REF>Lin, 1991</REF>~~~The Jensen-Shannon divergence of two discrete distributions p and q over the same domain is defined as 1 gSp, q   It is easy to see that JSp, q is always defined.
Opinion-piece data are used for training, and a different set of opinion-piece data and the subjective-element data are used for testing~~~With distributional similarity, words are judged to be more or less similar based on their distributional patterning in text <TREF>Lee 1999</TREF>; <REF>Lee and Pereira 1999</REF>~~~Our Table 5 Random sample of fixed-3-gram collocations in OP1~~~one-noun of-prep his-det worst-adj of-prep all-det quality-noun of-prep the-det to-prep do-verb so-adverb in-prep the-det company-noun you-pronoun and-conj your-pronoun have-verb taken-verb the-det rest-noun of-prep us-pronoun are-verb at-prep least-adj but-conj if-prep you-pronoun as-prep a-det weapon-noun continue-verb to-to do-verb purpose-noun of-prep the-det could-modal have-verb be-verb it-pronoun seem-verb to-prep to-pronoun continue-verb to-prep have-verb be-verb the-det do-verb something-noun about-prep cause-verb you-pronoun to-to evidence-noun to-to back-adverb that-prep you-pronoun are-verb i-pronoun be-verb not-adverb of-prep the-det century-noun of-prep money-noun be-prep 291 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language Table 6 Random sample of unique generalized collocations in OP1.
Thus, to decide whether to retain a word as a PSE, we consider the precision not of the individual word, but of the word together with a cluster of words similar to it~~~Many variants of distributional similarity have been used in NLP <TREF>Lee 1999</TREF>; <REF>Lee and Pereira 1999</REF>~~~<REF>Dekang Lins 1998</REF> method is used here~~~In contrast to many implementations, which focus exclusively on verb-noun relationships, Lins method incorporates a variety of syntactic relations.
To evaluate word prediction as a simple language model~~~We chose the verb prediction task which is similar to other word prediction tasks eg ,<REF>Golding and Roth, 1999</REF> and, in particular, follows the paradigm in <REF>Lee and Pereira, 1999</REF>; <REF>Dagan et al , 1999</REF>; <TREF>Lee, 1999</TREF>~~~There, a list of the confusion sets is constructed first, each consists of two different verbs~~~The verb vl is coupled with v2 provided that they occur equally likely in the corpus.
Results are shown in percentage of improvement in accuracy over the baseline~~~Table 2 compares our method to methods that use similarity measures <REF>Dagan et al , 1999</REF>; <TREF>Lee, 1999</TREF>~~~Since we could not use the same corpus as in those experiments, we compare the ratio of improvement and not the WER~~~The baseline in this studies is different, but other than that the experiments are identical.
The cosine measure <REF>Salton and McGill, 1983</REF> returns the cosine of the angle between two vectors~~~The Jensen-Shannon JS divergence measure <REF>Rao, 1983</REF> and the -skew divergence measure <TREF>Lee, 1999</TREF> are based on the Kullback-Leibler KL divergence measure~~~The KL divergence, or relative entropy, Dpjjq, between two probability distribution functions p and q is de ned <REF>Cover and Thomas, 1991</REF> as the ine ciency of assuming that the distribution is q when the true distribution is p: Dpjjq  Pcplog pq~~~However, Dpjjq  1 if there are any contexts c for which pc > 0 and qc  0.
However, due to the lack of a tight de nition for the concept of distributional similarity and the broad range of potential applications, a large number of measures of distributional similarity have been proposed or adopted see Section 2~~~Previous work on the evaluation of distributional similarity methods tends to either compare sets of distributionally similar words to a manually created semantic resource <REF>Lin, 1998</REF>; <REF>Curran and Moens, 2002</REF> or be oriented towards a particular task such as language modelling <REF>Dagan et al , 1999</REF>; <TREF>Lee, 1999</TREF>~~~The rst approach is not ideal since it assumes that the goal of distributional similarity methods is to predict semantic similarity and that the semantic resource used is a valid gold standard~~~Further, the second approach is clearly advantageous when one wishes to apply distributional similarity methods in a particular application area.
As the vector for each target word must sum to 1, the marginal distributions of target words have little effect on the resulting similarity estimates~~~Many 649 similarity measures and weighting functions have been proposed for distributional vectors; comparative studies include <TREF>Lee 1999</TREF>, <REF>Curran 2003</REF> and <REF>Weeds and Weir 2005</REF>~~~22 Kernel Methods for Computing Similarity and Distance In this section we describe two classes of functions, positive semi-definite and negative semidefinite kernels, and state some relationships between these classes~~~The mathematical treatment follows Berg et al.
It seems likely that distance measures that are known to work well for comparing co-occurrence distributions will also give us suitable psd similarity measures~~~Negative semi-definite kernels are bydefinitionsymmetric, whichrulestheKullbackLeibler divergence and <TREF>Lees 1999</TREF>-skew divergence out of consideration~~~The nsd condition 2 ismetifthedistancefunctionisasquaredmetricin a Hilbert space~~~In this paper we use a parametric familyofsquaredHilbertianmetricsonprobability distributions that has been discussed by <REF>Hein and Bousquet 2005</REF>.
23 Distributional Kernels Given the effectiveness of distributional similarity measures for numerous tasks in NLP and the interpretation of kernels as similarity functions, it seems natural to consider the use of kernels tailored for co-occurrence distributions when performing semantic classification~~~As shown in Section 22 the standardly used linear and Gaussian kernelsderivefromtheL2 distance, yet<TREF>Lee1999</TREF> has shown that this distance measure is relatively poor at comparing co-occurrence distributions~~~Information theory provides a number of alternative distance functions on probability measures, of which the L1 distance also called variational distance, Kullback-Leibler divergence and JensenShannon divergence are well-known in NLP and 1Negated nsd functions are sometimes called conditionally psd; they constitute a superset of the psd functions~~~650 Distance Definition Derived linear kernel L2 distance2 summationtextcPcw1Pcw22 summationtextc Pcw1Pcw2 L1 distance summationtextcPcw1Pcw2 summationtextc minPcw1,Pcw2 Jensen-Shannon summationtextc Pcw1log2 2Pcw1Pcw1Pcw2  summationtextc Pcw1log2 Pcw1Pcw1Pcw2  divergence Pcw2log2 2Pcw2Pcw1Pcw2 Pcw2log2 Pcw2Pcw1Pcw2 Hellinger distance summationtextcradicalbigPcw1radicalbigPcw22 summationtextcradicalbigPcw1Pcw2 Table 1: Squared metric distances on co-occurrence distributions and corresponding linear kernels were shown by Lee to give better similarity estimates than the L2 distance.
A key feature of this type of smoothing is the function which measures distributional similarity from cooccurrence frequencies~~~Several measures of distributional similarity have been proposed in the literature <REF>Dagan et al , 1999</REF>; <TREF>Lee, 1999</TREF>~~~We used two measures, the Jensen-Shannon divergence and the confusion probability~~~Those two measures have been previously shown to give promising performance for the task of estimating the frequencies of unseen verb-argument pairs <REF>Dagan et al , 1999</REF>; <REF>Grishman and Sterling, 1994</REF>; <REF>Lapata, 2000</REF>; <TREF>Lee, 1999</TREF>.
We used two measures, the Jensen-Shannon divergence and the confusion probability~~~Those two measures have been previously shown to give promising performance for the task of estimating the frequencies of unseen verb-argument pairs <REF>Dagan et al , 1999</REF>; <REF>Grishman and Sterling, 1994</REF>; <REF>Lapata, 2000</REF>; <TREF>Lee, 1999</TREF>~~~In the following we describe these two similarity measures and show how they can be used to recreate the frequencies for unseen adjective-noun pairs~~~Jensen-Shannon Divergence.
If a bigram is unseen in a given corpus, conventional approaches recreate its frequency using techniques such as back-off, linear interpolation, class-based smoothing or distance-weighted averaging see Dagan et al~~~1999 and <TREF>Lee 1999</TREF> for overviews~~~The approach proposed here does not recreate the missing counts, but instead retrieves them from a corpus that is much larger but also much more noisy than any existing corpus: it launches queries to a search engine in order to determine how often a bigram occurs on the web~~~We systematically investigated the validity of this approach by using it to obtain frequencies for predicate-argument bigrams adjective-noun, nounnoun, and verb-object bigrams.
The two measures are shown in Figure 2~~~The Skew divergence represents a generalisation of the Kullback-Leibler divergence and was proposed by <TREF>Lee 1999</TREF> as a linguistically motivated distance measure~~~We use a value of   :99~~~We explored in detail the influence of different types and sizes of context by varying the context specification and path value functions.
This makes semantic spaces more flexible, different types of contexts can be selected and words do not have to physically co-occur to be considered contextually relevant~~~However, existing models either concentrate on specific relations for constructing the semantic space such as objects eg , <TREF>Lee, 1999</TREF> or collapse all types of syntactic relations available for a given target word <REF>Grefenstette, 1994</REF>; <REF>Lin, 1998</REF>~~~Although syntactic information is now used to select a words appropriate contexts, this information is not explicitly captured in the contexts themselves which are still represented by words and is therefore not amenable to further processing~~~A commonly raised criticism for both types of semantic space models ie , word-based and syntaxbased concerns the notion of semantic similarity.
Contexts are defined as a small number of words surrounding the target word <REF>Lund and Burgess, 1996</REF>; <REF>Lowe and McDonald, 2000</REF> or as entire paragraphs, even documents <REF>Landauer and Dumais, 1997</REF>~~~Context is typically treated as a set of unordered words, although in some cases syntactic information is taken into account <REF>Lin, 1998</REF>; <REF>Grefenstette, 1994</REF>; <TREF>Lee, 1999</TREF>~~~A word can be thus viewed as a point in an n-dimensional semantic space~~~The semantic similarity between words can be then mathematically computed by measuring the distance between points in the semantic space using a metric such as cosine or Euclidean distance.
There are a number of studies that, starting from this hypothesis, have built automatic or semi-automatic procedures for clustering words <REF>Brill and Marcus, 1992</REF>; <REF>Pereira et al , 1993</REF>; <REF>Martin et al , 1998</REF>, especially in the field of cognitive sciences <REF>Redington et al , 1998</REF>; <REF>Gobet and Pine, 1997</REF>; <REF>Clark, 2000</REF>~~~They examine the distributional behaviour of some target words, comparing the lexical distribution of their respective collocates using quantitative measures of distributional similarity <TREF>Lee, 1999</TREF>~~~In <REF>Brill and Marcus, 1992</REF> it is given a semiautomatic procedure that, starting from lexical statistical data collected from a large corpus, aims to arrange target words in a tree more precisely a dendrogram, instead of clustering them automatically~~~This procedure requires a linguistic examination of the resulting tree, in order to identify the word classes that are most appropriate to describe the phenomenon under investigation.
The formula is symmetric but does not satisfy the triangle inequality~~~For speed the estimate may be calculated from the shared features alone <TREF>Lee, 1999</TREF>~~~After calculating all the pairwise estimates, we retained lists of the 100 most similar nouns for each of the nouns in the corpus data~~~No other data is used in the similarity calculations.
There are many approaches to computing semantic similarity between words based on their distribution in a corpus~~~For a general overview of similarity measures, see <REF>Manning and Schutze, 1999</REF>, and for some recent and extensive overviews and evaluations of similarity measures for ia automatic thesaurus construction, see <REF>Weeds, 2003</REF>; <REF>Curran, 2003</REF>; <REF>Lee, 2001</REF>; <REF>Dagan et al , 1999</REF>~~~They show that the information radius and the skew distance are among the best for finding distributional proxies for words~~~If we assume that a word w is represented as a sum of its contexts and that we can calculate the similarities between such word representations, we get a list Lw of words with quantifications of how similar they are to w Each similarity <REF>CompuTerm 2004</REF> 3rd International Workshop on Computational Terminology 63 list Lw contains a mix of words related to the senses of the word w If we wish to identify groups of synonyms and other related words in a list of similarityrated words, we need to find clusters of similar words that are more similar to one another than they are to other words.
The intuition behind the cosine measure is that the similarity between two distributions of words should be independent of the length of either document~~~However, researchers have demonstrated that cosine is not the best relevance metric for other applications, so we evaluated two other topical similarity scores: Jacquards coefficient, which performed better than most other similarity measures in a different task for <TREF>Lee 1999</TREF> and Nave Bayes, which gave better results than cosine in topic-adapted language models for <REF>Seymore and Rosenfeld 1997</REF>~~~We evaluated all three similarity metrics using Switchboard topics as the training data and each of our corpora for testing using cross-validation~~~We found that cosine is consistently better than both Jacquards coefficient and Nave Bayes, across all corpora tested.
1993 and <TREF>Lee 1999</TREF>, among others~~~We use the cosine similarity measure for windowbased contexts and the following commonly used similarity measures for the syntactic vector space: <REF>Hindles 1990</REF> measure, the weighted Lin measure <REF>Wu and Zhou, 2003</REF>, the -Skew divergence measure <TREF>Lee, 1999</TREF>, the Jensen-Shannon JS divergence measure <REF>Lin, 1991</REF>, Jaccards coef cient van <REF>Rijsbergen, 1979</REF> and the Confusion probability <REF>Essen and Steinbiss, 1992</REF>~~~The Jensen-Shannon measure JS x1, x2  summationtext yY summationtext xx1,x2 parenleftbigg P yx log parenleftbigg Pyx 1 2 Pyx1Pyx2 parenrightbiggparenrightbigg subsequently performed best for our task~~~We compare the different ranking methodologies and data sets with respect to a manually-de ned gold standard list of 20 goal-type verbs and 20 nouns.
We use verb-object relations in both active and passive voice constructions as did Pereira et al~~~1993 and <TREF>Lee 1999</TREF>, among others~~~We use the cosine similarity measure for windowbased contexts and the following commonly used similarity measures for the syntactic vector space: <REF>Hindles 1990</REF> measure, the weighted Lin measure <REF>Wu and Zhou, 2003</REF>, the -Skew divergence measure <TREF>Lee, 1999</TREF>, the Jensen-Shannon JS divergence measure <REF>Lin, 1991</REF>, Jaccards coef cient van <REF>Rijsbergen, 1979</REF> and the Confusion probability <REF>Essen and Steinbiss, 1992</REF>~~~The Jensen-Shannon measure JS x1, x2  summationtext yY summationtext xx1,x2 parenleftbigg P yx log parenleftbigg Pyx 1 2 Pyx1Pyx2 parenrightbiggparenrightbigg subsequently performed best for our task.
Although -Skew outperforms the simpler measures in ranking nouns, its performance on verbs is worse than the performance of Weighted Lin~~~<REF>While Lee 1999</REF> argues that -Skews asymmetry can be advantageous for nouns, this probably does not hold for verbs: verb hierarchies have much shallower structure than noun hierarchies with most verbs concentrated on one level <REF>Miller et al , 1990</REF>~~~This would explain why JS, which is symmetric compared to the -Skew metric, performed better in our experiments~~~In the evaluation presented here we therefore use Google Scholar data and the JS measure.
Table 1 summarises our expectations of the values of KL divergence and V, for the various substitutability relationships~~~KL divergence, unlike most similarity functions, is sensitive to the order of arguments related by hyponymy <TREF>Lee, 1999</TREF>~~~The 152 Something happened and something else happened~~~Something happened or something else happened.
149 This paper proposes that substitutability can be predicted through statistical analysis of the contexts in which connectives appear~~~Similar methods have been developed for predicting the similarity of nouns and verbs on the basis of their distributional similarity, and many distributional similarity functions have been proposed for these tasks <TREF>Lee, 1999</TREF>~~~However substitutability is a more complex notion than similarity, and we propose a novel variance-based function for assisting in this task~~~This paper constitutes a first step towards predicting substitutability of cnonectives automatically.
Here, xi and yj denote two words and c stands for a context~~~Similarly to <TREF>Lee, 1999</TREF>, we use unsmoothed relative frequencies to derive probability estimates P In the de nition of the dice coef cient, Fxi  c : Pcxi > 0~~~We are mainly interested in the symmetric measures dxi, yj  dyj, xi because of a symmetric positive semi-de nite matrix required by kernel methods~~~Consequently, such measures as the skew divergence were excluded from the consideration <TREF>Lee, 1999</TREF>.
There are a number of measures proposed over the years, including such metrics as cosine, dice coef cient, and Jaccard distance~~~Distributional similarity measures have been extensively studied in <TREF>Lee, 1999</TREF>; <REF>Weeds et al, 2004</REF>~~~We have chosen the following metrics: dice, cosine and l2 euclidean whose de nitions are given in Table 1~~~Here, xi and yj denote two words and c stands for a context.
We are mainly interested in the symmetric measures dxi, yj  dyj, xi because of a symmetric positive semi-de nite matrix required by kernel methods~~~Consequently, such measures as the skew divergence were excluded from the consideration <TREF>Lee, 1999</TREF>~~~The Euclidean measure as de ned in Table 1 does not necessarily vary from 0 to 1~~~It was therefore normalized by dividing an l2 score in Table 1 by a maximum score and retracting it from 1.
42 Experiment I: Distributional measures and their impact on the final performance Distributional similarity measures have been used for various tasks in the past~~~For instance, <TREF>Lee, 1999</TREF> employs them to detect similar nouns based on the verb-object cooccurrence pairs~~~The results suggest the Jaccards coef cient to be one of the best performing measures followed by some others including cosine~~~Euclidean distance fell into the group with the largest error rates.
This shows that we have extracted a reasonable number of features for each phrase, since distributional similarity techniques have been shown to work well for words which occur more than 100 times in a given corpus <REF>Lin, 1998</REF>; <REF>Weeds and Weir, 2003</REF>~~~We then computed the distributional similarity between each co-occurrence vector using the -skew divergence measure <TREF>Lee, 1999</TREF>~~~The -skew divergence measure is an approximation to the KullbackLeibler KL divergence meassure between two distributions p and q: Dpq  summationdisplay x pxlogpxqx 5We currently retain all of the distinctions between grammatical relations output by RASP~~~10 The -skew divergence measure is designed to be used when unreliable maximum likelihood estimates MLE of probabilities would result in the KL divergence being equal to .
Given this framework, many different methods of measuring distributional similarity have been proposed; see <REF>Dagan 2000</REF>, <REF>Weeds 2003</REF>, or <REF>Mohammad and Hirst 2005</REF> for a review~~~For example, the set of words that co-occur with w 1 and those that co-occur with w 2 may be regarded as a feature vector of each and their similarity measured as the cosine between the vectors; or a measure may be based on the KullbackLeibler divergence between the probability distributions Pww 1 andPww 2 , as, for example, <TREF>Lees 1999</TREF> -skew divergence~~~<REF>Lin 1998b</REF> uses his similarity theorem equation 19 above to derive a measure based on the degree of overlap of the sets of words with which w 1 and w 2 , respectively, have positive mutual information~~~22 Words that are distributionally similar do indeed often represent semantically related concepts, and vice versa, as the following examples demonstrate.
Second, whereas semantic relatedness is symmetric, distributional similarity is a potentially asymmetrical relationship~~~If distributional similarity is conceived of as substitutability, as <REF>Weeds and Weir 2005</REF> and <TREF>Lee 1999</TREF> emphasize, then asymmetries arise when one word appears in a subset of the contexts in which the other appears; for example, the adjectives that typically modify apple are a subset of those that modify fruit,sofruit substitutes for apple better than apple substitutes for fruit~~~While some distributional similarity measures, such as cosine, are symmetric, many, such as -skew divergence and the co-occurrence retrieval models developed by Weeds and Weir, are not~~~But this is simply not an adequate model of semantic relatedness, for which substitutability is far too strict a requirement: window and house are semantically related, but they are not plausibly substitutable in most contexts.
As a means of acknowledging the polysemy of language, in this paper the term concept will refer to a particular sense of a given word~~~We want to be very clear that, throughout this paper, when we say that two words are similar, this is a short way of saying that they denote similar concepts; we are not talking about similarity of distributional or co-occurrence behavior of the words, for which the term word similarity has also been used <REF>Dagan 2000</REF>; <REF>Dagan, Lee, and Pereira 1999</REF>~~~While similarity of denotation might be inferred from similarity of distributional or co-occurrence behavior <REF>Dagan 2000</REF>; <REF>Weeds 2003</REF>, the two are distinct ideas~~~We return to the relationship between them in Section 62.
Baseline1 and Baseline2 in our system use different back-off schema~~~The following formula is introduced in <TREF>Lee 1999</TREF> for word similarity-based smoothing: 4 , ,    1         tt tt wSw tt wSw tttt tt wwsim wtagPwwsim wtagP where Sw is a set of candidate similar words and simw,w is the similarity between word w and w~~~Word similarity-based smoothing approach is used in our system to make advantage of the huge unlabeled corpus~~~In order to plug the word similarity-based smoothing into our HMM model, we made several extensions to formula 4.
The constant and multiplying factors are required, since the CRM defines a similarity in the range 0,1, whereas the L 1 Norm defines a distance in the range 0,2 where 0 distance is equivalent to 1 on the similarity scale~~~44 The -skew Divergence Measure The -skew divergence measure <REF>Lee 1999, 2001</REF> is a popular approximation to the Kullback-Leibler divergence measure 8 <REF>Kullback and Leibler 1951</REF>; <REF>Cover and Thomas 1991</REF>~~~It is an approximation developed to be used when unreliable MLE probabilities 7 Distance measures, also referred to as divergence and dissimilarity measures, can be viewed as the inverse of similarity measures; that is, an increase in distance correlates with a decrease in similarity~~~8 The Kullback-Leibler divergence measure is also often referred to as relative entropy 456 Weeds and Weir Co-occurrence Retrieval would result in the actual Kullback-Leibler divergence measure being equal to Itis defined <TREF>Lee 1999</TREF> as: dist  q, r  Drq  1 r 35 for 0    1, and where: Dpq  summationdisplay x pxlog px qx 36 In effect, the q distribution is smoothed with the r distribution, which results in it always being non-zero when the r distribution is non-zero.
In our experiments, the development-set similarity using the harmonic mean in the additive MI-based CRM was 0312 for high-frequency nouns and 0153 for low-frequency nouns, and the development-set similarity using the harmonic mean in the additive t-test based CRM was 0294 for high-frequency nouns and 0129 for low-frequency nouns~~~52 Pseudo-Disambiguation Task Pseudo-disambiguation tasks have become a standard evaluation technique <REF>Gale, Church, and Yarowsky 1992</REF>; Sch utze 1992; <REF>Pereira, Tishby, and Lee 1993</REF>; Sch utze 1998; <TREF>Lee 1999</TREF>; <REF>Dagan, Lee, and Pereira 1999</REF>; <REF>Golding and Roth 1999</REF>; <REF>Rooth et al 1999</REF>; <REF>EvenZohar and Roth 2000</REF>; <REF>Lee 2001</REF>; <REF>Clark and Weir 2002</REF> and, in the current setting, we may use a nouns neighbors to decide which of two co-occurrences is the most likely~~~Although pseudo-disambiguation is an artificial task, it has relevance in at least two application areas~~~First, by replacing occurrences of a particular word in a test suite with 465 Computational Linguistics Volume 31, Number 4 a pair of words from which a technique must choose, we recreate a simplified version of the word sense disambiguation task; that is, we choose between a fixed number of homonyms based on local context.
In order to study the relationship between parameter settings and error rate, we combine three of the sets to form a development set and two of the sets to form a test set~~~The development set is used to optimize parameters and the test set 10 <REF>Unlike Lee 1999</REF>, we do not delete instances from the test data that occur in the training data~~~This is discussed in detail in <REF>Weeds 2003</REF>, but our main justification for this approach is that a single co-occurrence of n, v 1  compared to zero co-occurrences of n, v 2  is not necessarily sufficient evidence to conclude that the population probability of n, v 1  is greater than that of n, v 2 ~~~11 Ten being less than the minimum number 14 of possibly indistinct co-occurrences for any target noun in the original test data.
This is advantageous in the computation of similarity, since computing the sums over all co-occurrence types rather than just those co-occurring with at least one of the words is 1 very computationally expensive and 2 due to their vast number, the effect of these zero frequency co-occurrence types tends to outweigh the effect of those co-occurrence types that have actually occurred~~~Giving such weight to these shared non-occurrences seems unintuitive and has been shown by <TREF>Lee 1999</TREF> to be undesirable in the calculation of distributional similarity~~~Hence, when using the 448 Weeds and Weir Co-occurrence Retrieval ALLR as the weight function, we use the additional restriction that Pc, w > 0 when selecting features~~~24 Difference-Weighted Models In additive models, no distinction is made between features that have occurred to the same extent with each word and features that have occurred to different extents with each word.
It is an approximation developed to be used when unreliable MLE probabilities 7 Distance measures, also referred to as divergence and dissimilarity measures, can be viewed as the inverse of similarity measures; that is, an increase in distance correlates with a decrease in similarity~~~8 The Kullback-Leibler divergence measure is also often referred to as relative entropy 456 Weeds and Weir Co-occurrence Retrieval would result in the actual Kullback-Leibler divergence measure being equal to Itis defined <TREF>Lee 1999</TREF> as: dist  q, r  Drq  1 r 35 for 0    1, and where: Dpq  summationdisplay x pxlog px qx 36 In effect, the q distribution is smoothed with the r distribution, which results in it always being non-zero when the r distribution is non-zero~~~The parameter  controls the extent to which the measure approximates the Kullback-Leibler divergence measure~~~When  is close to 1, the approximation is close while avoiding the problem with zero probabilities associated with using the Kullback-Leibler divergence measure.
Further, the noun hyponymy hierarchy in WordNet, which will be used as a pseudo-gold standard for comparison, is widely recognized in this area of research~~~Some previous work on distributional similarity between nouns has used only a single grammatical relation eg , <TREF>Lee 1999</TREF>, whereas other work has considered multiple grammatical relations eg , <REF>Lin 1998a</REF>~~~We consider only a single grammatical relation because we believe that it is important to evaluate the usefulness of each grammatical relation in calculating similarity before deciding how to combine information from 5 This results in a single 80:20 split of the complete data set, in which we are guaranteed that the original relative frequencies of the target nouns are maintained~~~6 The use of grammatical relations to model context precludes finding similarities between words of different parts of speech.
A statistical technique using a language model that assigns a zero probability to these previously unseen events will rule the correct parse or interpretation of the utterance impossible~~~Similarity-based smoothing <REF>Hindle 1990</REF>; <REF>Brown et al 1992</REF>; <REF>Dagan, Marcus, and Markovitch 1993</REF>; <REF>Pereira, Tishby, and Lee 1993</REF>; <REF>Dagan, Lee, and Pereira 1999</REF> provides an intuitively appealing approach to language modeling~~~In order to estimate the probability of an unseen co-occurrence of events, estimates based on seen occurrences of similar events can be combined~~~For example, in a speech recognition task, we might predict that cat is a more likely subject of growl than the word cap, even though neither co-occurrence has been seen before, based on the fact that cat is similar to words that do occur as the subject of growl eg , dog and tiger, whereas cap is not.
However, the next section will present a small-scale study that compares the performance of several smoothing techniques with the performance of Web counts on a standard task from the literature~~~34 Pseudodisambiguation In the smoothing literature, re-created frequencies are typically evaluated using pseudodisambiguation <REF>Clark and Weir 2001</REF>; <REF>Dagan, Lee, and Pereira 1999</REF>; <TREF>Lee 1999</TREF>; <REF>Pereira, Tishby, and Lee 1993</REF>; <REF>Prescher, Riezler, and Rooth 2000</REF>; <REF>Rooth et al 1999</REF>~~~477 Keller and Lapata Web Frequencies for Unseen Bigrams The aim of the pseudodisambiguation task is to decide whether a given algorithm re-creates frequencies that make it possible to distinguish between seen and unseen bigrams in a given corpus~~~A set of pseudobigrams is constructed according to a set of criteria detailed below that ensure that they are unattested in the training corpus.
Conclusions This article explored a novel approach to overcoming data sparseness~~~If a bigram is unseen in a given corpus, conventional approaches re-create its frequency using techniques such as back-off, linear interpolation, class-based smoothing or distanceweighted averaging see Dagan, Lee, and Pereira 1999 and Lee 1999 for overviews~~~The approach proposed here does not re-create the missing counts but instead retrieves them from a corpus that is much larger but also much more noisy than any existing corpus: it launches queries to a search engine in order to determine how often the bigram occurs on the Web~~~We systematically investigated the validity of this approach by using it to obtain frequencies for predicate-argument bigrams adjective-noun, noun-noun, and verbobject bigrams.
Other, more sophisticated class-based methods do away with the simplifying assumption that the argument co-occurring with a given predicate adjective, noun, verb is distributed evenly across its conceptual classes and attempt to find the right level of generalization in a concept hierarchy, by discounting, for example, the contribution of very general classes <REF>Clark and Weir 2001</REF>; <REF>McCarthy 2000</REF>; <REF>Li and Abe 1998</REF>~~~Other smoothing approaches such as discounting <REF>Katz 1987</REF> and distance-weighted averaging <REF>Grishman and Sterling 1994</REF>; <REF>Dagan, Lee, and Pereira 1999</REF> re-create counts of unseen word combinations by exploiting only corpus-internal evidence, without relying on taxonomic information~~~Our goal was to demonstrate that frequencies retrieved from the Web are a viable alternative to conventional smoothing methods when data are sparse; we do not claim that our Web-based method is necessarily superior to smoothing or that it should be generally preferred over smoothing methods~~~However, the next section will present a small-scale study that compares the performance of several smoothing techniques with the performance of Web counts on a standard task from the literature.
They demonstrate that the counts re-created using this smoothing technique correlate significantly with plausibility judgments for adjective-noun bigrams~~~They also show that this class-based approach outperforms distance-weighted averaging <REF>Dagan, Lee, and Pereira 1999</REF>, a smoothing method that re-creates unseen word co-occurrences on the basis of distributional similarity without relying on a predefined taxonomy, in predicting plausibility~~~In the current study, we used the smoothing technique of <REF>Lapata, Keller, and McDonald 2001</REF> to re-create not only adjective-noun bigrams, but also noun-noun 475 Keller and Lapata Web Frequencies for Unseen Bigrams Table 12 Correlation of counts re-created using class-based smoothing with Web counts~~~Adjective-Noun Noun-Noun Verb-Object Seen Bigrams AltaVista 344 362 361 Google 330 343 349 Unseen Bigrams AltaVista 439 386 412 Google 444 421 397 p <05 one-tailed.
Despite their imperfect output, heuristic methods for the extraction of syntactic relations are relatively common in statistical NLP~~~Several statistical models employ frequencies obtained from the output of partial parsers and other heuristic methods; these include models for disambiguating the attachment site of prepositional phrases <REF>Hindle and Rooth 1993</REF>; <REF>Ratnaparkhi 1998</REF>, models for interpreting compound nouns <REF>Lauer 1995</REF>; <REF>Lapata 2002</REF> and polysemous adjectives <REF>Lapata 2001</REF>, models for the induction of selectional preferences <REF>Abney and Light 1999</REF>, methods for automatically clustering words according to their distribution in particular syntactic contexts <REF>Pereira, Tishby, and Lee 1993</REF>, automatic thesaurus extraction <REF>Grefenstette 1994</REF>; <REF>Curran 2002</REF>, and similarity-based models of word co-occurrence probabilities <TREF>Lee 1999</TREF>; <REF>Dagan, Lee, and Pereira 1999</REF>~~~In this article we investigate alternative ways for obtaining bigram frequencies that are potentially useful for such models despite the fact that some of these bigrams are identified in a heuristic manner and may be noisy~~~22 Sampling Bigrams from the NANTC We also obtained corpus counts from a second corpus, the North American News Text Corpus NANTC.
By using Japanese HTML documents, we empirically show that our proposed method can obtain a significant number of hyponymy relations which would otherwise be missed by alternative methods~~~Hyponymy relations can play a crucial role in various NLP systems, and there have been many attempts to develop automatic methods to acquire hyponymy relations from text corpora <REF>Hearst, 1992</REF>; <TREF>Caraballo, 1999</TREF>; <REF>Imasumi, 2001</REF>; <REF>Fleischman et al , 2003</REF>; <REF>Morin and Jacquemin, 2003</REF>; <REF>Ando et al , 2003</REF>~~~Most of these techniques have relied on particular linguistic patterns, such as NP such as NP The frequencies of use for such linguistic patterns are relatively low, though, and there can be many expressions that do not appear in such patterns even if we look at large corpora~~~The effort of searching for other clues indicating hyponymy relations is thus significant.
<REF>Pantel and Lin, 2002</REF> improves on the latter by clustering by committee~~~<TREF>Caraballo 1999</TREF> uses conjunction and appositive annotations in the vector representation~~~2We did not compare against methods that use richer syntactic information, both because they are supervised and because they are much more computationally demanding~~~3We are not aware of any multilingual evaluation previously reported on the task.
For instance, <REF>Lin 1998</REF> used dependency relation as word features to compute word similarities from large corpora, and compared the thesaurus created in such a way with WordNet and Roget classes~~~<TREF>Caraballo 1999</TREF> selected head nouns from conjunctions and appositives in noun phrases, and used the cosine similarity measure with a bottomup clustering technique to construct a noun hierarchy from text~~~<REF>Curran and Moens 2002</REF> explored a new similarity measure for automatic thesaurus extraction which better compromises with the speed/performance tradeoff~~~<REF>You and Chen 2006</REF> used a feature clustering method to create a thesaurus from a Chinese newspaper corpus.
Previous work on automatic methods for building semantic lexicons could be divided into two main groups~~~One is automatic thesaurus acquisition, that is, to identify synonyms or topically related words from corpora based on various measures of similarity eg <REF>Riloff and Shepherd, 1997</REF>; <REF>Lin, 1998</REF>; <TREF>Caraballo, 1999</TREF>; <REF>Thelen and Riloff, 2002</REF>; <REF>You and Chen, 2006</REF>~~~Another line of research, which is more closely related to the current study, is to extend existing thesauri by classifying new words with respect to their given structures eg <REF>Tokunaga et al, 1997</REF>; <REF>Pekar, 2004</REF>~~~An early effort along this line is <REF>Hearst 1992</REF>, who attempted to identify hyponyms from large text corpora, based on a set of lexico-syntactic patterns, to augment and critique the content of WordNet.
In contrast, in this paper we focus on the problem of determining the categories of interest~~~Another thread of work is on finding synonymous terms and word associations, as well as automatic acquisition of IS-A or genus-head relations from dictionary definitions and free text <REF>Hearst, 1992</REF>; <TREF>Caraballo, 1999</TREF>~~~That work focuses on finding the right position for a word within a lexicon, rather than building up comprehensible and coherent faceted hierarchies~~~A major class of solutions for creating subject hierarchies uses data clustering.
One way to overcome this problem might be to give judges information about a sequence of higher ancestors, in order to make the judgement easier~~~It is difficult to compare these results with results from other studies such as that of <TREF>Caraballo 1999</TREF>, as the data used is not the same~~~However, it seems that our figures are in the same range as those reported in previous studies~~~<REF>Charniak  Roark 1998</REF>, evaluating the semantic lexicon against gold standard resources the MUC-4 and the WSJ corpus, reports that the ratio of valid to total entries for their system lies between 20 and 40.
For example, while the plural form of the word boy ie boys is a valid hyponym of the hypernym group, the singular form would not be~~~As was also reported by <TREF>Caraballo 1999</TREF>, the judges sometimes found proper nouns as hyponyms hard to evaluate~~~Eg it might be hard to tell if Simon Le Bon is a valid hyponym to the hypernym rock star if his identity is unknown to the judge~~~One way to overcome this problem might be to give judges information about a sequence of higher ancestors, in order to make the judgement easier.
Generally, principle 1-2 above are meant to prevent the hierarchies from containing ambiguity~~~The built-in ambiguity in the hyponymy hierarchy presented in <TREF>Caraballo, 1999</TREF> is primarily an effect of the fact that all information is composed into one tree~~~Part of the ambiguity could have been solved if the requirement of building one tree had been relaxed~~~Principle 2, regarding keeping the hierarchy ambiguity-free, is especially important, as we are working with acquisition from a corpus that is not domain restricted.
<REF>Charniak  Roark 1998</REF>, evaluating the semantic lexicon against gold standard resources the MUC-4 and the WSJ corpus, reports that the ratio of valid to total entries for their system lies between 20 and 40~~~<TREF>Caraballo 1999</TREF> let three judges evaluate ten internal nodes in the hyponymy hierarchy, that had at least twenty descendants~~~Cases where judges had problems with proper nouns as hyponyms, corresponding to these mentioned above, were corrected~~~When the best hypernym was evaluated, the result reported for a majority of the judges was 33.
Continue at 1~~~<TREF>Caraballo 1999</TREF> uses a hierarchical clustering technique to build a hyponymy hierarchy of nouns~~~The internal nodes are labeled by the syntactic constructions from <REF>Hearst 1992</REF>~~~Each internal node in the hierarchy can be represented by up to three nouns.
Automatically extracting world knowledge from MRDs was attempted by projects such as MindNet at Microsoft Research <REF>Richardson, Dolan, and Vanderwende 1998</REF>, and Barrierre and <REF>Popowichs 1996</REF> project, which learns from childrens dictionaries~~~IS-A hierarchies have been learned automatically from MRDs <REF>Hearst 1992</REF> and from corpora Caraballo 1999 among others~~~14 http://wwwclcamacuk/Research/NL/acquilex/acqhomehtml 240 Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences Research on merging information from various lexical resources is related to the present work in the sense that the consistency issues to be resolved are similar~~~One example is the construction of Unified Medical Language System UMLS 15 <REF>Lindberg, Humphreys, and McCray 1993</REF>, in the medical domain.
Based on these seeds, she proposes a bootstrapping algorithm to semi-automatically acquire new more specific patterns~~~Similarly, <TREF>Caraballo, 1999</TREF> uses predefined patterns such as X is a kind of Y or X, Y, and other Zs to identify hypernym/hyponym relationships~~~This approach to information extraction is based on a technique called selective concept extraction as defined by <REF>Riloff, 1993</REF>~~~Selective concept extraction is a form of text skimming that selectively processes relevant text while effectively ignoring surrounding text that is thought to be irrelevant to the domain.
However, it is well known that any knowledge-based system suffers from the so-called knowledge acquisition bottleneck, ie the difficulty to actually model the domain in question~~~As stated in <TREF>Caraballo, 1999</TREF>, WordNet has been an important lexical knowledge base, but it is insufficient for domain specific texts~~~So, many attempts have been made to automatically produce taxonomies <REF>Grefenstette, 1994</REF>, but <TREF>Caraballo, 1999</TREF> is certainly the first work which proposes a complete overview of the problem by 1 automatically building a hierarchical structure of nouns based on bottom-up clustering methods and 2 labeling the internal nodes of the resulting tree with hypernyms from the nouns clustered underneath by using patterns such as B is a kind of A~~~2008.
As stated in <TREF>Caraballo, 1999</TREF>, WordNet has been an important lexical knowledge base, but it is insufficient for domain specific texts~~~So, many attempts have been made to automatically produce taxonomies <REF>Grefenstette, 1994</REF>, but <TREF>Caraballo, 1999</TREF> is certainly the first work which proposes a complete overview of the problem by 1 automatically building a hierarchical structure of nouns based on bottom-up clustering methods and 2 labeling the internal nodes of the resulting tree with hypernyms from the nouns clustered underneath by using patterns such as B is a kind of A~~~2008~~~Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 30 Unported license http://creativecommonsorg/licenses/by-ncsa/30/.
For example, the phrase France, Germany, Italy, and other European countries suggests that France, Germany and Italy are part of the class of European countries~~~Such hierarchical examples are quite sparse, and greater coverage was later attained by <REF>Riloff and Shepherd 1997</REF> and <REF>Roark and Charniak 1998</REF> in extracting relations not of hierarchy but of similarity, by finding conjunctions or co-ordinations such as cloves, cinammon, and nutmeg and cars and trucks This work was extended by <TREF>Caraballo 1999</TREF>, who built classes of related words in this fashion and then reasoned that if a hierarchical relationship could be extracted for any member of this class, it could be applied to all members of the class~~~This technique can often mistakenly reason across an ambiguous middle-term, a situation that was improved upon by <REF>Cederberg and Widdows 2003</REF>, by combining pattern-based extraction with contextual filtering using latent semantic analysis~~~Prior work in discovering non-compositional phrases has been carried out by <REF>Lin 1999</REF> and Baldwin et al.
The extraction patterns used in our research are linguistically richer patterns, requiring shallow parsing and syntactic role assignment~~~In recent years several techniques have been developed for semantic lexicon creation eg , <REF>Hearst, 1992</REF>; <REF>Riloff and Shepherd, 1997</REF>; <REF>Roark and Charniak, 1998</REF>; <TREF>Caraballo, 1999</TREF>~~~Semantic word learning is different from subjective word learning, but we have shown that MetaBootstrapping and Basilisk could be successfully applied to subjectivity learning~~~Perhaps some of these other methods could also be used to learn subjective words.
For this reason, knowledge-poor approaches such as the distributional approach are particularly suited for this task~~~Its previous applications eg , <REF>Grefenstette 1993</REF>, <REF>Hearst and Schuetze 1993</REF>, <REF>Takunaga et al 1997</REF>, <REF>Lin 1998</REF>, <TREF>Caraballo 1999</TREF> demonstrated that cooccurrence statistics on a target word is often sufficient for its automatical classification into one of numerous classes such as synsets of WordNet~~~Distributional techniques, however, are poorly applicable to rare words, ie, those words for which a corpus does not contain enough cooccurrence data to judge about their meaning~~~Such words are the primary concern of many practical NLP applications: as a rule, they are semantically focused words and carry a lot of important information.
<REF>Hearst 1992</REF> used textual patterns eg such as to identify common class members~~~<REF>Caraballo and Charniak 1999</REF> and <TREF>Caraballo 1999</TREF> augmented these lexical patterns with more general lexical co-occurrence statistics such as relative entropy~~~<REF>Berland and Charniak 1999</REF> use Hearst style techniques to learn meronym relationships part-whole from corpora~~~There has also been work in building ontologies from structured Correct Answer Question Debbie Reynolds What actress once held the title of Miss Burbank.
In addition, <REF>Strzalkowski and Wang 1996</REF> used a bootstrapping technique to identify types of references, and <REF>Riloff and Jones 1999</REF> adapted bootstrapping techniques to lexicon building targeted to information extraction~~~In the same vein, researchers at Brown University <REF>Caraballo and Charniak, 1999</REF> <REF>Berland and Charniak, 1999</REF>, <TREF>Caraballo, 1999</TREF> and <REF>Roark and Charniak, 1998</REF> focused on target constructions, in particular complex noun thrases, and searched for information not only on identifying classes of nouns, lint also hypernyms, noun specificity and meronymy~~~We have a diflbrent perspective than these lines of inquiry~~~They were specifying various semantic relationships and seeking ways to collect similar pairs.
To do this, many approaches to lexical acquisition employ the distributional model of word meaning induced from the distribution of the word across various lexical contexts of its occurrence found in the corpus~~~The approach is now being actively explored for a wide range of semantics-related tasks including automatic construction of thesauri <REF>Lin, 1998</REF>; <TREF>Caraballo, 1999</TREF>, their enrichment <REF>Alfonseca and Manandhar, 2002</REF>; <REF>Pekar and Staab, 2002</REF>, acquisition of bilingual lexica from nonaligned <REF>Kay and Rscheisen, 1993</REF> and nonparallel corpora <REF>Fung and Yee, 1998</REF>, learning of information extraction patterns from un-annotated text <REF>Riloff and Schmelzenbach, 1998</REF>~~~However, because of irregularities in corpus data, corpus statistics cannot guarantee optimal performance, notably for rare lexical items~~~In order to improve robustness, recent research has attempted a variety of ways to incorporate external knowledge into the distributional model.
However, such clustering algorithms fail to name their classes~~~<TREF>Caraballo 1999</TREF> was the first to use clustering for labeling is-a relations using conjunction and apposition features to build noun clusters~~~<REF>Recently, Pantel and Ravichandran 2004</REF> extended this approach by making use of all syntactic dependency features for each noun~~~3 Syntactical co-occurrence approach Much of the research discussed above takes a similar approach of searching text for simple surface or lexico-syntactic patterns in a bottom-up approach.
larity measure could be defined so that, for example: simexecutives, spouses > simbusloads, spouses then it is potentially useful for coordination disambiguation~~~The idea that nouns co-occurring in conjunctions tend to be semantically related has been noted in <REF>Riloff and Shepherd, 1997</REF> and used effectively to automatically cluster semantically similar words <REF>Roark and Charniak, 1998</REF>; <TREF>Caraballo, 1999</TREF>; <REF>Widdows and Dorow, 2002</REF>~~~The tendency for conjoined nouns to be semantically similar has also been exploited for coordinate noun phrase disambiguation by <REF>Resnik 1999</REF> who employed a measure of similarity based on WordNet to measure which were the head nouns being conjoined in certain types of coordinate noun phrase~~~In this paper we look at different measures of word similarity in order to discover whether they can detect empirically a tendency for conjoined nouns to be more similar than nouns which co-occur but are not conjoined.
This includes the extraction of hyponymy and synonymy relations <REF>Hearst 1992</REF>; <TREF>Caraballo 1999</TREF>, among others as well as meronymy <REF>Berland and Charniak 1999</REF>; <REF>Meyer 2001</REF>~~~10 One approach to the extraction of instances of a particular lexical relation is the use of patterns that express lexical relations structurally explicitly in a corpus <REF>Hearst 1992</REF>; <REF>Berland and Charniak 1999</REF>; <TREF>Caraballo 1999</TREF>; <REF>Meyer 2001</REF>, and this is the approach we focus on here~~~As an example, the pattern NP 1 and other NP 2 usually expresses a hyponymy/similarity relation between the hyponym NP 1 and its hypernym NP 2 <REF>Hearst 1992</REF>, and it can therefore be postulated that two noun phrases that occur in such a pattern in a corpus should be linked in an ontology via a hyponymy link~~~Applications of the extracted relations to anaphora resolution are less frequent.
372 Markert and Nissim Knowledge Sources for Anaphora Resolution consuming hand-modeling~~~This includes the extraction of hyponymy and synonymy relations <REF>Hearst 1992</REF>; <TREF>Caraballo 1999</TREF>, among others as well as meronymy <REF>Berland and Charniak 1999</REF>; <REF>Meyer 2001</REF>~~~10 One approach to the extraction of instances of a particular lexical relation is the use of patterns that express lexical relations structurally explicitly in a corpus <REF>Hearst 1992</REF>; <REF>Berland and Charniak 1999</REF>; <TREF>Caraballo 1999</TREF>; <REF>Meyer 2001</REF>, and this is the approach we focus on here~~~As an example, the pattern NP 1 and other NP 2 usually expresses a hyponymy/similarity relation between the hyponym NP 1 and its hypernym NP 2 <REF>Hearst 1992</REF>, and it can therefore be postulated that two noun phrases that occur in such a pattern in a corpus should be linked in an ontology via a hyponymy link.
The literature on automated text categorization is enormous, but assumes that a set of categories has already been created, whereas the problem here is to determine the categories of interest~~~There has also been extensive work on finding synonymous terms and word associations, as well as automatic acquisition of IS-A or genus-head relations from dictionary definitions and glosses <REF>Klavans and Whitman, 2001</REF> and from free text <REF>Hearst, 1992</REF>; <TREF>Caraballo, 1999</TREF>~~~<REF>Sanderson and Croft 1999</REF> propose a method called subsumption for building a hierarchy for a set of documents retrieved for a query~~~For two terms x and y, x is said to subsume y if the following conditions hold: a2a4a3a6a5a8a7a9a11a10a13a12a15a14a17a16a18a20a19a21a2a4a3a6a9a22a7a5a23a10a25a24a27a26.
Other kinds of models that have been studied in the context of lexical acquisition are those based on lexico-syntactic patterns of the kind X, Y and other Zs, as in the phrase bluejays, robins and other birds~~~These types of models have been used for hyponym discovery <REF>Hearst, 1992</REF>; <REF>Roark and Charniak, 1998</REF>, meronym discovery <REF>Berland and Charniak, 1999</REF>, and hierarchy building <TREF>Caraballo, 1999</TREF>~~~These methods are very interesting but of limited applicability, because nouns that do not appear in known lexico-syntactic patterns cannot be learned~~~7 Conclusion All the approaches cited above focus on some aspect of the problem of lexical acquisition.
Hale, Ge, and Charniak <REF>Ge et al , 1998</REF> devised a technique to learn the gender of words~~~Caraballo <TREF>Caraballo, 1999</TREF> and Hearst <REF>Hearst, 1992</REF> created techniques to learn hypernym/hyponym relationships~~~None of these previous algorithms used extraction patterns or similar contexts to infer semantic class associations~~~Several learning algorithms have also been developed for named entity recognition eg , <REF>Collins and Singer, 1999</REF>; <REF>Cucerzan and Yarowsky, 1999</REF>.
Alternatively, some systems are based on the observation that related terms appear together in particular contexts~~~These systems extract related terms directly by recognising linguistic patterns eg X, Y and other Zs which link synonyms and hyponyms <REF>Hearst, 1992</REF>; <TREF>Caraballo, 1999</TREF>~~~Our previous work <REF>Curran and Moens, 2002</REF> has evaluated thesaurus extraction performance and ef ciency using several different context models~~~In this paper, we evaluate some existing similarity metrics and propose and motivate a new metric which outperforms the existing metrics.
These methods use clustering algorithms to group words according to their meanings in text, label the clusters using its members lexical or syntactic dependencies, and then extract an is-a relation between each cluster member and the cluster label~~~<TREF>Caraballo 1999</TREF> proposed the first attempt, which used conjunction and apposition features to build noun clusters~~~<REF>Recently, Pantel and Ravichandran 2004</REF> extended this approach by making use of all syntactic dependency features for each noun~~~The advantage of clustering approaches is that they permit algorithms to identify is-a relations that do not explicitly appear in text, however they generally fail to produce coherent clusters from fewer than 100 million words; hence they are unreliable for small corpora.
In Section 3, we show how latent semantic analysis can be used to filter potential relationships according to their semantic plausibility~~~In Section 4, we show how correctly extracted relationships can be used as seed-cases to extract several more relationships, thus improving recall; this work shares some similarities with that of <TREF>Caraballo 1999</TREF>~~~In Section 5 we show that combining the techniques of Section 3 and Section 4 improves both precision and recall~~~Section 6 demonstrates that 1Another possible view is that hyponymy should only refer to core relationships, not contingent ones so pheasant a60 bird might be accepted but pheasant a60 food might not be, because it depends on context and culture.
We use the broader subset definition because contingent relationships are an important part of world-knowledge and are therefore worth learning, and because in practice we found the distinction difficult to enforce~~~Another definition is given by <TREF>Caraballo 1999</TREF>: ~~~a word A is said to be a hypernym of a word B if native speakers of English accept the sentence B is a kind of A  linguistic tools such as lemmatization can be used to reliably put the extracted relationships into a normalized or canonical form for addition to a semantic resource~~~2 Pattern-Based Hyponymy Extraction The first major attempt to extract hyponyms from text was that of <REF>Hearst 1992</REF>, described in more detail in <REF>Hearst, 1998</REF>, who extracted relationships from the text of Groliers Encyclopedia.
4 Improving Recall Using Coordination Information One of the main challenges facing hyponymy extraction is that comparatively few of the correct relations that might be found in text are expressed overtly by the simple lexicosyntactic patterns used in Section 2, as was apparent in the results presented in that section~~~This problem has been addressed by <TREF>Caraballo 1999</TREF>, who describes a system that first builds an unlabelled hierarchy of noun clusters using agglomerative bottom-up clustering of vectors of noun coordination information~~~The leaves of this hierarchy corresponding to nouns are assigned hypernyms using Hearst-style lexicosyntactic patterns~~~Internal nodes in the hierarchy are then labelled with hypernyms of the leaves they subsume according to a vote of these subsumed leaves.
This paper suggests many possibilities for future work~~~First of all, it would be interesting to apply LSA to a system for building an entire hypernym-labelled ontology in roughly the way described in <TREF>Caraballo, 1999</TREF>, perhaps by using an LSA-weighted voting method to determine which hypernym would be used to label each node~~~We are considering how to extend our techniques to such a task~~~Also, systematic comparison of the lexicosyntactic patterns used for extraction to determine the relative productiveness and accuracy of each pattern might prove illuminating, as would comparison across different corpora to determine the impact of the topic area and medium/format of documents on the effectiveness of hyponymy extraction.
Further, we explore a problem faced by the automatic thesaurus generation community, which is that distributional similarity methods do not seem to o er any obvious way to distinguish between the semantic relations of synonymy, antonymy and hyponymy~~~Previous work on this problem <TREF>Caraballo, 1999</TREF>; <REF>Lin et al , 2003</REF> involves identifying speci c phrasal patterns within text eg, Xs and other Ys is used as evidence that X is a hyponym of Y Our work explores the connection between relative frequency, distributional generality and semantic generality with promising results~~~The rest of this paper is organised as follows~~~In Section 2, we present ten distributional similarity measures that have been proposed for use in NLP.
The sparseness of these patterns prevents this from being an effective approach to the problem we address here~~~<REF>In Caraballo 1999</REF>, we construct a hierarchy of nouns, including hypernym relations~~~However, there are several areas where that work could benefit from the research presented here~~~The hypernyms used to label the internal nodes of that hierarchy are chosen in a simple fashion; pattern-matching as in <REF>Hearst 1992</REF> is used to identify candidate hypernyms of the words dominated by a particular node, and a simple voting scheme selects the hypernyms to be used.
This project is meant to provide a tool to support other methods~~~<REF>See Caraballo 1999</REF> for a detailed description of a method to construct such a hierarchy~~~2 Previous work To the best of our knowledge, this is the first attempt to automatically rank nouns based on specificity~~~<REF>Hearst 1992</REF> found individual pairs of hypernyms and hyponyms from text using pattern-matching techniques.
our disposal, WordNet <REF>Fellbaum, 1998</REF> contains very little information that would be considered as being about attributesonly information about parts, not about qualities such as height, or even to the values of such attributes in the adjective networkand this information is still very sparse~~~On the other hand, the only work on the extraction of lexical semantic relations we are aware of has concentrated on the type of relations found in WordNet: hyponymy <REF>Hearst, 1998</REF>; <TREF>Caraballo, 1999</TREF> and meronymy <REF>Berland and Charniak, 1999</REF>; <REF>Poesio et al, 2002</REF>~~~2 The work discussed here could be perhaps best described as an example of empirical ontology: using linguistics and philosophical ideas to improve the results of empirical work on lexical / ontology acquisition, and vice versa, using findings from empirical analysis to question some of the assumptions of theoretical work on ontology and the lexicon~~~Specifically, we discuss work on the acquisition of nominal concept attributes whose goal is twofold: on the one hand, to clarify the notion of attribute and its role in lexical semantics, if any; on the other, to develop methods to acquire such information automatically eg , to supplement WordNet.
Finally, some systems extract synonyms directly without extracting and comparing contextual representations for each term~~~Instead, these systems recognise terms within certain linguistic patterns eg X, Y and other Zs which associate synonyms and hyponyms <REF>Hearst, 1992</REF>; <TREF>Caraballo, 1999</TREF>~~~Thesaurus extraction is a good task to use to experiment with scaling context spaces~~~The vectorspace model with nearest neighbour searching is simple, so we neednt worry about interactions between the contexts we select and a learning algorithm such as independence of the features.
Node numbers represent hierarchical structure of terms Contextual information has been mainly used to represent the characteristics of terms~~~<TREF>Caraballo, 1999</TREF>A <REF>Grefenstette, 1994</REF> <REF>Hearst, 1992</REF> <REF>Pereira, 1993</REF> and <REF>Sanderson, 1999</REF> used contextual information to find hyponymy relation between terms~~~<TREF>Caraballo, 1999</TREF>B also used contextual information to determine the specificity of nouns~~~Contrary, compositional information of terms has not been commonly discussed.
This paper describes a classifier that assigns semantic thesaurus categories to unknown Chinese words~~~<REF>The Caraballo 1999</REF>s system adopted the contextual information to assign nouns to their hyponyms~~~<REF>Roark and Charniak 1998</REF> used the co-occurrence of words as features to classify nouns~~~While context is clearly an important feature, this paper focuses on non-contextual features, which may play a key role for unknown words that occur only once 1 The Sinica Corpus is a balanced corpus contained five million part-of-speech words in Mandarin Chinese.
My analysis of the Sinica Corpus shows that contrary to expectation, most of unknown words in Chinese are common nouns, adjectives, and verbs rather than proper nouns~~~Other previous research has focused on features related to unknown word contexts <TREF>Caraballo 1999</TREF>; <REF>Roark and Charniak 1998</REF>~~~While context is clearly an important feature, this paper focuses on non-contextual features, which may play a key role for unknown words that occur only once and hence have limited context~~~The feature I focus on, following <REF>Ciaramita 2002</REF>, is morphological similarity to words whose semantic category is known.
The existing approaches to ontology induction include those that start from structured data, merging ontologies or database schemas <REF>Doan et al 2002</REF>~~~Other approaches use natural language data, sometimes just by analyzing the corpus <REF>Sanderson and Croft 1999</REF>, <TREF>Caraballo 1999</TREF> or by learning to expand WordNet with clusters of terms from a corpus, eg, <REF>Girju et al 2003</REF>~~~Information extraction approaches that infer labeled relations either require substantial handcreated linguistic or domain knowledge, eg, <REF>Craven and Kumlien 1999</REF> <REF>Hull and Gomez 1993</REF>, or require human-annotated training data with relation information for each domain <REF>Craven et al 1998</REF>~~~The number of relations in H that our system missed relations that were more than distance 1 away in the system ontology, is 3493.
<REF>CompuTerm 2004</REF> 3rd International Workshop on Computational Terminology 49 234 Explicit Patterns Relations This knowledge source infers specific relations between terms based on characteristic cue-phrases which relate them~~~For example, the cue-phrase such as <REF>Hearst 1992</REF> <TREF>Caraballo 1999</TREF> suggest a kind-of relation, eg, a ligand such as triethylphosphine tells us that triethylphosphene is a kind of ligand~~~Likewise, in the TREC domain, air toxics such as benzene can suggest that benzene is a kind of air toxic~~~However, since such cue-phrase patterns tend to be sparse in occurrence, we do not use them in the evaluations described below.
Corpus statistics can be used to weight the links~~~For example, based on <TREF>Caraballo 1999</TREF>, each parent of a leaf node could be viewed as a cluster label for its children, with the weight of a parent-child link being determined based on how strongly the child is associated with the cluster~~~10 The mean distance in H between terms that are distance 1 apart in M is 517, with a standard deviation of 212~~~The mean distance in M between terms which are distance 1 apart in H is 385, with a standard deviation of 169.
The goal of this work is to become able to automatically acquire hyponymy relations for a wide range of words or phrases from HTML documents on the WWW~~~We do not use particular lexicosyntactic patterns, as previous attempts have <REF>Hearst, 1992</REF>; <TREF>Caraballo, 1999</TREF>; <REF>Imasumi, 2001</REF>; <REF>Fleischman et al , 2003</REF>; <REF>Morin and Jacquemin, 2003</REF>; <REF>Ando et al , 2003</REF>~~~The frequencies of use for such lexicosyntactic patterns are relatively low, and there can be many words or phrases that do not appear in such patterns even if we look at a large number of texts~~~The effort of searching for other clues indicating hyponymy relations is thus significant.
Fully unsupervised semantic clustering eg, <REF>Lin, 1998</REF>; <REF>Lin and Pantel, 2002</REF>; <REF>Davidov and Rappoport, 2006</REF> has the disadvantage that it may or may not produce the types and granularities of semantic classes desired by a user~~~Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes eg, <TREF>Caraballo, 1999</TREF>; <REF>Cimiano and Volker, 2005</REF>; <REF>Mann, 2002</REF>, and learning semantic relations such as meronymy <REF>Berland and Charniak, 1999</REF>; <REF>Girju et al, 2003</REF>~~~Our research focuses on semantic lexicon induction, which aims to generate lists of words that belong to a given semantic class eg, lists of FISH or VEHICLE words~~~Weakly supervised learning methods for semantic lexicon generation have utilized co-occurrence statistics <REF>Riloff and Shepherd, 1997</REF>; <REF>Roark and Charniak, 1998</REF>, syntactic information <REF>Tanev and Magnini, 2006</REF>; <REF>Pantel and Ravichandran, 2004</REF>; <REF>Phillips and Riloff, 2002</REF>, lexico-syntactic contextual patterns eg, resides in <location> or moved to <location> <REF>Riloff and Jones, 1999</REF>; <REF>Thelen and Riloff, 2002</REF>, and local and global contexts <REF>Fleischman and Hovy, 2002</REF>.
Accordingly, we try to extract a hierarchical relation of words automatically and statistically~~~In previous research, ways of extracting from definition sentences in dictionaries <REF>Tsurumaru et al , 1986</REF>; <REF>Shoutsu et al , 2003</REF> or from a corpus by using patterns such as a part of, is-a, or and <REF>Berland and Charniak, 1999</REF>; <TREF>Caraballo, 1999</TREF> have been proposed~~~Also, there is a method that uses the dependence relation between words taken from a corpus <REF>Matsumoto et al , 1996</REF>~~~In contrast, we propose a method based on the inclusion relation of appearance patterns from corpora.
Roark and Charniak <REF>Roark and Charniak, 1998</REF> followed up on this work by using a parser to explicitly capture these structures~~~Caraballo <TREF>Caraballo, 1999</TREF> also exploited these syntactic structures and applied a cosine vector model to produce semantic groupings~~~In our view, these previous systems used weak syntactic models because the syntactic structures sometimes identified desirable semantic associations and sometimes did not~~~To compensate, statistical models were used to separate the meaningful semantic associations from the spurious ones.
There are inherent problems in evaluating automatic thesaurus extraction techniques, and much research assumes a gold standard that does not exist see Kilgarriff 2003 and Weeds 2003 for more discussion of this~~~A further problem for distributional similarity methods for automatic thesaurus generation is that they do not offer any obvious way to distinguish between linguistic relations such as synonymy, antonymy, and hyponymy see Caraballo 1999 and Lin et al 2003 for work on this~~~Thus, one may question 1 You shall know a word by the company it keeps<REF>Firth 1957</REF> 440 Weeds and Weir Co-occurrence Retrieval the benefit of automatically generating a thesaurus if one has access to large-scale manually constructed thesauri eg , WordNet <REF>Fellbaum 1998</REF>, Rogets <REF>Roget 1911</REF>, the Macquarie <REF>Bernard 1990</REF> and Moby 2 ~~~Automatic techniques give us the opportunity to model language change over time or across domains and genres.
The number of dependency types may be reduced in future work~~~3 The Probability Model The DAG-like nature of the dependency structures makes it difficult to apply generative modelling techniques <REF>Abney, 1997</REF>; <TREF>Johnson et al , 1999</TREF>, so we have defined a conditional model, similar to the model of <REF>Collins 1996</REF> see also the conditional model in <REF>Eisner 1996b</REF>~~~While the model of <REF>Collins 1996</REF> is technically unsound <REF>Collins, 1999</REF>, our aim at this stage is to demonstrate that accurate, efficient wide-coverage parsing is possible with CCG, even with an over-simplified statistical model~~~Future work will look at alternative models4 4The reentrancies creating the DAG-like structures are fairly limited, and moreover determined by the lexical categories.
The features are shown with hidden variables corresponding to wordspecific hidden values, such as shares1 or bought3~~~In our experiments, we made use of features such as those in Figure 2 in combination with the following four definitions of the hiddenvalue 3We also performed some experiments using the conjugate gradient descent algorithm <TREF>Johnson et al , 1999</TREF>~~~However, we did not find a significant difference between the performance of either method~~~Since stochastic gradient descent was faster and required less memory, our final experiments used the stochastic gradient method.
Mainstream approaches in statistical parsing are based on nondeterministic parsing techniques, usually employing some kind of dynamic programming, in combination with generative probabilistic models that provide an n-best ranking of the set of candidate analyses derived by the parser <REF>Collins, 1997</REF>; <REF>Collins, 1999</REF>; <REF>Charniak, 2000</REF>~~~These parsers can be enhanced by using a discriminative model, which reranks the analyses output by the parser <TREF>Johnson et al , 1999</TREF>; <REF>Collins and Duffy, 2005</REF>; <REF>Charniak and Johnson, 2005</REF>~~~Alternatively, discriminative models can be used to search the complete space of possible parses <REF>Taskar et al , 2004</REF>; <REF>McDonald et al , 2005</REF>~~~A radically different approach is to perform disambiguation deterministically, using a greedy parsing algorithm that approximates a globally optimal solution by making a sequence of locally optimal choices, guided by a classifier trained on gold standard derivations from a treebank.
A recent development in data-driven parsing is the use of discriminative training methods <REF>Riezler et al , 2002</REF>; <REF>Taskar et al , 2004</REF>; <REF>Collins and Roark, 2004</REF>; <REF>Turian and Melamed, 2006</REF>~~~One popular approach is to use a log-linear parsing model and maximise the conditional likelihood function <TREF>Johnson et al , 1999</TREF>; <REF>Riezler et al , 2002</REF>; <REF>Clark and Curran, 2004b</REF>; Malouf and van <REF>Noord, 2004</REF>; <REF>Miyao and Tsujii, 2005</REF>~~~Maximising the likelihood involves calculating feature expectations, which is computationally expensive~~~Dynamic programming DP in the form of the inside-outside algorithm can be used to calculate the expectations, if the features are sufficiently local <REF>Miyao and Tsujii, 2002</REF>; however, the memory requirements can be prohibitive, especially for automatically extracted, wide-coverage grammars.
If a complete structure is represented with a feature forest of a tractable size, the parameters can be efficiently estimated by dynamic programming~~~A series of studies on parsing with wide-coverage LFG <TREF>Johnson et al , 1999</TREF>; <REF>Riezler et al , 2000</REF>; <REF>Riezler et al , 2002</REF> have had a similar motivation to ours~~~Their models have also been based on a discriminative model to select a parsing result from all candidates given by the grammar~~~A significant difference is that we apply maximum entropy estimation for feature forests to avoid the inherent problem with estimation: the exponential explosion of parsing results given by the grammar.
Wellknown computational linguistic models such as MLE MCLE Y  yi; X  xi X  xi Y  yi; X  xi Figure 1: The MLE makes the training data yi; xi as likely as possible relative to , while the MCLE makes yi; xi as likely as possible relative to other pairs y0; xi~~~Maximum-Entropy Markov Models <REF>McCallum et al , 2000</REF> and Stochastic Unification-based Grammars <TREF>Johnson et al , 1999</TREF> are standardly estimated with conditional estimators, and it would be interesting to know whether conditional estimation affects the quality of the estimated model~~~It should be noted that in practice, the MCLE of a model with a large number of features with complex dependencies may yield far better performance than the MLE of the much smaller model that could be estimated with the same computational effort~~~Nevertheless, as this paper shows, conditional estimators can be used with other kinds of models besides MaxEnt models, and in any event it is interesting to ask whether the MLE differs from the MCLE in actual applications, and if so, how.
However, because these constraints can be non-local or context-sensitive, developing stochastic versions of UBGs and associated estimation procedures is not as straight-forward as it is for, eg, PCFGs~~~Recent work has shown how to define probability distributions over the parses of UBGs <REF>Abney, 1997</REF> and efficiently estimate and use conditional probabilities for parsing <TREF>Johnson et al , 1999</TREF>~~~Like most other practical stochastic grammar estimation procedures, this latter estimation procedure requires a parsed training corpus~~~Unfortunately, large parsed UBG corpora are not yet available.
Maximum entropy ME models, variously known as log-linear, Gibbs, exponential, and multinomial logit models, provide a general purpose machine learning technique for classification and prediction which has been successfully applied to fields as diverse as computer vision and econometrics~~~In natural language processing, recent years have seen ME techniques used for sentence boundary detection, part of speech tagging, parse selection and ambiguity resolution, and stochastic attribute-value grammars, to name just a few applications <REF>Abney, 1997</REF>; <REF>Berger et al , 1996</REF>; <REF>Ratnaparkhi, 1998</REF>; <TREF>Johnson et al , 1999</TREF>~~~A leading advantage of ME models is their flexibility: they allow stochastic rule systems to be augmented with additional syntactic, semantic, and pragmatic features~~~However, the richness of the representations is not without cost.
2 Maximum likelihood estimation Suppose we are given a probability distribution p over a set of events X which are characterized by a d dimensional feature vector function f : X Rd In addition, we have also a set of contexts W and a function Y which partitions the members of X In the case of a stochastic context-free grammar, for example, X might be the set of possible trees, the feature vectors might represent the number of times each rule applied in the derivation of each tree, W might be the set of possible strings of words, and Yw the set of trees whose yield is w2W~~~A conditional maximum entropy model qxjw for p has the parametric form <REF>Berger et al , 1996</REF>; <REF>Chi, 1998</REF>; <TREF>Johnson et al , 1999</TREF>: qxjw  exp T f x y2Yw expT f y 1 where  is a d-dimensional parameter vector and T f x is the inner product of the parameter vector and a feature vector~~~Given the parametric form of an ME model in 1, fitting an ME model to a collection of training data entails finding values for the parameter vector  which minimize the Kullback-Leibler divergence between the model q and the empirical distribution p: Dpjjq   w;x px;wlog pxjwq xjw or, equivalently, which maximize the log likelihood: L   w;x pw;xlogqxjw 2 The gradient of the log likelihood function, or the vector of its first derivatives with respect to the parameter  is: G  Ep f  Eq f  3 Since the likelihood function 2 is concave over the parameter space, it has a global maximum where the gradient is zero~~~Unfortunately, simply setting G  0 and solving for  does not yield a closed form solution, so we proceed iteratively.
23 Log-Linear CCGs We can generalize CCGs to weighted, or probabilistic, models as follows~~~Our models are similar to several other approaches <REF>Ratnaparkhi et al , 1994</REF>; <TREF>Johnson et al , 1999</TREF>; <REF>Lafferty et al , 2001</REF>; <REF>Collins, 2004</REF>; <REF>Taskar et al , 2004</REF>~~~We will write x to denote a sentence, and y to denote a CCG parse for a sentence~~~We use GENx; to refer to all possible CCG parses for x under some CCG lexicon .
Therefore there are a large number of features available that could be used by stochastic models for disambiguation~~~Other researchers have worked on extracting features useful for disambiguation from unification grammar analyses and have built log linear models aka Stochastic Unification Based Grammars <TREF>Johnson et al , 1999</TREF>; <REF>Riezler et al , 2000</REF>~~~Here we also use log linear models to estimate conditional probabilities of sentence analyses~~~Since feature selection is almost prohibitive for these models, because of high computational costs, we use PCFG models to select features for log linear models.
One is for a simple model with a relatively small number of features, and the other is for a model with a large number of features~~~The usefulness of priors in maximum entropy models is not new to this work: Gaussian prior smoothing is advocated in <REF>Chen and Rosenfeld 2000</REF>, and used in all the stochastic LFG work <TREF>Johnson et al , 1999</TREF>~~~However, until recently, its role and importance have not been widely understood~~~For example, <REF>Zhang and Oles 2001</REF> attribute the perceived limited success of logistic regression for text categorization to a lack of use of regularization.
After filtering by the generator, the remaining fstructures were weighted by the stochastic disambiguation component~~~Similar to stochastic disambiguation for constraint-based parsing <TREF>Johnson et al , 1999</TREF>; <REF>Riezler et al , 2002</REF>, an exponential aka log-linear or maximumentropy probability model on transferred structures is estimated from a set of training data~~~The data for estimation consists of pairs of original sentences y and goldstandard summarized f-structures s which were manually selected from the transfer output for each sentence~~~For training data sj,yjmj1 and a set of possible summarized structures Sy for each sentence y, the objective was to maximize a discriminative criterion, namely the conditional likelihood L of a summarized f-structure given the sentence.
If on the other hand most features are noisy but at least weakly correlated with the target, it may be reasonable to attempt to reduce noise by averaging over all of the features~~~ME estimators with L2 regularization, which have been widely used in NLP tasks eg , <REF>Chen and Rosenfeld 2000</REF>; <REF>Charniak and Johnson 2005</REF>; <TREF>Johnson et al 1999</TREF>, tend to produce models that have this property~~~In addition, the perceptron algorithm and its variants, eg, the voted or averaged perceptron, is becoming increasingly popular due to their competitive performance, simplicity in implementation and low computational cost in training eg , <REF>Collins 2002</REF>~~~While recent studies claim advantages for L1 regularization, this study is the first of which we are aware to systematically compare it to a range of estimators on a diverse set of NLP tasks.
The problems with history-based models and the desire to be able to specify features as arbitrary predicates of the entire tree have been noted before~~~In particular, previous work <REF>Ratnaparkhi, Roukos, and Ward 1994</REF>; <REF>Abney 1997</REF>; Della Pietra, <REF>Della Pietra, and Lafferty 1997</REF>; <TREF>Johnson et al 1999</TREF>; <REF>Riezler et al 2002</REF> has investigated the use of Markov random fields MRFs or log-linear models as probabilistic models with global features for parsing and other NLP tasks~~~Log-linear models are often referred to as maximum-entropy models in the NLP literature~~~Similar methods have also been proposed for machine translation <REF>Och and Ney 2002</REF> and language understanding in dialogue systems <REF>Papineni, Roukos, and Ward 1997, 1998</REF>.
Experiments on the Penn WSJ treebank show that the model achieves state-of-the-art performance, for both constituent and dependency accuracy~~~In global linear models GLMs for structured prediction, eg, <TREF>Johnson et al, 1999</TREF>; <REF>Lafferty et al, 2001</REF>; <REF>Collins, 2002</REF>; <REF>Altun et al, 2003</REF>; <REF>Taskar et al, 2004</REF>, the optimal label y for an input x is y  arg max yYx w fx,y 1 where Yx is the set of possible labels for the input x; fx,y  Rd is a feature vector that represents the pair x,y; and w is a parameter vector~~~This paper describes a GLM for natural language parsing, trained using the averaged perceptron~~~The parser we describe recovers full syntactic representations, similar to those derived by a probabilistic context-free grammar PCFG.
This section describes the relationship between our work and this previous work~~~In reranking approaches, a first-pass parser is used to enumerate a small set of candidate parses for an input sentence; the reranking model, which is a GLM, is used to select between these parses eg, <REF>Ratnaparkhi et al, 1994</REF>; <TREF>Johnson et al, 1999</TREF>; <REF>Collins, 2000</REF>; <REF>Charniak and Johnson, 2005</REF>~~~A crucial advantage of our approach is that it considers a very large set of alternatives in Yx, and can thereby avoid search errors that may be made in the first-pass parser1 Another approach that allows efficient training of GLMs is to use simpler syntactic representations, in particular dependency structures McDon1Some features used within reranking approaches may be difficult to incorporate within dynamic programming, but it is nevertheless useful to make use of GLMs in the dynamicprogramming stage of parsing~~~Our parser could, of course, be used as the first-stage parser in a reranking approach.
Moreover, property design can be carried out in a targeted way, ie properties can be designed in order to improve the disambiguation of grammatical relations that, so far, are disambiguated particularly poorly or that are of special interest for the task that the systems output is used for~~~By demonstrating that property design is the key to good log-linear models for deepsyntactic disambiguation, our work confirms that specifying the features of a SUBG stochastic unification-based grammar is as much an empirical matter as specifying the grammar itself<TREF>Johnson et al , 1999</TREF>~~~Acknowledgements The work described in this paper has been carried out in the DLFG project, which was funded by the German Research Foundation DFG~~~Furthermore, I thank the audiences at several ParGram meetings, at the Research Workshop of the Israel Science Foundation on Large-scale Grammar Development and Grammar Engineering at the University of Haifa and at the SFB 732 Opening Colloquium in Stuttgart for their important feedback on earlier versions of this work.
Note that HPSG is one of the lexicalized grammar formalisms, in which lexical entries determine the dominant syntactic structures~~~Previous studies <REF>Abney, 1997</REF>; <TREF>Johnson et al , 1999</TREF>; <REF>Riezler et al , 2000</REF>; Malouf and van <REF>Noord, 2004</REF>; <REF>Kaplan et al , 2004</REF>; <REF>Miyao and Tsujii, 2005</REF> defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model <REF>Berger et al , 1996</REF>~~~The probability that a parse result T is assigned to a given sentence w  w1,,wn is Probabilistic HPSG phpsgTw  1Z w exp parenleftBiggsummationdisplay u ufuT parenrightBigg Zw  summationdisplay Tprime exp parenleftBiggsummationdisplay u ufuTprime parenrightBigg, where u is a model parameter, fu is a feature function that represents a characteristic of parse tree T, and Zw is the sum over the set of all possible parse trees for the sentence~~~Intuitively, the probability is defined as the normalized product of the weights expu when a characteristic corresponding to fu appears in parse result T The model parameters, u, are estimated using numerical optimization methods <REF>Malouf, 2002</REF> to maximize the log-likelihood of the training data.
The main difficulty of developing parsers in these formalisms was how to model a well-defined probabilistic model for graph structures such as feature structures~~~This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model <REF>Berger et al , 1996</REF> with many features for parse trees <REF>Abney, 1997</REF>; <TREF>Johnson et al , 1999</TREF>; <REF>Riezler et al , 2000</REF>; Malouf and van <REF>Noord, 2004</REF>; <REF>Kaplan et al , 2004</REF>; <REF>Miyao and Tsujii, 2005</REF>~~~Following this discriminative approach, techniques for efficiency were investigated for estimation <REF>Geman and Johnson, 2002</REF>; <REF>Miyao and Tsujii, 2002</REF>; Malouf and van <REF>Noord, 2004</REF> and parsing <REF>Clark and Curran, 2004b</REF>; <REF>Clark and Curran, 2004a</REF>; <REF>Ninomiya et al , 2005</REF>~~~An interesting approach to the problem of parsing efficiency was using supertagging Clark and Cur60 ran, 2004b; <REF>Clark and Curran, 2004a</REF>; <REF>Wang, 2003</REF>; <REF>Wang and Harper, 2004</REF>; <REF>Nasr and Rambow, 2004</REF>; <REF>Ninomiya et al , 2006</REF>; <REF>Foth et al , 2006</REF>; <REF>Foth and Menzel, 2006</REF>, which was originally developed for lexicalized tree adjoining grammars LTAG <REF>Bangalore and Joshi, 1999</REF>.
Firstly, the rudimentary character of functional annotations in standard treebanks has hindered the direct use of such data for statistical estimation of linguistically fine-grained statistical parsing systems~~~Rather, parameter estimation for such models had to resort to unsupervised techniques <REF>Bouma et al , 2000</REF>; <REF>Riezler et al , 2000</REF>, or training corpora tailored to the specific grammars had to be created by parsing and manual disambiguation, resulting in relatively small training sets of around 1,000 sentences <TREF>Johnson et al , 1999</TREF>~~~Furthermore, the effort involved in coding broadcoverage grammars by hand has often led to the specialization of grammars to relatively small domains, thus sacrificing grammar coverage ie the percentage of sentences for which at least one analysis is found on free text~~~The approach presented in this paper is a first attempt to scale up stochastic parsing systems based on linguistically fine-grained handcoded grammars to the UPenn Wall Street Journal henceforth WSJ treebank <REF>Marcus et al , 1994</REF>.
The clustering model itself is then used to yield smoothed probabilities as values for property functions on head-argument-relation tuples of LFG parses~~~32 Discriminative Estimation Discriminative estimation techniques have recently received great attention in the statistical machine learning community and have already been applied to statistical parsing <TREF>Johnson et al , 1999</TREF>; <REF>Collins, 2000</REF>; <REF>Collins and Duffy, 2001</REF>~~~In discriminative estimation, only the conditional relation of an analysis given an example is considered relevant, whereas in maximum likelihood estimation the joint probability of the training data to best describe observations is maximized~~~Since the discriminative task is kept in mind during estimation, discriminative methods can yield improved performance.
Recent work in statistical approaches to parsing and tagging has begun to consider methods which incorporate global features of candidate structures~~~Examples of such techniques are Markov Random Fields <REF>Abney 1997</REF>; Della <REF>Pietra et al 1997</REF>; <TREF>Johnson et al 1999</TREF>, and boosting algorithms <REF>Freund et al 1998</REF>; <REF>Collins 2000</REF>; <REF>Walker et al 2001</REF>~~~One appeal of these methods is their flexibility in incorporating features into a model: essentially any features which might be useful in discriminating good from bad structures can be included~~~A second appeal of these methods is that their training criterion is often discriminative, attempting to explicitly push the score or probability of the correct structure for each training sentence above the score of competing structures.
A second appeal of these methods is that their training criterion is often discriminative, attempting to explicitly push the score or probability of the correct structure for each training sentence above the score of competing structures~~~This discriminative property is shared by the methods of <TREF>Johnson et al 1999</TREF>; <REF>Collins 2000</REF>, and also the Conditional Random Field methods of <REF>Lafferty et al 2001</REF>~~~In a previous paper <REF>Collins 2000</REF>, a boosting algorithm was used to rerank the output from an existing statistical parser, giving significant improvements in parsing accuracy on Wall Street Journal data~~~Similar boosting algorithms have been applied to natural language generation, with good results, in <REF>Walker et al 2001</REF>.
The framework is derived by the transformation from ranking problems to a margin-based classification problem in <REF>Freund et al 1998</REF>~~~It is also related to the Markov Random Field methods for parsing suggested in <TREF>Johnson et al 1999</TREF>, and the boosting methods for parsing in <REF>Collins 2000</REF>~~~We consider the following set-up: a15 Training data is a set of example input/output pairs~~~In tagging we would have training examples a147 a71 a28 a30a37a17 a28a19a148 where each a71 a28 is a sentence and each a17 a28 is the correct sequence of tags for that sentence.
Experiments of the parsing of realworld sentences can properly evaluate the effectiveness and possibility of parsing models for HPSG~~~2 Disambiguation models for HPSG Discriminative log-linear models are now becoming a de facto standard for probabilistic disambiguation models for deep parsing <TREF>Johnson et al , 1999</TREF>; <REF>Riezler et al , 2002</REF>; <REF>Geman and Johnson, 2002</REF>; <REF>Miyao and Tsujii, 2002</REF>; <REF>Clark and Curran, 2004b</REF>; <REF>Kaplan et al , 2004</REF>~~~Previous studies on probabilistic models for HPSG <REF>Toutanova and Manning, 2002</REF>; <REF>Baldridge and Osborne, 2003</REF>; Malouf and van <REF>Noord, 2004</REF> also adopted log-linear models~~~HPSG exploits feature structures to represent linguistic constraints.
Following Abney, we propose a loglinear framework which incorporates long-range dependencies as features without loss of consistency~~~Log-linear models have previously been applied to statistical parsing <TREF>Johnson et al , 1999</TREF>; <REF>Toutanova et al , 2002</REF>; <REF>Riezler et al , 2002</REF>; <REF>Osborne, 2000</REF>~~~Typically, these approaches have enumerated all possible parses for model estimation and finding the most probable parse~~~For grammars extracted from the Penn Treebank in our case CCGbank <REF>Hockenmaier, 2003</REF>, enumerating all parses is infeasible.
As expected, we observed that the regularization term increases the accuracy, especially when the training data is small; but we did not observe much difference when we used different regularization terms~~~The results we report are with the Gaussian prior regularization term described in <TREF>Johnson et al , 1999</TREF>~~~Our goal in this paper is not to build the best tagger or recognizer, but to compare different loss functions and optimization methods~~~Since we did not spend much effort on designing the most useful features, our results are slightly worse than, but comparable to the best performing models.
This method generates 50-best lists that are of substantially higher quality than previously obtainable~~~We used these parses as the input to a MaxEnt reranker <TREF>Johnson et al , 1999</TREF>; <REF>Riezler et al , 2002</REF> that selects the best parse from the set of parses for each sentence, obtaining an f-score of 910 on sentences of length 100 or less~~~We describe a reranking parser which uses a regularized MaxEnt reranker to select the best parse from the 50-best parses returned by a generative parsing model~~~The 50-best parser is a probabilistic parser that on its own produces high quality parses; the maximum probability parse trees according to the parsers model have an f-score of 0897 on section 23 of the Penn Treebank <REF>Charniak, 2000</REF>, which is still state-of-the-art.
Given set W of words and set F of feature structures, an HPSG is formulated as a tuple, G  L,R, where L  l  w,Fw  W,F  F is a set of lexical entries, and R is a set of schemata, ie, r  R is a partial function: F F  F Given a sentence, an HPSG computes a set of phrasal signs, ie, feature structures, as a result of parsing~~~Previous studies <REF>Abney, 1997</REF>; <TREF>Johnson et al , 1999</TREF>; <REF>Riezler et al , 2000</REF>; <REF>Miyao et al , 2003</REF>; Malouf and van <REF>Noord, 2004</REF>; <REF>Kaplan et al , 2004</REF>; <REF>Miyao and Tsujii, 2005</REF> defined a probabilistic model of unification-based grammars as a log-linear model or maximum entropy model <REF>Berger et al , 1996</REF>~~~The probability of parse result T assigned to given sentence w  w1,,,wn is pTw  1Z w exp parenleftBiggsummationdisplay i ifiT parenrightBigg Zw  summationdisplay T prime exp parenleftBiggsummationdisplay i ifiTprime parenrightBigg, where i is a model parameter, and fi is a feature function that represents a characteristic of parse tree T Intuitively, the probability is defined as the normalized product of the weights expi when a characteristic corresponding to fi appears in parse result T Model parameters i are estimated using numer104 ical optimization methods <REF>Malouf, 2002</REF> so as to maximize the log-likelihood of the training data~~~However, the above model cannot be easily estimated because the estimation requires the computation of pTw for all parse candidates assigned to sentence w Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences.
The second instantiation finds the borders of phrases beginning and end and then pairs them in an optimal way into different phrases~~~These problems formulations are similar to those studied in <REF>Ramshaw and Marcus, 1995</REF> and <TREF>Church, 1988</TREF>; <REF>Argamon et al , 1998</REF>, respectively~~~The experimental results presented using the SNoW based approach compare favorably with previously published results, both for NPs and SV phrases~~~A s important, we present a few experiments that shed light on some of the issues involved in using learned predictors that interact to produce the desired inference.
Our earlier example would be marked for base NPs as: I wont to California last May~~~This approach has been studied in <TREF>Church, 1988</TREF>; <REF>Argamon et al , 1998</REF>~~~331 Architecture The architecture used for the Open/Close predictors is shown in Figure 2~~~Two SNoW predictors are used, one to predict if the word currently in consideration is the first in the phrase an open bracket, and the other to predict if it is the last a close bracket.
A lot of the work on shallow parsing over the past years has concentrated on manual construction of rules~~~The observation that shallow syntactic information can be extracted using local information by examining the pattern itself, its nearby context and the local part-of-speech information has motivated the use of learning methods to recognize these patterns <TREF>Church, 1988</TREF>; <REF>Ramshaw and Marcus, 1995</REF>; <REF>Argamon et al , 1998</REF>; <REF>Cardie and Pierce, 1998</REF>~~~ Research supported by NSF grants IIS-9801638 and SBR-9873450~~~t Research supported by NSF grant CCR-9502540.
HMMs have long been central in speech recognition <REF>Rabiner, 1989</REF>~~~Their application to partof-speech tagging <TREF>Church, 1988</TREF>; <REF>DeRose, 1988</REF> kicked off the era of statistical NLP, and they have found additional NLP applications to phrase chunking, text segmentation, word-sense disambiguation, and information extraction~~~The algorithm is also important to teach for pedagogical reasons, as the entry point to a family of EM algorithms for unsupervised parameter estimation~~~Indeed, it is an instructive special case of 1 the inside-outside algorithm for estimation of probabilistic context-free grammars; 2 belief propagation for training singly-connected Bayesian networks and junction trees <REF>Pearl, 1988</REF>; <REF>Lauritzen, 1995</REF>; 3 algorithms for learning alignment models such as weighted edit distance; 4 general finitestate parameter estimation <REF>Eisner, 2002</REF>.
Subsequent analysis suggested that half the errors could be removed with only a little additional work, suggesting that over 90 performance is achievable~~~In a related test, we explored the bracketings produced by Churchs PARTS program <TREF>Church, 1988</TREF>~~~We extracted 200 sentences of WSJ text by taking every tenth sentence from a collection of manually corrected parse trees data from the TREEBANK Project at the University of Pennsylvania~~~We evaluated the NP bracketings in these 200 sentences by hand, and tried to classify the errors.
Given that each token has on the average more than 2 possible tags, the procedural description above is very inefficient for all but very short sentences~~~However, the observation that our constraints are localized to a window of a small number of tokens say at most 5 tokens in a sequence, suggests a more efficient scheme originally used by <TREF>Church 1988</TREF>~~~Assume our constraint windows are allowed to look at a window of at most size k sequential parses~~~Let us take the first k tokens of a sentence and generate all possible paths of k arcs spanning k  1 nodes, and apply all constraints to these short paths.
Part-of-speech tagging is one of the preliminary steps in many natural language processing systems in which the proper part-of-speech tag of the tokens comprising the sentences are disambiguated using either statistical or symbolic local contextual information~~~Tagging systems have used either a statistical approach where a large corpora is employed to train a probabilistic model which then is used to tag unseen text, eg , <TREF>Church 1988</TREF>, Cutting et al~~~1992, <REF>DeRose 1988</REF>, or a constraint-based approach which employs a large number of hand-crafted linguistic constraints that are used to eliminate impossible sequences or morphological parses for a given word in a given context, recently most prominently exemplified by the Constraint Grammar work <REF>Karlsson et al , 1995</REF>; <REF>Voutilainen, 1995b</REF>; <REF>Voutilainen et al , 1992</REF>; <REF>Voutilainen and Tapanainen, 1993</REF>~~~BriU 1992; 1994; 1995 has presented a transformationbased learning approach.
 Right close double quote 231 Automated Stage~~~During the early stages of the Penn Treebank project, the initial automatic POS assignment was provided by PARTS <TREF>Church 1988</TREF>, a stochastic algorithm developed at ATT Bell Labs~~~PARTS uses a modified version of the Brown Corpus tagset close to our own and assigns POS tags with an error rate of 3-5~~~The output of PARTS was automatically tokenized 8 and the tags assigned by PARTS were automatically mapped onto the Penn Treebank tagset.
Various methods for POS tagging have been proposed in recent years~~~For simplicity, we adapted the method proposed by <REF>Churchl1988</REF> to tag the definition sentence~~~In the second stage, we select the label which is associated with word lists most similar to the definition as the result~~~We sum up the above descriptions and outline the procedure for labeling a dictionary sense.
The experimental results show the proposed method significantly outperforms both hand-crafted and conventional statistical methods~~~The last few years have seen the great success of stochastic part-of-speech POS taggers <TREF>Church, 1988</TREF>: <REF>Kupiec, 1992</REF>; Charniak et M , 1993; <REF>Brill, 1992</REF>; <REF>Nagata, 1994</REF>~~~The stochastic approach generally attains 94 to 96 accuracy and replaces the labor-intensive compilation of linguistics rules by using an automated learning algorithm~~~However, 1NTT is an abbreviation of Nippon Telegraph and Telephone Corporation.
1~~~A recent trend in natural language processing has been toward a greater emphasis on statistical approaches, beginning with the success of statistical part-of-speech tagging programs <TREF>Church 1988</TREF>, and continuing with other work using statistical part-of-speech tagging programs, such as BBN PLUM <REF>Weischedel et al 1993</REF> and NYU Proteus <REF>Grishman and Sterling 1993</REF>~~~More recently, statistical methods have been applied to domain-specific semantic parsing <REF>Miller et al 1994</REF>, and to the more difficult problem of wide-coverage syntactic parsing <REF>Magerman 1995</REF>~~~Nevertheless, most natural language systems remain primarily rule based, and even systems that do use statistical techniques, such as ATT Chronus <REF>Levin and Pieraccini 1995</REF>, continue to require a significant rule based component.
The training is performed on ambiguity classes and not on individual word tokens~~~<REF>Kallgren 1996</REF> gives a more covering description of how XPOST is used on the Swedish material and also sketches the major differences between this algorithm and some others used for tagging, such as PARTS <TREF>Church 1988</TREF> and VOLSUNGA <REF>DeRose 1988</REF>~~~A characteristic tbature of the SUC is its high number of different tags~~~The number of part-ofspeech tags used in the SUC is 21.
Disambiguation of capitalized words is usually handled by POS taggers, which treat capitalized words in the same way as other categories, that is, by accounting for the immediate syntactic context and using estimates collected from a training corpus~~~<REF>As Church 1988</REF> rightly pointed out, however, Proper nouns and capitalized words are particularly problematic: some capitalized words are proper nouns and some are not~~~Estimates from the Brown Corpus can be misleading~~~For example, the capitalized word Acts is found twice in the Brown Corpus, both times as a proper noun in a title.
The morphological ambiguity will differ depending on the level of tagging used in each case, as shown in table 2~~~There are two kinds of methods for morphological disambiguation: on one hand, statistical methods need little effort and obtain very good results <TREF>Church, 1988</TREF>; Cutting el al, 1992, at least when applied to English, but when we try to apply them to Basque we encounter additional problems; on the other hand, some rule-based systems <REF>Brill, 1992</REF>; <REF>Voutilainen et al, 1992</REF> are at least as good as statistical systems and are better adapted to free-order languages and agglutinative languages~~~So, we 381 have selected one of each group: Constraint Grammar formalism <REF>Karlsson et al, 1995</REF> and the HMM based TATOO tagger <REF>Armstrong et al, 1995</REF>, which has been designed to be applied it to the output of a morphological analyser and the tagset can be switched easily without changing the input text~~~second  third 70 ks I M M MCG MCG Figure 1-Initial ambiguity3.
We are tagging this material with a much simpler tagset than used by previous projects, as discussed at the Oct 1989 DARPA Workshop~~~The material is first processed using Ken Churchs tagger <TREF>Church 1988</TREF>, which labels it as if it were Brown Corpus material, and then is mapped to our tagset by a SEDscript~~~Because of fundamental differences in tagging strategy between the Penn Treebank Project and the Brown project, the resulting mapping is about 9 inaccurate, given the tagging guidelines of the Penn Treebank project as given in 40 pages of explicit tagging guidelines~~~This material is then hand-corrected by our annotators; the result is consistent within annotators to about 3 cf.
H90-1055:17~~~Deducing Linguistic Structure from the Statistics of Large Corpora Eric Brill David Magerman Mitchell Marcus Beatrice Santorini Department of Computer and Information Science University of Pennsylvania Philadelphia, PA 19104 1 Introduction Within the last two years, approaches using both stochastic and symbolic techniques have proved adequate to deduce lexical ambiguity resolution rules with less than 3-4 error rate, when trained on moderate sized 500K word corpora of English text eg <TREF>Church, 1988</TREF>; <REF>Hindle, 1989</REF>~~~The success of these techniques suggests that much of the grammatical structure of language may be derived automatically through distributional analysis, an approach attempted and abandoned in the 1950s~~~We describe here two experiments to see how far purely distributional techniques can be pushed to automatically provide both a set of part of speech tags for English, and a grammatical analysis of free English text.
This line of research was motivated by a series of successful applications of mutual information statistics to other problems in natural language processing~~~In the last decade, research in speech recognition <REF>Jelinek 1985</REF>, noun classification <REF>Hindle 1988</REF>, predicate argument relations <REF>Church  Hanks 1989</REF>, and other areas have shown that mutual information statistics provide a wealth of information for solving these problems~~~22 Mutual Information Statistics The mutual information statistic <REF>Fano 1961</REF> is a measure of the interdependence of two signals in a message~~~It is a function of the probabilities of the two events: Mz, u  log u xzPvy In this paper, the events x and y will be part-of-speech n-grams instead of single parts-of-speech, as in some earlier work.
Each of these three steps will be described below~~~3 The preprocessing stage The noun phrase parser identifies simple non-recursive noun phrases such as DetAdjN or NN The method used for this process involves an algorithm of the type described in <TREF>Church 1988</TREF> which was trained on a manually marked part of our corpus~~~The module is thus geared to the particular type of second language text the checker needs to deal with~~~The resulting information is passed on to a preprocessing module consisting of a number of automata groups.
Measures/NNS of/IN manufacturing/VBG activity/NN fell/VBD more/RBR than/IN the/DT overall/JJ measures/NNS/~~~Figure 1: An example sentence with baseNP brackets A number of researchers have dealt with the problem of baseNP identification <TREF>Church 1988</TREF>; <REF>Bourigault 1992</REF>; <REF>Voutilainen 1993</REF>; <REF>Justeson  Katz 1995</REF>~~~Recently some researchers have made experiments with the same test corpus extracted from the 20 th section of the Penn Treebank Wall Street Journal Penn Treebank~~~<REF>Ramshaw  Markus 1998</REF> applied transformbased error-driven algorithm <REF>Brill 1995</REF> to learn a set of transformation rules, and using those rules to locally updates the bracket positions.
4 Concluding remarks Though there can be little doubt that the ruling system of bakeoffs actively encourages a degree of oneupmanship, our paper and our software are not offered in a competitive spirit~~~As we said at the out211 set, we dont necessarily believe HunPos to be in any way better than TnT, and certainly the main ideas have been pioneered by <REF>DeRose 1988</REF>, <TREF>Church 1988</TREF>, and others long before this generation of HMM work~~~But to improve the results beyond what a basic HMM can achieve one needs to tune the system, and progress can only be made if the experiments are end to end replicable~~~There is no doubt many other systems could be tweaked further and improve on our results what matters is that anybody could now also tweak HunPos without any restriction to improve the state of the art.
In this paper we describe experiments we performed to ascertain how well humans, given an annotated training set, can generate rules for base noun phrase chunking~~~Much previous work has been done on this problem and many different methods have been used: Churchs PARTS 1988 program uses a Markov model; <REF>Bourigault 1992</REF> uses heuristics along with a grammar; Voutilainens NP<REF>Tool 1993</REF> uses a lexicon combined with a constraint grammar; <REF>Juteson and Katz 1995</REF> use repeated phrases; <REF>Veenstra 1998</REF>, Argamon, Dagan  <REF>Krymolowski1998</REF> and Daelemaus, van den <REF>Bosch  Zavrel 1999</REF> use memory-based systems; Ramshaw  Marcus In Press and <REF>Cardie  Pierce 1998</REF> use rule-based systems~~~2 Learning Base Noun Phrases by Machine We used the base noun phrase system of Ramshaw and Marcus RM as the machine learning system with which to compare the human learners~~~It is difficult to compare different machine learning approaches to base NP annotation, since different definitions of base NP are used in many of the papers, but the RM system is the best of those that have been tested on the Penn Treebank.
The suggestion which we want to explore is that the association revealed by textual distribution whether its source is a complementation relation, a modification relation, or something else gives us information needed to resolve the prepositional attachment~~~Discovering Lexical Association in Text A 13 million word sample of Associated Press new stories from 1989 were automatically parsed by the Fidditch parser <REF>Hindle 1983</REF>, using Churchs part of speech analyzer as a preprocessor <TREF>Church 1988</TREF>~~~From the syntactic analysis provided by the parser for each sentence, we extracted a table containing all the heads of all noun phrases~~~For each noun phrase head, we recorded the following preposition if any occurred ignoring whether or not the parser attached the preposition to the noun phrase, and the preceding verb if the noun phrase was the object of that verb.
We also looked at whether the token constituted an entire intermediate or intonational phrase--possibly with other cue phrases--or not, and what each tokens position within its intermediate phrase and larger intonational phrase was--first-inphrase again, including tokens preceded only by other cue phrases as well as tokens that were absolutely first in intermediate phrase, last, or other~~~We also examined each items part of speech, using Churchs 1988 part-of-speech tagger~~~Finally, we investigated orthographic features of the transcript that might be associated with a discourse/sentential distinction, such as immediately preceding and succeeding punctuation and paragraph boundaries~~~In both the syntactic and orthographic analyses we were particularly interested in discovering how successful nonprosodic features that might be obtained automatically from a text would be in differentiating discourse from sentential uses.
While the use of orthographic and part-of-speech data represents only a fractional improvement over orthographic information alone, it is possible that, since the latter is not subject to transcriber idiosyncracy, such an approach may prove more reliable than orthography alone in the general case~~~And, for text-to-speech applications, it 7 The parbof-speech tagger employed in this analysis <TREF>Church 1988</TREF> uses a subset of the part-of-speech tags used in Francis and Kuera 1982~~~We have translated these for Table 12~~~Note that intensifier corresponds to QU in Francis and Kuera 1982.
Research on corpus-based natural language learning and processing is rapidly accelerating following the introduction of large on-line corpora, faster computers, and cheap storage devices~~~Recent work involves novel ways to employ annotated corpus in part of speech tagging <TREF>Church 1988</TREF> <REF>Derose 1988</REF> and the application of mutual information statistics on the corpora to uncover lexical information <REF>Church 1989</REF>~~~The goal of the research is the construction of robust and portable natural language processing systems~~~The wide range of topics available on the Internet calls for an easily adaptable information extraction system for different domains.
Part-of-speech tagging is to assign the correct tag to each word in the context of the sentence~~~here are three main approaches in tagging problem: rule-based approach Klein and Simmons 13; <REF>Brodda 1982</REF>; <REF>Paulussen and Martin 1992</REF>; <REF>Brill et al 1990</REF>, statistical approach Church :1988; <REF>Merialdo 1994</REF>; <REF>Foster 1991</REF>; <REF>Weischedel et al 1993</REF>; <REF>Kupiec 1992</REF> and connectionist approach <REF>Benello et al 1989</REF>; <REF>Nakanmra et al 1989</REF>~~~In these approaches, statistical approach has the following advantages :  a theoretical framework is provided  automatic learning facility is provided  the probabilities provide a straightforward way to disambiguate Many information sources must be combined to solve tagging problem with statistical approach~~~It is a significant assumption that tire correct tag can generally be chosen from Ihe local context.
In this study, we measure performance solely through the cross-entropy of test data; it would be interesting to see how these cross-entropy differences correlate with performance in end applications such as speech recognition~~~In addition, it would be interesting to see whether these results extend to fields other than language modeling where smoothing is used, such as prepositional phrase attachment <REF>Collins and Brooks, 1995</REF>, part-of-speech tagging <TREF>Church, 1988</TREF>, and stochastic parsing <REF>Magerman, 1994</REF>~~~317 Acknowledgements The authors would like to thank Stuart Shieber and the anonymous reviewers for their comments on previous versions of this paper~~~We would also like to thank William Gale and Geoffrey Sampson for supplying us with code for Good-Turing frequency estimation without tears.
In addition, we introduce two novel smoothing techniques, one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique, both of which outperform existing methods~~~Smoothing is a technique essential in the construction of n-gram language models, a staple in speech recognition <REF>Bahl, Jelinek, and Mercer, 1983</REF> as well as many other domains <TREF>Church, 1988</TREF>; <REF>Brown et al , 1990</REF>; <REF>Kernighan, Church, and Gale, 1990</REF>~~~A language model is a probability distribution over strings Ps that attempts to reflect the frequency with which each string s occurs as a sentence in natural text~~~Language models are used in speech recognition to resolve acoustically ambiguous utterances.
In practice, computational limitations do not allow the enumeration of all possible assignments for long sentences, and smoothing is required for infrequent events~~~This is described in more detail in the original publication <TREF>Church, 1988</TREF>~~~Although more sophisticated algorithms for unsupervised learning which can be trained on plain text instead on manually tagged corpora are well established see eg <REF>Merialdo, 1994</REF>, we decided not to use them~~~The main reason is that with large tag sets, the sparse-data-problem can become so severe that unsupervised training easily ends up in local minima, which can lead to poor results without any indication to the user.
<REF>Lezius, Rapp  Wettler 1996</REF> give an overview on some German tagging projects~~~Although we considered a number of algorithms, we decided to use the trigram algorithm described by <TREF>Church 1988</TREF> for tagging~~~It is simple, fast, robust, and among the statistical taggers still more or less unsurpassed in terms of accuracy~~~Conceptually, the Church-algorithm works as follows: For each sentence of a text, it generates all possible assignments of part-of-speech tags to words.
A more detailed discussion of LTAGs with an example and some of the key properties of elementary trees is presented in Appendix A 4~~~Supertags Part-of-speech disambiguation techniques POS taggers <TREF>Church 1988</TREF>; <REF>Weischedel et al 1993</REF>; <REF>Brill 1993</REF> are often used prior to parsing to eliminate or substantially reduce the part-of-speech ambiguity~~~The POS taggers are all local in the sense that they use information from a limited context in deciding which tags to choose for each word~~~As is well known, these taggers are quite successful.
We tested the performance of the unigram model on the previously discussed two sets of data~~~The words are first assigned standard parts of speech using a conventional tagger <TREF>Church 1988</TREF> and then are assigned supertags according to the unigram model~~~A word in a sentence is considered correctly supertagged if it is assigned the same supertag as it is associated with in the correct parse of the sentence~~~The results of these experiments are tabulated in Table 4.
Some external mechanism is assumed to consistently or stochastically annotate substrings as phrases 2~~~Our goal is to come up with a mechanism that, given an input string, identifies the phrases in this string, this is a fundamental task with applications in natural language <TREF>Church, 1988</TREF>; <REF>Ramshaw and Marcus, 1995</REF>; <REF>Mufioz et al , 1999</REF>; <REF>Cardie and Pierce, 1998</REF>~~~The identification mechanism works by using classifiers that process the input string and recognize in the input string local signals which  This research is supported by NSF grants IIS-9801638, SBR-9873450 and IIS-9984168~~~1Full version is in <REF>Punyakanok and Roth, 2000</REF>.
Much research has been donc Oll knowledge acquisition fiom large-scalc annotated corpora as a rich source of linguistic knowledge~~~Mtior works done to create English POS taggers henceforth, taggers, for example, include <TREF>Church 1988</TREF>, <REF>Kupicc 1992</REF>, <REF>Brill 1992</REF>and <REF>Voutilaincn et al 1992</REF>~~~The problem with this framework, however, is that such reliable corpora are hardly awdlable duc to a huge amount of the labor-intensive work required~~~In case of the acquisition of non-core knowledge, such as specific, lexically or dolnain dependent knowledge, preparation of annotated corpora becomes more serious problem.
2~~~PART:OF-SPEECH TAG SEQUENCE GRAMMAR We utilised the ANLT metagrammatical formalism to develop a feature-based, declarative description of part-of-speech PoS label sequences see eg <TREF>Church, 1988</TREF> for English~~~This grammar compiles into a DCG-like grammar of approximately 400 rules~~~It has been designed to enumerate possible valencies for predicates verbs, adjectives and nouns by including separate rules for each pattern of possible complementation in English.
But dictionaries of technical terminology have many one-word terms~~~Simplex or complex NPs eg , <TREF>Church 1988</TREF>; <REF>Hindle and Rooth 1991</REF>; <REF>Wacholder 1998</REF> identify simplex or base NPs  NPs which do not have any component NPs -at least in part because this bypasses the need to solve the quite difficult attachment problem, ie, to determine which simpler NPs should be combined to output a more complex NP~~~But if people find complex NPs more useful than simpler ones, it is important to focus on improvement of techniques to reliably identify more complex terms~~~Semantic and syntactic terms variants.
As described in Section 3, each indicator has a unique value for each verb, which corresponds to the frequency of the aspectual marker with the verb except verb frequency, which is an absolute measure over the corpus~~~6 Similar baselines for comparison have been used for many classification problems <REF>Duda and Hart 1973</REF>, eg, part-of-speech tagging <TREF>Church 1988</TREF>; <REF>Allen 1995</REF>~~~611 Computational Linguistics Volume 26, Number 4 The second and third columns of Table 9 show the average value for each indicator over stative and event clauses, as measured over the training examples which exclude be and have~~~These values are computed solely over the 739 training cases in order to avoid biasing the classification experiments in the sections below, which are evaluated over the unseen test cases.
To assign capitalized unknown words the category proper noun seems a good heuristic, but may not always work~~~As argued in <TREF>Church 1988</TREF>, who proposes a more elaborated heuristic, <REF>Dermatas and Kokkinakis 1995</REF> proposed a simple probabilistic approach to unknown-word guessing: HCRC, Language Technology Group, University of Edinburgh, 2 Buccleuch Place, Edinburgh EH8 9LW, Scotland, UK~~~Q 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 3 Table 1 The most frequent open-class tags from the Penn tag set~~~Tag Meaning Example Tag Meaning Example NN common noun table NNS noun plural tables NNP proper noun John NNPS plural proper noun Vikings JJ adjective green RB adverb naturally VB verb base form take VBD verb past took VBG gerund taking VBN past participle taken VBZ verb present, 3d person takes VBP verb, present, non-3d take the probability that an unknown word has a particular Pos-tag is estimated from the probability distribution of hapax words words that occur only once in the previously seen texts.
Although our primary goal was not to compare the taggers themselves but rather their performance with the guessing components, we attribute the difference in their performance to the fact that Brills tagger uses the information about the most likely tag for a word whereas the HMM tagger did not have this information and instead used the priors for a set of POS-tags ambiguity class~~~When we removed from the lexicon all the hapax words and, following the recommendation of <TREF>Church 1988</TREF>, all the capitalized words with frequency less than 20, we obtained some 51,522 unknown word-tokens 25,359 wordtypes out of more than a million word-tokens in the Brown Corpus~~~We tagged the fifteen subcorpora of the Brown Corpus by the four combinations of the taggers and the guessers using the lexicon of 22,260 word-types~~~42 Results of the Experiment Table 4 displays the tagging results on the unknown words obtained by the four different combinations of taggers and guessers.
In the 329 rule-based approach a large number of hand crafted rules are used to select the correct morphological parse or POS tag of a given word in a given context <REF>Karlsson et al , 1995</REF>; Oflazer and Tcurrency1ur, 1997~~~In the statistical approach a hand tagged corpus is used to train a probabilistic model which is then used to select the best tags in unseen text <TREF>Church, 1988</TREF>; Hakkani-Tcurrency1ur et al , 2002~~~Examples of statistical and machine learning approaches that have been used for tagging include transformation based learning <REF>Brill, 1995</REF>, memory based learning <REF>Daelemans et al , 1996</REF>, and maximum entropy models <REF>Ratnaparkhi, 1996</REF>~~~It is also possible to train statistical models using unlabeled data with the expectation maximization algorithm <REF>Cutting et al , 1992</REF>.
After presenting our results and evaluation, we discuss simulation experiments that show how our method performs under different conditions of sparseness of data~~~3 Data Collection For our experiments, we use the 21 million word 1987 Wall Street Journal corpus 4, automatically annotated with part-of-speech tags using the PARTS tagger <TREF>Church, 1988</TREF>~~~In order to verify our hypothesis about the orientations of conjoined adjectives, and also to train and evaluate our subsequent algorithms, we need a 3Certain words inflected with negative affixes such as inor un- tend to be mostly negative, but this rule applies only to a fraction of the negative words~~~Furthermore, there are words so inflected which have positive orientation, eg, independent and unbiased.
For example, ve O, dead can be used tkr emphasis, and relet am relet as in her lhce became redder and redder can be used to indicate a progression of coloring, qb distinguish between truly gradablc adjectives and non-gradable adjectives in these exceptional contexts, we have developed a trainable log-linear statistical model that lakes into account tile number of times an adiective has been observed in a form or context indicating gradability relative to the number of limes it has been seen in non-gradable contexts~~~We use a shallow parser to retrieve from a large corpus tagged for part-of-speech with Churchs PARTS tagger <TREF>Church, 1988</TREF> all adjectives and their modifiers~~~Although the most common use of an adverb modifying an adjective is to function as an intensilier or diminisher <REF>Quirk et al , 1985</REF>, p 445, adverbs can also add to tile semantic content of the adjectival phrase instead of providing a grading effect eg , immediately available, politically vuhmrable, or function as cmphasizers, adding to the force o1 tile base adjective and not lo its degree eg , virtually impossible; compare re O, impossible~~~Therefore, we compiled by hand a list of 73 adverbs and noun phrases such as a little, exceedingly, somewhat, and veo that are fiequently used as grading moditicrs.
More precisely, assume that the word wh occurs in a sentence W  wlWkwn, and that w is a word we are considering substituting for it, yielding sentence W I Word w is then preferred over wk iff PW > PW, where PW and PW are the probabilities of sentences W and W f respectively~~~1 We calculate PW using the tag sequence of W as an intermediate quantity, and summing, over all possible tag sequences, the probability of the sentence with that tagging; that is: PW   PW, T T where T is a tag sequence for sentence W The above probabilities are estimated as is traditionally done in trigram-based part-of-speech tagging <TREF>Church, 1988</TREF>; <REF>DeRose, 1988</REF>: PW,T  PWITPT  1  HPwiti HPt, lt,2t,l2 i i where T  tltn, and Ptitl-2ti-1 is the prob ability of seeing a part-of-speech tag tl given the two preceding part-of-speech tags ti-2 and ti-1~~~Equations 1 and 2 will also be used to tag sentences W and W  with their most likely part-of-speech sequences~~~This will allow us to determine the tag that 1To enable fair comparisons between sequences of different length as when considering maybe and may be, we actually compare the per-word geometric mean of the sentence probabilities.
Its recall is very high 997 of all words receive the correct morphological analysis, but this system leaves 3-7 of all words ambiguous, trading precision for recall~~~157 ena or the linguists abstraction capabilities eg knowledge about what is relevant in the context, they tend to reach a 95-97 accuracy in the analysis of several languages, in particular English <REF>Marshall 1983</REF>; Black et aL 1992; <TREF>Church 1988</TREF>; <REF>Cutting et al 1992</REF>; de <REF>Marcken 1990</REF>; <REF>DeRose 1988</REF>; <REF>Hindle 1989</REF>; <REF>Merialdo 1994</REF>; <REF>Weischedel et al 1993</REF>; <REF>Brill 1992</REF>; <REF>Samuelsson 1994</REF>; Eineborg and Gambick 1994, etc~~~Interestingly, no significant improvement beyond the 97 barrier by means of purely data-driven systems has been reported so far~~~In terms of the accuracy of known systems, the data-driven approach seems then to provide the best model of part-of-speech distribution.
Figure 1: Base NP Examples base noun phrases with initial determiners and modifiers removed: <REF>Justeson  Katz 1995</REF> look for repeated phrases; <REF>Bourigault 1992</REF> uses a handcrafted noun phrase grammar in conjunction with heuristics for finding maximal length noun phrases; Voutilainens NP<REF>Tool 1993</REF> uses a handcrafted lexicon and constraint grammar to find terminological noun phrases that include phrase-final prepositional phrases~~~Churchs PARTS program 1988, on the other hand, uses a probabilistic model automatically trained on the Brown corpus to locate core noun phrases as well as to assign parts of speech~~~More recently, Ramshaw  Marcus In press apply transformation-based learning <REF>Brill, 1995</REF> to the problem~~~Unfortunately, it is difficult to directly compare approaches.
1993 call the core noun phrase, that is a noun phrase with no modification to the right of the head~~~Several approaches provide similar output based on statistics <TREF>Church 1988</TREF>, <REF>Zhai 1997</REF>, for example, a finite-state machine <REF>AitMokhtar and Chanod 1997</REF>, or a hybrid approach combining statistics and linguistic rules <REF>Voutilainen and Padro 1997</REF>~~~The SPECIALIST parser is based on the notion of barrier words <REF>Tersmette et al 1988</REF>, which indicate boundaries between phrases~~~After lexical look-up and resolution of category label ambiguity by the Xerox tagger, complementizers, conjunctions, modals, prepositions, and verbs are marked as boundaries.
Finally, memory-based learning is adopted to further improve the performance of the chunk tagger~~~The idea of using statistics for chunking goes back to <TREF>Church1988</TREF>, who used corpus frequencies to determine the boundaries of simple nonrecursive noun phrases~~~Skut and <REF>Brants1998</REF> modified Churchs approach in a way permitting efficient and reliable recognition of structures of limited depth and encoded the structure in such a way that it can be recognised by a Viterbi tagger~~~Our approach follows Skut and Brants way by employing HMM-based tagging method to model the chunking process.
Thus, Examples 3-5 illustrate how the syntactic context of a word can help determine its meaning~~~22 Motivation from previous work 221 Parsing In recent years, the success of statistical parsing techniques can be attributed to several factors, such as the increasing size of computing machinery to accommodate larger models, the availability of resources such as the Penn Treebank <REF>Marcus et al , 1993</REF> and the success of machine learning techniques for lowerlevel NLP problems, such as part-of-speech tagging <TREF>Church, 1988</TREF>; <REF>Brill, 1995</REF>, and PPattachment <REF>Brill and Resnik, 1994</REF>; <REF>Collins and Brooks, 1995</REF>~~~However, perhaps even more significant has been the lexicalization of the grammar formalisms being probabilistically modeled: crucially, all the recent, successful statistical parsers have in some way made use of bilexical dependencies~~~This includes both the parsers that attach probabilities to parser moves <REF>Magerman, 1995</REF>; <REF>Ratnaparkhi, 1997</REF>, but also those of the lexicalized PCFG variety <REF>Collins, 1997</REF>; <REF>Charniak, 1997</REF>.
to  NP only  18 billion  PP in  NP September  While earlier work in this direction concentrated on manual construction of rules, most of the recent work has been motivated by the observation that shallow syntactic information can be extracted using local information  by examining the pattern itself, its nearby context and the local part-of-speech information~~~Thus, over the past few years, along with advances in the use of learning and statistical methods for acquisition of full parsers <REF>Collins, 1997</REF>; <REF>Charniak, 1997a</REF>; <REF>Charniak, 1997b</REF>; <REF>Ratnaparkhi, 1997</REF>, significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns  syntactic phrases or words that participate in a syntactic relationship <TREF>Church, 1988</TREF>; <REF>Ramshaw and Marcus, 1995</REF>; <REF>Argamon et al , 1998</REF>; <REF>Cardie and Pierce, 1998</REF>; <REF>Munoz et al , 1999</REF>; <REF>Punyakanok and Roth, 2001</REF>; <REF>Buchholz et al , 1999</REF>; Tjong <REF>Kim Sang and Buchholz, 2000</REF>~~~Research on shallow parsing was inspired by psycholinguistics arguments <REF>Gee and Grosjean, 1983</REF> that suggest that in many scenarios eg , conversational full parsing is not a realistic strategy for sentence processing and analysis, and was further motivated by several arguments from a natural language engineering viewpoint~~~First, it has been noted that in many natural language applications it is sufficient to use shallow parsing information; information such as noun phrases NPs and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization <REF>Grishman, 1995</REF>; <REF>Appelt et al , 1993</REF>.
33 Dialogue Act Decoding The HMM representation allows us to use efficient dynamic programming algorithms to compute relevant aspects of the model, such as  the most probable DA sequence the Viterbi algorithm  the posterior probability of various DAs for a given utterance, after considering all the evidence the forward-backward algorithm The Viterbi algorithm for HMMs <REF>Viterbi 1967</REF> finds the globally most probable state sequence~~~When applied to a discourse model with locally decomposable likelihoods and Markovian discourse grammar, it will therefore find precisely the DA 348 Stolcke et al Dialogue Act Modeling sequence with the highest posterior probability: U  argmaxPUIE  4 u The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition <REF>Bahl, Jelinek, and Mercer 1983</REF> and tagging <TREF>Church 1988</TREF>~~~It maximizes the probability of getting the entire DA sequence correct, but it does not necessarily find the DA sequence that has the most DA labels correct <REF>Dermatas and Kokkinakis 1995</REF>~~~To minimize the total number of utterance labeling errors, we need to maximize the probability of getting each DA label correct individually, ie, we need to maximize PUilE for each i  1  n We can compute the per-utterance posterior DA probabilities by summing: PuE  E PUIE 5 U: Uiu where the summation is over all sequences U whose ith element matches the label in question.
Various methods for POS tagging have been proposed in recent years~~~For simplicity, we adopted the method proposed by <TREF>Church 1988</TREF> to tag definition sentences~~~Experiments indicated an average error rate for tagging of less than 10~~~Tagging errors have limited negative impact, because words in the LLOCE are organized primarily according to topic, not part of speech.
I think we I need l to uh I I I need l r I m I et I r m I Algorithm Our algorithm for labeling potential repair patterns encodes the assumption that speech repairs can be processed one at a time~~~The algorithm runs in lockstep with a part-of-speech tagger <TREF>Church, 1988</TREF>, which is used for deciding possible word replacements~~~Words are fed in one at a time~~~The detection clues are checked first.
7About half of the difference between the detection recall rate and the correction recall rate is due to abridged repairs being misclassified as modification repairs~~~299 Part-of-speech tagging is the process of assigning to a word the category that is most probable given the sentential context <TREF>Church, 1988</TREF>~~~The sentential context is typically approximated by only a set number of previous categories, usually one or two~~~Good part-of-speech results can be obtained using only the preceding category <REF>Weischedel et al , 1993</REF>, which is what we will be using.
Recent research advances may lead to the development of viable book indexing methods for Chinese books~~~These include the availability of efficient and high precision word segmentation methods for Chinese text <REF>Chang et al , 1991</REF>; <REF>Sproat and Shih, 1990</REF>; <REF>Wang et al , 1990</REF>, the availability of statistical analysis of a Chinese corpus <REF>Liu et al , 1975</REF> and large-scale electronic Chinese dictionaries with partof-speech information <REF>Chang et al , 1988</REF>; BDC, 1992, the corpus-based statistical part-of-speech tagger <TREF>Church, 1988</TREF>; <REF>DeRose, 1988</REF>; <REF>Beale, 1988</REF>, as well as phrasal and clausal analyzers <TREF>Church 1988</TREF>; <REF>Ejerhed 1990</REF> 2~~~Problem description As being pointed out in <REF>Salton, 1988</REF>, back-of-book indexes may consist of more than one word that are derived from a noun phrase~~~Given the text of a book, an indexing system, must perform some kind of phrasal and statistical analysis in order to produce a list of candidate indexes and their occurrence statistics in order to generate indexes as shown in Figure 1 which is an excerpt from the reconstruction of indexes of a book on transformational grammar for Mandarin Chinese <REF>Tang, 1977</REF>.
NPtool parse Apparent correct parse less time less time the other hand the other hand many advantages many advantages bnary addressing binary addressing and and instruction formats instruction formats a purely binary computer a purely binary computer Table 1: Apparent errors made by Voutilainens N<REF>Ptool Kupiec 1993</REF> also briefly mentions the use of finite state NP recognizers for both English and French to prepare the input for a program that identified the correspondences between NPs in bilingual corpora, but he does not directly discuss their performance~~~Using statistical methods, Churchs Parts program 1988, in addition to identifying parts of speech, also inserted brackets identifying core NPs~~~These brackets were placed using a statistical model trained on Brown corpus material in which NP brackets had been inserted semi-automatically~~~In the small test sample shown, this system achieved 98 recall for correct brackets.
In the small test sample shown, this system achieved 98 recall for correct brackets~~~At about the same time, <REF>Ejerhed 1988</REF>, working with Church, performed comparisons between finite state methods and Churchs stochastic models for identifying both non-recursive clauses and non-recursive NPs in English text~~~In those comparisons, the stochastic methods outperformed the hand built finite-state models, with claimed accuracies of 935 clauses and 986 NPs for the statistical models compared to to 87 clauses and 978 NPs for the finite-state methods~~~Running Churchs program on test material, however, reveals that the definition of NP embodied in Churchs program is quite simplified in that it does not include, for example, structures or words conjoined within NP by either explicit conjunctions like and and or, or implicitly by commas.
They consider a range of input variables, including textderived information such as detailed POS labels and syntactic constituent structure, and in some experiments, acoustic information~~~POS labels were given by Churchs tagger <TREF>Church 1988</TREF> and syntactic constituents by Hindles parser <REF>Hindle 1987</REF>~~~The acoustic information previous boundary location, pitch accent location, and phrase duration, which was based on hand-labeled prosodic markers, did not improve performance but resulted in a much smaller tree for prediction~~~All of these approaches have influenced the model proposed here.
These tools can then be used by other systems to address more complex tasks~~~For example, previous work has addressed low-level tasks such as tagging a free-style corpus with part-of-speech information <TREF>Church 1988</TREF>, aligning a bilingual corpus <REF>Gale and Church 1991b</REF>; <REF>Brown, Lai, and Mercer 1991</REF>, and producing a list of collocations <REF>Smadja 1993</REF>~~~While each of these tools is based on simple statistics and tackles elementary tasks, we have demonstrated with our work on Champollion that by combining them, one can reach new levels of complexity in the automatic treatment of natural languages~~~Acknowledgments This work was supported jointly by the Advanced Research Projects Agency and the Office of Naval Research under grant N00014-89-J-1782, by the Office of Naval Research under grant N00014-95-1-0745, by the National Science Foundation under grant GER-90-24069, and by the New York State Science and Technology Foundation under grants NYSSTF-CAT91-053 and NYSSTF-CAT94-013.
The parser will eventually disambiguate all the descriptions and pick one per object, for a given reading of the sentence~~~This is what the parser is expected to do for disambiguating the standard POS, unless a separate POS disambiguation module is used <TREF>Church, 1988</TREF>~~~Many parsers, including XTAG, use such a module alhd a POS tagger~~~LTAGs present a novel opportunity to reduce the amount of disambiguation done by the parser.
Much recent research in the field of natural language processing NLP has focused on an empirical, corpus-based approach <REF>Church and Mercer, 1993</REF>~~~The high accuracy achieved by a corpus-based approach to part-of-speech tagging and noun phrase parsing, as demonstrated by <TREF>Church, 1988</TREF>, has inspired similar approaches to other problems in natural language processing, including syntactic parsing and word sense disambiguation WSD~~~The availability of large quantities of part-ofspeech tagged and syntactically parsed sentences like the Penn Treebank corpus <REF>Marcus, Santorini, and Marcinkiewicz, 1993</REF> has contributed greatly to the development of robust, broad coverage partof-speech taggers and syntactic parsers~~~The Penn Treebank corpus contains a sufficient number of partof-speech tagged and syntactically parsed sentences to serve as adequate training material for building broad coverage part-of-speech taggers and parsers.
One method of handling large vocabularies is simply increasing the size of the lexicon~~~Research efforts at IBM Chodorow, et al 1988; Neff, et al 1989, Bell Labs Church, et al 1989, New Mexico State University <REF>Wilks 1987</REF>, and elsewhere have used mechanical processing of on-line dictionaries to infer at least minimal syntactic and semantic information from dictionary definitions~~~However, even assuming a very large lexicon already exists, it can never be complete~~~Systems aiming for coverage of unrestricted language in broad domains must continually deal with new words and novel word senses.
In order to make these improvements, we need access to word-class information Pos information <REF>Johansson et al 1986</REF>; <REF>Black, Garside, and Leech 1993</REF> or semantic information <REF>Beckwith et al 1991</REF>, which is usually obtained in three main ways: Firstly, we can use corpora that have been manually tagged by linguistically informed experts <REF>Derouault and Merialdo 1986</REF>~~~Secondly, we can construct automatic part-ofspeech taggers and process untagged corpora <REF>Kupiec 1992</REF>; <REF>Black, Garside, and Leech 1993</REF>; this method boasts a high degree of accuracy, although often the construction of the automatic tagger involves a bootstrapping process based on a core corpus which has been manually tagged <TREF>Church 1988</TREF>~~~The third option is to derive a fully automatic word-classification system from untagged corpora~~~Some advantages of this last approach include its applicability to any natural language for which some corpus exists, independent of the degree of development of its grammar, and its parsimonious commitment to the machinery of modern linguistics.
If an external resource is used in the form of a morphological analyzer MA, this will almost always overgenerate, yielding false ambiguity~~~But even if the MA is tight, a considerable proportion of ambiguous tokens will come from legitimate but rare analyses of frequent types <TREF>Church, 1988</TREF>~~~For example the word nem, can mean both not and gender, so both ADV and NOUN are valid analyses, but the adverbial reading is about five orders of magnitude more frequent than the noun reading, 12596 vs 4 tokens in the 1 m word manually annotated Szeged Korpusz <REF>Csendes et al , 2004</REF>~~~Thus the difficulty of the task is better measured by the average information required for disambiguating a token.
Almost all approaches to sequence problems such as partof-speech tagging take a unidirectional approach to conditioning inference along the sequence~~~Regardless of whether one is using HMMs, maximum entropy conditional sequence models, or other techniques like decision trees, most systems work in one direction through the sequence normally left to right, but occasionally right to left, eg, <TREF>Church 1988</TREF>~~~There are a few exceptions, such as Brills transformation-based learning <REF>Brill, 1995</REF>, but most of the best known and most successful approaches of recent years have been unidirectional~~~Most sequence models can be seen as chaining together the scores or decisions from successive local models to form a global model for an entire sequence.
Part-of-speech tagging is required to detect new terms formed through conversion~~~This is quite feasible using statistical taggers like those of <REF>Garside 1987</REF>, <TREF>Church 1988</TREF> or <REF>Foster 1991</REF> which achieve performance upwards of 97 on unrestricted text~~~Terms formed through semantic drift are the wolves in sheeps clothing stealing through terminological pastures~~~They are well enough conceMcd to allude at times even the human reader and no automatic term-recognition system has attempted to distinguish such terms, despite the prevalence ofpolysemy in such fields as the social sciences Riggs, 1993 and the importance for purposes of terminological standardization that deviant usage be tracked.
4~~~Partof-Speech Tagging Part-of-speech tagging is the process of assigning to a word the category that is most probable given the sentential context <TREF>Church, 1988</TREF>~~~The sentential context is typically approximated by only a set number of previous categories, usually one or two~~~Since the context is limited, we are making the Markov assumption, that the next transition depends only on the input, which is the word that we the following changes: 1 we -parated Weposifiom from subordinating conjunctions; 2 we separated uses of to as a preposition from in me as part of a to-infinilive; 3 rather than classify verbs by tense, we classified them into four groups, conjugations of be, conjugations of have, verbs that are followed by a to-infinitive, and verbs that are followed immediately by another verb.
On sentences with <40 words, the former model performs at 69 precision, 75 recall, and the latter at 77 precision and 78 recall~~~Ever since the success of HMMs application to part-of-speech tagging in <TREF>Church, 1988</TREF>, machine learning approaches to natural language processing have steadily become more widespread~~~This increase has of course been due to their proven efficacy in many tasks, but also to their engineering effiCacy~~~Many machine learning approaches let the data speak for itself data ipsa loquuntur, as it were, allowing the modeler to focus on what features of the data are important, rather than on the complicated interaction of such features, as had often been the case with hand-crafted NLP systems.
In conclusion, we argue strongly that the use of an independent morphological dictionary is the preferred choice to more annotated data under such circumstances~~~1 Full Morphological Tagging English Part of Speech POS tagging has been widely described in the recent past, starting with the <TREF>Church, 1988</TREF> paper, followed by numerous others using various methods: neural networks <REF>Julian Benello and Anderson, 1989</REF>, HMM tagging <REF>Merialdo, 1992</REF>, decision trees <REF>Schmid, 1994</REF>, transformation-based error-driven learning <REF>Brill, 1995</REF>, and maximum entropy <REF>Ratnaparkhi, 1996</REF>, to select just a few~~~However different the methods were, English dominated in these tests~~~Unfortunately, English is a morphologically impoverished language: there are no complicated agreement relations, word order variation is minireal, and the morphological categories are either extremely simple -s for plural of nouns, for example, or almost nonexistent cases expressed by inflection, for example with not too many exceptions and irregularities.
The suggestion which we want to explore is that the association revealed by textual distribution whether its source is a complementation relation, a modification relation, or something else gives us information needed to resolve the prepositional attachment~~~Discovering Lexical Association in Text A 13 million word sample of Associated Press new stories from 1989 were automatically parsed by the Fidditch parser <REF>Hindle 1983</REF>, using Churchs part of speech analyzer as a preprocessor <TREF>Church 1988</TREF>~~~From the syntactic analysis provided by the parser for each sentence, we extracted a table containiffg all the heads of all noun phrases~~~For each noun phrase head, we recorded the following preposition if any occurred ignoring whether or not the parser attached the preposition to the noun phrase, and the preceding verb if the noun phrase was the object of that verb.
460 of this data has an impact on the tagging accuracy of both the HMM itself and the derived transducer~~~The training of the HMM can be done on either a tagged or untagged corpus, and is not a topic of this paper since it is exhaustively described in the literature <REF>Bahl and Mercer, 1976</REF>; <TREF>Church, 1988</TREF>~~~An HMM can be identically represented by a weighted FST in a straightforward way~~~We are, however, interested in non-weighted transducers.
In the early nineties, <REF>Abney 1991</REF> proposed to approach parsing by starting with finding related chunks of words~~~By then, <TREF>Church 1988</TREF> had already reported on recognition of base noun phrases with statistical methods~~~<REF>Ramshaw and Marcus 1995</REF> approached chunking by using a machine learning method~~~Their work has inspired many others to study the application of learning methods to noun phrase chunking 5.
Realizing the difficulties o1 complete parsing, many researches turned to explore the partial parsing techniques~~~<TREF>Church1988</TREF> proposed a silnple stochastic technique for lecognizing the non-recursive base noun phrases in English~~~;outilaimen1993 designed an English noun phrase recognition tool -- NPTbol~~~<REF>Abney1997</REF> applied both rule-based and statistics-based approaches for parsing chunks in English.
Several approaches have been proposed to construct automatic taggers~~~Most work on statistical methods has used n-gram models or Hidden Markov Model-based taggers eg <TREF>Church, 1988</TREF>; <REF>DeRose, 1988</REF>; <REF>Cutting et al 1992</REF>; <REF>Merialdo, 1994</REF>, etc~~~In 14 these approaches, a tag sequence is chosen for a sentence that maximizes the product of lexical and contextual probabilities as estimated from a tagged corpus~~~In rule-based approaches, words are assigned a tag based on a set of rules and a lexicon.
Categorical ambiguity, however, is of a different kind and is resolved in a different way~~~For the purposes of the present paper, it will be assumed that only content words are at issue, and that the syntactic category of all content words in the text that is under study can be determined automatically <TREF>Church, 1988</TREF>; <REF>DeRose, 1988</REF>~~~The problem is simply to decide which sense of a content word--noun, verb, adjective, or adverb---is appropriate in a given linguistic context~~~It will also be assumed that sense resolution for individual words can be accomplished on the basis of information about the irnrnediate linguistic context.
The program can be trained even with a relatively small amount of treebank data; then it can be J used for parsing unrestricted pre-tagged text~~~As far as coverage is concerned, our parser can handle recursive structures, which is an advantage compared to simpler techniques such as that described by <TREF>Church 1988</TREF>~~~On the other hand, the Markov assumption underlying our approach means that only strictly local dependencies are recognised~~~For full parsing, one would probably need non-local contextual information, such as the long-range trigrams in Link Grammar Della <REF>Pietra et al , 1994</REF>.
Regardless of whether or not abstractions such as phrases occur in the model, most of the relevant information is contained directly in the sequence of words and part-of-speech tags to be processed~~~An archetypal representative of this approach is the method described by <TREF>Church 1988</TREF>, who used corpus frequencies to determine the boundaries of simple non, recursive NPs~~~For each pair of part-of-speech tags ti, tj, the probability of an NP boundary   or  occurring between ti and tj is computed~~~On the basis of these context probabilities, the program inserts the symbols  and  into sequences of part-of-speech tags.
However, it does undeniably reduce confusion with respect to the proper noun category~~~Some well-known previous efforts <TREF>Church 1988</TREF>; de <REF>Marcken 1990</REF> have dealt with unknown words using various heuristics~~~For instance, Churchs program PARTS has a prepass prior to applying the tri-tag probability model that predicts proper nouns based on capitalization~~~The new aspects of our work are 1 incorporating the treatment of unknown words uniformly within the probability model, 2 approximating the component probabilities for unknowns directly from the training data, and 3 measuring the contribution of the tri-tag model, of the ending, and of capitalization.
Purely rule-based techniques seemed too brittle for dealing with the variety of constructions, the long sentences averaging 29 words per sentence, and the degree of unexpected input~~~Statistical models based on local information eg , <REF>DeRose 1988</REF>; <TREF>Church 1988</TREF> might operate effectively in spite of sentence length and unexpected input~~~To see whether our four hypotheses in italics above effectively addressed the four concerns above, we chose to test the hypotheses on two well-known problems: ambiguity both at the structural level and at the part-of-speech level and inferring syntactic and semantic information about unknown words~~~Guided by the past success of probabilistic models in speech processing, we have integrated probabilistic models into our language processing systems.
We report in Section 2 on our experiments on the assignment of part of speech to words in text~~~The effectiveness of such models is well known <REF>DeRose 1988</REF>; <TREF>Church 1988</TREF>; <REF>Kupiec 1989</REF>; <REF>Jelinek 1985</REF>, and they are currently in use in parsers eg de <REF>Marcken 1990</REF>~~~Our work is an incremental improvement on these models in three ways: 1 Much less training data than theoretically required proved adequate; 2 we integrated a probabilistic model of word features to handle unknown words uniformly within the probabilistic model and measured its contribution; and 3 we have applied the forward-backward algorithm to accurately compute the most likely tag set~~~In Section 3, we demonstrate that probability models can improve the performance of knowledge-based syntactic and semantic processing in dealing with structural ambiguity and with unknown words.
1 is a typical example of the ambiguities encountered in a running text: little POS ambiguity, but a lot of gender, number and case ambiguity columns 3 to 5~~~485 3 The Model Instead of employing the source-channel paradigm for tagging more or less explicitly present eg in <REF>Merialdo, 1992</REF>, <TREF>Church, 1988</TREF>, Hajji, Hladk, 1997 used in the past notwithstanding some exceptions, such as Maximum Entropy and rule-based taggers, we are using here a direct approach to modeling, for which we have chosen an exponential probabilistic model~~~Such model when predicting an event 5 y E Y in a context x has the general form PAC,e YIX  exp-in----1 Aifi y, x Zx 3 where fi Y, x is the set of size n of binary-valued yes/no features of the event value being predicted and its context, hi is a weigth in the exponential sense of the feature fi, and the normalization factor Zx is defined naturally as zx  exp z x 4 yEY i----1 ,Ve use a separate model for each ambiguity class AC which actually appeared in the training data of each of the 13 morphological categories 6~~~The final PAC Yix distribution is further smoothed using unigram distributions on subtags again, separately for each category.
This was expanded upon by <REF>Gale et al , 1992</REF>, and in a class-based variant by <REF>Yarowsky, 1992</REF>~~~Decision trees <REF>Brown, 1991</REF> have been usefully applied to word-sense ambiguities, and HMM part-of-speech taggers <REF>Jelinek 1985</REF>, <TREF>Church 1988</TREF>, <REF>Merialdo 1990</REF> have addressed the syntactic ambiguities presented here~~~<REF>Hearst 1991</REF> presented an effective approach to modeling local contextual evidence, while <REF>Resnik 1993</REF> gave a classic treatment of the use of word classes in selectional constraints~~~An algorithm for combining syntactic and semantic evidence in lexical ambiguity resolution has been realized in <REF>Chang et al , 1992</REF>.
Preprocessing the Corpus with a Part of Speech Tagger Phrasal verbs involving the preposition to raise an interesting problem because of the possible confusion with the infinitive marker to~~~We have found that if we first tag every word in the corpus with a part of speech using a method such as <TREF>Church 1988</TREF> or <REF>DeRose 1988</REF>, and then measure associations between tagged words, we can identify interesting contrasts between verbs associated with a following preposition toin and verbs associated with a following infinitive marker toto~~~Part of speech notation is borrowed from <REF>Francis and Kucera 1982</REF>; m  preposition; to  infinitive marker; vb  bare verb; vbg  verb  ing; vbd  verb  ed; vbz  verb  s; vbn  verb  en~~~The score identifies quite a number of verbs associated in an interesting way with to; restricting our attention to pairs with a score of 30 or more, there are 768 verbs associated with the preposition toin and 551 verbs with the infinitive marker toto.
In the partof-speech tagging field, the disambiguation of capitalized words is treated similarly to the disambiguation of common words~~~However, as <TREF>Church 1988</TREF> rightly pointed out Proper nouns and capitalized words are particularly problematic: some capitalized words are proper nouns and some are not~~~Estimates from the Brown Corpus can be misleading~~~For example, the capitalized word Acts is found twice in Brown Corpus, both times as a proper noun in a title.
Automatic morphological disambiguation is an important component in higher level analysis of natural language text corpora~~~There has been a large number of studies in tagging and morphological disambiguation using various techniques such as statistical techniques, eg, <TREF>Church, 1988</TREF>; <REF>Cutting et al , 1992</REF>; <REF>DeRose, 1988</REF>, constraint-based techniques <REF>Karlsson et al , 1995</REF>; <REF>Voutilainen, 1995b</REF>; Voutilainen, Heikkil/i, and <REF>Anttila, 1992</REF>; <REF>Voutilainen and Tapanainen, 1993</REF>; <REF>Oflazer and KuruSz, 1994</REF>; <REF>Oflazer and Till 1996</REF> and transformation-based techniques <REF>Brilt, 1992</REF>; <REF>Brill, 1994</REF>; <REF>Brill, 1995</REF>~~~This paper presents a novel approach to constraint based morphological disambiguation which relieves the rule developer from worrying about conflicting rule ordering requirements~~~The approach depends on assigning votes to constraints according to their complexity and specificity, and then letting constraints cast votes on matching parses of a given lexical item.
Given that each token has on the average more than 2 possible tags, the procedural description above is very inefficient for M1 but very short sentences~~~However, the observation that our constraints are localized to a window of a small number of tokens say at most 5 tokens in a sequence, suggests a more efficient scheme originally used by <TREF>Church 1988</TREF>~~~Assume our constraint windows are allowed to look at a window of at most size k sequential parses~~~Let us take the first k tokens of a sentence and generate all possible paths of k arcs spanning k  1 nodes, and apply all constraints to these short paths.
Part-of-speech tagging is one of the preliminary steps in many natural language processing systems in which the proper part-of-speech tag of the tokens comprising the sentences are disambiguated using either statistical or symbolic local contextual information~~~Tagging systems have used either a statistical approach where a large corpora is employed to train a probabilistic model which then is used to tag unseen text, eg, <TREF>Church 1988</TREF>, Cutting et al~~~1992, DeR,ose 1988, or a constraint-based approach which employs a large number of hand-crafted linguistic constraints that are used to eliminate impossible sequences or morphological parses for a given word in a given context, recently most prominently exemplified by the Constraint Grammar work <REF>Karlsson et al, 1995</REF>; <REF>Voutilainen, 1995b</REF>; <REF>Voutilainen et al, 1992</REF>; <REF>Voutilainen and Tapanainen, 1993</REF>~~~Brill 1992; 1994; 1995 has presented a transformationbased learning approach.
Models G and C For model G, I induced a simple grammar from the training corpus~~~I used Ken Churchs tagger <TREF>Church 1988</TREF> to 234 assign part-of-speech probabilities to words~~~The grammar contains a rule x ---> T for every Treebank chunk x t in the training corpus~~~x is the syntactic category of the chunk, and y is the part-of-speech sequence assigned to the words of the chunk.
The corpus used in our first experiment was derived from newswire text automatically parsed by 183 Hindles parser Fidditch <REF>Hindle, 1993</REF>~~~More recently, we have constructed similar tables with the help of a statistical part-of-speech tagger <TREF>Church, 1988</TREF> and of tools for regular expression pattern matching on tagged corpora <REF>Yarowsky, 1992</REF>~~~We have not yet compared the accuracy and coverage of the two methods, or what systematic biases they might introduce, although we took care to filter out certain systematic errors, for instance the misparsing of the subject of a complement clause as the direct object of a main verb for report verbs like say~~~We will consider here only the problem of classifying nouns according to their distribution as direct objects of verbs; the converse problem is formally similar.
1; they closely replicate Brills results 1993b, page 96, allowing for the fact that his tests used more templates, including templates like if one of the three previous tags is A~~~Brills results demonstrate that this approach can outperform the Hidden Markov Model approaches that are frequently used for part-of-speech tagging <REF>Jelinek, 1985</REF>; <TREF>Church, 1988</TREF>; <REF>DeRose, 1988</REF>; <REF>Cutting et al , 1992</REF>; <REF>Weischedel et al , 1993</REF>, as well as showing promise for other applications~~~The resulting model, encoded as a list of rules, is also typically more compact and for some purposes more easily interpretable than a table of HMM probabilities~~~An Incremental Algorithm It is worthwhile noting first that it is possible in some circumstances to significantly speed up the straightforward algorithm described above.
We show the method to be efficient and easily adaptable to different text genres, including single-case texts~~~Labeling of sentence boundaries is a necessary prerequisite for many natural language processing NLP tasks, including part-of-speech tagging <TREF>Church, 1988</TREF>, <REF>Cutting et al , 1991</REF>, and sentence alignment <REF>Gale and Church, 1993</REF>, Kay and R<REF>Sscheisen, 1993</REF>~~~End-of-sentence punctuation marks are ambiguous; for example, a period can denote an abbreviation, the end of a sentence, or both, as shown in the examples below: 1 The group included Dr JM Freeman and T Boone Pickens Jr~~~2 This issue crosses party lines and crosses philosophical lines.
21 Assignment of Descriptors The first stage of the process is lexical analysis, which breaks the input text a stream of characters into tokens~~~Our implementation uses a slightlymodified version of the tokenizer from the PARTS part-of-speech tagger <TREF>Church, 1988</TREF> for this task~~~A token can be a sequence of alphabetic characters, a sequence of digits numbers containing periods acting as decimal points are considered a single token, or a single non-alphanumeric character~~~A lookup module then uses a lexicon with part-of-speech tags for each token.
The list of candidate terms contains both multi-word noun phrases and single words~~~The multi-word terms match a small set of syntactic patterns defined by regular expressions and are found by searching a version of the document tagged with parts of speech <TREF>Church, 1988</TREF>~~~The set of syntactic patterns is considered as a parameter and can be adopted to a specific domain by the user~~~Currently our patterns match only sequences of nouns, which seem to yield the best hit rate in our environment.
This current practice is very laborious and runs the risk of missing many important terms~~~Termight uses a part of speech tagger <TREF>Church, 1988</TREF> to identify a list of candidate terms which is then filtered by a manual pass~~~We have found, however, that the manual pass dominates the cost of the monolingual task, and consequently, we have tried to design an interactive user interface see Figure 1 that minimizes the burden on the expert terminologist~~~The terminologist is presented with a list of candidate terms, and corrects the list with a minimum number of key strokes.
We rewrite this term as follows: PrW1,ND1,N N  I-IPrWiDilWl,ilDl,il i1 N  l-I PrWilWl,i-lDl,i PrDilWl,i-lDl,i-1 i1 7 Equation 7 involves two probability distributions that need to be estimated~~~These are the same distributions that are needed by previous POS-based language models Equation 5 and POS taggers <TREF>Church 1988</TREF>; <REF>Charniak et al 1993</REF>~~~However, these approaches simplify the context so that the lexical probability is just conditioned on the POS category of the word, and the POS probability is conditioned on just the preceding POS tags, which leads to the following two approximations~~~PrWiIWl,ilDl,i  PrWilDi 8 PrDiIWulDl,il  PrDiIDul 9 However, to successfully incorporate POS information, we need to account for the full richness of the probability distributions, as will be demonstrated in Section 344.
Figure 1: Base NP Examples base noun phrases with initial determiners and modifiers removed: <REF>Justeson  Katz 1995</REF> look for repeated phrases; <REF>Bourigault 1992</REF> uses a handcrafted noun phrase grammar in conjunction with heuristics for finding maximal length noun phrases; Voutilainens NP<REF>Tool 1993</REF> uses a handcrafted lexicon and constraint grammar to find terminological noun phrases that include phrase-final prepositional phrases~~~Churchs PARTS program 1988, on the other hand, uses a probabilistic model automatically trained on the Brown corpus to locate core noun phrases as well as to assign parts of speech~~~More recently, Ramshaw  Marcus In press apply transformation-based learning <REF>Brill, 1995</REF> to the problem~~~Unfortunately, it is difficult to directly compare approaches.
The recursive expansion of the tree stops if either the information gained by consulting further fv-pairs or the frequencies upon which the calculus is based are smaller than defined thresholds~~~4 TAGGING ALGORITHM Starting point for the implementation of a feature structure tagger was a second-0rdcr-IIMM tagger trigrams based on a modified version of the Viterbi algorithm <REF>Viterbi, 1967</REF>; <TREF>Church, 1988</TREF> which we had earlier implemented in C Kempe,1994~~~There we replaced the function which estimated the contextual probability of a tag state transition probability hy dividing a trigram frequency by a bigram frequency eq~~~3 with a flmction which accomplished this calculus either using PF1Ls in the above-described way eqs 6, 7 or by consulting a decision tree fig.
1992 circumvent this problem by training their taggers on untagged data using tile Itaum-Welch algorithm also know as the forward-backward algorithm~~~They report rates of correctly tagged words which are comparable to that presented by <TREF>Church 1988</TREF> and <REF>Kempe 1993</REF>~~~A third and rather new approach is tagging with artificial neural networks~~~In the area of speech recognition neural networks have been used for a decade r, ow.
For example, if we choose to create a pseudo-word out of the words make and take, we would change the test data like this: make plans  make, take plans take action  make, take action The method being tested must choose between the two words that make up the pseudo-word~~~32 Data We used a statistical part-of-speech tagger <TREF>Church, 1988</TREF> and pattern matching and concordancing tools due to David Yarowsky to identify transitive main verbs and head nouns of the corresponding direct objects in 44 million words of 1988 Associated Press newswire~~~We selected the noun-verb pairs for the 1000 most frequent nouns in the corpus~~~These pairs are undoubtedly somewhat noisy given the errors inherent in the part-of-speech tagging and pattern matching.
151 Computational Linguistics Volume 19, Number 1 <REF>Garside and Leech 1987</REF> have been shown to reach 95-99 performance on free-style text~~~We preprocessed the corpus with a stochastic part-of-speech tagger developed at Bell Laboratories by Ken Church <TREF>Church 1988</TREF>~~~9 In the rest of this section, we describe the algorithm used for the first stage of Xtract in some detail~~~We assume that the corpus is preprocessed by a part of speech tagger and we note wi a collocate of w if the two words appear in a common sentence within a distance of 5 words.
Such techniques have various applications~~~Speech recognition <REF>Bahl, Jelinek, and Mercer 1983</REF> and text compression eg , <REF>Bell, Witten, and Cleary 1989</REF>; <REF>Guazzo 1980</REF> have been of long-standing interest, and some new applications are currently being investigated, such as machine translation <REF>Brown et al 1988</REF>, spelling correction <REF>Mays, Damerau, and Mercer 1990</REF>; <REF>Church and Gale 1990</REF>, parsing <REF>Debili 1982</REF>; <REF>Hindle and Rooth 1990</REF>~~~As pointed out by <REF>Bell, Witten, and Cleary 1989</REF>, these applications fall under two research paradigms: statistical approaches and lexical approaches~~~In the statistical approach, language is modeled as a stochastic process and the corpus is used to estimate probabilities.
On one hand, according to the linguistic approach, experts encode handcrafted rules or constraints based on abstractions derived from language paradigms usually with the aid of corpora <REF>Green and Rubin, 1971</REF>; <REF>Voutilainen 1995</REF>~~~On the other hand, according to the data-driven approach, a frequency-based language model is acquired from corpora and has the forms of ngrams <TREF>Church, 1988</TREF>; <REF>Cutting et al , 1992</REF>, rules <REF>Hindle, 1989</REF>; <REF>Brill, 1995</REF>, decision trees <REF>Cardie, 1994</REF>; <REF>Daelemans et al , 1996</REF> or neural networks <REF>Schmid, 1994</REF>~~~In order to increase their robusmess, most POS taggers include a guesser, which tries to extract the POS of words not present in the lexicon~~~As a common strategy, POS guessers examine the endings of unknown words <REF>Cutting et al 1992</REF> along with their capitalization, or consider the distribution of unknown words over specific parts-of-speech Weischedel et aL, 1993.
In practice, computational limitations do not allow the enumeration of all possible assignments for long sentences, and smoothing is required for infrequent events~~~This is described in more detail in the original publication <TREF>Church, 1988</TREF>~~~Although more sophisticated algorithms for unsupervised learning which can be trained on plain text instead on manually tagged corpora are well established see eg <REF>Merialdo, 1994</REF>, we decided not to use them~~~The main reason is that with large tag sets, the sparse-data-problem can become so severe that unsupervised training easily ends up in local minima, which call lead to poor results without any indication to the user.
<REF>Lezius, Rapp  Wettler 1996</REF> give an overview on some German tagging projects~~~Although we considered a number of algorithms, we decided to use the trigram algorithm described by <TREF>Church 1988</TREF> for tagging~~~It is simple, fast, robust, and among the statistical taggers still more or less unsurpassed in terms of accuracy~~~Conceptually, the Church-algorithm works as follows: For each sentence of a text, it generates all possible assignments of part-of-speech tags to words.
2~~~Part-of-Speech Tagging The prototype source-channel application in natural language is part-of-speech tagging <TREF>Church 1988</TREF>~~~We review it here for purposes of comparison with machine translation~~~Source strings comprise sequences of part-of-speech tags like noun, verb, etc A simple source model assigns a probability to a tag sequence tl tm based on the probabilities of the tag pairs inside it.
Indeed, recent increased interest in the problem of disambiguating lexical category in English has led to significant progress in developing effective programs for assigning lexical category in unrestricted text~~~The most successful and comprehensive of these are based on probabilistic modeling of category sequence and word category <REF>Church 1987</REF>; <REF>Garside, Leech and Sampson 1987</REF>; <REF>DeRose 1988</REF>~~~These stochastic methods show impressive performance: Church reports a success rate of 95 to 99, and shows a sample text with an error rate of less than one percent~~~What may seem particularly surprising is that these methods succeed essentially without reference to syntactic structure; purely surface lexical patterns are involved.
Part-of-Speech Tagging Many of the very same methods are being applied to problems in natural language processing by many of the very same researchers~~~As a result, the empirical approach has been adopted by almost all contemporary part-of-speech programs: <REF>Bahl and Mercer 1976</REF>, <REF>Leech, Garside, and Atwell 1983</REF>, <REF>Jelinek 1985</REF>, <REF>Deroualt and Merialdo 1986</REF>, <REF>Garside, Leech, and Sampson 1987</REF>, <TREF>Church 1988</TREF>, <REF>DeRose 1988</REF>, <REF>Hindle 1989</REF>, Kupiec 1989, 1992, Ayuso et al~~~1990, de<REF>Marcken 1990</REF>, <REF>Karlsson 1990</REF>, <REF>Boggess, Agarwal, and Davis 1991</REF>, <REF>Merialdo 1991</REF>, and <REF>Voutilainen, Heikkila, and Anttila 1992</REF>~~~These programs input a sequence of words, eg, The chair will table the motion, and output a sequence of part-of-speech tags, eg, art noun modal verb art noun.
English parsing is divided into two tasks: shallow parsing and deep parsing~~~The shallow parser constructs Verb Groups VGs and basic Noun Phrases NPs, also called BaseNPs <TREF>Church 1988</TREF>~~~The deep parser utilizes syntactic subcategorization features and semantic features of a head eg , VG to decode both syntactic and logical dependency relationships such as Verb-Subject, Verb-Object, Head-Modifier, etc Part-of-Speech POS Tagging General Lexicon Lexical lookup Named Entity NE Taggig Shallow Parsing PV Identification Deep parsing General Lexicon PV Expert Lexicon Figure 1~~~System Architecture The general lexicon lookup component involves stemming that transforms regular or irregular inflected verbs into the base forms to facilitate the later phrasal verb matching.
We redistribute the probability mass of low count sequences to unseen sequences~~~Generalized Forward Backward Reestimation Generalization of the Forward and Viterbi Algorithm In English part of speech taggers, the maximization of Equation 1 to get the most likely tag sequence, is accomplished by the Viterbi algorithm <TREF>Church, 1988</TREF>, and the maximum likelihood estimates of the parameters of Equation 2 are obtained from untagged corpus by the ForwardBackward algorithm <REF>Cutting et al , 1992</REF>~~~However, it is impossible to apply the Viterbi algorithm and the Forward-Backward algorithm for word segmentation of those languages that have no delimiter between words, such as Japanese and Chinese, because word segmentation hypotheses overlap one another~~~Figure 3 shows an example of overlapping word hypotheses and possible word segmentations for the string Ntig-f all prefectures in the nation.
An event greater improvement over the baseline is illustrated by the increase in the number of event clauses correctly classified, ie event rrall~~~As shown in Table 7, an event recall of 677 was achieved by the classification rule, as compared to speech tagging <TREF>Church, 1988</TREF>; <REF>Alien, 1995</REF>~~~13 I I I I I I I I I I I I I I I I I I the 00 event recall achieved by the baseline, while suffering no loss in overall accuracy~~~This difference in recall is more dramatic than the accuracy improvement because of the dominance of stative clauses in the test set.
What can be done at the present stage is the recognition of relatively simple structures such as NPs and PPs~~~<TREF>Church, 1988</TREF> used a simple mechanism to mark the boundaries of NPs~~~He used part-of-speech tagging and added two flags to the part-of-speech tags to mark the beginning and the end of an NP~~~Our goal is more ambitious in that we mark not only the phrase boundaries of NPs but also the complete structure of a wider class of phrases, starting with APs, NPs and PPs.
For a larger dataset, such as the Canadian Hansards, it was not possible to check the results by hand~~~We used the same procedure which is used in <TREF>Church, 1988</TREF>~~~This procedure was developed by Kathryn Baker private communication~~~ratio.
In fact, whereas stochastic taggers have to store word-tag, bigram, and trigram probabilities, the rule-based tagger and therefore the finite-state one only have to encode a small number of rules between 200 and 300~~~We empirically compared our tagger with Eric Brills implementation of his tagger, and with our implementation of a trigram tagger adapted from the work of <TREF>Church 1988</TREF> that we previously implemented for another purpose~~~We ran the three programs on large files and piped their output into a file~~~In the times reported, we included the time spent reading the input and writing the output.
Although finite-state machines have been used for part-of-speech tagging <REF>Tapanainen and Voutilainen 1993</REF>; <REF>Silberztein 1993</REF>, none of these approaches has the same flexibility as stochastic techniques~~~Unlike stochastic approaches to part-of-speech tagging <TREF>Church 1988</TREF>; <REF>Kupiec 1992</REF>; <REF>Cutting et al 1992</REF>; <REF>Merialdo 1990</REF>; <REF>DeRose 1988</REF>; <REF>Weischedel et al 1993</REF>, up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired~~~<REF>Recently, Brill 1992</REF> described a rule-based tagger that performs as well as taggers based upon probabilistic models and overcomes the limitations common in rule-based approaches to language processing: it is robust and the rules are automatically ac Mitsubishi Electric Research Laboratories, 201 Broadway, Cambridge, MA 02139~~~E-mail: rocbe/schabesmerlcom.
A corpus is manually tagged with the categories and transition probabilities between two or three categories are estimated from their relative frequencies~~~This method is commonly used for part-of-speech tagging <TREF>Church, 1988</TREF>~~~The fourth method is a variation of the third method and is also used for part-of-speech tagging~~~This method does not need a pre-annotated corpus for parameter estimation.
2~~~Previous <REF>Works Church 1988</REF> proposes a part of speech tagger and a simple noun phrase extractor~~~His noun phrase extractor brackets the noun phrases of input tagged texts according to two probability matrices: one is starting noun phrase matrix; the other is ending noun phrase matrix~~~The methodology is a simple version of Garside and Leechs probabilistic parser 1985.
The testing scale is large enough about 150,000 words~~~In contrast, <TREF>Church 1988</TREF> tests a text and extracts the simple noun phrases only~~~Bourigaults work 1992 is evaluated manually, and dose not report the precision~~~Hence, the real performance is not known.
For more details, we refer the reader to <REF>Mgrquez and Rodrfguez, 1997</REF>~~~22 STT: A Statistical Tree-based Tagger The aim of statistical or probabilistic tagging <TREF>Church, 1988</TREF>; <REF>Cutting et al , 1992</REF> is to assign the most likely sequence of tags given the observed sequence of words~~~For doing so, two kinds of information are used: the lexical probabilities, ie, the probability of a particular tag conditional on the particular word, and the contextual probabilities, which describe the probability of a particular tag conditional on the surrounding tags~~~Contextual or transition probabilities are usually reduced to the conditioning of the preceding tag bigrams, or pair of tags trigrams, however, the general formulation allows a broader definition of context.
based approach implemented with finite-state machines <REF>Koskenniemi et al , 1992</REF>; <REF>Voutilainen and Tapanainen, 1993</REF>~~~A completely different approach to tagging uses statistical methods, eg , <TREF>Church, 1988</TREF>; <REF>Cutting et al , 1993</REF>~~~These systems essentially train a statistical model using a previously hand-tagged corpus and provide the capability of resolving ambiguity on the basis of most likely interpretation~~~The models that have been widely used assume that the part-ofspeech of a word depends on the categories of the two preceding words.
As far as coreference resolution is concerned, the goal of these NLP modules is to determine the boundary of the markables, and to provide the necessary information about each markable for subsequent generation of features in the training examples~~~Our part-of-speech tagger is a standard statistical tagger based on the Hidden Markov Model HMM <TREF>Church 1988</TREF>~~~Similarly, we built a statistical HMM-based noun phrase identification module that determines the noun phrase boundaries solely based on the part-of-speech tags assigned to the words in a sentence~~~We also implemented a module that recognizes MUC-style named entities, that is, organization, person, location, date, time, money, and percent.
There has recently been a great deal of work exploring methods for automatically training part of speech taggers, as an alternative to laboriously hand-crafting rules for tagging, as was done in the past <REF>Klein and Simmons, 1963</REF>; <REF>Harris, 1962</REF>~~~Almost all of the work in the area of automatically trained taggers has explored Markov-model based part of speech tagging <REF>Jelinek, 1985</REF>; <TREF>Church, 1988</TREF>; <REF>Derose, 1988</REF>; <REF>DeMarcken, 1990</REF>; <REF>Cutting et al , 1992</REF>; <REF>Kupiec, 1992</REF>; <REF>Charniak et al , 1993</REF>; <REF>Weischedel et al , 1993</REF>; <REF>Schutze and Singer, 1994</REF>; <REF>Lin et al , 1994</REF>; <REF>Elworthy, 1994</REF>; <REF>Merialdo, 1995</REF>~~~2 For a Markov-model based tagger, training consists of learning both lexical probabilities Pwordltag and contextual probabilities Ptagiltagil tagi-n~~~Once trained, a sentence can be tagged by searching for the tag sequence that maximizes the product of lexical and contextual probabilities.
In the examples ahove, tagging of presents as vbz in the first sentence cuts off a potentially long and cosily garden path with presents as a plural noun followed by a headless relative clause starting with that a proposal  In the second sentence, tagging resolves ambiguity of used vim vs vbd, and associates vbz vs nns~~~Perhaps more imlxmantly, elimination of word-level lexical ambiguity allows the parser to make projection about the input which is yet to be parsed, using a simple lookabead; in particular, phrase boundaries can be determined with a degree of confidence <TREF>Church, 1988</TREF>~~~This latter property is critical for implementing skip-and-fit recovery technique outlined in the previous section~~~Tagging of input also helps to reduce the number of parse structures that can be assigned to a sentence, decreases the demand for consulting of the dictionary, and simplifies dealing with unknown words.
The typical examples are the recognition of BaseNP in English and Chinese~~~In English BNP base noun phrase is defined as simple and non-nesting noun phrases, ie noun phrases that do not contain other noun phrase descendants <TREF>Church, 1988</TREF>~~~After that researches on BNP identification reports promising results for such task in English~~~Observing that the Chinese BNP is different form English, <REF>Zhao  Huang, 1999</REF> puts forward the definition of Chinese BNP in terms of combination of determinative modifier and head noun.
More sophisticated linguistic information comes in several forms, all of which may need to be represented if performance in an automatic categorisation experiment is to be improved~~~Typical examples of linguistically sophisticated annotation include tagging words with their syntactic category although this has not been found to be effective for 1R, lemma of the word eg corpus for corpora, phrasal information eg identifying noun groups and phrases <REF>Lewis 1992c</REF>, <TREF>Church 1988</TREF>, and subject-predicate identification eg <REF>Hindle 1990</REF>~~~For the RAPRA corpus, we currently identify noun groups and adjective groups~~~This is achieved in a manner similar to Churchs 1988 PARTS algorithm used by Lewis 1992bc, in the sense that its main properties are robustness and corpus sensitivity.
For the RAPRA corpus, we currently identify noun groups and adjective groups~~~This is achieved in a manner similar to Churchs 1988 PARTS algorithm used by Lewis 1992bc, in the sense that its main properties are robustness and corpus sensitivity~~~All that is important for this paper is that the technique identifies various groupings of words for example, noun-groups, adjective groups, and so on with a high level of accuracy~~~Major parts of the technique are described in detail in <REF>Finch, 1993</REF>.
problem~~~Excellent methods have been developed for part-of-speech POS tagging using stochastic models trained on partially tagged corpora <TREF>Church, 1988</TREF>; Cutting, <REF>Kupiec, Pedersen  Sibun, 1992</REF>~~~Semantic issues have been addressed, particularly for sense disambiguation, by using large contexts, eg, 50 nearby words <REF>Gale, Church  Yarowsky, 1992</REF> or by reference to on-line dictionaries <REF>Krovetz, 1991</REF>; <REF>Lesk, 1986</REF>; <REF>Liddy  Paik, 1992</REF>; <REF>Zernik, 1991</REF>~~~More recently, methods to work with entirely untagged corpora have been developed which show great promise <REF>Brill  Marcus, 1992</REF>; <REF>Finch  Chater, 1992</REF>; <REF>Myaeng  Li, 1992</REF>; <REF>Schutze, 1992</REF>.
Although methods for unsupervised training of HMMs do exist, training is usually done in a supervised way by estimation of the above probabilities from relative frequencies in the training data~~~The HMM approach to tagging is by far the most studied and applied <TREF>Church 1988</TREF>; <REF>DeRose 1988</REF>; <REF>Charniak 1993</REF>~~~In van <REF>Halteren, Zavrel, and Daelemans 1998</REF> we used a straightforward implementation of HMMs, which turned out to have the worst accuracy of the four competing methods~~~In the present work, we have replaced this by the TnT system we will refer to this tagger as HMM below.
Two-Word Descriptions Three-Word Descriptions Stage Entities Unique Entities Entities Unique Entities POS tagging only 9,079 1,546 2,617 604 After WordNet checkup 1,509 395 81 26  Extraction of candidates for proper nouns~~~After tagging the corpus using the POS part-of-speech tagger <TREF>Church 1988</TREF>, we used a CREP <REF>Duford 1993</REF> regular grammar to first extract all possible candidates for entities~~~These consist of all sequences of words that were tagged as proper nouns NP by POS~~~Our manual analysis showed that out of a total of 2150 entities recovered in this way, 1139 529 are not names of entities.
A trained system would probably be more accurate in classifying new verbs~~~Finally, the lexical ambiguity problem could probably be reduced substantially in the applied context by using a statistical tagging program <REF>Brill 1992</REF>; <TREF>Church 1988</TREF>~~~For addressing basic questions in machine learning of natural language the solutions outlined above are not attractive~~~All of those solutions provide the learner with additional specific knowledge of English, whereas the goal for the machine learning effort should be to replace specific knowledge with general knowledge about the types of regularities to be found in natural language.
to appear, <REF>Hearst 1991</REF>, <REF>Lesk 1986</REF>, <REF>Smadja and McKeown 1990</REF>, <REF>Walker 1987</REF>, <REF>Veronis and Ide 1990</REF>, <REF>Yarowsky 1992</REF>, Zemik 1990, 1991~~~Much of this work offers the prospect that a disambiguation system might be able to input unrestricted text and tag each word with the most likely sense with fairly reasonable accuracy and efficiency, just as part of speech taggers eg , <TREF>Church 1988</TREF> can now input unrestricted text and assign each word with the most likely part of speech with fairly reasonable accuracy and efficiency~~~The availability of massive lexicographic databases offers a promising route to overcoming the knowledge acquisition bottleneck~~~More than thirty years ago, BarI-<REF>Iillel 1960</REF> predicted that it would be futile to write expert-system-like rules by-hand as they had been doing at Georgetown at the time because there would be no way to scale up such rules to cope with unrestricted input.
<REF>Choueka and Lusignan 1985</REF> presented a system for the morphological tagging of large texts that is based on the short context of the word but also depends heavily on human interaction~~~Methods using the short context of a word in order to resolve ambiguity usually categorical ambiguity are very common in English and other languages <REF>DeRose 1988</REF>; <TREF>Church 1988</TREF>; <REF>Karlsson 1990</REF>~~~A system using this approach was developed by Levinger and Ornan in order to serve as a component in their project of morphological disambiguation in Hebrew <REF>Levinger 1992</REF>~~~The main resource, used by this system for disambiguation, is a set of syntactic constraints that were defined manually by the authors and followed two theoretical works that defined short context rules for Hebrew <REF>Pines 1975</REF>; <REF>Albeck 1992</REF>.
Another application which is more difficult in Hebrew than in other languages is text-to-speech systems, which cannot be implemented in Hebrew without first solving the morphological ambiguity, since in many cases different analyses of a word imply different pronunciations~~~A much simpler problem occurs in English, where for some words the correct syntactic tag is necessary for pronunciation <TREF>Church 1988</TREF>~~~The notion that this ambiguity problem in Hebrew is very complicated and that it can be dealt with only by using vast syntactic and semantic knowledge has led researchers to look for solutions involving a considerable amount of human interaction~~~<REF>Ornan 1986</REF> for instance, developed a new writing system for Hebrew, called The Phonemic Script.
Estimating the Lexical Priors for Rare Forms For a common form such as lopen walk a reasonable estimate of the lexical prior probabilities is the MLE, computed over all occurrences of this form~~~So, in the UdB corpus, lopen occurs 92 times as an infinitive and 43 times as a finite plural, so the MLE 1 Even models of disambiguation that make use of context, such as statistical n-gram taggers, often presume some estimate of lexical priors, in addition to requiring estimates of the transition probabilities of sequences of lexical tags <TREF>Church 1988</TREF>; <REF>DeRose 1988</REF>; <REF>Kupiec 1992</REF>, and this again brings up the question of what to do about unseen or low-frequency forms~~~In working taggers, a common approach is simply to apply a uniform small probability to the various senses of unseen or low-frequency forms: this was done in the tagger discussed in <TREF>Church 1988</TREF>, for example~~~156 Baayen and Sproat Lexical Priors for Low-Frequency Forms to> 8 Figure 1 I I I I 0 2 4 6 log frequency class Relative frequency of Dutch infinitives versus finite plurals in the Uit den Boogaart corpus, as a function of the natural log of the frequency of the word forms.
So, in the UdB corpus, lopen occurs 92 times as an infinitive and 43 times as a finite plural, so the MLE 1 Even models of disambiguation that make use of context, such as statistical n-gram taggers, often presume some estimate of lexical priors, in addition to requiring estimates of the transition probabilities of sequences of lexical tags <TREF>Church 1988</TREF>; <REF>DeRose 1988</REF>; <REF>Kupiec 1992</REF>, and this again brings up the question of what to do about unseen or low-frequency forms~~~In working taggers, a common approach is simply to apply a uniform small probability to the various senses of unseen or low-frequency forms: this was done in the tagger discussed in <TREF>Church 1988</TREF>, for example~~~156 Baayen and Sproat Lexical Priors for Low-Frequency Forms to> 8 Figure 1 I I I I 0 2 4 6 log frequency class Relative frequency of Dutch infinitives versus finite plurals in the Uit den Boogaart corpus, as a function of the natural log of the frequency of the word forms~~~The horizontal solid line represents the overall MLE, the relative frequency of the infinitive as computed over all tokens; the horizontal dashed line represents the relative frequency of the infinitive among the hapax legomena.
Statistical anMyses of linguistic data were very popular in the 50s and 60s, mainly, even though not only, for literary types of analyses and for studies on the lexicon <REF>Guiraud 1959</REF>, <REF>Muller 1964</REF>, <REF>Moskovich 1977</REF>~~~Stochastic approaches to linguistic analyses have been strongly reevaluated in the past few years, either for syntactic analysis Gmside et al 1987, <TREF>Church 1988</TREF>, or for NLP applications <REF>Brown et al 1988</REF>, or for semantic analysis <REF>Zemik 1989</REF>, <REF>Smadja 1989</REF>~~~Quantitative not statistical evidence on eg word-sense occurrences in a large corpus have been taken into account for lexicographic descriptions Cobuild 17~~~I llere and in the following we have not translated idiomatic phrases and compounds, because there is no point in giving the literal translation of the single words.
Furthermore, we might expect that some words, such as prepositions and determiners, for example, do not constitute the typical end to an intonational phrase~~~We test these possibilities by examining part-of-speech in a window of four words surrounding each potential phrase break, using Churchs part-of-speech tagger 1988~~~Recall that each intermediate phrase is composed of one or more pitch accents plus a phrase accent, and each intonational phrase is composed of one or more intermediate phrases plus a boundary tone~~~Informal observation suggests that phrase boundaries are more likely to occur in some accent contexts than in others.
Discussion The application of CART techniques to the problem of predicting and detecting phrasing boundaries not only provides a classification procedure for predicting intonational boundaries from text, but it increases our understanding of the importance of several among the numerous variables which might plausibly be related to boundary location~~~In future, we plan to extend the set of variables for analysis to include counts of stressed syllables, automatic NP-detection <TREF>Church, 1988</TREF>, MUTUAL INFORMATION, GENERALIZED MUTUAL INFORMATION scores can serve as indicators of intonational phrase boundaries <REF>Magerman and Marcus, 1990</REF>~~~We will also examine possible interactions among the statistically important variables which have emerged from our initial study~~~CART techniques have worked extremely well at classifying phrase boundaries and indicating which of a set of potential variables appear most important.
The main atvantage of the linguistic approach is that the model is constructed from a linguistic Ioint of view and contains many and complex kinds of knowledge iI1 tim lemning approach, tile most extended tbrmalism is based on n-grains or IIMM~~~In tiffs case, the language inodel can be estimated from a labelled corpus supervised methods <TREF>Church, 1988</TREF>Weisehedel et al , 1993 or from a nonlabelled corpus unsupervised methods Cutting et 21~~~, 1992~~~In the first; case, the model is trained from the relative observed Dequencies.
122 LKarning Techniques These allnoachcs automatically :onstruel; a language model from a labellod alld brackKted corpus~~~The lirst probabilistic approach was proposed in <TREF>Church, 1988</TREF>~~~This method learn; a bigram model for detecting simph3 noun phrasKs on the Brown corpus~~~Civn a sequene of parts of st3eeh as inlug, the Church program inserts the most prolable openings and Kndings of NPs, using a Viterbiqiko.
It shows the descriptive power of low-level morphology-based constraints~~~The most successful achievements so far in the domain of large-scale morphological disambiguation of running text have been those for English reported by <REF>Garside, Leech, and Sampson 1987</REF>, on tagging the LOB corpus, and <TREF>Church 1988</TREF>, on assigning part-of-speech labels and parsing noun phrases~~~Success rates ranging between 95-99 are reported, depending on how success is defined~~~These approaches are probabilistic and based on transitional probabilities calculated from extensive pretagged corpora.
The methods we investi1This gives the Viterbi model <REF>Merialdo, 1994</REF>, which we use here~~~2This version of the method uses Bayes theorem  <TREF>Church, 1988</TREF>~~~Pwdt, o Pt, J gate approach this evaluation implicitly, measuring an examples informativeness as the uncertainty in its classification given the current training data <REF>Seung, Opper, and Sompolinsky, 1992</REF>; <REF>Lewis and Gale, 1994</REF>; <REF>MacKay, 1992</REF>~~~The reasoning is that if an examples classification is uncertain given current training data then the example is likely to contain unknown information useful for classifying similar examples in the future.
Our work focuses on sample selection for training probabilistic classifiers~~~In statistical NLP, probabilistic classifiers are often used to select a preferred analysis of the linguistic structure of a text for example, its syntactic structure <REF>Black et al , 1993</REF>, word categories <TREF>Church, 1988</TREF>, or word senses <REF>Gale, Church, and Yarowsky, 1993</REF>~~~As a representative task for probabilistic classification in NLP, we experiment in this paper with sample selection for the popular and well-understood method of stochastic part-of-speech tagging using Hidden Markov Models~~~We first review the basic approach of committeebased sample selection and its application to partof-speech tagging.
Additionally, there is a slight but not significant improvement of tagging accuracy~~~Statistical part-of-speech disambiguation can be efficiently done with n-gram models <TREF>Church, 1988</TREF>; <REF>Cutting et al , 1992</REF>~~~These models are equivalent to Hidden Markov Models HMMs <REF>Rabiner, 1989</REF> of order n 1~~~The states represent parts of speech categories, tags, there is exactly one state for each category, and each state outputs words of a particular category.
A specialised version of the chunking task is NP CHUNKING or baseNP identification in which the goal is to identify the base noun phrases~~~The first work on this topic was done back in the eighties <TREF>Church, 1988</TREF>~~~The data set that has become standard for evaluation machine learning approaches is the one first used by <REF>Ramshaw and Marcus 1995</REF>~~~It consists of the same training and test data segments of the Penn Treebank as the chunking task respectively sections 15-18 and section 20.
We report in this paper on one application of probabilistic models to language processing, the assignment of part of speech to words in open text~~~The effectiveness of such models is well known <TREF>Church 1988</TREF> and they are currently in use in parsers eg de <REF>Marcken 1990</REF>~~~Our work is an incremental improvement on these models in two ways: 1 We have run experiments regarding the amount of training data needed in moving to a new domain; 2 we have added probabilistic models of word features to handle unknown words effectively~~~We describe POST and its algorithms and then we describe our extensions, showing the results of our experiments.
Using the Viterbi algorithm, we selected the path whose overall probability was highest, and then took the tag predictions from that path~~~We replicated the result <TREF>Church 1988</TREF> that this process is able to predict the parts of speech with only a 3-4 error rate when the possible parts of speech of each the words in the corpus are known~~~This is in fact about the rate of discrepancies among human taggers on the TREEBANK project <REF>Marcus, Santorini  Magerman 1990</REF>~~~22 Quantity of training data While supervised training is shown here to be very effective, it requires a correctly taed corpus.
They mainly differ in the emphasis they give to syntactic and statistical control of the induction process~~~In Church,1988 a well-know purely statistical method for POS tagging is applied to the derivation of simple noun phrases that are relevant in the underlying corpus~~~On the contrary more language oriented methods are those where specialized grammar are used~~~LEXTER Bourigault,1992 extracts maximal length noun phrases mlnp from a corpus, and then applies a special purpose noun phrase parsing to hem in order to focus on significant complex nominals.
There are a number of large tagged corpora available, allowing for a variety of experiments to be run~~~Part-of-speech tagging is an active area of research; a great deal of work has been done in this area over the past few years eg , <REF>Jelinek 1985</REF>; <TREF>Church 1988</TREF>; <REF>Derose 1988</REF>; <REF>Hindle 1989</REF>; <REF>DeMarcken 1990</REF>; <REF>Merialdo 1994</REF>; <REF>Brill 1992</REF>; <REF>Black et al 1992</REF>; <REF>Cutting et al 1992</REF>; <REF>Kupiec 1992</REF>; <REF>Charniak et al 1993</REF>; <REF>Weischedel et al 1993</REF>; <REF>Schutze and Singer 1994</REF>~~~Part-of-speech tagging is also a very practical application, with uses in many areas, including speech recognition and generation, machine translation, parsing, information retrieval and lexicography~~~Insofar as tagging can be seen as a prototypical problem in lexical ambiguity, advances in part-of-speech tagging could readily translate to progress in other areas of lexical, and perhaps structural, ambiguity, such as wordsense disambiguation and prepositional phrase attachment disambiguation.
It has recently become clear that automatically extracting linguistic information from a sample text corpus can be an extremely powerful method of overcoming the linguistic knowledge acquisition bottleneck inhibiting the creation of robust and accurate natural language processing systems~~~A number of part-of-speech taggers are readily available and widely used, all trained and retrainable on text corpora <TREF>Church 1988</TREF>; <REF>Cutting et al 1992</REF>; <REF>Brill 1992</REF>; <REF>Weischedel et al 1993</REF>~~~Endemic structural ambiguity, which can lead to such difficulties as trying to cope with the many thousands of possible parses that a grammar can assign to a sentence, can be greatly reduced by adding empirically derived probabilities to grammar rules <REF>Fujisaki et al 1989</REF>; <REF>Sharman, Jelinek, and Mercer 1990</REF>; <REF>Black et al 1993</REF> and by computing statistical measures of lexical association <REF>Hindle and Rooth 1993</REF>~~~Word-sense disambiguation, a problem that once seemed out of reach for systems without a great deal of handcrafted linguistic and world knowledge, can now in some cases be done with high accuracy when all information is derived automatically from corpora <REF>Brown, Lai, and Mercer 1991</REF>; <REF>Yarowsky 1992</REF>; Gale, Church, and <REF>Yarowsky 1992</REF>; <REF>Bruce and Wiebe 1994</REF>.
However, stochastic taggers have the disadvantage that linguistic information is captured only indirectly, in large tables of statistics~~~Almost all recent work in developing automatically trained part-of-speech taggers has been on further exploring Markovmodel based tagging <REF>Jelinek 1985</REF>; <TREF>Church 1988</TREF>; <REF>Derose 1988</REF>; <REF>DeMarcken 1990</REF>; <REF>Merialdo 1994</REF>; <REF>Cutting et al 1992</REF>; <REF>Kupiec 1992</REF>; <REF>Charniak et al 1993</REF>; <REF>Weischedel et al 1993</REF>; <REF>Schutze and Singer 1994</REF>~~~41 Transformation-based Error-driven Part-of-Speech Tagging Transformation-based part of speech tagging works as follows~~~9 The initial-state annotator assigns each word its most likely tag as indicated in the training corpus.
Extraction of candidates for proper nouns~~~After tagging the corpus using the POS part-of-speech tagger <TREF>Church, 1988</TREF>, we used a CREP <REF>Duford, 1993</REF> regular grammar to first extract all possible candidates for entities~~~These consist of all sequences of words that were tagged as proper nouns NP by POS~~~Our manual analysis showed that out of a total of 2150 entities recovered in this way, 1139 529 are not names of entities.
In the past decade, the speech recognition community has had huge successes in applying hidden Markov models, or HMMs to their problems~~~More recently, the natural language processing community has effectively employed these models for part-ofspeech tagging, as in the seminal <TREF>Church, 1988</TREF> and other, more recent efforts <REF>Weischedel et al , 1993</REF>~~~We would now propose that HMMs have successfully been applied to the problem of name-finding~~~We have built a named-entity NE recognition system using a slightly-modified version of an HMM; we call our system Nymble.
For example, there is less than a 005 chance that the difference between stative and event means for the first four indicators listed 2This test was suggested by Judith Klavans personal communication~~~3Similar baselines for comparison have been used for many classification problems <REF>Duda and Hart, 1973</REF>, eg, part-of-speech tagging <TREF>Church, 1988</TREF>; <REF>Allen, 1995</REF>~~~159 is due to chance~~~Overall, this shows that the differences in stative and event averages are statistically significant for the first seven indicators listed p < 01.
Both types of ambiguity, syntactic and lexical, may cause the system to acquire or use inappropriate patterns~~~This problems is consid ered very important when dealing with a corpus: it was the re,Leon for the substantial human intervention in the procedure of <REF>Grishman et al 1986</REF>, and it is the reason why other techniques use manually tagged corpora eg <TREF>Church 1988</TREF>~~~In practice, however, we have discovered that the problem is not so cruciah semantically vMid patterns have occurred many more times in syntactically unambiguous constructs than in mnbiguous ones~~~Thus, they could be identified without the need of first disambiguating the sentences.
6 PREPROCESSING WITH A PART OF SPEECH TAGGER Phrasal verbs involving the preposition to raise an interesting problem because of the possible confusion with the infinitive marker to~~~We have found that if we first tag every word in the corpus with a part of speech using a method such as <TREF>Church 1988</TREF>, and then measure associations between tagged words, we can identify interesting contrasts between verbs associated with a following preposition toin and verbs associated with a following infinitive marker toto~~~Part of speech notation is borrowed from <REF>Francis and Kucera 1982</REF>; in  preposition; to  infinitive marker; vb  bare verb; vbg  verb  ing; vbd  verb  ed; vbz  verb  s; vbn  verb  en~~~The association ratio identifies quite a number of verbs associated in an interesting way with to; restricting our attention to pairs with a score of 30 or more, there are 768 verbs associated with the preposition toin and 551 verbs with the infinitive marker to/to.
The computational tools available for studying machinereadable corpora are at present still rather primitive~~~These are concordancing programs see Figure 1, which are basically KWIC key word in context; <REF>Aho et al 1988</REF> indexes with additional features such as the ability to extend the context, sort leftward as well as rightward, and so on~~~There is very little interactive software~~~In a typical situation in the lexicography of the 1980s, a lexicographer is giwen the concordances for a word, marks up the printout with colored pens to identify the salient senses, and then writes syntactic descriptions and definitions.
/3 2~~~precision  recall 1 HMM-based Chunk Tagger The idea of using statistics for chunking goes back to <TREF>Church1988</TREF>, who used corpus frequencies to determine the boundaries of simple non-recursive noun phrases~~~Skut and <REF>Brants1998</REF> modified Churchs approach in a way permitting efficient and reliable recognition of structures of limited depth and encoded the structure in such a way that it can be recognised by a Viterbi tagger~~~This makes the process run in time linear to the length of the input string.
Input Text Tokenization Part-of-speech Lookup Descriptor array construction Classification by learning algorithm Text withsentence boundaries disambiguated 31 Tokenization The first stage of the process is lexical analysis, which breaks the input text a stream of characters into tokens~~~The Satz tokenizer is implemented using the UNIX tool LEX <REF>Lesk and Schmidt 1975</REF> and is modeled on the tokenizer used by the PARTS part-of-speech tagger <TREF>Church 1988</TREF>~~~The tokens returned by the LEX program can be a sequence of alphabetic characters, a sequence of digits, 8 or a sequence of one or more non-alphanumeric characters such as periods or quotation marks~~~32 Part-of-Speech Lookup The individual tokens are next assigned a series of possible parts of speech, based on a lexicon and simple heuristics described below.
The lexicon and thus the frequency counts used to calculate the descriptor arrays were derived from the Brown corpus <REF>Francis and Kucera 1982</REF>~~~In initial experiments we used the extensive lexicon from the PARTS part-of-speech tagger <TREF>Church 1988</TREF>, which contains 30,000 words~~~We later experimented with a much smaller lexicon, and these results are discussed in Section 44~~~In Sections 41-49 we describe the results of our experiments with the Satz system using the neural network as the learning algorithm.
We regard our use of probabilities as being consistent with Bauers claim that accounting for semi-productivity is an issue of performance, not competence <REF>Bauer 1983</REF>:71f~~~The frequency with which a given word form is associated with a particular lexical entry ie sense or grammatical realization is often highly skewed; <TREF>Church 1988</TREF> points out that a model of part-of-speech assignment in context will be 90 accurate for English if it simply chooses the lexically most frequent part-of-speech for a given word~~~<REF>Briscoe and Carroll 1995</REF> found in one corpus that there were about 18 times as many instances of believe in the most common subcategorizati0n class as in the 4 least common classes combined~~~In the absence of other factors, it seems very likely that language users utilize frequency information to resolve indeterminacies in both generation and interpretation.
12 Survey of Related Work Chunking has been studied for English and other languages, though not very extensively~~~The earliest work on chunking based on machine learning goes to Church K, 1988 for English~~~<REF>Ramshaw and Marcus, 1995</REF> used transformation based learning using a large annotated corpus for English~~~<REF>Skut and Brants, 1998</REF> modi ed Churchs approach, and used standard HMM based tagging methods to model the chunking process.
27 3 Empirical Comparison We evaluated the similarity functions introduced in the previous section on a binary decision task, using the same experimental framework as in our previous preliminary comparison <REF>Dagan et al , 1999</REF>~~~That is, the data consisted of the verb-object cooccurrence pairs in the 1988 Associated Press newswire involving the 1000 most frequent nouns, extracted via Churchs 1988 and Yarowskys processing tools~~~587,833 80 of the pairs served as a training set from which to calculate base probabilities~~~From the other 20, we prepared test sets as follows: after discarding pairs occurring in the training data after all, the point of similarity-based estimation is to deal with unseen pairs, we split the remaining pairs into five partitions, and replaced each nounverb pair n, vl with a noun-verb-verb triple n, vl, v2 such that Pv2  Pvl.
In this paper, we propose a new space and a new metric for computing this distance~~~Being inspired by the success of noisy-channel-based approaches in applications as diverse as speech recognition <REF>Jelinek, 1997</REF>, part of speech tagging <TREF>Church, 1988</TREF>, machine translation <REF>Brown et al , 1993</REF>, information retrieval <REF>Berger and Lafferty, 1999</REF>, and text summarization <REF>Knight and Marcu, 2002</REF>, we develop a noisy channel model for QA~~~This model explains how a given sentence S A that contains an answer sub-string A to a question Q can be rewritten into Q through a sequence of stochastic operations~~~Given a corpus of questionanswer pairs Q, S A , we can train a probabilistic model for estimating the conditional probability PQ  S A .
7 0 7 3 Word Segmentation Algorithm 31 Statistical Language Model For the language model in Equation 1, we used the part of speech trigram nlodel POS trigranl or 2nd-order HMM~~~It is used,as tagging mode in English <TREF>Church, 1988</TREF>; <REF>Cutting et al , 1992</REF> and morphological analysis nlodel word segmentation and tagging in Japanese <REF>Nagata, 1994</REF>~~~Let the input character sequence be /  ccec  We approxinlate PCby PW, 7, the joint prol>ability of word sequence W  wlw2u, and part of speech sequence   tlte, t,,~~~PW,T is then approximated t>y the product of parts of speech trigram probabilities Ptiti-2, i-l and word output probabilities for given part of speech Pwiltl, 71 pc pw, -- IX pt, lt,-,t,-,p,lt, 5 i1 Ptilti-,e,ti- and /-wlti  are estimated >y computing the relative frequencies of the corresponding events in training corpus a 32 Forward-DP Backward-A Algorithm /sing the language model 5, Japanese morplological analysis can be detined,as finding tile set of word segmentation and parts of speech 1/, 7 that maximizes the joint probability of word sequence and tag sequence PW, 7.
In this seelion, we will outline the three lexicalist, linguistically perspicuous, qualitatiwly different models that we have leveloped a, nd tested~~~21 Model A: Bigram lexieal affinities N-gram tatters like <TREF>Church, 1988</TREF>; lelinek 1985; <REF>Kupiec 1992</REF>; <REF>Merialdo 1990</REF> take the following view of row /, tagged sentctrce enters the worhl~~~Iirst, a setuenee of tags is gnexated aecordittg to a Markov lrocess, with th random choice of ech tag conditioned ou the previous two tags~~~Second, a word is choseu conditional on each tag.
Nevertheless, if a large POS set is specified, the number of rules increases significantly and rule definition becomes highly costly and cumbersome~~~Stochastic taggers use both contextual and morphological information, and the model parameters are usually defined or updated automatically from tagged texts Cerf-Danon and E1-<REF>Beze 1991</REF>; <TREF>Church 1988</TREF>; <REF>Cutting et al 1992</REF>; <REF>Dermatas and Kokkinakis 1988, 1990, 1993, 1994</REF>; <REF>Garside, Leech, and Sampson 1987</REF>; <REF>Kupiec 1992</REF>; Maltese  Department of Electrical Engineering, Wire Communications Laboratory WCL, University of Patras, 265 00 Patras, Greece~~~E-mail: dermataswcleeupatrasgr~~~ 1995 Association for Computational Linguistics Computational Linguistics Volume 21, Number 2 and <REF>Mancini 1991</REF>; <REF>Meteer, Schwartz, and Weischedel 1991</REF>; <REF>Merialdo 1991</REF>; <REF>Pelillo, Moro, and Refice 1992</REF>; <REF>Weischedel et al 1993</REF>; <REF>Wothke et al 1993</REF>.
2 Relation to Previous Works Quite a few works have dealt with extending a given POS tagger, mainly by smoothing it using extra-information about untreated words~~~For example, <TREF>Church, 1988</TREF> uses the simple heuristic of predicting proper nouns from capitalization~~~This method is not applicable to Arabic and Hebrew, which lack typographical marking of proper nouns~~~More advanced methods like those described by Weischedel et al.
In particular, it would be interesting to see if the accuracy ranking of the seven algorithms is affected by a change in the representation~~~Similar comparisons of a range of algorithms should also be performed on other natural language problems such as part-of-speech tagging <TREF>Church, 1988</TREF>, prepositional phrase attachment <REF>Hindle  Rooth, 1993</REF>, anaphora resolution Anoe  <REF>Bennett, 1995</REF>, etc Since the requirements of individual tasks vary, different algorithms may be suitable for different sub-problems in natural language processing~~~87 o 0 : 0 0  / 350 300 250 200 150 1 O0 50 I I I I I ~~~Naive Bales o J 3 Nearest Neighbor --0-- Perceptron -m- C45 ---x   PFOIL-DNF ---1 PFOIL-CNF ---1 PFOIL-DLIST --- I  G    :::::-;    ii212  x2C:222Z/2: 200 400 600 800 1000 1200 Training Examples Figure 3: Testing Time for Line Corpus Conclusions This paper has presented fairly comprehensive experiments comparing seven quite different empirical methods on learning to disambiguate words in context.
As far as coreference resolution is concerned, the goal of these NLP modules is to determine the boundary of the markables, and to provide the necessary information about each markable for subsequent generation of features in the training examples~~~Our part-of-speech tagger is a standard sta285 tistical bigram tagger based on the Hidden Markov Model HMM <TREF>Church, 1988</TREF>~~~Similarly, we built a statistical HMM-based noun phrase identification module where the noun phrase boundaries are determined solely based on the part-of-speech tags assigned to the words in a sentence~~~We also implemented a module that recognizes MUC-style named entities, ie, organization, person, location, date, time, money, and percent.
1 is a typical example of the ambiguities encountered in a running text: little POS ambiguity, but a lot of gender, number and case ambiguity columns 3 to 5~~~485 3 The Model Instead of employing the source-channel paradigm for tagging more or less explicitly present eg in <REF>Merialdo, 1992</REF>, <TREF>Church, 1988</TREF>, <REF>HajiS, HladkA, 1997</REF> used in the past notwithstanding some exceptions, such as Maximum Entropy and rule-based taggers, we are using here a direct approach to modeling, for which we have chosen an exponential probabilistic model~~~Such model when predicting an event  y E Y in a context x has the general form PAC,eYl x  expEiI ifiy,x Zx 3 where fiY, x is the set of size n of binary-valued yes/no features of the event value being predicted and its context, Ai is a weigth in the exponential sense of the feature fi, and the normalization factor Zx is defined naturally as n Zx   exp Aifiy,x 4 yEY i:1 We use a separate model for each ambiguity class AC which actually appeared in the training data of each of the 13 morphological categories 6~~~The final PAC YlX distribution is further smoothed using unigram distributions on subtags again, separately for each category.
For a larger dataset, such as the Canadian Hansards, it was not possible to check the results by hand~~~We used the same procedure that is used in <TREF>Church 1988</TREF>~~~This procedure was developed by Kathryn Baker unpublished~~~79 Computational Linguistics Volume 19, Number 1 fefQ t e E 0 0 0 0 P, o Ol I 0 .
The accuracy of this data has an impact on the tagging accuracy of both the HMM itself and the derived transducer~~~The training of the HMM can be done on either a tagged or untagged corpus, and is not a topic of this paper since it is exhaustively described in the literature <REF>Bahl and Mercer, 1976</REF>; <TREF>Church, 1988</TREF>~~~An HMM can be identically represented by a weighted FST in a straightforward way~~~We are, however, interested in non-weighted transducers.
The first feature represents the part of speech of the word~~~Vve use an in-house statistical tagger based on <TREF>Church, 1988</TREF> to tag the text in which the unknown word occurs~~~The tag set used is a simplified version of the tags used in the machinereadable version of the Oxford Advanced Learners Dictionary OALD~~~The tag set contains just one tag to identify nouns.
The first major use of HMMs for part of speech tagging was in CLAWS <REF>Garside et al , 1987</REF> in the 1970s~~~With the availability of large corpora and fast computers, there has been a recent resurgence of interest, and a number of variations on and alter53 natives to the FB, Viterbi and BW algorithms have been tried; see the work of, for example, Church <TREF>Church, 1988</TREF>, Brill <REF>Brill and Marcus, 1992</REF>; <REF>Brill, 1992</REF>, DeRose <REF>DeRose, 1988</REF> and gupiec <REF>Kupiec, 1992</REF>~~~One of the most effective taggers based on a pure HMM is that developed at Xerox <REF>Cutting et al , 1992</REF>~~~An important aspect of this tagger is that it will give good accuracy with a minimal amount of manually tagged training data.
Specifically speaking, the content chunking contains two subtasks: 1 to recognize the maximum phrase in a sequence of content words; 2 to analyze the hierarchical structure within the phrase down to words~~~Like baseNP chunking<TREF>Church, 1988</TREF>; <REF>Ramshaw  Marcus 1995</REF>, content chunk parsing is also a kind of shallow parsing~~~Content chunk parsing is deeper than baseNP chunking in two aspects: 1 a content chunk may contain verb phrases and other phrases even a full sentence as long as the all the components are content words; 2 it may contain recursive NPs~~~Thus the content chunk can supply more structural information than a baseNP.
From the perspective of using a lexicalized grammar developed for parsing and importing parsing techniques, our method is similar to the following approaches~~~The Fergus system <REF>Bangalore and Rambow, 2000</REF> uses LTAG Lexicalized Tree Adjoining Grammar <TREF>Schabes et al , 1988</TREF> for generating a word lattice containing realizations and selects the best one using a trigram model~~~<REF>White and Baldridge 2003</REF> developed a chart generator for CCG Combinatory Categorial Grammar <REF>Steedman, 2000</REF> and proposed several techniques for efficient generation such as best-first search, beam thresholding and chunking the input logical forms <REF>White, 2004</REF>~~~Although some of the techniques look effective, the models to rank candidates are still limited to simple language models.
Work on the use of synchronous TAGs to capture quantifier scoping possibilities makes use of so-called multicomponent TAGs~~~Finally, the base TAGs may be lexicalized <TREF>Schabes et al , 1988</TREF> or not~~~Once the base formalism has been decided upon we currently are using lexicalized multi-component TAGs with substitution and adjunction, a simple translation strategy from a source string to a target is to parse the string using an appropriate TAG parser for the base formalism~~~Each derivation of the source string can be mapped according to the synchronizing links in the grammar to a target derivation.
Also, the provision of conceptual entities which are incrementally generated by the semantic interpretation process supplies the necessary anchoring points for the continuous resolution of textual anaphora and ellipses <REF>Strube  Hahn, 1995</REF>; <REF>Hahn et al , 1996</REF>~~~The lexical distribution of grammatical knowledge one finds in many lexiealized grammar formalisms eg , LTAGS <TREF>Schabes et al , 1988</TREF> or HPSG <REF>Pollard  Sag, 1994</REF> is still constrained to declarative notions~~~Given that the control flow of text understanding is globally unpredictable and, also, needs to be purposefully adapted to critical states of the analysis eg , cases of severe extragrammaticality, we drive lexicalization to its limits in that we also incorporate procedural control knowledge at the lexical gr,unmar level~~~The specification of lexiealized communication primitives allows heterogeneous and local lorms of interaction among groups of lexical items.
This will allow for easy maintenance and facilitate updates to the grammar~~~1 Motivations Lexicalized tree-adjoining grammar LTAG <TREF>Schabes et al , 1988</TREF>; <REF>Schabes, 1990</REF> is a tree-rewriting formalism used for specifying the syntax of natural languages~~~It combines elementary lexical trees with two operations, adjoining and substitution~~~In a LTAG, lexical itenm are associated with complex syntactic structures in the form of trees that define the various phrase structures they can participate in.
Formally, a derivation tree is represented as a set of dependencies: D   i,  j,r i , where  i is an elementary tree,   i represents a node in  j where substitution/adjunction has occurred, and r i is a label of the applied rule, ie, adjunction or substitution~~~A probability of derivation tree D   i,  j,r i  is generally defined as follows <TREF>Schabes et al , 1988</TREF>; <REF>Chiang, 2000</REF>~~~pD productdisplay i p i   j,r i  Note that each probability on the right represents the syntactic/semantic preference of a dependency of two lexical items~~~We can readily see that the model is very similar to LPCFG models.
That is, the models are still based on decomposition into primitive lexical dependencies~~~Derivation trees, the structural description in LTAG <TREF>Schabes et al , 1988</TREF>, represent the association of lexical items ie, elementary trees~~~In LTAG, all syntactic constraints of words are described in an elementary tree, and the dependencies of elementary trees, ie, a derivation tree, describe the semantic relations of words more directly than lexicalized parse trees~~~For example, Figure 3 has a derivation tree corresponding to the parse tree in Figure 1 2.
A more linguistically motivated approach is to expand the domain of productions downward to incorporate more tree structures~~~The Lexicalized Tree-Adjoining Grammar LTAG formalism <TREF>Schabes et al , 1988</TREF>, <REF>Schabes, 1990</REF>, although not context-free, is the most well-known instance in this category~~~PLTIGs belong to this third category and generate only context-free languages~~~LTAGs and LTIGs are tree-rewriting systems, consisting of a set of elementary trees combined by tree operations.
Thus CCG assigns the following two groupings to John likes apples: 2 John likes apples 3 John likes apples The work on CCG was presented by Mark Steedman in an earlier DARPA SLS Workshop <REF>Steedman, 1989</REF>~~~In this paper, we show how a CCG-like account for coordination can be constructed in the framework of lexicalized tree-adjoining grammars TAGs <REF>Joshi, 1987</REF>; <TREF>Schabes et al , 1988</TREF>; <REF>Schabes, 1990</REF>~~~2~~~In particular, we show how a fixed constituency can be maintained at the level of the elementary trees of lexicalized TAGs and yet be able to achieve the kind of flexibility needed for dealing with the so-called non-constituents.
which cannot be felicitously uttered except in a context where there is something in the discourse that a restriction could apply to~~~Conventional approaches to subcategorization, such as Definite Clause Grammar <REF>Pereira and Warren, 1980</REF>, Categorial Grammar <REF>Ades and Steedman, 1982</REF>, PATR-II <REF>Shieber, 1986</REF>, and lexicalized TAG <TREF>Schabes et al, 1988</TREF> all deal with complementation by including in one form or another a notion of subcategorization frame that specifies a sequence of complement phrases and constraints on them~~~Handling all the possible variations in complement distribution in such formalisms inevitably leads to an explosion in the number of such frames, and a correspondingly more difficult task in porting to a new domain~~~In our approach, on the other hand, it becomes possible to view subcategorization of a lexical item as a set of constraints on the outgoing arcs of its semantic graph node.
Recently there has been a gain in interest in the so-called mildly context-sensitive formalisms Vijay-<REF>Shanker, 1987</REF>; <REF>Weir, 1988</REF>; <REF>Joshi, VijayShanker, and Weir, 1991</REF>; Vijay-<REF>Shanker and Weir, 1993a</REF> that generate only a small superset of context-free languages~~~One such formalism is lexicalized tree-adjoining grammar LTAG Schabes, Abeill, and <REF>Joshi, 1988</REF>; <REF>Abeillfi et al , 1990</REF>; <REF>Joshi and Schabes, 1992</REF>, which provides a number of attractive properties at the cost of decreased efficiency, On6-time in the worst case <REF>VijayShanker, 1987</REF>; <REF>Schabes, 1991</REF>; <REF>Lang, 1990</REF>; <REF>VijayShanker and Weir, 1993b</REF>~~~An LTAG lexicon consists of a set of trees each of which contains one or more lexical items~~~These elementary trees can be viewed as the elementary clauses including their transformational variants in which the lexical items participate.
Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation~~~At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar LFG <REF>Bresnan, 1982</REF>, Lexicalized Tree Adjoining Grammar LTAG <TREF>Schabes et al , 1988</TREF>, Headdriven Phrase Structure Grammar HPSG <REF>Pollard and Sag, 1994</REF> and Combinatory Categorial Grammar CCG <REF>Steedman, 2000</REF>, which represent deep syntactic structures that cannot be expressed in a shallower formalism designed to represent only aspects of surface syntax, such as the dependency formalism used in current mainstream dependency parsing~~~We present a novel framework that combines strengths from surface syntactic parsing and deep syntactic parsing, specifically by combining dependency and HPSG parsing~~~We show that, by using surface dependencies to constrain the application of wide-coverage HPSG rules, we can benefit from a number of parsing techniques designed for high-accuracy dependency parsing, while actually performing deep syntactic analysis.
This paper will concentrate on context-free grammars CFG and their associated parsers~~~However, virtually all Tree Adjoining Grammars TAG, see eg, <TREF>Schabes et al , 1988</TREF> used in NLP applications can almost be seen as lexicalized Tree Insertion Grammars TIG, which can be converted into strongly equivalent CFGs <REF>Schabes and Waters, 1995</REF>~~~Hence, the parsing techniques and tools described here can be applied to most TAGs used for NLP, with, in the worst case, a light over-generation which can be easily and efficiently eliminated in a complementary pass~~~This is indeed what we have achieved with a TAG automatically extracted from Villemonte de <REF>La Clergerie, 2005</REF>s large-coverage factorized French TAG, as we will see in Section 4.
We can thus define lexical transfer rules that avoid the defects of a mere word-to-word approach but still benefit from the simplicity and elegance of a lexical approach~~~We rely on the French and English LTAG grammars Abeille 1988, Abeille 1990 b, Abeilld et al 1990, Abeill6 and Schabes 1989, 1990 that have been designed over the past two years jointly at University of Pennsylvania and University of Paris 7-Jussieu~~~1 Strategy for Machine Translation with LTAGs The idea of using grammars written with lexicalist formalisms for machine translation is not new This research was partially ftmded by ARO grant DAAG29-84-K-0061, DARPA grant N00014-85-K0018, and NSF grant MCS-82-19196 at the University of Pen nsylvania~~~We are indebted to Stuart Shieber for his valuable comments.
3 A TAG Analysis The TAG formalism for a recent introduction, see <REF>Joshi 1987a</REF> is well suited for linguistic description because 1 it provides a larger domain of locality than a CFG or other augmented CFG-based formalisms such as tlPSG or LFG, and 2 it allows factoring of recursion from the domain of dependencies~~~This extended domain of locality, provided by the elementary trees of TAG, allows us to lexicalize a TAG grammar: we can associate each tree in a grammar with a lexical item <TREF>Schabes et al 1988</TREF>, <REF>Schabes 1990</REF> 4~~~The tree will contain the lexical item, and all of its syntac3Some verbs allow scrambling out of their Complements more freely than others~~~It appears that all subject-control verbs and most object-control verbs governing the dative allow scrambling fairly fely, while scrambling with objectcontrol verbs governing the accusative is more restricted cir.
In Section 8 we conclude with some directions for future work~~~2 Lexicalized Tree-Adjoining Grammar Lexicalized Tree-Adjoining Grammar LTAG <TREF>Schabes et al , 1988</TREF>; <REF>Schabes, 1990</REF> consists of ELEMENTARY TREES, with each elementary tree having a lexical item anchor on its frontier~~~An elementary tree serves as a complex description of the anchor and provides a domain of locality over which the anchor can specify syntactic and semantic predicate-argument constraints~~~Elementary trees are of two kinds a INITIAL TREES and b AUXILIARY TREES.
178 6 Comparison to PSG Approaches One feature of word order domains is that they factor ordering alternatives from the syntactic tree, much like feature annotations do for morphological alternatives~~~Other lexicalized grammars collapse syntactic and ordering information and are forced to represent ordering alternatives by lexical ambiguity, most notable L-TAG <TREF>Schabes et al , 1988</TREF> and some versions of CG <REF>Hepple, 1994</REF>~~~This is not necessary in our approach, which drastically reduces the search space for parsing~~~This property is shared by the proposal of <REF>Reape 1993</REF> to associate HPSG signs with sequences of constituents, also called word order domains.
This high coverage allowed us to evaluate the parser in terms of the accuracy of dependency analysis on real-world texts, the evaluation measure that is previously used for more statistically-oriented parsers~~~2 HPSG Head-Driven Phrase Structure Grammar HPSG is classified into lexicalized grammars <TREF>Schabes et al , 1988</TREF>~~~It attempts to model linguistic phenomena by interactions between a small number of grammar rules and a large number of lexical entries~~~Figure 1 shows an example of an HPSG derivation of a Japanese sentence kare ga shinda, which means, He died In HPSG, linguistic entities such as words and phrases are represented by typed feature structures called signs, and the grammaticality of a sentence is verified by applying grammar rules to a sequence of signs.
We also investigate the reason for that difference~~~Various parsing techniques have been developed for lexicalized grammars such as Lexicalized Tree Adjoining Grammar LTAG <TREF>Schabes et al , 1988</TREF>, and Head-Driven Phrase Structure Grammar HPSG <REF>Pollard and Sag, 1994</REF>~~~Along with the independent development of parsing techniques for individual grammar formalisms, some of them have been adapted to other formalisms <TREF>Schabes et al , 1988</TREF>; van <REF>Noord, 1994</REF>; <REF>Yoshida et al , 1999</REF>; <REF>Torisawa et al , 2000</REF>~~~However, these realizations sometimes exhibit quite different performance in each grammar formalism <REF>Yoshida et al , 1999</REF>; <REF>Yoshinaga et al , 2001</REF>.
Various parsing techniques have been developed for lexicalized grammars such as Lexicalized Tree Adjoining Grammar LTAG <TREF>Schabes et al , 1988</TREF>, and Head-Driven Phrase Structure Grammar HPSG <REF>Pollard and Sag, 1994</REF>~~~Along with the independent development of parsing techniques for individual grammar formalisms, some of them have been adapted to other formalisms <TREF>Schabes et al , 1988</TREF>; van <REF>Noord, 1994</REF>; <REF>Yoshida et al , 1999</REF>; <REF>Torisawa et al , 2000</REF>~~~However, these realizations sometimes exhibit quite different performance in each grammar formalism <REF>Yoshida et al , 1999</REF>; <REF>Yoshinaga et al , 2001</REF>~~~If we could identify an algorithmic difference that causes performance difference, it would reveal advantages and disadvantages of the different realizations.
The parser achieves an OGn6-time worst case behavior, OG2n4-time for unambiguous grammars and linear time for a large class of grammars~~~The parser uses the following two-pass parsing strategy originally defined for lexicalized grammars <TREF>Schabes et al , 1988</TREF> which improves its performance in practice <REF>Schabes and Joshi, 1990</REF>:  In the first step the parser will select, the set of structures corresponding to each word in the sentence~~~Each structure can be considered as encoding a set of rules~~~In the second step, the parser tries to see whether these structures can be combined to obtain a wellformed structure.
XTAG runs under Common Lisp and X Window CLX~~~Tree-adjoining grammar TAG <REF>Joshi et al , 1975</REF>; <REF>Joshi, 1985</REF>; <REF>Joshi, 1987</REF> and its lexicalized variant <TREF>Schabes et al , 1988</TREF>; <REF>Schabes, 1990</REF>; <REF>Joshi and Schabes, 1991</REF> are tree-rewriting systems in which the syntactic properties of words are encoded as tree structured-objects of extended size~~~TAG trees can be combined with adjoining and substitution to form new derived trees~~~1 Tree-adjoining grammar differs from more traditional tree-generating systems such as context-free grammar in two ways: 1.
This information is particularly useful for a top-down component of the parser <REF>Schabes and Joshi, 1990</REF>~~~XTAG provides all the utilities required for designing a lexicalized TAG structured as in Schabes et al 1988~~~All the syntactic concepts of lexicalized TAG such as the grouping of the trees in tree families which represents the possible variants on a basic subcategorization frame are accessible through mouse-sensitive items~~~Also, all the operations required to build a grammar such as load trees, define tree families, load syntactic and morphological lexicon can be predefined with a macro-like language whose instructions can be loaded from a file See Figure 5.
See the introduction by Joshi 1987 for an introduction to tree-adjoining grammar~~~We refer the reader to Joshi 1985, Joshi 1987, Kroch and Joshi 1985, Abeill et al 1990a, Abeill 1988 and to Joshi and Schabes 1991 for more information on the linguistic characteristics of TAG such as its lexicalization and factoring recursion out of dependencies~~~2The TAG derivation tree is the basis for semantic interpretation <REF>Shieber and Schabes, 1990b</REF>, generation <REF>Shieber and Schabes, 1991</REF> and machine translation Abeill et al , 1990b since the information given in this data-structure is richer than the one found in the derived tree~~~Furthermore, it is at the level of the derivation tree that ambiguity must be defined.
Interestingly, several studies suggested that the identification of PropBank annotations would require linguistically-motivated features that can be obtained by deep linguistic analysis <REF>Gildea and Hockenmaier, 2003</REF>; <REF>Chen and Rambow, 2003</REF>~~~They employed a CCG <REF>Steedman, 2000</REF> or LTAG <TREF>Schabes et al , 1988</TREF> parser to acquire syntactic/semantic structures, which would be passed to statistical classifier as features~~~That is, they used deep analysis as a preprocessor to obtain useful features for training a probabilistic model or statistical classifier of a semantic argument identifier~~~These results imply the superiority of deep linguistic analysis for this task.
All errors are of course our own~~~As for Lexicalized TAGs, in <TREF>Schabes et al , 1988</TREF> a two step algorithm has been presented: during the first step the trees corresponding to the input string are selected and in the second step the input string is parsed with respect to this set of trees~~~Another paper by <REF>Schabes and Joshi 1989</REF> shows how parsing strategies can take advantage of lexicalization in order to improve parsers performance~~~Two major advantages have been discussed in the cited work: grammar filtering the parser can use only a subset of the entire grammar and bottom-up information further constraints are imposed on the way trees can be combined.
In <REF>Kroch and Joshi, 1985</REF> a detailed discussion of the linguistic relevance of TAGs can be found~~~Lexicalized Tree Adjoining Grammars <TREF>Schabes et al , 1988</TREF> are a refinement of TAGs such that each elementary tree is associated with a lexieal item, called the anchor of the tree~~~Therefore, Lexicalized TAGs conform to a common tendency in modem theories of grammar, namely the attempt to embed grammatical information within lexical items~~~Notably, the association between elementary trees and anchors improves also parsing performance, as will be discussed below.
Early mechanisms of this sort included categorial grammar Bar-<REF>Hillel, 1953</REF> and subcategorization frames <REF>Chomsky, 1965</REF>~~~Other lexicalized formalisms include <TREF>Schabes et al , 1988</TREF>; Meluk, 1988; <REF>Pollard and Sag, 1994</REF>~~~Besides the possible arguments of a word, a natural-language grammar does well to specify possible head words for those arguments~~~Convene requires an NP object, but some NPs are more semantically or lexically appropriate here than others, and the appropriateness depends largely on the NPs head eg , meeting.
A more linguistically motivated approach is to expand the domain of productions downward to incorporate more tree structures~~~The Lexicalized Tree-Adjoining Grammar LTAG formalism <TREF>Schabes et al, 1988</TREF>, <REF>Schabes, 1990</REF> , although not context-free, is the most well-known instance in this category~~~PLTIGs belong to this third category and generate only context-free languages~~~LTAGs and LTIGs are tree-rewriting systems, consisting of a set of elementary trees combined by tree operations.
E87-1042:140~~~TEMPORAL REASONING IN NATURAL LANGUAGE UNDERSTANDING: THE TEMPORAL STRUCTURE OF THE NARRATIVE Alexander Nakhimovsky Department of Computer Science Colgate University Hamilton, NY 13346 USA CSNet: sashacolgate Abstract This paper proposes a new framework for discourse analysis, in the spirit of <TREF>Grosz and Sidner 1986</TREF>, Webber 1987a,b but differentiated with respect to the type or genre of discourse~~~It is argued that different genres call for different representations and processing strategies; particularly important is the distinction between subjective, pefformative discourse and objective discourse, of which narrative is a primary example~~~This paper concentrates on narratives and introduces the notions of temporal focus proposed also in <REF>Webber 1987b</REF> and narrative move.
Preprocessing the Corpus with a Part of Speech Tagger Phrasal verbs involving the preposition to raise an interesting problem because of the possible confusion with the infinitive marker to~~~We have found that if we first tag every word in the corpus with a part of speech using a method such as <REF>Church 1988</REF> or <TREF>DeRose 1988</TREF>, and then measure associations between tagged words, we can identify interesting contrasts between verbs associated with a following preposition toin and verbs associated with a following infinitive marker toto~~~Part of speech notation is borrowed from <REF>Francis and Kucera 1982</REF>; m  preposition; to  infinitive marker; vb  bare verb; vbg  verb  ing; vbd  verb  ed; vbz  verb  s; vbn  verb  en~~~The score identifies quite a number of verbs associated in an interesting way with to; restricting our attention to pairs with a score of 30 or more, there are 768 verbs associated with the preposition toin and 551 verbs with the infinitive marker toto.
Automatic morphological disambiguation is an important component in higher level analysis of natural language text corpora~~~There has been a large number of studies in tagging and morphological disambiguation using various techniques such as statistical techniques, eg, <REF>Church, 1988</REF>; <REF>Cutting et al , 1992</REF>; <TREF>DeRose, 1988</TREF>, constraint-based techniques <REF>Karlsson et al , 1995</REF>; <REF>Voutilainen, 1995b</REF>; Voutilainen, Heikkil/i, and <REF>Anttila, 1992</REF>; <REF>Voutilainen and Tapanainen, 1993</REF>; <REF>Oflazer and KuruSz, 1994</REF>; <REF>Oflazer and Till 1996</REF> and transformation-based techniques <REF>Brilt, 1992</REF>; <REF>Brill, 1994</REF>; <REF>Brill, 1995</REF>~~~This paper presents a novel approach to constraint based morphological disambiguation which relieves the rule developer from worrying about conflicting rule ordering requirements~~~The approach depends on assigning votes to constraints according to their complexity and specificity, and then letting constraints cast votes on matching parses of a given lexical item.
HMMs have long been central in speech recognition <REF>Rabiner, 1989</REF>~~~Their application to partof-speech tagging <REF>Church, 1988</REF>; <TREF>DeRose, 1988</TREF> kicked off the era of statistical NLP, and they have found additional NLP applications to phrase chunking, text segmentation, word-sense disambiguation, and information extraction~~~The algorithm is also important to teach for pedagogical reasons, as the entry point to a family of EM algorithms for unsupervised parameter estimation~~~Indeed, it is an instructive special case of 1 the inside-outside algorithm for estimation of probabilistic context-free grammars; 2 belief propagation for training singly-connected Bayesian networks and junction trees <REF>Pearl, 1988</REF>; <REF>Lauritzen, 1995</REF>; 3 algorithms for learning alignment models such as weighted edit distance; 4 general finitestate parameter estimation <REF>Eisner, 2002</REF>.
1; they closely replicate Brills results 1993b, page 96, allowing for the fact that his tests used more templates, including templates like if one of the three previous tags is A~~~Brills results demonstrate that this approach can outperform the Hidden Markov Model approaches that are frequently used for part-of-speech tagging <REF>Jelinek, 1985</REF>; <REF>Church, 1988</REF>; <TREF>DeRose, 1988</TREF>; <REF>Cutting et al , 1992</REF>; <REF>Weischedel et al , 1993</REF>, as well as showing promise for other applications~~~The resulting model, encoded as a list of rules, is also typically more compact and for some purposes more easily interpretable than a table of HMM probabilities~~~An Incremental Algorithm It is worthwhile noting first that it is possible in some circumstances to significantly speed up the straightforward algorithm described above.
We describe a part-of-speech tagger built on these principles and we suggest a methodology for developing an adequate training corpus~~~In the part-of-speech hterature, whether taggers are based on a rule-based approach <REF>Klein and Simmons, 1963</REF>, <REF>Brill, 1992</REF>, <REF>Voutilainen, 1993</REF>, or on a statistical one <REF>Bahl and Mercer, 1976</REF>, <REF>Leech et al , 1983</REF>, <REF>Merialdo, 1994</REF>, <TREF>DeRose, 1988</TREF>, <REF>Church, 1989</REF>, <REF>Cutting et al , 1992</REF>, there is a debate as to whether more attention should be paid to lexical probabilities rather than contextual ones~~~<REF>Church, 1992</REF> claims that part-of-speech taggers depend almost exclusively on lexical probabilities, whereas other researchers, such as Voutilainen <REF>Karlsson et al , 1995</REF> argue that word ambiguities vary widely in function of the specific text and genre~~~Indeed, part of Churchs argument is relevant if a system is based on a large corpus such as the Brown corpus Francis and Kuera, 1982 which represents one million surface forms of morpho-syntacticaJly disambiguated words from a range of balanced texts.
The work is based on some similarity metrics~~~ Bahl, <REF>Brown, DeSouza and Mercer, 1989</REF>; Brown, Pietra, deSouza and Mercer,1992; <REF>Chang, 1995</REF>; DeRose,1988; <REF>Garside, 1987</REF>; <REF>Hughes, 1994</REF>; Jardino,1993; <REF>Jelinek, Mercer, and Roukos, 1990b</REF>; Wu, <REF>Wang, Yu and Wang, 1995</REF>; <REF>Magerman, 1994</REF>; <REF>McMahon, 1994</REF>; <REF>McMahon, 1995</REF>; <REF>Pereira, 1992</REF>; <REF>Resnik, 1992</REF>; <REF>Zhao, 1995</REF>; <REF>Brill 1993</REF> and <REF>Pop 1996</REF> present a transformation-based tagging~~~Before a part-of-speech tagger can be built, the word classifications are performed to help us choose a set of part-of-speech~~~They use the sum of two relative entropies obtained from neighboring words as the similarity metric to compare two words.
The training is performed on ambiguity classes and not on individual word tokens~~~<REF>Kallgren 1996</REF> gives a more covering description of how XPOST is used on the Swedish material and also sketches the major differences between this algorithm and some others used for tagging, such as PARTS <REF>Church 1988</REF> and VOLSUNGA <TREF>DeRose 1988</TREF>~~~A characteristic tbature of the SUC is its high number of different tags~~~The number of part-ofspeech tags used in the SUC is 21.
Several workers have addressed the problem of tagging text~~~Methods have ranged from locally-operating rules <REF>Greene and Rubin, 1971</REF>, to statistical methods <REF>Church, 1989</REF>; <TREF>DeRose, 1988</TREF>; <REF>Garside, Leech and Sampson, 1987</REF>; <REF>Jelinek, 1985</REF> and back-propagation <REF>Benello, Mackie and Anderson, 1989</REF>; <REF>Nakamura and Shikano, 1989</REF>~~~The statistical methods can be described in terms of Markov models~~~States in a model represent categories clc n is the number of different categories used.
Indeed, recent increased interest in the problem of disambiguating lexical category in English has led to significant progress in developing effective programs for assigning lexical category in unrestricted text~~~The most successful and comprehensive of these are based on probabilistic modeling of category sequence and word category <REF>Church 1987</REF>; <REF>Garside, Leech and Sampson 1987</REF>; <TREF>DeRose 1988</TREF>~~~These stochastic methods show impressive performance: Church reports a success rate of 95 to 99, and shows a sample text with an error rate of less than one percent~~~What may seem particularly surprising is that these methods succeed essentially without reference to syntactic structure; purely surface lexical patterns are involved.
Part-of-Speech Tagging Many of the very same methods are being applied to problems in natural language processing by many of the very same researchers~~~As a result, the empirical approach has been adopted by almost all contemporary part-of-speech programs: <REF>Bahl and Mercer 1976</REF>, <REF>Leech, Garside, and Atwell 1983</REF>, <REF>Jelinek 1985</REF>, <REF>Deroualt and Merialdo 1986</REF>, <REF>Garside, Leech, and Sampson 1987</REF>, <REF>Church 1988</REF>, <TREF>DeRose 1988</TREF>, <REF>Hindle 1989</REF>, Kupiec 1989, 1992, Ayuso et al~~~1990, de<REF>Marcken 1990</REF>, <REF>Karlsson 1990</REF>, <REF>Boggess, Agarwal, and Davis 1991</REF>, <REF>Merialdo 1991</REF>, and <REF>Voutilainen, Heikkila, and Anttila 1992</REF>~~~These programs input a sequence of words, eg, The chair will table the motion, and output a sequence of part-of-speech tags, eg, art noun modal verb art noun.
4 Concluding remarks Though there can be little doubt that the ruling system of bakeoffs actively encourages a degree of oneupmanship, our paper and our software are not offered in a competitive spirit~~~As we said at the out211 set, we dont necessarily believe HunPos to be in any way better than TnT, and certainly the main ideas have been pioneered by <TREF>DeRose 1988</TREF>, <REF>Church 1988</REF>, and others long before this generation of HMM work~~~But to improve the results beyond what a basic HMM can achieve one needs to tune the system, and progress can only be made if the experiments are end to end replicable~~~There is no doubt many other systems could be tweaked further and improve on our results what matters is that anybody could now also tweak HunPos without any restriction to improve the state of the art.
54 29 3 43 40 4 97 69 7 These results are remarkably good, in spite of the fact that many other systems are reported to reach an accuracy of 9697~~~<REF>Garside 1987</REF>, <REF>Marshall 1987</REF>, <TREF>DeRose 1988</TREF>, <REF>Church 1988</REF>, <REF>Ejerhed 1987</REF>, O<REF>Shaughnessy 1989</REF>~~~Those systems, however, all use heavier artillery than MorP, that has been deliberately restricted in accordance with the hypotheses presented above~~~This restrictiveness concerns both the size of the lexicon and the ways of carrying out disambiguation.
Although finite-state machines have been used for part-of-speech tagging <REF>Tapanainen and Voutilainen 1993</REF>; <REF>Silberztein 1993</REF>, none of these approaches has the same flexibility as stochastic techniques~~~Unlike stochastic approaches to part-of-speech tagging <REF>Church 1988</REF>; <REF>Kupiec 1992</REF>; <REF>Cutting et al 1992</REF>; <REF>Merialdo 1990</REF>; <TREF>DeRose 1988</TREF>; <REF>Weischedel et al 1993</REF>, up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired~~~<REF>Recently, Brill 1992</REF> described a rule-based tagger that performs as well as taggers based upon probabilistic models and overcomes the limitations common in rule-based approaches to language processing: it is robust and the rules are automatically ac Mitsubishi Electric Research Laboratories, 201 Broadway, Cambridge, MA 02139~~~E-mail: rocbe/schabesmerlcom.
Research on corpus-based natural language learning and processing is rapidly accelerating following the introduction of large on-line corpora, faster computers, and cheap storage devices~~~Recent work involves novel ways to employ annotated corpus in part of speech tagging <REF>Church 1988</REF> <REF>Derose 1988</REF> and the application of mutual information statistics on the corpora to uncover lexical information <REF>Church 1989</REF>~~~The goal of the research is the construction of robust and portable natural language processing systems~~~The wide range of topics available on the Internet calls for an easily adaptable information extraction system for different domains.
There has recently been a great deal of work exploring methods for automatically training part of speech taggers, as an alternative to laboriously hand-crafting rules for tagging, as was done in the past <REF>Klein and Simmons, 1963</REF>; <REF>Harris, 1962</REF>~~~Almost all of the work in the area of automatically trained taggers has explored Markov-model based part of speech tagging <REF>Jelinek, 1985</REF>; <REF>Church, 1988</REF>; <REF>Derose, 1988</REF>; <REF>DeMarcken, 1990</REF>; <REF>Cutting et al , 1992</REF>; <REF>Kupiec, 1992</REF>; <REF>Charniak et al , 1993</REF>; <REF>Weischedel et al , 1993</REF>; <REF>Schutze and Singer, 1994</REF>; <REF>Lin et al , 1994</REF>; <REF>Elworthy, 1994</REF>; <REF>Merialdo, 1995</REF>~~~2 For a Markov-model based tagger, training consists of learning both lexical probabilities Pwordltag and contextual probabilities Ptagiltagil tagi-n~~~Once trained, a sentence can be tagged by searching for the tag sequence that maximizes the product of lexical and contextual probabilities.
Unknown Words Unknown Common Words Unknown Proper Nouns Tagger Guesser Metrics Error Error Coverage Error Coverage HMM Xerox mean 17851643 30022169 37567270 10785563 63797113 s-error 0484710 0469922 1687396 0613745 1714969 HMM Cascade mean 12378716 21266264 36507909 7776456 64795969 s-error 0917656 0403957 2336381 0853958 2206457 Brill Brill mean 14688501 27411736 38998687 6439525 62160917 s-error 0908172 0539634 2627234 0501082 4010992 Brill Cascade mean 11327863 20986240 37933048 5548990 63816586 s-error 0761576 0480798 2353510 0561009 3775991 the Brown Corpus, we obtained the error rate mean 0 4003093 with the standard error deB0155599~~~This agrees with the results on the closed dictionary ie , without unknown words obtained by other researchers for this class of the model on the same corpus <REF>Kupiec 1992</REF>; <TREF>DeRose 1988</TREF>~~~The Brill tagger showed some better results: error rate mean 0 3327366 with the standard error deBO 123903~~~Although our primary goal was not to compare the taggers themselves but rather their performance with the guessing components, we attribute the difference in their performance to the fact that Brills tagger uses the information about the most likely tag for a word whereas the HMM tagger did not have this information and instead used the priors for a set of POS-tags ambiguity class.
A lot of effort has been devoted in the past to the problem of tagging text, ie assigning to each word the correct tag part of speech in the context of the sentence~~~Two main approaches have generally been considered: rule-based <REF>Klein and Simmons 1963</REF>; <REF>Brodda 1982</REF>; <REF>Paulussen and Martin 1992</REF>; <REF>Brill et al 1990</REF> probabilistic <REF>Bahl and Mercer 1976</REF>; <REF>Debili 1977</REF>; <REF>Stolz, Tannenbaum, and Carstensen 1965</REF>; <REF>Marshall 1983</REF>; <REF>Leech, Garside, and Atwell 1983</REF>; <REF>Derouault and Merialdo 1986</REF>; <TREF>DeRose 1988</TREF>; <REF>Church 1989</REF>; <REF>Beale 1988</REF>; <REF>Marcken 1990</REF>; <REF>Merialdo 1991</REF>; <REF>Cutting et al 1992</REF>~~~More recently, some work has been proposed using neural networks <REF>Benello, Mackie, and Anderson 1989</REF>; <REF>Nakamura and Shikano 1989</REF>~~~Multimedia Communications Department, Institut EURECOM, 2229 Route des Cretes, BP 193, 06904 Valbonne Cedex France; merialdoeurecomfr.
for evaluation at word level, choose the most probable tag for each word in the sentence argmax argmax Wi  t pti  t/W  t  pW, T T:tit where Wi is the tag assigned to word wi by the tagging procedure b in the context of the sentence W, We call this procedure Maximum Likelihood ML tagging~~~It is interesting to note that the most commonly used method is Viterbi tagging see <TREF>DeRose 1988</TREF>; <REF>Church 1989</REF> although it is not the optimal method for evaluation at word level~~~The reasons for this preference are presumably that:  Viterbi tagging is simpler to implement than ML tagging and requires less computation although they both have the same asymptotic complexity  Viterbi tagging provides the best interpretation for the sentence, which is linguistically appealing  ML tagging may produce sequences of tags that are linguistically impossible because the choice of a tag depends on all contexts taken together~~~However, in our experiments, we will show that Viterbi and ML tagging result in very similar performance.
Although methods for unsupervised training of HMMs do exist, training is usually done in a supervised way by estimation of the above probabilities from relative frequencies in the training data~~~The HMM approach to tagging is by far the most studied and applied <REF>Church 1988</REF>; <TREF>DeRose 1988</TREF>; <REF>Charniak 1993</REF>~~~In van <REF>Halteren, Zavrel, and Daelemans 1998</REF> we used a straightforward implementation of HMMs, which turned out to have the worst accuracy of the four competing methods~~~In the present work, we have replaced this by the TnT system we will refer to this tagger as HMM below.
<REF>Choueka and Lusignan 1985</REF> presented a system for the morphological tagging of large texts that is based on the short context of the word but also depends heavily on human interaction~~~Methods using the short context of a word in order to resolve ambiguity usually categorical ambiguity are very common in English and other languages <TREF>DeRose 1988</TREF>; <REF>Church 1988</REF>; <REF>Karlsson 1990</REF>~~~A system using this approach was developed by Levinger and Ornan in order to serve as a component in their project of morphological disambiguation in Hebrew <REF>Levinger 1992</REF>~~~The main resource, used by this system for disambiguation, is a set of syntactic constraints that were defined manually by the authors and followed two theoretical works that defined short context rules for Hebrew <REF>Pines 1975</REF>; <REF>Albeck 1992</REF>.
Most successful methods have followed speech recognition systems <REF>Jelinek, Mercer, and Roukos 1992</REF> and used large corpora to deduce the probability of each part of speech in the current context usually the two previous words--trigrams~~~These methods have reported performance in the range of 95-99 correct by word <TREF>DeRose 1988</TREF>; <REF>Cutting et al 1992</REF>; <REF>Jelinek, Mercer, and Roukos 1992</REF>; <REF>Kupiec 1992</REF>~~~The difference in performance is due to different evaluation methods, different tag sets, and different corpora~~~<REF>See Church 1992</REF> for a survey.
More precisely, assume that the word wh occurs in a sentence W  wlWkwn, and that w is a word we are considering substituting for it, yielding sentence W I Word w is then preferred over wk iff PW > PW, where PW and PW are the probabilities of sentences W and W f respectively~~~1 We calculate PW using the tag sequence of W as an intermediate quantity, and summing, over all possible tag sequences, the probability of the sentence with that tagging; that is: PW   PW, T T where T is a tag sequence for sentence W The above probabilities are estimated as is traditionally done in trigram-based part-of-speech tagging <REF>Church, 1988</REF>; <TREF>DeRose, 1988</TREF>: PW,T  PWITPT  1  HPwiti HPt, lt,2t,l2 i i where T  tltn, and Ptitl-2ti-1 is the prob ability of seeing a part-of-speech tag tl given the two preceding part-of-speech tags ti-2 and ti-1~~~Equations 1 and 2 will also be used to tag sentences W and W  with their most likely part-of-speech sequences~~~This will allow us to determine the tag that 1To enable fair comparisons between sequences of different length as when considering maybe and may be, we actually compare the per-word geometric mean of the sentence probabilities.
More recently, Koskenniemi also used a rule-based approach implemented with finite-state machines <REF>Koskenniemi, 1990</REF>~~~Statistical methods have also been used eg , <TREF>DeRose, 1988</TREF>, <REF>Garside et al , 1987</REF>~~~These provide the capability of resolving ambiguity on the basis of most likely interpretation~~~A form of Markov model has been widely used that assumes that a word depends probabilistically on just its part-of-speech category, which in turn depends solely on the categories of the preceding two words.
Estimating the Lexical Priors for Rare Forms For a common form such as lopen walk a reasonable estimate of the lexical prior probabilities is the MLE, computed over all occurrences of this form~~~So, in the UdB corpus, lopen occurs 92 times as an infinitive and 43 times as a finite plural, so the MLE 1 Even models of disambiguation that make use of context, such as statistical n-gram taggers, often presume some estimate of lexical priors, in addition to requiring estimates of the transition probabilities of sequences of lexical tags <REF>Church 1988</REF>; <TREF>DeRose 1988</TREF>; <REF>Kupiec 1992</REF>, and this again brings up the question of what to do about unseen or low-frequency forms~~~In working taggers, a common approach is simply to apply a uniform small probability to the various senses of unseen or low-frequency forms: this was done in the tagger discussed in <REF>Church 1988</REF>, for example~~~156 Baayen and Sproat Lexical Priors for Low-Frequency Forms to> 8 Figure 1 I I I I 0 2 4 6 log frequency class Relative frequency of Dutch infinitives versus finite plurals in the Uit den Boogaart corpus, as a function of the natural log of the frequency of the word forms.
Its recall is very high 997 of all words receive the correct morphological analysis, but this system leaves 3-7 of all words ambiguous, trading precision for recall~~~157 ena or the linguists abstraction capabilities eg knowledge about what is relevant in the context, they tend to reach a 95-97 accuracy in the analysis of several languages, in particular English <REF>Marshall 1983</REF>; Black et aL 1992; <REF>Church 1988</REF>; <REF>Cutting et al 1992</REF>; de <REF>Marcken 1990</REF>; <TREF>DeRose 1988</TREF>; <REF>Hindle 1989</REF>; <REF>Merialdo 1994</REF>; <REF>Weischedel et al 1993</REF>; <REF>Brill 1992</REF>; <REF>Samuelsson 1994</REF>; Eineborg and Gambick 1994, etc~~~Interestingly, no significant improvement beyond the 97 barrier by means of purely data-driven systems has been reported so far~~~In terms of the accuracy of known systems, the data-driven approach seems then to provide the best model of part-of-speech distribution.
This will partly be based on another important step in the process, namely the construction of constituents, in particular noun phrases and prepositional phrases <REF>Church 1988</REF>, <REF>Kfillgren 1984c</REF>, and partly on a more general algorithm that for pairs or longer sequences of tags calculates the relative probability of alternative tag assignments~~~The principles behind such algorithms are known, but they have never been tried on Swedish material <TREF>DeRose 1988</TREF>, <REF>Marshall 1987</REF>, <REF>EegOlofsson 1985</REF>~~~An indispensable step in the disambiguation process is the assignment of clause boundaries, which presupposes established constituents at the same time as it forms an important basis for disambiguating chains of tags~~~Methods for this are being tested out on Swedish material <REF>Ejerhed 1989</REF>.
There are a number of large tagged corpora available, allowing for a variety of experiments to be run~~~Part-of-speech tagging is an active area of research; a great deal of work has been done in this area over the past few years eg , <REF>Jelinek 1985</REF>; <REF>Church 1988</REF>; <REF>Derose 1988</REF>; <REF>Hindle 1989</REF>; <REF>DeMarcken 1990</REF>; <REF>Merialdo 1994</REF>; <REF>Brill 1992</REF>; <REF>Black et al 1992</REF>; <REF>Cutting et al 1992</REF>; <REF>Kupiec 1992</REF>; <REF>Charniak et al 1993</REF>; <REF>Weischedel et al 1993</REF>; <REF>Schutze and Singer 1994</REF>~~~Part-of-speech tagging is also a very practical application, with uses in many areas, including speech recognition and generation, machine translation, parsing, information retrieval and lexicography~~~Insofar as tagging can be seen as a prototypical problem in lexical ambiguity, advances in part-of-speech tagging could readily translate to progress in other areas of lexical, and perhaps structural, ambiguity, such as wordsense disambiguation and prepositional phrase attachment disambiguation.
However, stochastic taggers have the disadvantage that linguistic information is captured only indirectly, in large tables of statistics~~~Almost all recent work in developing automatically trained part-of-speech taggers has been on further exploring Markovmodel based tagging <REF>Jelinek 1985</REF>; <REF>Church 1988</REF>; <REF>Derose 1988</REF>; <REF>DeMarcken 1990</REF>; <REF>Merialdo 1994</REF>; <REF>Cutting et al 1992</REF>; <REF>Kupiec 1992</REF>; <REF>Charniak et al 1993</REF>; <REF>Weischedel et al 1993</REF>; <REF>Schutze and Singer 1994</REF>~~~41 Transformation-based Error-driven Part-of-Speech Tagging Transformation-based part of speech tagging works as follows~~~9 The initial-state annotator assigns each word its most likely tag as indicated in the training corpus.
Statistical taggers usually work as follows: First, each word in the input word string 1471,   , W, is assigned all possible tags according to the lexicon, thereby creating a lattice~~~A dynamic programming technique is then used to find tag the sequence 5/,, , that maximizes PT1,,Tn I Wl,   , Wn  tt  IIPTk T1,,Tk-1;Wl,,Wn kl 1:1 PTk Tk-NI,,Tk-1; VIZk  7 PT wk PTk k-Nl,, k-l  kl fl PTk PT, Tk-N,, - PWk I Tk : PWk Since the maximum does not depend on the factors PWk, these can be omitted, yielding the standard statistical PoS tagging task: max - PTk IU-V,,Tk-JPWk JT TI,,T, tl This is well-described in for example <TREF>DeRose 1988</TREF>~~~We thus have to estimate the two following sets of probabilities:  Lexical probabilities: The probability of each tag T i conditional on the word W that is to be tagged, pr I I wr~~~i Often the converse probabilities PW are given instead, but we will for reasons soou to become apparent use the former formulation.
Recent research advances may lead to the development of viable book indexing methods for Chinese books~~~These include the availability of efficient and high precision word segmentation methods for Chinese text <REF>Chang et al , 1991</REF>; <REF>Sproat and Shih, 1990</REF>; <REF>Wang et al , 1990</REF>, the availability of statistical analysis of a Chinese corpus <REF>Liu et al , 1975</REF> and large-scale electronic Chinese dictionaries with partof-speech information <REF>Chang et al , 1988</REF>; BDC, 1992, the corpus-based statistical part-of-speech tagger <REF>Church, 1988</REF>; <TREF>DeRose, 1988</TREF>; <REF>Beale, 1988</REF>, as well as phrasal and clausal analyzers <REF>Church 1988</REF>; <REF>Ejerhed 1990</REF> 2~~~Problem description As being pointed out in <REF>Salton, 1988</REF>, back-of-book indexes may consist of more than one word that are derived from a noun phrase~~~Given the text of a book, an indexing system, must perform some kind of phrasal and statistical analysis in order to produce a list of candidate indexes and their occurrence statistics in order to generate indexes as shown in Figure 1 which is an excerpt from the reconstruction of indexes of a book on transformational grammar for Mandarin Chinese <REF>Tang, 1977</REF>.
In this paper, we report on experimental work dealing with the part-of-speech tagging of a corpus of transcribed spoken Swedish~~~The tagger used implements a standard probabilistic biclass model see, e g, <TREF>DeRose 1988</TREF> trained on a tagged subset of the Stockhohn-Ume Corpus of written Swedish <REF>Ejerhed et al 1992</REF>~~~Given that the transcriptions contain many modifications of standard orthography in order to capture spoken language variants, reductions, etc~~~a special lexicon had to be developed to map spoken langnage variants onto their canonical written language forms.
Several approaches have been proposed to construct automatic taggers~~~Most work on statistical methods has used n-gram models or Hidden Markov Model-based taggers eg <REF>Church, 1988</REF>; <TREF>DeRose, 1988</TREF>; <REF>Cutting et al 1992</REF>; <REF>Merialdo, 1994</REF>, etc~~~In 14 these approaches, a tag sequence is chosen for a sentence that maximizes the product of lexical and contextual probabilities as estimated from a tagged corpus~~~In rule-based approaches, words are assigned a tag based on a set of rules and a lexicon.
Categorical ambiguity, however, is of a different kind and is resolved in a different way~~~For the purposes of the present paper, it will be assumed that only content words are at issue, and that the syntactic category of all content words in the text that is under study can be determined automatically <REF>Church, 1988</REF>; <TREF>DeRose, 1988</TREF>~~~The problem is simply to decide which sense of a content word--noun, verb, adjective, or adverb---is appropriate in a given linguistic context~~~It will also be assumed that sense resolution for individual words can be accomplished on the basis of information about the irnrnediate linguistic context.
Purely rule-based techniques seemed too brittle for dealing with the variety of constructions, the long sentences averaging 29 words per sentence, and the degree of unexpected input~~~Statistical models based on local information eg , <TREF>DeRose 1988</TREF>; <REF>Church 1988</REF> might operate effectively in spite of sentence length and unexpected input~~~To see whether our four hypotheses in italics above effectively addressed the four concerns above, we chose to test the hypotheses on two well-known problems: ambiguity both at the structural level and at the part-of-speech level and inferring syntactic and semantic information about unknown words~~~Guided by the past success of probabilistic models in speech processing, we have integrated probabilistic models into our language processing systems.
We report in Section 2 on our experiments on the assignment of part of speech to words in text~~~The effectiveness of such models is well known <TREF>DeRose 1988</TREF>; <REF>Church 1988</REF>; <REF>Kupiec 1989</REF>; <REF>Jelinek 1985</REF>, and they are currently in use in parsers eg de <REF>Marcken 1990</REF>~~~Our work is an incremental improvement on these models in three ways: 1 Much less training data than theoretically required proved adequate; 2 we integrated a probabilistic model of word features to handle unknown words uniformly within the probabilistic model and measured its contribution; and 3 we have applied the forward-backward algorithm to accurately compute the most likely tag set~~~In Section 3, we demonstrate that probability models can improve the performance of knowledge-based syntactic and semantic processing in dealing with structural ambiguity and with unknown words.
Trento, Italy, pages 133-140, Association for Computational Linguistics~~~Steven J <TREF>DeRose 1988</TREF>~~~Grammatical Category Disambiguation by Statistical Optimization~~~Computational Linguistics, 141 :31-39.
The first major use of HMMs for part of speech tagging was in CLAWS <REF>Garside et al , 1987</REF> in the 1970s~~~With the availability of large corpora and fast computers, there has been a recent resurgence of interest, and a number of variations on and alter53 natives to the FB, Viterbi and BW algorithms have been tried; see the work of, for example, Church <REF>Church, 1988</REF>, Brill <REF>Brill and Marcus, 1992</REF>; <REF>Brill, 1992</REF>, DeRose <TREF>DeRose, 1988</TREF> and gupiec <REF>Kupiec, 1992</REF>~~~One of the most effective taggers based on a pure HMM is that developed at Xerox <REF>Cutting et al , 1992</REF>~~~An important aspect of this tagger is that it will give good accuracy with a minimal amount of manually tagged training data.
Different accessibility relations can be modeled, eg to distinguish a local context for resolving reflexive anaphors like himself  from a global context <REF>Kruijff, 2001</REF>~~~Finally, the rich temporal ontology underlying models of tense and aspect such as <TREF>Moens and Steedman 1988</TREF> can be captured using the sorting strategy~~~Earlier work like <REF>Blackburn and Lascarides 1992</REF> already explored such ideas~~~HLDS employs hybrid logic to integrate Moens and Steedmans notion of the event nucleus directly into meaning representations.
However, at least 30 of the dates and times in the MUC test were fixed-format ones occurring in document headers, trailers, and copyright notices~~~ Finally, there is a large body of work, eg, <TREF>Moens and Steedman 1988</TREF>, <REF>Passoneau 1988</REF>, <REF>Webber 1988</REF>, <REF>Hwang 1992</REF>, <REF>Song and Cohen 1991</REF>, that has focused on a computational analysis of tense and aspect~~~While the work on event chronologies is based on some of the notions developed in that body of work, we hope to further exploit insights from previous work~~~Conclusion We have developed a temporal annotation specification, and an algorithm for resolving a class of time expressions found in news.
Other aspects of our ontology are designed following proposals by <REF>Jackendoff 1990</REF>, in particular his analysis of movement events~~~2 <TREF>Moens and Steedman 1988</TREF> also use this term, but they restrict it to momentaneous events~~~Unfortunately, the terminology used in the literature for these kinds of categories varies so much that a standardization seems out of reach~~~404 Stede Verb Alternations event1 fill conf---- > not-full -state-1    f > pa>btination  fill-state-2   water-1  value > full Figure 2 SitSpec representing a fill-event.
As their central feature we take them to always involve some change of state: the building loses its integrity, the book comes into existence, or gets finished~~~<REF>While Bach 1986</REF> did not investigate the internal structure of events, others suggested that this needs to be done eg , <TREF>Moens and Steedman 1988</TREF>; <REF>Parsons 1990</REF>~~~<REF>Pustejovsky 1991</REF> treated Vendlerian accomplishments and achievements as transitions from a state Qy to NOT-Qy, and suggested that accomplishments in addition have an intrinsic agent performing an activity that brings about the change of state~~~We follow this line, but modify it in some ways.
In this paper, we show how one can find and exploit approximate solutions for both of these problems by capitalizing on the occurrences of certain lexicogrammatical constructs~~~Such constructs can include tense 96 and aspect <TREF>Moens and Steedman, 1988</TREF>; <REF>Webber, 1988</REF>; <REF>Lascarides and Asher, 1993</REF>, certain patterns of pronominalization and anaphoric usages <REF>Sidner, 1981</REF>; <REF>Grosz and Sidner, 1986</REF></REF>; <REF>Sumita et al , 1992</REF>; <REF>Grosz, Joshi, and Weinstein, 1995</REF>,/t-clefts <REF>Delin and Oberlander, 1992</REF>, and discourse markers or cue phrases <REF>Ballard, Conrad, and Longacre, 1971</REF>; <REF>Halliday and Hasan, 1976</REF>; <REF>Van Dijk, 1979</REF>; <REF>Longacre, 1983</REF>; <REF>Grosz and Sidner, 1986</REF></REF>; <REF>Schiffrin, 1987</REF>; <REF>Cohen, 1987</REF>; <REF>Redeker, 1990</REF>; <REF>Sanders, Spooren, and Noordman, 1992</REF>; <REF>Hirschberg and Litman, 1993</REF>; <REF>Knott, 1995</REF>; <REF>Fraser, 1996</REF>; <REF>Moser and Moore, 1997</REF>~~~In the work described here, we investigate how far we can get by focusing our attention only on discourse markers and lexicogrammatical constructs that can be detected by a shallow analysis of natural language texts~~~The intuition behind our choice relies on the following facts:  Psycholinguistic and other empirical research <REF>Kintsch, 1977</REF>; <REF>Schiffrin, 1987</REF>; <REF>Segal, Duchan, and Scott, 1991</REF>; <REF>Cahn, 1992</REF>; <REF>Sanders, Spooren, and Noordman, 1992</REF>; <REF>Hirschberg and Litman, 1993</REF>; <REF>Knott, 1995</REF>; <REF>Costermans and Fayol, 1997</REF> has shown that discourse markers are consistently used by human subjects both as cohesive ties between adjacent clauses and as macroconnectors between larger textual units.
The feature specification of this ompositionally derived accomplishment is therefore identical to that of a sentence containing a telic accomplishment verb, such as destroy~~~According to many researchers, knowledge of lexical aspect--how verbs denote situations as developing or holding in time-may be used to interpret event sequences in discourse <REF>Dowty, 1986</REF>; <TREF>Moens and Steedman, 1988</TREF>; <REF>Passoneau, 1988</REF>~~~In particular, Dowty suggests that, absent other cues, a relic event is interpreted as completed before the next event or state, as with ran into lhe room in 4a; in contrast, atelic situations, such as run, was hungry in 4b and 4 are interpreted as contemporaneous with the following situations: fell and made a pizza, respectively~~~4 a Mary ran into the room.
In 10, s is the consequent state of the event of John greeting Max, and it holds at the time t which precedes now~~~So our semantics of the perfect is like that in <TREF>Moens and Steedman 1988</TREF>: a perfect transforms an event into a consequent state, and asserts that the consequent state holds~~~The pluperfect of a state, such as 11, therefore, is assumed to first undergo a transformation into an event~~~11 John had loved Mary.
Other genres of text might depend on other kinds of structuring devices~~~Changes in time scale or time, as we redefined the category, may require world knowledge reasoning to recoguize but are often indicated by either cue words and phrases eg , n/he years ago ,  a year, for months; several months ago, a change in grammatical time of the verb eg , past tense versus present tense, or changes in aspect eg , atomic versus extended events versus states as defined by <TREF>Moens  Steedman 1988</TREF>~~~In considering how time change might affect anaphoric expression choice, we consider the choice for the first mention of a discourse entity in a sentence where that entity has recently been referred to in the discourse~~~Our hypothesis is that: Changes in time reliably signal changes of the thread in newspaper articles; definite descriptions should appear when the current reference to a discourse entity is in a different thread from the last reference to that entity and pronouns should occur when the previous mention is in the same thread 3.
The imperfective point of view brought by IMP imposes a change of point of view on the term of the eventuality~~~As for accomplishments, we can assume that they can be decomposed into several stages, according to <TREF>Moens and Steedman, 1988</TREF>: first a preparatory phase, second a culmination or achievement we are not concerned here with the result state~~~We can then say that IMP refers only to the preparatory phase, so that the term of the eventuality loses all relevance~~~This explains the so-called imperfective paradox: it is possible to use IMP even though the eventuality never reaches its term: 6 a I1 traversait la rue quand la voiture la 6cras6 He was crossing the street when the car hit him b  I1 traversa la rue quand la voiture la 6cras6 He crossed the street when the car hit him As for achievements, we can assume that they are reduced to a culmination.
Secondly, it has been argued that A when B permits many possible temporal relationships between the eventualities denoted by A and B cf~~~<TREF>Moens and Steedman 1988</TREF>; its for this reason that 2c can be interpreted as denoting El; 260 but given this permissiveness, why is 2d not as acceptable as 2c~~~3 The basic explanation: temporal presuppositions The basic explanation for the inappropriateness of 2b and 2d is actually quite simple~~~Sentences containing temporal connectives are presuppositional: the temporal clause introduces an eventuality that must be presupposed to have occurred, for the sentence as a whole to have a truth-value cf.
Second, with when, the almost equal distribution of preposed and postposed tokens suggests either free variation of the two patterns or different uses of the two patterns, with each use favoring a different pattern~~~The latter would accord with a theoretical distinction that has been made between postposed when expressing a purely temporal relation between the two clauses, and preposed when expressing a contingent relation between them <TREF>Moens and Steedman, 1988</TREF>~~~Integrated evidence from the PTB and PropBank may help distinguish the two possibilities~~~Third, there is a striking contrast between the patterning of although and even though, especially if one assumes that even though like even when, even after, even if, etc.
Weneedamuchbettermodelofhowtocommunicate time, and how this communication depends on the semantics and linguistic expression of the events being described~~~An obvious first step, which we are currently working on, is to include a linguisticallymotivatedtemporalontology <REF>MoensandSteedman, 1988</REF>, which will be separate from the existing domain ontology~~~We also need better techniques for communicating the temporal relationships between events in cases where they are not listed in chronological order <REF>Oberlander and Lascarides, 1992</REF>~~~6 Discussion Two discourse analysts from Edinburgh University, Dr Andy McKinlay and Dr Chris McVittie, kindly examined and compared some of the human and BT45 texts.
The domain is limited to trajectoryof-motion events specified by the verbs run, jog, sit is worth noting that as an alternative to positing a lexical ambiguity, one could just as easily invoke a coercion operator on an event predicate Pz mapping it to the process predicate he~~~plurPx  e, which would bring the present treatment more in line with <TREF>Moens and Steedman 1988</TREF> and <REF>Jackendoff 1991</REF>~~~plod, and walk; the locative prepositions to, towards, from, away from, along, eastwards, westwards, and to and back; various landmarks; the distance adverbials n miles; the frequency adverbials twice and n times; and finally the temporal adverbials for and in~~~Trajectory-of-motion events are modeled as continuous constant rate changes of location in one dimension of the TRAJECTOR relative to one or more LANDMARKS following <REF>Regier 1992</REF> in his use of Langackers 1987 terminology.
2 3 Theory 31 Ontology Various authors including <REF>Link, 1983</REF>, <REF>Bach, 1986</REF>, <REF>Krifka, 1989</REF>, <REF>Eberle, 1990</REF> have proposed modeltheoretic treatments in which a parallel ontological distinction is made between substances and things, processes and events, etc A similarly parallel distinction is employed here, but in a rather different way: unlike the above treatments, the present account models substances, processes, and other such entities as abstract kinds whose realizations vary in amount~~~As such, the approach developed here may be seen as building upon the work of <REF>Carlson 1977</REF> and his successors; it also represents one way to further formalize the intuitions found in <TREF>Moens and Steedman 1988</TREF> and <REF>Jackendoff 1991</REF>~~~<REF>Following Schubert and Pelletier 1987</REF>, the present account distinguishes individuals from kinds, but not from stages or quantities~~~Extending their ontology, the same distinction is assumed to hold not only in the domain of materials but also in the domain of eventualities, and derivatively in the domains of space and time as well.
Note that in a terminal DRS ready for an embedding test, all the auxiliary Rpts disappear do not participate in the embedding~~~The perfect is analyzed by using the notion of a nucleus <TREF>Moens and Steedman, 1988</TREF> to account for the inner structure of an eventuality~~~A nucleus is defined as a structure containing a preparatory process, culmination and consequent state~~~The categorization of verb phrases into different aspectual classes can be phrased in terms of which part of the nucleus they refer to.
Moreover, the semantic category of these features can also play a role~~~For example, Sue played the piano is nonc,lminated, while Sue played the sonata signifies a c-lminated event this example comes from <TREF>Moens and Steedman 1988</TREF>~~~32 Classes of Ambiguous Verbs Placing aspectually ambiguous verbs into semantic categories will help predict how these verbs combine with their arguments to determine aspectual class~~~This is because many verbs with related meanings combine with their arguments in similar ways.
For example, shaw denotes a state in, H/ lumbar puncture showed evidence of white cells, but denotes an event in, He showed me the photographs~~~This ambiguity presents a diculty for automatically classifying a verb because the aspectual class of a clause is a function of several clausal constituents in addition to the main verb <REF>Dowry, 1979</REF>; <TREF>Moens and Steedman, 1988</TREF>; <REF>Pustejovsky, 1991</REF>~~~However, previous work that numerically evaluates aspectual classification has looked at verbs in isolation <REF>Klavans and Chodorow, 1992</REF>; <REF>Siegel, 1997</REF>~~~10 The verb have is particularly problematic.
For example, I made a fire is culminated, whereas, I gazed at the sunset is non-culminated~~~Aspectual classification is necessary for interpreting temporal modifiers and assessing temporal entailments <TREF>Moens and Steedman, 1988</TREF>; <REF>Dorr, 1992</REF>; <REF>Klavans, 1994</REF>, and is therefore a necessary component for applications that perform certain language interpretation, summarization, information retrieval, and machine translation tasks~~~Aspectual classification is a diflqcult problem because many verbs, like have, are aspectually ambiguous~~~In this paper, I demonstrate that verbs can be disambiguated according to aspect by the semantic category of the direct object.
Finally, Section 8 provides conclusions and describes future work~~~2 Aspect in Natural Language Aspectual classification is a key component of models that assess temporal constraints between clauses <TREF>Moens and Steedman, 1988</TREF>; <REF>Hwang and Schubert, 1991</REF>; <REF>Dorr, 1992</REF>; <REF>Hitzeman et al , 1994</REF>~~~For example, stativity must be identified to detect temporal constraints between clauses connected with when~~~For example, in interpreting I, I She had good strength when objectively tested.
In 26i, the event is associated with the features d,t,-a, whereas, in 26ii the event is associated with the features d,t,a~~~According to <REF>Bennett et al , 1990</REF> in the spirit of <TREF>Moens and Steedman, 1988</TREF>, predicates are allowed to undergo an atomicity coercion in which an inherently non-atomic predicate such as dio may become atomic under certain conditions~~~These conditions are language-specific in nature, ie, they depend on the lexical-semantic structure of the predicate in question~~~Given the featural scheme that is imposed on top of the lexical-semantic framework, it is easy to specify coercion functions for each language.
In light of these observations, the lexicM-semantic structure adopted for UNITRAN is an augmented form of Jackendoffs representation in which events are distinguished from states as before, but they are further subdivided into activities, achievements, and accomplishments~~~The subdivision is achieved by means of three features proposed by <REF>Bennett et al , 1990</REF> following the framework of <TREF>Moens and Steedman, 1988</TREF> in the spirit of <REF>Dowty, 1979</REF> and <REF>Vendler, 1967</REF>: dynamic ie , events vs states, as in the Jackendoff framework, :t:telic i e, culminative events transitions vs nonculminative events activities, and atomic ie , point events vs extended events~~~This featural system is imposed on top of the lexical-semantic framework proposed by Jackendoff~~~For example, the primitive GO would be annotated with the features d,t,-a for the verb destroy, but d,t,a for the verb obliterate, thus providing the appropriate distinction for cases such as 12.
Figure 2 relates the four types of lexical-semantic frameworks outlined above~~~Note that the system of features proposed by <REF>Bennett et al , 1990</REF> and <TREF>Moens and Steedman, 1988</TREF> provide the finest tuning given that five distinct categories of predicates are identified by the feature settings~~~This system is essentially equivalent to the Dowty/Vendler proposal, but features are used to distinguish the categories more precisely~~~In the next section, we will see how the tense and aspect structure described in section 21 and the lexicM-semantic representation described in this section are combined to provide the framework for generating a target-language surface form.
2b John finished drawing the circle~~~<REF>Dowty 1986</REF> and <TREF>Moens and Steedman 1988</TREF> decisively questioned the coherence of the class of achievement verbs, arguing that not all of them are non-durative~~~As noted above, Vendler identifies punctual events through the conjunction of the positive at and negative finish tests~~~However, they do not always yield comparable results : 3a 3b 4a 4b Karpov beat Kasparov at 1000 PM The Allies beat Germany at I000 PM  Karpov finished beating Kasparov The Allies finished beating Germany.
They trigger a change-of-state COS, henceforth, result states RSs, henceforth being entailments of CoSs~~~<TREF>Moens and Steedman 1988</TREF>, <REF>Smith 1991</REF>, <REF>Pustejovsky 1995</REF>, and others argue that it is a defining property of telic events~~~They should therefore include an undergoer argument, whose CoS determines the telicity of the event ie , it acts as a measuring-out argument~~~<REF>Tenny 1987</REF> thus claims that telic events require such an argument, which she calls an affected argument.
The subdivision is achieved by means of three features proposed by Bennett etal~~~1990 following the framework of <TREF>Moens and Steedman 1988</TREF>: -t-dynamic ie , events vs states, as in the Jackendoff framework, telic ie , culminative events transitions vs noneulminative events activities, and -I-atomic ie , point events vs extended events~~~We impose this system of features on top of the current lexical-semantic framework~~~For example, the lexical entry for all three verbs, ransack, obliterate, and destroy, would contain the following lexical-semantic representation: 6 Event CAUSE Thing X, Event GOLoc Thing X, Position TOLoc X John, Property DESTROYED The three verbs would then be distinguished by annotating this representation with the aspectual features d,t,-a for the verb ransack, d,t,-a for the verb destroy, and d,t,a for the verb obliterate, thus providing the appropriate distinction for cases such as 4.
SUMMARY This paper has examined a two-level knowledge representation model for machine translation that integrates aspectual information based on theories by <REF>Bach 1986</REF>, <REF>Comrie 1976</REF>, <REF>Dowty 1979</REF>, mourelatos 1981, <REF>Passonneau 1988</REF>, Pustejovsky 1988, 1989, 1991, and <REF>Vendler 1967</REF>, and more recently by Bennett et al~~~1990 and <TREF>Moens and Steedman 1988</TREF>, with lexicalsemantic information based on Jackendoff 1983, 1990~~~We have examined the question of cross-linguistic applicability showing that the integration of aspect with lexical-semantics is especially critical in machine translation when there are a large number of temporal connectives and verbal selection/realization possibilities that may be generated from a lexical semantic representation~~~Furthermore, we have illustrated that the selection/realization processes may be parameterized, by means of selection charts and coercion functions, so that the processes may operate uniformly across more than one language.
S: Juan le dio una puflaJada a Marls John gave a knife-wound to Mary S: Juan le dio pufialadas a Marls John gave knife-wounds to Mary b Duratlve Divergence, E: John met/knew Mary 4 S: Juan coaoci6 a Marls John met Mary S: Juan conoci a Mrfa John knew Merit Figure 1: Three Levels of MT Divergences et el~~~1990 have examined aspect and verb semantics within the context of machine translation in the spirit of <TREF>Moens and Steedman 1988</TREF>~~~This paper borrows from, and extends, these ideas by demonstrating how this theoretical framework might be adapted for crosslinguistic applicability~~~The framework has been tested within the context of an interlingual machine translation system and is currently being used as the basis for extraction of aspectual information from corpora.
Although there have been quite a few studies on individual aspects of sentence planning, little attention has been paid to the interaction between the various tasks--exceptions are <REF>Rambow and Korelsky 1992</REF> and <REF>Wanner and Hovy 1996</REF>--and in particular to the role of marker choice in the overall sentence planning process~~~There exists a large body of research in NLU on analysing the temporal structure of texts, including the role of temporal markers, though again restricted to English <TREF>Moens and Steedman 1988</TREF>; <REF>Lascarides and Oberlander 1993</REF>; <REF>Hitzeman et al 1995</REF>~~~We turn to these studies when it comes to identifying the information that needs to be assembled for representing temporal markers~~~3 Linguistic perspective: Describing temporal markers Selecting an appropriate German temporal marker given two events in a temporal relationship requires detailed knowledge of the semantic, pragmatic and syntactic properties that characterize temporal markers.
edu Abstract Verbal and compositional lexical aspect provide the underlying temporal structure of events~~~Knowledge of lexical aspect, eg, atelicity, is therefore required for interpreting event sequences in discourse <REF>Dowty, 1986</REF>; <TREF>Moens and Steedman, 1988</TREF>; <REF>Passoneau, 1988</REF>, interfacing to temporal databases <REF>Androutsopoulos, 1996</REF>, processing temporal modifiers <REF>Antonisse, 1994</REF>, describing allowable alternations and their semantic effects <REF>Resnik, 1996</REF>; <REF>Tenny, 1994</REF>, and selecting tense and lexical items for natural language generation <REF>Dorr and Olsen, 1996</REF>; <REF>Klavans and Chodorow, 1992</REF>, cf~~~<REF>Slobin and Bocaz, 1988</REF>~~~We show that it is possible to represent lexical aspect--both verbal and compositional--on a large scale, using Lexical Conceptual Structure LCS representations of verbs in the classes cataloged by <REF>Levin 1993</REF>.
Finally, we illustrate how knowledge of lexical aspect facilitates the interpretation of events in NLP applications~~~Knowledge of lexical aspect--how verbs denote situations as developing or holding in time--is required for interpreting event sequences in discourse <REF>Dowty, 1986</REF>; <TREF>Moens and Steedman, 1988</TREF>; <REF>Passoneau, 1988</REF>, interfacing to temporal databases <REF>Androutsopoulos, 1996</REF>, processing temporal modifiers <REF>Antonisse, 1994</REF>, describing allowable alternations and their semantic effects <REF>Resnik, 1996</REF>; <REF>Tenny, 1994</REF>, and for selecting tense and lexical items for natural language generation Dorr and Olsen~~~1996: <REF>Klavans and Chodorow, 1992</REF>, cf~~~<REF>Slobin and Bocaz, 1988</REF>.
How does 37b get its interpretation~~~As with 36d, the relevant elements of 37b can be represented as   then R   after S  turn right on County Line   e 3 :turn-rightyou, county line and the unresolved interpretation of 37b is thus  xafterx, EVe 3  aftere 3, EV 560 Computational Linguistics Volume 29, Number 4 As for resolving EV, in a well-known article, <TREF>Moens and Steedman 1988</TREF> discuss several ways in which an eventuality of one type eg , a process can be coerced into an eventuality of another type eg , an accomplishment, which Moens and Steedman call a culminated process~~~In this case, the matrix argument of then the eventuality of turning right on County Line can be used to coerce the process eventuality in 37b into a culminated process of going west on Lancaster Avenue until County Line~~~We treat this coercion as a type of associative or bridging inference, as in the examples discussed in section 31.
The alternatives arise when more than one event can be used~~~The temporal ontology is based on a recent theory of temporal semantics developed by <TREF>Moens and Steedman 1988</TREF>~~~This allows a modular representation of the semantics of temporal adverbials like until and by, and also aids in the generation of tense and aspect~~~This system looks at the mechanics of how the alternatives can be generated from the initial data, but we will have less to say about choosing between them.
21 Aspectual Categories of Verbs A number of aspectually oriented lexical-semantic representations have been proposed~~~Ve adopt and extend the feature-based framework proposed by <REF>Bennett et al , 1990</REF> in the spirit of <TREF>Moens and Steedman, 1988</TREF>~~~They uses three features: dynamic, telic, and atomic~~~We add two more features: process and gradual.
In principle, it is possible for this second module to detect aspectual transformations that apply to any input clause, independent of the manner in which the core constituents interact to produce its fundamental aspectual class~~~600 Siegel and McKeown Improving Aspectual Classification 26 Applications of Aspectual Classification Aspectual classification is a required component of applications that perform natural language interpretation, natural language generation, summarization, information retrieval, and machine translation tasks <TREF>Moens and Steedman 1988</TREF>; <REF>Klavans and Chodorow 1992</REF>; <REF>Klavans 1994</REF>; <REF>Dorr 1992</REF>; <REF>Wiebe et al 1997</REF>~~~These applications require the ability to reason about time, ie, temporal reasoning~~~Assessing temporal relationships is a prerequisite for inferring sequences of medical procedures in medical domains.
598 Siegel and McKeown Improving Aspectual Classification Table 3 Several aspectual entailments~~~If a clause occurring: necessarily entails: then it must be: in past progressive tense as argument of stopped in simple present tense past tense reading past tense reading the habitual reading Nonculminated Event Nonculminated Event or State Event 23 Interpreting Temporal Connectives and Modifiers Several researchers have developed models that incorporate aspectual class to assess temporal constraints between connected clauses <REF>Hwang and Schubert 1991</REF>; <REF>Schubert and Hwang 1990</REF>; <REF>Dorr 1992</REF>; <REF>Passonneau 1988</REF>; <TREF>Moens and Steedman 1988</TREF>; <REF>Hitzeman, Moens, and Grover 1994</REF>~~~For example, stativity must be identified to detect temporal constraints between clauses connected with when~~~For example, in interpreting, 7 She had good strength when objectively tested.
An understanding system can recognize the aspectual transformations that have affected a clause only after establishing the clauses fundamental aspectual category~~~Linguistic models motivate the division between a module that first detects fundamental aspect and a second that detects aspectual transformations <REF>Hwang and Schubert 1991</REF>; <REF>Schubert and Hwang 1990</REF>; <REF>Dorr 1992</REF>; <REF>Passonneau 1988</REF>; <TREF>Moens and Steedman 1988</TREF>; <REF>Hitzeman, Moens, and Grover 1994</REF>~~~In principle, it is possible for this second module to detect aspectual transformations that apply to any input clause, independent of the manner in which the core constituents interact to produce its fundamental aspectual class~~~600 Siegel and McKeown Improving Aspectual Classification 26 Applications of Aspectual Classification Aspectual classification is a required component of applications that perform natural language interpretation, natural language generation, summarization, information retrieval, and machine translation tasks <TREF>Moens and Steedman 1988</TREF>; <REF>Klavans and Chodorow 1992</REF>; <REF>Klavans 1994</REF>; <REF>Dorr 1992</REF>; <REF>Wiebe et al 1997</REF>.
Some aspectual markers such as the pseudocleft and many manner adverbs test for intentional events in particular not all events in general, and therefore are not compatible with all events, eg, I died diligently~~~Aspectual coercion such as iteration can allow a punctual event to appear in the progressive, eg She was sneezing for a week point  process  culminated process 4 <TREF>Moens and Steedman 1988</TREF>~~~The predictive power of some indicators is uncertain, since several measure phenomena that are not linguistically constrained by any aspectual category, eg, the present tense, durative for-PPs, frequency and notnever indicators~~~Therefore, the predictive power of individual linguistic indicators is incomplete; only the subset of verbs that adhere to the respective constraints or trends can be correctly classified.
the second sentence describes a state, which begins before the event described by the first sentence~~~Aspectual classification is also a necessary prerequisite for interpreting certain adverbial adjuncts, as well as identifying temporal constraints between sentences in a discourse <TREF>Moens and Steedman 1988</TREF>; <REF>Dorr 1992</REF>; <REF>Klavans 1994</REF>~~~In addition, it is crucial for lexical choice and tense selection in machine translation <TREF>Moens and Steedman 1988</TREF>; <REF>Klavans and Chodorow 1992</REF>; <REF>Klavans 1994</REF>; <REF>Dorr 1992</REF>~~~Table 1 sunnarizes the three aspectual distinctions, which compose five aspectual categories.
Some aspectual auxiliaries also perform an aspectual transformation of the clause they modify, eg, 11 I finished staring at it culminated process~~~Aspectual coercion, a second type of aspectual transformation, can take place when a clause is modified by an aspectual marker that violates an aspectual constraint <TREF>Moens and Steedman 1988</TREF>; <REF>Pustejovsky 1991</REF>~~~In this case, an alternative interpretation of the clause is inferred which satisfies the aspectual constraint~~~For example, the progressive marker is constrained to appear with an extended event.
E-mail: evscscolumbiaedu t Computer Science Dept , 1214 Amsterdam Ave , New York, NY 10027~~~E-mail: kathycscolumbiaedu  2001 Association for Computational Linguistics Computational Linguistics Volume 26, Number 4 Aspectual classification is necessary for interpreting temporal modifiers and assessing temporal entailments <TREF>Moens and Steedman 1988</TREF>; <REF>Dorr 1992</REF>; <REF>Klavans 1994</REF> and is therefore a required component for applications that perform certain natural language interpretation, generation, summarization, information retrieval, and machine translation tasks~~~Each of these applications requires the ability to reason about time~~~A verbs aspectual category can be predicted by co-occurrence frequencies between the verb and linguistic phenomena such as the progressive tense and certain temporal modifiers <REF>Klavans and Chodorow 1992</REF>.
Aspectual classification is also a necessary prerequisite for interpreting certain adverbial adjuncts, as well as identifying temporal constraints between sentences in a discourse <TREF>Moens and Steedman 1988</TREF>; <REF>Dorr 1992</REF>; <REF>Klavans 1994</REF>~~~In addition, it is crucial for lexical choice and tense selection in machine translation <TREF>Moens and Steedman 1988</TREF>; <REF>Klavans and Chodorow 1992</REF>; <REF>Klavans 1994</REF>; <REF>Dorr 1992</REF>~~~Table 1 sunnarizes the three aspectual distinctions, which compose five aspectual categories~~~In addition to the two distinctions described in the previous section, atomicity distinguishes punctual events eg , She noticed the picture on the wall from extended events, which have a time duration eg , She ran to the store.
Therefore, if it appears with an atomic event, eg, 12 He hiccupped point, the event is transformed to an extended event, eg, 13 He was hiccupping process~~~in this case with the iterated reading of the clause <TREF>Moens and Steedman 1988</TREF>~~~25 The First Problem: Fundamental Aspect We define fundamental aspectual class as the aspectual class of a clause before any aspectual transformations or coercions~~~That is, the fundamental aspectual category is the category the clause would have if it were stripped of any and all aspectual markers that induce an aspectual transformation, as well as all components of the clauses pragmatic context that induce a transformation.
Aspect in Natural Language Because, in general, the sequential order of clauses is not enough to determine the underlying chronological order, aspectual classification is required for interpreting 596 Siegel and McKeown Improving Aspectual Classification Table 1 Aspectual classes~~~This table is adapted from Moens and Steedman 1988, p 17~~~Culminated Nonculminated EVENTS punctual extended CULMINATION CULMINATED PROCESS recognize build a house POINT PROCESS hiccup run, swim, walk STATES understand even the simplest narratives in natural language~~~For example, consider: 1 Sue mentioned Miami event.
THE IMPERFECTIVE PARADOX AND TRAJECTORY-OF-MOTION EVENTS  Michael White Department of Computer and Information Science University of Pennsylvania Philadelphia, PA, USA mwhit el inc c is upenn, edu Abstract In the first part of the paper, I present a new treatment of THE IMPERFICTIVE PARADOX <REF>Dowty 1979</REF> for the restricted case of trajectoryof-motion events~~~This treatment extends and refines those of <TREF>Moens and Steedman 1988</TREF> and <REF>Jackendoff 1991</REF>~~~In the second part, I describe an implemented algorithm based on this treatment which determines whether a specified sequence of such events is or is not possible under certain situationally supplied constraints and restrictive assumptions~~~Bach 1986:12 summarizes THE IMPERFECTIVE PARADOX <REF>Dowty 1979</REF> as follows: how can we characterize the meaning of a progressive sentence like la 17 on the basis of the meaning of a simple sentence like lb 18 when la can be true of a history without lb ever being true.
<REF>White 1993</REF>~~~5Much as in <TREF>Moens and Steedman 1988</TREF> and <REF>Jackendoff 1991</REF>, the introduction of gr is necessary to avoid having an ill-sorted formula~~~284 the manner of motion supplied by a verb, it does nevertheless permit one to consider factors such as the normal speed as well as the meanings of the prepositions 10, lowards, etc By making two additional restrictive assumptions, namely that these events be of constant velocity and in one dimension, I have been able to construct and implement an algorithm which determines whether a specified sequence of such events is or is not possible under certain situationally supplied constraints~~~These constraints include the locations of various landmarks assumed to remain stationary and the minimum, maximum, and normal rates associated with various manners of motion eg running, jogging for a given individual.
Capitalizing on Bachs insight, I present in the first part of the paper a new treatment of the imperfective paradox which relies on the possibility of having actual events standing in the part-of relation to hypothetical super-events~~~This treatment extends and refines those of <TREF>Moens and Steedman 1988</TREF> and <REF>Jackendoff 1991</REF>, at least for the restricted case of trajectory-of-motion events~~~1 In particular, the present treatment correctly accounts not only for what 2a fails to entail -namely, that John eventually reaches the museum -but also for what 2a does in fact entail -namely, that John follows by jogging at least an initial part of a path that leads to the museum~~~In the second part of the paper, I briefly describe an implemented algorithm based on this theoretical treatment which determines whether a specified sequence of trajectory-of-motion is or is not possible under certain situationally supplied constraints and restrictive assumptions.
1 SonyCorp~~~hasheavilypromotedtheVideoWalkman since the products introduction last summer, 2 but Bob Gerson, video editor of This Week in Consumer Electronics, says 3 Sony conceives of 8mm as a family of products, camcorders and VCR decks,  SE classification is a fundamental component in determining the discourse mode of texts <REF>Smith, 2003</REF> and, along with aspectual classification, for temporal interpretation <TREF>Moens and Steedman, 1988</TREF>~~~It may be useful for discourse relation projection and discourse parsing~~~Though situation entities are well-studied in linguistics, they have received very little computational treatment.
The imperfective point of view brought by IMP imposes a change of point of view on the term of the eventuality~~~As for accomplishments, we can assume that they can be decomposed into several stages, according to <TREF>Moens and Steedman, 1988</TREF>: first  preparatory phase, second a cuhnination or achievement we are not concerned here with the result state~~~We can then say that IMP refers only to the preparatory phase, so that the term of the eventuality loses all relevance~~~This explains the so-called imperfective paradox: it is possible to use IMP even though the eventnality never reaches its term: 6 a I1 traversait la rue quand la voiture la ras6 He was crossing the street when the car hit him b  I1 traversa la rue quand la voiture la 6cras6 Ile crossed the street when the car hit him As for achievements, we can assume that they are reduced to a culmination.
Three machine learning methods are compared for this task: decision tree induction, a genetic algorithm, and log-linear regression~~~The ability to distinguish states, eg, Mark seems happy, from events, eg, Rende ran down the street, is a necessary prerequisite for interpreting certain adverbial adjuncts, as well as identifying temporal constraints between sentences in a discourse <TREF>Moens and Steedman, 1988</TREF>; <REF>Doff, 1992</REF>; <REF>Klavans, 1994</REF>~~~Furthermore, stativity is the first of three fundamental temporal distinctions that compose the aspectual class of a clause~~~Aspectual classification is a necessary component for a system that analyzes temporal constraints, or performs lexical choice and tense selection in machine translation <TREF>Moens and Steedman, 1988</TREF>; <REF>Passonneau, 1988</REF>; <REF>Doff, 1992</REF>; <REF>Klavans, 1994</REF>.
Furthermore, stativity is the first of three fundamental temporal distinctions that compose the aspectual class of a clause~~~Aspectual classification is a necessary component for a system that analyzes temporal constraints, or performs lexical choice and tense selection in machine translation <TREF>Moens and Steedman, 1988</TREF>; <REF>Passonneau, 1988</REF>; <REF>Doff, 1992</REF>; <REF>Klavans, 1994</REF>~~~Researchers have used empirical analysis of corpora to develop linguistically-based numerical indicators that aid in aspectual classification <REF>Klavans and Chodorow, 1992</REF>; <REF>Siegel and McKeown, 1996</REF>~~~Specifically, this technique takes advantage of linguistic constraints that pertain to aspect, eg, only clauses that describe an event can appear in the progressive.
And that they should be seen as the result of an attempt to take a good metaphor too literally~~~<TREF>Moens and Steedman 1988</TREF> conceived temporal adverbials as functions which coerce their inputs to the appropriate type, by a loose sic~~~analogy with type-coercion in programming languages~~~Under this perspective, aspectual shift is triggered by a conflict between the aspectual type of the situation to be modified and the aspectual constraint set by the temporal preposition heading the modifier1 Coercion operators, then, are thought to adapt the verbal input on the level of model-theoretical interpretation by mapping one sort of situation onto another.
By using TAGs we get the additional benefit of an existing parser that yields derivations and derived trees fiom which we can construct the compositional semantics of a given sentence~~~We decompose each event E into a tripartite structure in a manner similar to <TREF>Moens and Steedman 1988</TREF>, introducing a time function for each predicate to specify whether the predicate is true in the preparatory dringE, cuhnination erdE, or consequent resll:E stage of an event~~~hfitial trees capture tile semantics of the basic senses of verbs in each class~~~For example, many IThese restrictions are more like preferences that generate a preferred reading of a sentence.
However, this incompleteness is also a consequence of the linguistic characteristics of various indicators~~~For example:  Aspectual coercion such as iteration compromises indicator measurements <TREF>Moens and Steedman, 1988</TREF>~~~For example, a punctual event appears with the progressive in, She was sneezing for a week~~~point --, process --.
For example, I made a fire is culminated, since a new state is introduced something is made, whereas, I gazed at the sunset is non-culminated~~~Aspectual classification is necessary for interpreting temporal modifiers and assessing temporal entailments <REF>Vendler, 1967</REF>; <REF>Dowty, 1979</REF>; <TREF>Moens and Steedman, 1988</TREF>; <REF>Dorr, 1992</REF>, and is therefore a necessary component for applications that perform certain natural language interpretation, natural language generation, summarization, information retrieval, and machine translation tasks~~~Aspect introduces a large-scale, domaindependent lexical classification problem~~~Although an aspectual lexicon of verbs would suffice to classify many clauses by their main verb only, a verbs primary class is often domaindependent <REF>Siegel, 1998b</REF>.
112 Table 1: Aspectual classes~~~This table comes from Moens and Steedman <TREF>Moens and Steedman, 1988</TREF>~~~Culm EVENTS STATES punctual extended CULM CULM PROCESS recognize build a house NonPOINT PROCESS Culm hiccup run, swim understand 2 Aspect in Natural Language Table 1 summarizes the three aspectual distinctions, which compose five aspectual categories~~~In addition to the two distinctions described in the previous section, atomicity distinguishes events according to whether they have a time duration punctual versus extended.
This can effect not only the semantic interpretation of the text itself, but also translation and the choice of adverb~~~3 3Many of these issues are discussed in the CL Special Issue on Tense and Aspect <REF>June, 1988</REF> in articles by Hinniche, Moens and Steedman, Nakhimovsky, Passoneau, and Webber~~~For example, Passoneau demonstrates how, without an ccurate specification of the pectual tendencies of the verb coupled with the effect of temporal and aspectual adjuncts, messages, which tend to be in the present tense, ttre not correctly understood nor generated in the PUNDIT system~~~For instance, the pressure is low must be interpreted at statlve, whereas the pump operates must be interpreted as a process.
It is also clear that events are not undifferentiated masses, but rather have subparts that can be picked out by the choice of phrase type or the addition of adverbial phrases~~~<TREF>Moens  Steedman 1988</TREF> identify three constituents to an event nucleus, a preparatory process, culmination, and consequent state, whereas <REF>Nakhimovsky 1988</REF> identifies five: preparatory, initial, body, final, result, exemplified by the following: 1 15~~~When the children crossed the road, a they waited for the teacher to give a signal b they stepped onto its concrete surface as if it were about to swallow them up~~~c they were nearly hit by a car d they reached the other side stricken with fear.
I will call this level the event structure of a word cf~~~<REF>Pustejovsky 1991</REF>; <TREF>Moens and Steedman 1988</TREF>~~~The event structure of a word is one level of the semantic specification 419 Computational Linguistics Volume 17, Number 4 for a lexical item, along with its argument structure, qualia structure, and inheritance structure~~~Because it is recursively defined on the syntax, it is also a property of phrases and sentences.
Weather would seem selfcontained, but change, creation and stative are not semantic fields at all~~~Stative belongs to the Aktionsart categorisation of verbs distinguishing it from verbs of activity, achievement and accomplishment, which is orthogonal to the categorisation of verbs into semantic fields <REF>Vendler, 1967</REF>, <TREF>Moens  Steedman 1988</TREF>, <REF>Amaro, 2006</REF>~~~Moreover, a verb can belong to more than one Aktionsart category, as these apply to verbs in contexts~~~33 Suggested Revision of Categories Among verbs, the level of arbitrariness and incorrectness of the WordNet categories seems greater than that of the relations.
Differences in annotation could be due to the differences in interpretations of the event; however, we found that the vast majority of radically different judgments can be categorized into a relatively small number of classes~~~Some of these correspond to aspectual features of events, which have been intensively investigated eg , <REF>Vendler, 1967</REF>; <REF>Dowty, 1979</REF>; <TREF>Moens and Steedman, 1988</TREF>; <REF>Passonneau, 1988</REF>~~~We then developed guidelines to cover those cases see the next section~~~22 Event Classes Action vs State: Actions involve change, such as those described by words like speaking, gave, and skyrocketed.
Events of type eat differ from those of type run in the way the result d is brought about~~~This can be illustrated by means of the notion of a nucleus-structure Moens/<REF>Steedman 1988</REF>~~~A nucleus-structure consists of three parts: the inception-point IP, the development-portion DP and the culmination-point CP~~~Nucleus-Structure e S S  IP DP CP Figure 1 The result  is evaluated at each part of the nucleus-structure.
Further, we do not have a theory of the interaction of temporal interpretation with aspect~~~<REF>See Dowty, 1979</REF>; <REF>Dowty, 1986</REF>; <REF>Moens, 1987</REF>; <TREF>Moens and Steedman, 1988</TREF>; <REF>Nakhimovsky, 1988</REF>; and <REF>Passoneau, 1988</REF>~~~found in these works~~~Section 5 provides a more detailed comparison with <REF>Yip 1986</REF> and <REF>Hornstein 1990</REF>.
The decisive units for this selection are phases and boundaries <REF>Bickel, 1996</REF>~~~Presuming a tripartite event structure <TREF>Moens and Steedman, 1988</TREF> consisting of a preparation phase dynamic phase  dyn , a culmination point boundary  and a consequent state static phase  stat , there are three possibilities for aspect to select~~~English and Turkish both have  dyn -selecting aspectual markers, Turkish also a marker for explicit  stat selection; Russian pf aspect explicitly selects ~~~The unmarked members of the aspectual oppositions may assert anything else  Russian ipf aspect may assert anything but the explicit selection of a boundary.
2 For the problem with multi-sentence discourses, and the threads that sentences continue, we use an implementation of temporM centering <REF>Kameyama et al , 1993</REF>; <REF>Poesio, 1994</REF>~~~This is a technique similar to the type of centering used for nominal anaphora <REF>Sidner, 1983</REF>; <TREF>Grosz et al , 1983</TREF>~~~Centering assumes that discourse understanding requires some notion of aboutness~~~While nominal centering assumes there is one object that the current discourse is about, temporal centering assumes that there is one thread that the discourse is currently following, and that, in addition to tense and aspect constraints, there is a preference for a new utterance to continue a thread which has a parallel tense or which is semantically related to it and a preference to continue the current thread rather than switching to another thread.
Passonneau to appear examined some of the few claims relating discourse anaphoric noun phrases to global discourse structure in the Pear corpus~~~Resuits included an absence of correlation of segmental structure with centering <TREF>Grosz et al , 1983</TREF>; <REF>Kameyama, 1986</REF>, and poor correlation with the contrast between full noun phrases and pronouns~~~As noted in <REF>Passonneau and Litman, 1993</REF>, the NP features largely reflect Passonneaus hypotheses that adjacent utterances are more likely to contain expressions that corefer, or that are inferentially linked, if they occur within the same segment; and that a definite pronoun is more likely than a full NP to refer to an entity that was mentioned in the current segment, if not in the previous utterance~~~33 Evaluation The segmentation algorithms presented in the next two sections were developed by examining only a training set of narratives.
Any communicative act, be it spoken, written, gestured, or system-initiated, can give rise to DEs~~~As a discourse progresses, an adequate discourse model must represent the relevant entities, and the relationships between them <REF>Grosz and Sidner, 1986</REF>, A speaker may then felicitously refer anaphorically to an object subject to focusing or centering constraints <TREF>Grosz et al , 1983</TREF>, <REF>Sidner 1981, 1983</REF>, <REF>Brennan et al 1987</REF>  if there is an existing DE representing it, or if a corresponding DE may be directly inferred from an existing DE~~~For example, the utterance Every senior in Milford High School has a car gives rise to at least 3 entities, describable in English as the seniors in Milford High School, Milford High School, and the set of cars each of which is owned by some senior in Milford High School~~~These entities may then be accessed by the following next utterances, respectively: They graduate in June.
This is, to our knowledge, the first implementation of Webbers DE generation ideas~~~We designed the 243 algorithms and structures necessary to generate discourse entities from our logical representation of the meaning of utterances, and from pointing gestures, and currently use them in Januss <REF>Weischedel et al , 1987</REF>, BSN, 1988 pronoun resolution component, which applies centering techniques <TREF>Grosz et al , 1983</TREF>, <REF>Sidner 1981, 1983</REF>, <REF>Brennan et al 1987</REF> to track and constrain references~~~Janus has been demonstrated in the Navy domain for DARPAs Fleet Command Center Battle Management Program FCCBMP, and in the Army domain for the Air Land Battle Management Program ALBM~~~2 Meaninq Representation for DE Generation Webber found that appropriate discourse entities could be generated from the meaning representation of a sentence by applying rules to the representation that are strictly structural in nature, as long as the representation reflects certain crucial aspects of the sentence.
Its identification needs peech act cal;egorization of sentences~~~This topic-based approach is in contrast to Kameyamas,Japanese version <REF>Kameyama 1985</REF>, <REF>Kameyama 1986</REF> of tbcus-based spproach to anaphora by <TREF>Grosz et al 1983</TREF>~~~In her framewock, subjecthood and predicate deixis play the principal role, and the fact that topic provides the most important clue to anaphora identification in actual spoken Japanese discourse is not utilized eexplicitly~~~,-L3 Extension of topic introduction One of the pobems with the topicobased approach is that topics reerred to by zero pronouns are not always e:pliitiy marked by the topic postposition wa.
The parser can be modified to simulate other types of machines such LRk-like or SLR-like automata~~~It can also be extended to handle unification based grammars using a similar method as that employed by <TREF>Shieber 1985</TREF> for extending Earleys algorithm~~~Furthermore, the algorithm can be tuned to a particular grammar and therefore be made more efficient by carefully determinizing portions of the nondeterministic machine while making sure that the number of states in not increased~~~These variants lead to more efficient parsers than the one based on the basic non-deterministic push-down machine.
Finally note that in cases where substantial material has to be supplied, as it were, by the target grammar eg if a transitive verb is supplied but no object, then Definition 3 would allow arbitrary lexicalisations, giving rise to a very large number of permissible outputs~~~If this is felt to be problem, then estricting in the sense of <TREF>Shieber 1985</TREF> the subsumption test in the second half of Definition 3 to ignore the values of certain features, ie pred, would bepstraight-forward~~~This would have the effect of producing a single, exemplary lexicalisation for each significantly different ie different ignoring differences under pred structure which satisfies the minimaximal requirements~~~II4 A Problem with the Mini-maximal Approach One potential problem clearly arises with this approach.
So, top-down constraints must be weakened in order for parsing to be guaranteed to terminate~~~In order to solve the nontermination problem, <TREF>Shieber 1985</TREF> proposes restrictor, a statically predefined set of features to consider in propagation, and restriction, a filtering function which removes the features not in restrictor from top-down expectation~~~However, not only does this approach fail to provide a method to automatically generate the restrictor set, it may weaken the predicative power of top-down expectation more than necessary: a globally defined restrictor can only specify the least common features for all propagation paths~~~In this paper, a general method of maximizing top-down constraints is proposed.
Manual detection is also problematic: when a grammar is large, particularly if semantic features are included, complete detection is nearly impossible~~~As for the techniques developed so far which partially solve prediction nontermination eg , <TREF>Shieber 1985</TREF>; <REF>Haas 1989</REF>; <REF>Samuelsson 1993</REF>, they do not apply to nonminimal derivations because nonminimal derivations may arise without left recursion or recursion in general s One way is to define p to filter out all features except the context-free backbone of predictions~~~However, this severely restricts the range of possible instantiations of Shiebers algorithm~~~9 A third possibility is to manually fix the grammar so that nonminimal derivations do not occur, as we noted in Section 4.
However, simple subsumption may filter out valid parses for some grammars, thus sacrificing completeness~~~7 Another possibility is to filter out problematic features in the Prediction step by using the function p However, automatic detection of such features ie , automatic derivation of p is undecidable for the same reason as the prediction nontermination problem caused by left recursion for unification grammars <TREF>Shieber 1985</TREF>~~~Manual detection is also problematic: when a grammar is large, particularly if semantic features are included, complete detection is nearly impossible~~~As for the techniques developed so far which partially solve prediction nontermination eg , <TREF>Shieber 1985</TREF>; <REF>Haas 1989</REF>; <REF>Samuelsson 1993</REF>, they do not apply to nonminimal derivations because nonminimal derivations may arise without left recursion or recursion in general s One way is to define p to filter out all features except the context-free backbone of predictions.
1~~~Unification grammar is a term often used to describe a family of feature-based grammar formalisms, including GPSG <REF>Gazdar et al 1985</REF>, PATR-II <REF>Shieber 1986</REF>, DCG <REF>Pereira and Warren 1980</REF>, and HPSG <REF>Pollard and Sag 1994</REF>~~~In an effort to formalize the common elements of unification-style grammars, <REF>Shieber 1992</REF> developed a logic for describing them, and used this logic to define an abstract parsing algorithm~~~The algorithm uses the same set of operations as Earleys 1970 algorithm for context-free grammars, but modified for unification grammars.
It is more so because 1 there is no restriction such as that there should be only one zero morpheme within an S clause, and 2 the stack is useless because zero morphemes are independent morphemes and are not bound to other morphemes comparable to wh-words~~~<TREF>Shieber 1985</TREF> proposes a more efficient approach to gaps in the PATR-II formalism, extending Earleys algorithm by using restriction to do top-down filtering~~~While an approach to zero morphemes similar to Shiebers gap treatment is possible, we can see one advantage of ours~~~That is, our approach does not depend on what kind of parsing algorithm we choose.
However, if features are carefully selected so as to increase the amount of pruning done by the chart, the net effect may 583 Computational Linguistics Volume 19, Number 4 be that even though the grammar allows more types of constituents, the chart may end up with fewer instances~~~It is interesting to compare this technique to the restriction proposal in <TREF>Shieber 1985</TREF>~~~Both approaches select functional features to be moved forward in processing order in the hope that some processing will be pruned~~~Shiebers approach changes the processing order of functional constraints so that some of them are processed top-down instead of bottom-up.
For the experiments discussed in the final section all goal-weakening operators were chosen by hand, based on small experiments and inspection of the goal table and item table~~~Even if goal weakening is reminiscent of Shiebers 1985 restriction operator, the rules of the game are quite different: in the case of goal weakening, as much information as possible is removed without risking nontermination of the parser, whereas in the case of Shiebers restriction operator, information is removed until the resulting parser terminates~~~For the current version of the grammar of OVIS, weakening the goal category in such a way that all information below a depth of 6 is replaced by fresh variables eliminates the problem caused by the absence of the occur check; moreover, this goal-weakening operator reduces parsing times substantially~~~In the latest version, we use different goal-weakening operators for each different functor.
Depending on the properties of a particular grammar, it may, for example, be worthwhile to restrict a given category to its syntactic features before attempting to solve the parse-goal of that category~~~Shiebers 1985 restriction operator can be used here~~~Thus we essentially throw some information away before an attempt is made to solve a memorized goal~~~For example, the category xA, B, f A, B, gA,hB, i C   may be weakened into: xA,B,f ,,g, If we assume that the predicate weaken/2 relates a term t to a weakened version tw, such that tw subsumes t, then 15 is the improved version of the parse predicate: parsewithweakening Cat, P0, P, E0, E  15 weakenCat,WeakenedCat, parseWeakenedCat,P0,P,E0,E, CatWeakenedCat.
Therefore, we generally cannot use all information available in the grammar but rather we should compute a weakened version of the linking table~~~This can be accomplished, for example, by replacing all terms beyond a certain depth by anonymous variables, or by other restrictors <TREF>Shieber 1985</TREF>~~~Secondly, the use of a linking table may give rise to spurious ambiguities~~~Consider the case in which the category we are trying to parse can be matched against two different items in the linking table, but in which case the predicted head-category may turn out to be the same.
One can obtain similar results for the class of grammars whose context-free backbone is finitely ambiguous--what <REF>Pereira and Warren 1983</REF> called the offline-parsable grammars~~~However, as <TREF>Shieber 1985b</TREF> observed, this class of grammars excludes many linguistically interesting grammars that do not use atomic category symbols~~~230 The present parser as opposed to the table-building algorithm is much like those in the literature~~~Like nearty all parsers using term unification, it is a special case of Earley deduction <REF>Pereira and Warren 1985</REF>.
<REF>Sato and Tamaki 1984</REF> proposed to analyze the behavior of Prolog programs, including parsers, by using something much like a weak prediction table~~~To guarantee that the table was finite, they restricted the depth of terms occurring in the table <TREF>Shieber 1985b</TREF> offered a more selective approach--his program predicts only those features chosen by the user as most useful for prediction~~~<REF>Pereira and Shieber 1987</REF> discuss both approaches~~~We will present a variation of Shiebers ideas that depends on using a sorted language.
Any general parsing method for definite clause grammar will enter an infinite loop in some cases, and it is the task of the grammar writer to avoid this~~~Generalized phrase structure grammar avoids the problem because it has only the formal power of context-free grammar <REF>Gazdar et al 1985</REF>, but according to <TREF>Shieber 1985a</TREF> this is not adequate for describing human language~~~Lexical functional grammar employs a better solution~~~A lexical functional grammar must include a finitely ambiguous context-free grammar, which we will call the context-free backbone <REF>Barton 1987</REF>.
We then define the set of terms in a standard way~~~All unification in this paper is unification of terms, as in <REF>Robinson 1965</REF>--not graphs or other structures, as in much recent work <TREF>Shieber 1985b</TREF>~~~A unification grammar is a five-tuple G  S, ,r T, P, Z where S is a set of sorts, ,r an S-ranked alphabet, T a finite set of terminal symbols, and Z a function letter of arity e in ,r~~~Z is called the start symbol of the grammar the standard notation is S not Z, but by bad luck that conflicts with standard notation for the set of sorts.
One could devise more elaborate examples, but this one suffices to make the point: not every natural unification grammar has an obvious context-free backbone~~~Therefore it is useful to have a parser that does not require us to find a context-free backbone, but works directly on a unification grammar <TREF>Shieber 1985b</TREF>~~~We propose to guarantee that the parsing problem is solvable by restricting ourselves to depth-bounded grammars~~~A unification grammar is depth-bounded if for every L > 0 there is a D > 0 such that every parse tree for a sentential form of L symbols has depth less than D In other words, the depth of a tree is bounded by the length of the string it derives.
The grammar is depth-bounded because the depth of a tree is a linear function of the length of the string it derives~~~A similar grammar can derive the crossed serial dependencies of Swiss German, which according to <TREF>Shieber 1985a</TREF> no context-free grammar can derive~~~It is clear where the extra formal power comes from: a contextfree grammar has a finite set of nonterminals, but a unification grammar can build arbitrarily large nonterminal symbols~~~It remains to show that there is a parsing algorithm for depth-bounded unification grammars.
Some additional penalty may also have been incurred by not using dotted grammar rules to generate reductions, as in standard leftcorner parsing algorithms~~~2 There are important differences between the technique for limited prediction in this parser, and other techniques for limited prediction such as Shiebers notion of restriction <TREF>Shieber, 1985</TREF> which we also use~~~In methods such as Shiebers, predictions are weakened in ways that can result in an overall gain in efficiency, but predictions nevertheless must be dynamically generated for every phrase that is built bottom-up~~~In our log version 314.
In addition, the parser maintains a skeletal copy of the chart in which edges are labeled only by the nonterminal symbols contained in their context-free backbone, which gives us more efficient indexing of the full grammar rules~~~Other optimizations include using one-word look-ahead before adding new predictions, and using restrictors <TREF>Shieber, 1985</TREF> to increase the generality of the predictions~~~Comparison with Other Parsers Table 1 compares the average number of edges, average number of predictions, and average parse times 1 in seconds per utterance for the limited 1All parse times given in this paper were produced on a Sun SPARCstation 10/51, running Quintus Pro111 For grammar with start symbol , phrase structure rules P, lexicon L, context-independent categories CI, and context-dependent categories CD; and for word string w  wlwn: Variant Edges Preds Secs Bottom-Up 1191 0 146 Limited Left-Context 203 25 10 Left-Corner 112 78 40 Table h Comparison of Syntax-Only Parsers if  E CD, predictT, 0; addemptycategories 0 ; for i from I to n do foreach C such that C--wi EL do addedgetochartC, i-i, i ; makenewpredictionsC, ii, i ; findnew-reductionsC, il,i end addemptycategories i ; end sub findmew-reductionsB, j, k  foreach A and a such that A- B 6 P do foreach i such that i  match, j do if A 6 CD and predictedA,i or A 6 CI addedgetochartA, i, k; makenewpredictionsA, i, k ; findnewreductionsA, i, k ; end end  sub addemptycategoriesi  foreach A such that A - e E P do if A 6 CD and predictedA,/ or A 6 CI addedgetochartA, i, i ; makenewpredictionsA, i, i ; findnewreductionsA, i, i ; end  sub makenewpredictionsA, i, j  foreach Aft E Predictionsi do predict fl, j end foreach H - ABfl 6 P such that H 6 CI and B E CD and fl 6 CI do predict B, j end foreach H -- AB 6 P such that H E CD and B E CD and fl E CI and predictedH, i or H left-corner-of C and predictedC, i do predict B, j end Figure 1: Limited Left-Context Algorithm left-context parser with those for a variant equivalent to a bottom-up parser when all categories are context independent and for a variant equivalent to a left-corner parser when all categories are context dependent~~~The tests were performed on a set of 194 utterances chosen at random from the ARPA ATIS corpus MADCOW, 1992, using a broad-coverage syntactic grammar of English having 84 coverage of the test set.
The situation becomes more complicated when we move to unification-based grammars, since there may be an unbounded number of different categories appearing in the accessible stack states~~~In the system implemented here we used restriction <TREF>Shieber, 1985</TREF> on the stack states to restrict attention to a finite number of distinct stack states for any given stack depth~~~Since the restriction operation maps a stack state to a more general one, it produces a finite-state approximation which accepts a superset of the language generated by the original unification grammar~~~Thus for general constraint-based grammars the language accepted by our finite-state approximation is not guaranteed to be either a superset or a subset of the language generated by the input grammar.
Since the size of the state sets possible with finite partitioning is now finite, the algorithm always terminates~~~After establishing a correspondence between attribute and unification grammar UG, we may see that the technique of restriction used by <TREF>Shieber 1985</TREF> in his extended algorithm is related to finite partitioning on attribute domains, in fact a particular case which takes advantage of the more structured attribute domains of UG~~~For attribute grammar, given that the domains involved are more general eg , the integers, finite partitioning is the required device~~~5.
1~~~Earleys 1970 algorithm is a general algorithm for context-free languages, widely used in natural language processing <REF>King, 1983</REF>; <TREF>Shieber, 1985</TREF> and syntactic pattern recognition <REF>Fu, 1982</REF>, where the full generative power of context-free grammar is required~~~The original algorithm and its common implementations, however, assume the atomic symbols of context-free grammars, thus limiting its applicability to systems with attributed symbols, or attribute grammars <REF>Knuth, 1968</REF>~~~Attribute grammar is an elegant formalization of the augmented context-free grammars characteristic of most current NLP systems.
However, their particular realization of the technique is severely restricted for NLP applications, since it uses a deterministic one-path LR algorithm, applicable only to semantically unambiguous grammars~~~<REF>Pereira and Warren 1983</REF> and <TREF>Shieber 1985</TREF> present v6rsions of Earleys algorithm for unification grammars, in which unification is the sole operation responsible for attribute evaluation~~~However, given the high computational cost of unification, important differences between attribute and unification grammars in their respective attribution domains and functions Correa, forthcoming, and the more general nature of attribute grammars in this regard, it is of interest to investigate the extension of Earleys algorithm directly to the main subclasses of attribute grammar~~~The paper is organized as follows: Section 2 presents pieliminary elements, including a definition of attribute grammar and Earleys algorithm.
Attribute grammar is an elegant formalization of the augmented context-free grammars characteristic of most current NLP systems~~~It is more general than members of the family of unification-based grammar formalisms <REF>Kay, 1985</REF>; <REF>Shieber, 1986</REF>, mainly in that it allows and encourages the use of simpler attribution functions than unification for the definition of attribute values, and hence can lead to computationally efficient grammatical definitions, while maintaining the advantages of a well-understood declarative formalism~~~Attribute grammar has been used in the past by the author to define computational models of Chomskys Government-binding theory, from which practical parsing programs were developed <REF>Correa, 1987a</REF>~~~Many systems based on Earleys algorithm have a clear division between the phases of syntactic and semantic analysis.
The parsing problem for offline parsable grammars ts solvable~~~Yet these grammars apparently have enough formal power to describe natural language at least, they can describe the crossed-serial dependencies of Dutch and Swiss German, which are presently the most widely accepted example of a construction that goes beyond context-free grammar <TREF>Shieber 1985a</TREF>~~~Suppose that the variable M ranges over integers, and the function letter s denotes the successor function~~~Consider the rule 1 pM --- psM A grammar containing this rule cannot be offline parsable, because erasing the arguments of the top-level terms in the rule gives 2 p --- p which immediately leads to infinite ambiguity.
These ideas can be generalized to other forms of unification~~~Consider dag unification as in <TREF>Shieber 1985b</TREF>~~~Given a set S of sorts, assign a sort to each label and to each atomic dag~~~The arity of a label is a set of sorts not a sequence of sorts as in term unification.
This does not deny that compilation methods may be able to convert a grammar into a program that generates without termination problems~~~In fact, the partial execution techniques described by two of us <REF>Pereira and Shieber 1985</REF> could form the basis of a compiler built by partial execution of the new algorithm we propose below relative to a grammar~~~However, the compiler will not generate a program that generates top-down, as Strzalkowskis does~~~v c,k,mj V mj I zag V k,m V e,k saw  helpen voeren help feed Figure 2 Schematic of Verb Subeategorization Lists for Dutch Example.
But even this ad hoc solution is problematic, as there may be no principled bound on the size of the subcategorization list~~~For instance, in analyses of Dutch cross-serial verb constructions <REF>Evers 1975</REF>; <REF>Huybrechts 1984</REF>, subcategorization lists may be concatenated by syntactic rules Moort32 Computational Linguistics Volume 16, Number 1, <REF>March 1990</REF> Shieber et al Semantic Head-Driven Grammar gat 1984; <REF>Fodor et al 1985</REF>; <REF>Pollard 1988</REF>, resulting in arbitrarily long lists~~~Consider the Dutch sentence dat Jan Marie de oppasser de olifanten zag helpen that John Mary the keeper the elephants saw help voeren feed that John saw Mary help the keeper feed the elephants The string of verbs is analyzed by appending their subcategorization lists as in Figure 2~~~Subcategorization lists under this analysis can have any length, and it is impossible to predict from a semantic structure the size of its corresponding subcategorization list merely by examining the lexicon.
The symptom is an ordering paradox in the sorting~~~For example, the complements rule given by <TREF>Shieber 1985a</TREF> in the PATR-II formalism VP 1 -- VP 2 X VPl head  VP2 head VP2 syncat first  X <VP2 syncat rest  VPI syncat can be encoded as the DCG rule: vpHead, Syncat/VP ->,Head, Compl/LFlSyncat/VP, Compl/LF~~~Top-down generation using this rule will be forced to expand the lower VP before its complement, since LF is uninstantiated initially~~~Any of the reordering methods must choose to expand the child VP node first.
where the cat i are terms representing the grammatical category of an expression and its subconstituents~~~Terminal symbols are introduced into rules by enclosing them in list brackets, for example sbar/S --> that, s/S Such rules can be translated into Prolog directly using a difference list encoding of string positions; we assume readers are familiar with this technique <REF>Pereira and Shieber, 1985</REF>~~~Because we concentrate on the relationship between expressions in a language and their logical forms, we will assume that the category terms have both a syntactic and a semantic component~~~In particular, the infix function symbol / will be used to form categories of the form Syn/Sem where Syn is the syntactic category of the expression and Sere is an encoding of its semantics as a logical form; the previous rule uses this notation, for example.
This is the correlate of the link relation used in left-corner parsers with top-down filtering; we direct the reader to the discussion by Matsumoto et al~~~1983 or <REF>Pereira and Shieber 1985</REF> for further information~~~applicable  non  chain  ruleRoot, Pivot, RHS : semantics ofroot andpivot ere serae node semanticsRoot, Sem, node semanticsPivot, Sere,  choose a nonchain rue non  chain  ruloLHS, RHS,   whose lhs matches the pivot unifyPivot, LHS,  make sttre tile categories can connect chained nodesPivot, Root~~~A chain rule is applicable to connect a pivot to a root if the pivot can serve as the semantic head of the rule and the left-hand side of the rule is appropriate for linking to the root.
The reason is simple~~~Consider, for example, a grammar with a gap-threading treatment of wh-movement <REF>Pereira 1981</REF>; <REF>Pereira and Shieber 1985</REF>, which might include the rule npAffr, npAgr/SemJX-X/Sem -->  ~~~stating that an NP with agreement Agr and semantics Sere can be empty provided that the list of gaps in the NP can be represented as the difference list npAgr/SemlX-X, that is, the list containing an NP gap with the same agreement features Agr~~~Because the above rule is a nonchain rule, it will be considered when trying to generate any nongap NP, such as the proper noun np3-sing, G-G/john.
Tile algorithm described in this paper is an attempt to resolve these problems in a satisfactory manner~~~Although we believe that this algorithm could be seen as an instance of a uniform architecture for parsing and generation--just as tile extended Earley parser <TREF>Shieber, 1985b</TREF> and the bottom-up generator were instances of the generalized Earley deduction architecture--our efforts to date have been aimed foremost toward the development of the algorithm for generation alone~~~We will mention efforts toward this end in Section 5~~~11 APPLICABILITY OF THE ALGORITHM As does the Earley-based generator, the new algorithm assumes that the grammar is a unification-based or logic grammar with a phrase structure backbone and complex nonterminals.
441 Identifying Functional Strata Manually Normally, the grammarian knows which information needs to be made explicit~~~Hence, instead of differentiating between the linguistic strata sYN and SEM, we let the linguist identify which constraints filter and which only serve as a means for representation; see also <TREF>Shieber, 1985</TREF>~~~In contrast to the separation along linguistic levels, this approach adopts a functional view, cutting across linguistic strata~~~On this view, the syntactic constraints together with, eg, semantic selection constraints would constitute a subgrammar.
Though theoretically very attractive, codescription has its price: i the grammar is difficult to modularize due to the fact that the levels constrain each other mutually and ii there is a computational overhead when parsers use the complete descriptions~~~Problems of these kinds which were already noted by <TREF>Shieber, 1985</TREF> motivated tile research described here~~~The goal was to develop more flexible ways of using codescriptive grammars than having them applied by a parser with full informational power~~~The underlying observation is that constraints in such grammars can play different roles:  Genuine constraints which relate directly to tile grammaticality wellformedness of the input.
Furthermore, the need to perform nondestructive unification means that a large proportion of the processing time is spent copying feature structures~~~One approach to this problem is to refine parsing algorithms by developing techniques such as restrictions, structure-sharing, and lazy unification that reduce the amount of structure that is stored and hence the need for copying of features structures <TREF>Shieber, 1985</TREF>; <REF>Pereira, 1985</REF>; <REF>Karttunen and Kay, 1985</REF>; <REF>Wroblewski, 1987</REF>; <REF>Gerdemann, 1989</REF>; <REF>Godden, 1990</REF>; <REF>Kogure, 1990</REF>; <REF>Emele, 1991</REF>; <REF>Tomabechi, 1991</REF>; <REF>Harrison and Ellison, 1992</REF>~~~While these techniques can yield significant improvements in performance, the generality of unification-based grammar formalisms means that there are still cases where expensive processing is unavoidable~~~This approach does not address the fundamental issue of the tradeoff between the descriptive capacity of a formalism and its computational power.
Original Earley prediction works on category symbols~~~An answer to these problems was presented by <TREF>Shieber 1985</TREF> who proposed to do Earley prediction on the basis of some finite quotient of all constituent DAGs which can be specified by the grammar writer~~~Another example for the influence of the CUG efforts on the development of PATR is a new template notation introduced by Lauri Karttunen in his Interlisp-D version of PATR~~~Since categorial grammars exhibit an extensive embedding of categories within other categories, it is useful to unify templates not only with the whole lexical DAG but also with its categorial subgraphs.
The sohltion to this problem is to define a finite number of equivalence classes into which the infinite uumber of nnnterminals inay be sorted~~~Fhese ,lasses may be established in a number of ways; the one we have adopted in that presented by Harrison and Ellison,  992 which builds on l;he work of <TREF>Shieber, 1985</TREF>: it introduces the nol;ion of a negative restrictor to define equivalence classes~~~In this solution a predefined portion of a category a specific set of paths is discarded when determining whether a category belongs to an equivalence :lass or not~~~For instance, in the above example we could define the negative restrictor to be orth.
It turns out to be the case that only in this way the effect of top-down filtering will pay-off against the increased overhead of having to check the left-corner table~~~6 Some Results The performance of the parsing algorithms discussed in the preceding sections a bottom-up parser for UG BU, a top-down parser for UG of <TREF>Shieber, 1985</TREF> TD, a top-down parser operating on an instantiated grammar TD/1, and a bottom-up parser with topdown filtering operating on an instantiated grammar BU/LC were tested on two experimental CUGs, one implementing the morphosyntactic features of German N Ps, and one implementing the syntax of WH-questions in Dutch by means of a gap-threading mechanism~~~Some illustrative results are listed in Tables 1 and 2~~~Sentencel Sentence2 items sees items sees TD: 93 59 160 105 TD/I: 45 20 68 25 BU: 68 20 120 30 Bu/ c: 12 o6 53 o9 Table1: German For German, an ideal restrictor R was < l > II  cat,val, arg, or dir.
In terms of parse times the two algorithms are almost equivalent~~~Comparing our results with those of <TREF>Shieber 1985</TREF> and <REF>Haas 1989</REF>, we see that in all cases top-down filtering may reduce the size of the chart significantly~~~<REF>Whereas Haas 1989</REF> found that top-down filtering never helps to actually decrease parse times in a bottom-up parser, we have found at least one example German where top-down filtering is useful~~~183 7 Conclusions There is a trend in modern linguistics to replace grammars that are completely language specific by grammars which combine universal rules and principles with language specific parameter settings, lexicons, etc This trend can be observed in such diverse frameworks as Lexical Functional Grammar, Government-Binding Theory, Head-driven Phrase Structure Grammar and Categorial Grammar.
CHART PARSING OF UNIFICATION GRAMMAR UG~~~Parsing methods for context-free grammar can be extended to unification-based grammar formalisms see <TREF>Shieber, 1985</TREF> or <REF>Haas, 1989</REF>, and therefore they can in principle be used to parse CUG~~~A chart-parser scans a sentence from left to right, while entering items, representing partial derivations, in a chart~~~Assume that items are represented as Prolog terms of the form itemBegin, End, LH S, Parsed, ToParse, where LHS is a feature-structure and Parsed and ToParse contain lists of feature-structures.
Contrary to bottomup parsing, however, the adaptation of a top-down algorithm for UG requires some special care~~~For UGs which lack a so-called context-free back-bone, such as CUG, the top-down prediction step can only be guaranteed to terminate if we make use of restriction, as defined in <TREF>Shieber 1985</TREF>~~~Top-down prediction with a restrictor R where R is a finite set of paths through a feature-structure amounts to the following: Restriction The restriction of a feature-structure F relative to a restrictor R is the most specific feature-structure F  E F, such that every path in F j has either an atomic value or is an element of R Predictor Step For each item, End, LHS, Parsed, Next I ToParse such that Rjve, is the restriction of Next relative to R, and each rule RNe:t  RHS, add itemi,i, Rge:t, , RHS~~~Restriction can be used to develop a top-down chart parser for CUG in which the top-down prediction step terminates.
The algorithm described in this paper is an attempt to resolve these problems in a satisfactory manner~~~Although we believe that this algorithm could be seen as an instance of a uniform architecture for parsing and generation--just as the extended Earley parser <TREF>Shieber, 1985b</TREF> and the bottom-up generator were instances of the generalized Earley deduction architecture our efforts to date have been aimed foremost toward the development of the algorithm for generation alone~~~We will have little to say about its relation to parsing, leaving such questions for later research1 2 Applicability of the Algorithm As does the Earley-based generator, the new algorithm assumes that the grammar is a unificationbased or logic grammar with a phrase-structure backbone and complex nonterminMs~~~Furthermore, and again consistent with previous work, we assume that the nonterminals associate to the phrases they describe logical expressions encoding their possible meanings.
This is the correlate of the link relation used in left-corner parsers with topdown filtering; we direct the reader to the discussion by Matsumoto et al~~~1983 or Pereira and Shieber 1985, p 182 for further information~~~applicablenonchainrule Root, Pivot, RHS :7o semantics of root and pivot are same nodesemantics Root, Sem, nodesemanticsPivot, Sem, o choose a nonchain rule nonehainrulerHS, RttS, whose lhs matches the pivot unifyPivot, LHS, make sure the categories can connect chainednodesPivot, Root~~~A chain rule is applicable to connect a pivot to a root if the pivot can serve as the semantic head of the rule and the left-hand-side of the rule is appropriate for linking to the root.
In particular, in order to derive a finite CF grammar, we will need to consider only those features that have a finite number of possible values, or at least consider only finitely many of the possible values for infinitely valued features~~~We can use the technique of restriction <TREF>Shieber 1985</TREF> to remove these features from our feature structures~~~Removing these features may give us a more permissive language model, but it will still be a sound approximation~~~The experimental results reported in this paper are based on a grammar under development at RIACS for a spoken dialogue interface to a semi-autonomous robot, the Personal Satellite Assistant PSA.
The situation becomes more complicated when we move to unification-based grammars, since there may be an unbounded number of different categories appearing in the accessible stack states~~~In the system implemented here we used restriction <TREF>Shieber, 1985</TREF> on the stack states to restrict attention to a finite number of distinct stack states for any given stack depth~~~Since the restriction operation maps a stack state to a more general one, it produces a finite-state approximation which accepts a superset of the language generated by the original unification grammar~~~Thus for general constraint-based grammars the language accepted by our finite-state aptroximation is not guaranteed to be either a superset or a subset of the language generated by the input grammar.
The rule builds up infinitely large subcategorization lists of which eventually only one is to be matched against the subcategorization list of, eg, the lexical entry for buys~~~Though this rule is not cyclic, it becomes cyclic upon off-line abstraction: magicvp VForm, CSem I3, SSem : magicvp VForm, CSem2l, SSem  Through trimming this magic rule, eg, given a bounded term depth <REF>Sato and Tamaki, 1984</REF> or a restrictor <TREF>Shieber, 1985</TREF>, constructing an abstract unfolding tree reveals the fact that a cycle results from the magic rule~~~This information can then be used to discard the culprit~~~312 Indexing Removing the direct or indirect cycles from the magic part of the compiled grammar does eliminate the necessity of subsumption checking in many cases.
As a result of the explicit representation of filtering we do not need to postpone abstraction until run-time, but can trim the magic predicates off-line~~~One can consider this as bringing abstraction into the logic as the definite clause representation of filtering is weakened such that only a mild form of connectedness results which does not affect completeness <TREF>Shieber, 1985</TREF>~~~Consider the following magic rule: magicvpVForm, CgemlArgs, SSem :magicvp VForm, Args, SSem  This is the rule that is derived from the headrecursive vp rule when the partially specified subcategorization list is considered as filtering information cf~~~, fn.
More specifically, magic generation falls prey to non-termination in the face of head recursion, ie, the generation analog of left recursion in parsing~~~This necessitates a dynamic processing strategy, ie, memoization, extended with an abstraction function like, eg, restriction <TREF>Shieber, 1985</TREF>, to weaken filtering and a subsumption check to discard redundant results~~~It is shown that for a large class of grammars the subsumption check which often influences processing efficiency rather dramatically can be eliminated through fine-tuning of the magic predicates derived for a particular grammar after applying an abstraction function in an off-line fashion~~~Unfolding can be used to eliminate superfluous filtering steps.
In the categorial grammar example only x/3 goals are memoized and thus only these goals incur the cost of table management~~~The abstraction step, which is used in most memoizing systems including complex feature grammar chart parsers where it is somewhat confusingly called restriction, as in <TREF>Shieber 1985</TREF>, receives an elegant treatment in a CLP approach; an abstracted goal is merely one in which not all of the equality constraints associated with the variables appearing in the goal are selected with that goal~~~2 For example, because of the backward application rule and the left-to-right evaluation our parser uses, eventually it will search at every left string position for an uninstantiated category the variable Y in the clause, we might as well abstract all memoized goals of the form xC, L, R to x, L, , ie, goals in which the category and right string position are uninstantinted~~~Making the equality constraints explicit, we see that the abstracted goal is obtained by merely selecting the underlined subset of these below: xXl,X2, X3,Xl  C, X2  L, Xa  R While our formal presentation does not discuss abstraction since it can be implemented in terms of constraint selection as just described, because our implementation uses the underlying Prologs unification mechanism to solve equality constraints over terms, it provides an explicit abstraction operation.
24 Top-Down Predictive Linking The aim of our proposal is to define equivalence relations that keep the linking relation finite while also preventing it from being too restrictive; this turns the linking relation into a weakpredietion table in the sense of Haas 1989: 227ff~~~Like Shieber 1985, 1992 with the notion of restriction, we confine our attention to a subset of specifications; in particular, we can define a feature structure that subsumes all VP-type feature structures of Shiebers recursive subcategorization rules~~~But unlike Shieber, our restrictors are computed automatically by building the generalization of the occurrences ofleftrecursive categories in a grammar~~~The intuitive idea is that we consider categories to be left recursive if their tokens can be unified rather than being identical, as in the case of atoms; we then use their generalization, or greatest lower bound, as a common denominator defining an equivalence relation.
A better solution, which we have adopted from <REF>Kilbury 1990</REF>, is to introduce rule numbers, which are then used to define a purely filtering linking relation~~~This amounts to the simplest case of the restriction technique of <TREF>Shieber 1985</TREF>~~~Only when a link between numerical pointers is first found is the linking relation between feature structures used to instantiate information~~~3 Consequences of Predictive Linking What is the advantage of predictive linking as discussed above in 2.
Such formalisms typically include a contextfree CF base, which allows the use of parsing algorithms designed for CF languages despite the fact that complex-feature-based formalisms are essentially more powerful than CF grammars~~~However, such an adaptation of CF algorithms involves their extension to possibly infinite nonterminal domains, which, as <TREF>Shieber 1985</TREF> and <REF>Haas 1989</REF> have shown, is nontrivial~~~Various CF algorithms make use of a binary relation between a goal category and the category of a constituent phrase or word which either has just been parsed or is to be parsed next~~~Different terms have been used to designate this relation; <REF>Kay 1980</REF> speaks of reachability, while Pereira/<REF>Shieber 1987</REF> and others before them use the term linking for the relation.
1990 have discussed similar techniques in the context of semantichead-driven generation, we are concerned here with parsing~~~We view the linking relation not simply as a filter to increase efficiency within the domain of syntactic analysis--this aspect is stressed by <TREF>Shieber 1985</TREF> and other investigators such as <REF>Bouma 1991</REF>--but rather as a device for the top-down predictive instantiation of information, as Shieber et al~~~1990 have shown for semantic-head-driven generation~~~In this paper we are concerned especially with morphosyntactic information and illustrate the relevance of predictive linking for morphological analysis and for the analysis of unknown or new lexical items.
Whatever term one takes, an important aspect of the relation is that it can be used to reduce the search space of possible syntactic analyses at an earlier point in parsing and thus serves to improve the efficiency of a parser~~~Shieber 1985, 1992 follows established terminology in speaking of top-down filtering in connection with the prediction step of the Earley algorithm~~~His central notion of restriction, whereby a restrictor is a finite subset of the paths specified in a feature structure, is related to the technique we introduce here, since both guarantee the finiteness of an otherwise possibly infinite domain of complex categories, but Shiebers restrictors are specified manually~~~We propose a general algorithmic method of compilation that avoids manual specification.
In particular, declaring certain category-valued features so that they cannot take variable values may lead to nontermination in the backbone construction for some grammars~~~However, it should be possible to restrict the set of features that are considered in category-valued features in an analogous way to Shiebers 1985 restrictors for Earleys 1970 algorithm, so that a parse table can still be constructed~~~36 Ted Briscoe and John Carroll Generalized Probabilistic LR Parsing 4~~~Building LR Parse Tables for Large NL Grammars The backbone grammar generated from the ANLT grammar is large: it contains almost 500 distinct categories and more than 1600 productions.
This requirement places a greater load on the grammar writer and is inconsistent with most recent unification-based grammar formalisms, which represent grammatical categories entirely as feature bundles eg <REF>Gazdar et al 1985</REF>; <REF>Pollard and Sag 1987</REF>; <REF>Zeevat, Calder, and Klein 1987</REF>~~~In addition, it violates the principle that grammatical formalisms should be declarative and defined independently of parsing procedure, since different definitions of the CF portion of the grammar will, at least, effect the efficiency of the resulting parser and might, in principle, lead to nontermination on certain inputs in a manner similar to that described by <TREF>Shieber 1985</TREF>~~~In what follows, we will assume that the unification-based grammars we are considering are represented in the ANLT object grammar formalism <REF>Briscoe et al 1987</REF>~~~This formalism is a notational variant of Definite Clause Grammar eg <REF>Pereira and Warren 1980</REF>, in which rules consist of a mother category and one or more daughter categories, defining possible phrase structure configurations.
In contrast to the symbols in context-free grammars, feature structures in unification-based grammars often include information encoding part of the derivation history, most notably semantics~~~In order to achieve successful packing rates, feature restriction <TREF>Shieber, 1985</TREF> is used to remove this information during creation of the packed parse forest~~~During the unpacking phase, which operates only on successful parse trees, these features are unified back in again~~~For their experiments with efficient subsumptionbased packing, <REF>Oepen and Carroll, 2000</REF> experimented with different settings of the packing restrictor for the English Resource Grammar ERG <REF>Copestake and Flickinger, 2000</REF>: they found that good packing rates, and overall good performance during forest creation and unpacking were achieved, for the ERG, with partial restriction of the semantics, eg keeping index features unrestricted, since they have an impact on external combinatorial potential, but restricting most of the internal MRS representation, including the list of elementary predications and scope constraints.
5 Choosing the Grammar Restrictor and Parsing Strategy In order for the subsumption relation to apply meaningfully to HPSG signs, two conditions must be met~~~Firstly, parse tree construction must not be duplicated in the feature structures by means of the HPSG DTRS feature but be left to the parser ie recorded in the chart; this is achieved in a standard way by feature structure restriction <TREF>Shieber, 1985</TREF> applied to all passive edges~~~Secondly, the processing of constraints that do not restrict the search space but build up new often semantic structure should be postponed, since they are likely to interfere with subsumption~~~For example, analyses that differ only with respect to PP attachment would have the same syntax, but differences in semantics may prevent them being packed.
Attention is restricted here to approximations of context-free grammars because context-free languages are the smallest class of formal language that can realistically be applied to the analysis of natural language~~~Techniques such as restriction <TREF>Shieber, 1985</TREF> can be used to construct context-free approximations of many unification-based formalisms, so techniques for constructing finite-state approximations of context-free grammars can then be applied to these formalisms too~~~2 Finite-state calculus A finite-state calculus or finite automata toolkit is a set of programs for manipulating finite-state automata and the regular languages and transducers that they describe~~~Standard operations include intersection, union, difference, determinisation and minimisation.
The ELU tormalism provides a generalization of the template facility of PATR-II, the relational abstractions, which are statements abstracting over sets of constraint equations~~~These statements 5 Restrictors are also used to restrict the search space in parsing see <TREF>Shieber 1985</TREF>~~~fbe use of linking information in generation was first proposed by van <REF>Noord 1988</REF>~~~may receive multiple and mcursive definitions.
Problems in the prediction step of the Earley parser used for unification-based formalisms no longer exist~~~The use of restrictors as proposed by <TREF>Shieber 1985</TREF> is no longer necessary and the difficulties caused by treating subcategorization as a feature is no longer a problem~~~By assuming that the number of structures associated with a lexical item is finite, since each structure has a lexical item attached to it, we implicitly make the assumption that an input string of finite length cannot be syntactically infinitely ambiguous~~~Since the trees are produced by the input string, the parser can use information that might be non-local to guide the search.
The resulting structures form equivalence classes, since they abstract from word-specific information, such as FORM or STEM~~~The abstraction is specified by means of a restrictor <TREF>Shieber, 1985</TREF>, the so-called lexicon restrictor~~~After that, the grammar rules are instantiated by unification, using the abstracted lexicon entries and resulting in derivation trees of depth 1~~~The rule restrictor is applied to each resulting feature structure FS, removing all information contained only in the daughters of a rule.
head-dtr syn  counter 1   Then, this can generate an infinite sequence of signs, each of which contains a part,  counter <bar, ba, r,,bar l and is not equivalent to any previously generated sign~~~In order to resolve this difficulty, we apply tim restriction <TREF>Shieber, 1985</TREF> to a rule schemata and a lexical entry, and split the feature structure F  fsR of a rule schema R or a lexical entry F  l, into two, namely, coreF and subF such that F  coreF U subF~~~The definition of the restriction here is given as follows~~~Definition 5 paths For arty node n in a feature structure F, pathsn,F is a set of all the paths that reaches n from the root of F Definition 6 Restriction Schema A restriction schema rs is a set of paths.
However, these models are inadequate for language interpretation, since they cannot express the relevant syntactic and semantic regularities~~~Augmented phrase structure grammar APSG formalisms, such as unification-based grammars <TREF>Shieber, 1985a</TREF>, can express many of those regularities, but they are computationally less suitable for language modeling, because of the inherent cost of computing state transitions in APSG parsers~~~The above problems might be circumvented by using separate grammars for language modeling and language interpretation~~~Ideally, the recognition grammar should not reject sentences acceptable by the interpretation grammar and it should contain as much as reasonable of the constraints built into the interpretation grammar.
procedure CLOSUREI; begin repeat for each item <AwBx> in I, and each production C-y such that C is unifiable with B and <CBy> is not in I do add <CB--,y> to I; until no more items can be added to I; return I end; procedure NEXT-SI,C for each category C that appears to the right ; of the dot in items begin let J be the set of items <A-wBx> such that <AwBx> is in I and B is unifiable with C; return CLOSUREJ end; Figure 6~~~Preliminary CLOSURE/NEXT-S Procedures The preliminary CLOSURE procedure Unifies the lhs of a predicted production, ie -71 Cy, and the category the prediction is made flom, ieB This approach is essentially top-down lrOlagation of instantiated features and well documented by <TREF>Shieber 1985</TREF> in the context of Earleys algorithm~~~A new item added to the state, <CB--,~~~y>, is not the production C--,y, but its partial instantiation, y is also instantiated to be y as a result of the unification CB if C and some members of y share tags.
That is, instantiation of productions introduces the nontermination problem of left-recursive productions to the procedure, as well as to the Predictor Step of Earleys algorithm~~~To overcome this problem, <TREF>Shieber 1985</TREF> proposes restrictor, which specifies a maximum depth of feature-based categories~~~When the depth of a category in a predicted item exceeds the limit imposed by a restrictor, further instantiation of the category in new items is prohibited~~~The Predictor Step eventually halts when it starts creating a new item whose feature specification within the depth allowed by the resuictor is identical to, or subsumed by, a previous one.
The situation is different for active chart items since daughters can affect their siblings~~~To be independent from a-certain grammatical theory or implementation, we use restrictors similar to <TREF>Shieber, 1985</TREF> as a flexible and easyto-use specification to perform this deletion~~~A positive restrictor is an automaton describing the paths in a feature structure that will remain after restriction the deletion operation, 3There are refinements of the technique which we have implemented and which in practice produce additional benefits; we will report these in a subsequent paper~~~Briefly, they involve an improvement to th e path collection method, and the storage of other information besides types in the vectors.
Estimates of the object-grammar size for typical systems vary from hundreds or thousands 3 up to trillions of rules <REF>Shieber 1983</REF>:4~~~With some formalisms, the context-free object-grammar approach is not even possible because the object grammar would be infinite <TREF>Shieber 1985</TREF>:145~~~Grammar size matters beyond questions of elegance and clumsiness, for it typically affects processing complexity~~~<REF>Berwick and Weinberg 1982</REF> argue that the effects of grammar size can actually dominate complexity for a relevant range of input lengths.
That is, for the reference determination, the subject roles of the candidates referent within a discourse segment will be checked intheflrstplace~~~Thisflndingsupportswell the suggestion in centering theory that the grammaticalrelationsshouldbeusedasthe key criteria to rank forward-looking centers in the process of focus tracking <TREF>Brennan et al , 1987</TREF>; <REF>Grosz et al , 1995</REF>~~~3~~~candi Pron and candi NoAntecedent are to be examined in the cases when the subject-role checking fails, which conflrms the hypothesis in the S-List model by <REF>Strube 1998</REF> that co-refereing candidates would have higher preference than other candidates in the pronoun resolution.
The S-List model <REF>Strube, 1998</REF>, for example, assumes that a co-referring candidate is a hearer-old discourse entity and is preferred to other hearer-new candidates~~~In the algorithms based on the centering theory <TREF>Brennan et al , 1987</TREF>; <REF>Grosz et al , 1995</REF>, if acandidateanditsantecedentarethebackwardlooking centers of two subsequent utterances respectively, the candidate would be the most preferred since the CONTINUE transition is always ranked higher than SHIFT or RETAIN~~~In this paper, we present a supervised learning-based pronoun resolution system which incorporates coreferential information of candidates in a trainable model~~~For each candidate, we take into consideration the properties of its antecedents in terms of features henceforth backward features, and use the supervised learning method to explore their in uences on pronoun resolution.
The first class is characterized by adaptations of previously known reference algonthms eg~~~<REF>Lappin and Leass, 1994</REF>, <TREF>Brennan et al , 1987</TREF> the scarce syntactic and semantic knowledge available m an w system eg~~~<REF>Kameyama, 1997</REF>~~~The second class is based on statistical and machine learning techniques that rely on the tagged corpora to extract features of the coreferential relations eg.
One of the most unusual features of centering theory is that the notions of utterance, previous utterance, ranking, and realization used in the definitions above are left unspecified, to be appropriately defined on the basis of empirical evidence, and possibly in a different way for each language~~~As a result, centering theory is best viewed as a cluster of theories, each of which specifies the parameters in a different ways: eg, ranking has been claimed to depend on grammatical function <REF>Kameyama, 1985</REF>; <TREF>Brennan et al , 1987</TREF>, on thematic roles <REF>Cote, 1998</REF>, and on the discourse status of the CFs <REF>Strube and Hahn, 1999</REF>; there are at least two definitions of what counts as previous utterance <REF>Kameyama, 1998</REF>; <REF>Suri and McCoy, 1994</REF>; and realization can be interpreted either in a strict sense, ie, by taking a CF to be realized in an utterance only if an NP in that utterance denotes that CF, or in a looser sense, by also counting a CF as realized if it is referred to indirectly by means of a bridging reference <REF>Clark, 1977</REF>, ie, an anaphoric expression that refers to an object which wasnt mentioned before but is somehow related to an object that already has, as in the vase the handle see, eg, the discussion in <REF>Grosz et al , 1995</REF>; <REF>Walker et al , 1998b</REF>~~~3 METHODS The fact that so many basic notions of centering theory do not have a completely specified definition makes empirical verification of the theory rather difficult~~~Because any attempt at directly annotating a corpus for utterances and their CBs is bound to force the annotators to adopt some specification of the basic notions of the theory, previous studies have tended to study a particular variant of the theory <REF>Di Eugenio, 1998</REF>; <REF>Kameyama, 1998</REF>; <REF>Passonneau, 1993</REF>; <REF>Strube and Hahn, 1999</REF>; <REF>Walker, 1989</REF>.
Another important factor in pronoun resolution is the grammatical role of the antecedent~~~The role hierarchy used in centering <TREF>Brennan et al , 1987</TREF>; <REF>Grosz et al , 1995</REF> ranks subjects over direct objects over indirect objects over others~~~<REF>Lappin and Leass 1994</REF> provide a more elaborate model which ranks NP complements and NP adjuncts lowest~~~Two other distinctions in their model express a preference of rhematic2 over thematic arguments: Existential subjects, which follow the verb, rank very high, between subjects and direct objects.
Similarly, we can assess other strategies of sentence ordering that have been proposed in the literature~~~Hard-core centering approaches only deal with the last sentence <TREF>Brennan et al , 1987</TREF>~~~In Negra, these approaches can consequently have at most a success rate of 442~~~Performance is particularly low with possessive pronouns which often only have antecedents in the current sentence.
While it appears that our existing linguistic bias set will be of use, we believe that the CBL system will benefit from additional linguistic biases~~~Centering constraints see <TREF>Brennan et al , 1987</TREF>, for example, can be encoded as linguistic biases and applied to the pronoun resolution task to increase system performance~~~Furthermore, we have focused on applying the linguistic bias approach to feature set selection for case-based learning algorithms only~~~In future work, we plan to investigate the use of the approach for feature selection in conjunction with other standard machine learning algorithms.
A number of studies have developed refinements and extensions of the theory eg~~~<TREF>Brennan et al , 1987</TREF>; <REF>Kameyama, 1986</REF>; <REF>Strube and Hahn, 1996</REF>; <REF>Walker et al , 1998</REF>, but few have attempted to extend the model to multi-party discourse cf~~~<REF>Brennan, 1998</REF>; <REF>Walker, 1998</REF>~~~For dialog systems, the benefits of using centering theory include improved reference resolution and generation of more coherent referring expressions.
Separately, the SPG also handles referring expression generation by converting proper names to pronouns when they appear in the previous utterance~~~The rules are applied locally, across adjacent sequences of utterances <TREF>Brennan et al , 1987</TREF>~~~Referring expressions are manipulated in the d-trees, either intrasententially during the creation of the sp-tree, or intersententially, if the full sp-tree contains any period operations~~~The third and fourth sentences for Alt 13 in Figure 4 show the conversion of a named restaurant Carmines to a pronoun.
Pragmatic level Working together, surface patterns and possessive relationships can deal with many PPAs found in our corpus, but we still have two problems to be solved: semantic ambiguity among two or more acceptable candidates and abstract anaphors/antecedents, which cannot be solved by simply applying possessive relationship rules~~~For these cases, and possibly for some other cases not included in previous rules, we suggest a pragmatic factor, adapted from S Brennans et al 1987 centering algorithm~~~Although sentence center plays a crucial role in many works in anaphor resolution, usually limiting the number of candidates to be considered, we notice that, because PPAs can refer to almost any NP in the sentence rather than, for example, personal pronouns, which are often related to the sentence center, pragmatic knowledge plays only a secondary but still important role in our approach~~~We have adapted basic aspects of center algorithm, considering subject/object preference, and domain concepts preference, 1012 suggested by R <REF>Mitkov 1996</REF>, aiming to estimate the most probable center for intrasentential PPAs.
In Section 3, I introduce my model, its only data structure, the S-list, and the accompanying algorithm~~~In Section 4, I compare the results of my algorithm with the results of the centering algorithm <TREF>Brennan et al , 1987</TREF> with and without specifications for complex sentences <REF>Kameyama, 1998</REF>~~~2 A Look Back: Centering The centering model describes the relation between the focus of attention, the choices of referring expressions, and the perceived coherence of discourse~~~The model has been motivated with evidence from preferences for the antecedents of pronouns <REF>Grosz et al , 1983</REF>; 1995 and has been applied to pronoun resolution Brennan et al.
Their approach has been proven as the point of departure for a new model which is valid for English as well~~~The use of the centering transitions in Brennan et als 1987 algorithm prevents it from being applied incrementally cf~~~<REF>Kehler 1997</REF>~~~In my approach, I propose to replace the functions of the backward-looking center and the centering transitions by the order among the elements of the list of salient discourse entities S-list.
RANK by transition orderings~~~To illustrate this algorithm, we consider example 1 <TREF>Brennan et al , 1987</TREF> which has two different final utterances ld and ld~~~Utterance ld contains one pronoun, utterance ld t two pronouns~~~We look at the interpretation of ld and ldt.
Table 1 illustrates the Pour transitions that are detined according to diese constraints~~~<TREF>Brennan et al , 1987</TREF></TREF> proposes a default ordering on transitions which correlates with discourse coherence: CONTINUE is preferred to RETAIN is prelbrred to SMOOTH-SHIFT is pre2The version of centering I presem; here is tiom <TREF>Brennan et al , 1987</TREF></TREF>~~~352 H CbUu  CbWn, Ct,Un 7 CtW,,~~~1 n CbU,  CtU, XNTINUI,; SMOOTII-SItlH CbU,,  CpU, IETAIN IIUIlI-SIIIFT Tatle 1: Ceutering Transitions ferred to IIIII-SIIllT.
Although they report that their method estimates over 90 of zero subjects correctly, there are several difficulties including the fact that the test corpus is identical with the corpus from which the pragmatic constraints are extracted, and the fact that there are so many rules46 rules to estimate 175 sentences~~~As for the identifying method available in general discourses, the centering theory<TREF>Brennan et al , 1987</TREF>; <REF>Walker et al , 1990</REF> and the property sharing theory<REF>Kameyama, 1988</REF> are proposed~~~The important feature of these theories is the fact that it is independent of the type of discourse~~~However, according to our experimental result, it seems that these kinds of theory do not estimate zero subjects in high precision for manual sentences 3.
This preference can be explained in terms of salience from the point of view of the centering theory~~~The latter proposes the ranking subject, direct object, indirect object <TREF>Brennan et al 1987</TREF> and noun phrases which are parts of prepositional phrases are usually indirect objects~~~Collocation pattern preference This preference is given to candidates which have an identical collocation pattern with a pronoun 2,0~~~The collocation preference here is restricted to the patterns noun phrase pronoun, verb and verb, noun phrase pronoun.
Grosz et al list seven such costraints, three of which can be directly evaluated~~~Even though we are not following here the distinction between constraints and rules introduced in <REF>Brennan, Friedman, and Pollard 1987</REF>, we will use for these three claims the names Brennan et al gave them, by which they are now best known: Constraint 1 Strong: All utterances of a segment except for the first have exactly one CB~~~Rule 1 GJW95: If any CF is pronominalized, the CB is Rule 2 GJW95: Sequences of continuations are preferred over sequences of retains, which are preferred over sequences of shifts~~~231 Constraint 1, Topic Uniqueness, and Entity Coherence.
162 In effect, we use this probability information to identify the topic of the segment with the belief that the topic is more likely to be referred to by a pronoun~~~The idea is similar to that used in the centering approach <TREF>Brennan et al , 1987</TREF> where a continued topic is the highest-ranked candidate for pronominalization~~~Given the above possible sources of informar tion, we arrive at the following equation, where Fp denotes a function from pronouns to their antecedents: Fp  argmaxP Ap  alp, h, l, t, l, so, d A where Ap is a random variable denoting the referent of the pronoun p and a is a proposed antecedent~~~In the conditioning events, h is the head constituent above p, l r is the list of candidate antecedents to be considered, t is the type of phrase of the proposed antecedent always a noun-phrase in this study, I is the type of the head constituent, sp describes the syntactic structure in which p appears, dspecifies the distance of each antecedent from p and M is the number of times the referent is mentioned.
A number of studies have developed refinements and extensions of the theory eg~~~Brennan et at, 1987; <REF>Kameyama, 1986</REF>; <REF>Strube and Hahn, 1996</REF>; <REF>Walker et al, 1998</REF>, but few have attempted to extend the model to mul party discourse cf~~~<REF>Brennan, 1998</REF>; <REF>Walker, 1998</REF>~~~For dialog systems, the benefits of using centering theory include improved reference resolution and generation of more coherent referring expressions.
In Section 3, I introduce my model, its only data structure, the S-list, and the accompanying algorithm~~~In Section 4, I compare the results of my algorithm with the results of the centering algorithm <TREF>Brennan et al, 1987</TREF> with and without specifications for complex sentences <REF>Kamcyama, 1998</REF>~~~2 A Look Back: Centering The centering model describes the relation between the focus of attention, the choices of referring expressions, and the perceived coherence of discourse~~~The model has been motivated with evidence from preferences for the antecedents of pronouns <REF>Grosz et al, 1983</REF>; 1995 and has been applied to pronoun resolution Brennan et al.
Their approach has been proven as the point of departure for a new model which is valid for English as well~~~The use of the centering transitions in Brennan et als 1987 algorithm prevents it from being applied incrementally cf~~~<REF>Kehler 1997</REF>~~~In my approach, I propose to replace the functions of the backward-looking center and the centering transitions by the order among the elements of the list of salient discourse entities S-list.
RANK by transition orderings~~~To illustrate this algorithm, we consider example 1 <TREF>Brennan et al, 1987</TREF> which has two different final utterances ld and ld~~~Utterance ld contains one pronoun, utterance ld t two pronouns~~~We look at the interpretation of ld and ld.
Mitkovs knowledge-poor pronoun resolution method <REF>Mitkov, 1998</REF>, for example, uses the scores from a set of antecedent indicators to rank the candidates~~~And centering algorithms <TREF>Brennan et al , 1987</TREF>; <REF>Strube, 1998</REF>; <REF>Tetreault, 2001</REF>, sort the antecedent candidates based on the ranking of the forward-looking or backwardlooking centers~~~In recent years, supervised machine learning approaches have been widely used in coreference resolution <REF>Aone and Bennett, 1995</REF>; <REF>McCarthy, 1996</REF>; <REF>Soon et al , 2001</REF>; <REF>Ng and Cardie, 2002a</REF>, and have achieved significant success~~~Normally, these approaches adopt a single-candidate model in which the classifier judges whether an antecedent candidate is coreferential to an anaphor with a confidence value.
Any communicative act, be it spoken, written, gestured, or system-initiated, can give rise to DEs~~~As a discourse progresses, an adequate discourse model must represent the relevant entities, and the relationships between them <REF>Grosz and Sidner, 1986</REF>, A speaker may then felicitously refer anaphorically to an object subject to focusing or centering constraints <REF>Grosz et al , 1983</REF>, <REF>Sidner 1981, 1983</REF>, <TREF>Brennan et al 1987</TREF>  if there is an existing DE representing it, or if a corresponding DE may be directly inferred from an existing DE~~~For example, the utterance Every senior in Milford High School has a car gives rise to at least 3 entities, describable in English as the seniors in Milford High School, Milford High School, and the set of cars each of which is owned by some senior in Milford High School~~~These entities may then be accessed by the following next utterances, respectively: They graduate in June.
This is, to our knowledge, the first implementation of Webbers DE generation ideas~~~We designed the 243 algorithms and structures necessary to generate discourse entities from our logical representation of the meaning of utterances, and from pointing gestures, and currently use them in Januss <REF>Weischedel et al , 1987</REF>, BSN, 1988 pronoun resolution component, which applies centering techniques <REF>Grosz et al , 1983</REF>, <REF>Sidner 1981, 1983</REF>, <TREF>Brennan et al 1987</TREF> to track and constrain references~~~Janus has been demonstrated in the Navy domain for DARPAs Fleet Command Center Battle Management Program FCCBMP, and in the Army domain for the Air Land Battle Management Program ALBM~~~2 Meaninq Representation for DE Generation Webber found that appropriate discourse entities could be generated from the meaning representation of a sentence by applying rules to the representation that are strictly structural in nature, as long as the representation reflects certain crucial aspects of the sentence.
Pragmatic level Working together, surface patterns and possessive relationships can deal with many PPAs found in our corpus, but we still have two problems to be solved: semantic ambiguity among two or more acceptable candidates and abstract anaphors/antecedents, which cannot be solved by simply applying possessive relationship rules~~~For these cases, and possibly for some other cases not included in previous rules, we suggest a pragmatic factor, adapted from S Brennans et al 1987 centering algorithm~~~Although sentence center plays a crucial role in many works in anaphor resolution, usually limiting the number of candidates to be considered, we notice that, because PPAs can refer to almost any NP in the sentence rather than, for example, personal pronouns, which are often related to the sentence center, pragmatic knowledge plays only a secondary but still important role in our approach~~~We have adapted basic aspects of center algorithm, considering subject/object preference, and domain concepts preference, 1012 suggested by R <REF>Mitkov 1996</REF>, aiming to estimate the most probable center for intrasentential PPAs.
The result is as follows: UI : Cb f a, Cp  a U2:Cba, Cpb U3 : Cb f b, Cp  b U4 : Cb  b, Cp  b In terms of the conventional transitions this works out as U/U2: RETIN U2/U3: SMOOTH SHIFT us/u: COnTINUa This is consistent with Strube and Hahns 1996 observation that a IIrAIN transition ideally predicts a SMOOTH sswr in the following utterance~~~<REF>Brenuan et al 1987</REF> make a very similar claim: A computational system for 9e-emtion would try to plan a retention as a signal of an impending shift, so that after a retention, a shift would be preferred rather than a continuation~~~<REF>Grosz et al 1995</REF> give the following example of the Am SHIFT pattern: 5 a John has had trouble arranging his vacation~~~b He Cb; John cannot find anyone to take over his responsibilities.
I suggmtthat these results should be treated with some caution since it is not dear that the authors have the same assumptions about the claims of CT or that what they are testing directly reflects formulations of CT in the more theoretical literature~~~For instance <REF>Passoneau 1998</REF> refers to two variantS of CT: Version A based on <TREF>Brennan et al 1987</TREF> and Version B taken from <REF>Kameyama et al 1993</REF>~~~Passoneau does nt address the issue of direct vs indirect realisation and it appears from the examples given that she only takes account of entities realised by a full NP or possibly null pronoun~~~The analysis according to Version B results in a count of 52 NULL transitions, ie no Cb, which gives the impression that CT is in fact a rather poor measure of coherence, It is probable that a higher measure might have been obtained if Passoneau had allowed entities to be added to the U/s by inference, as discussed in Brennan et al, op cit.
Centering theory C is a theory of discourse structure which models the interaction of cohesion and salience in the internal organlsation of a text~~~The main assumptions of the theory as presented by Gross et a11995 GJW, <TREF>Brennan et al 1987</TREF> rare: 1~~~For each utterance in a discourse there is precisely one entity which is the centre of attention or center~~~2.
Take the Cb as given and plan the realisation of Ui- to make this entity the highestranked~~~The first strategy is clearly appropriate for interpretation cf <TREF>Brennan et al 1987</TREF> but for generation the issue is less clear-cut~~~Either the generator interprets its own output to designate Cb in terms of the grammatical structure of the previous utterance, in which case there have to be separate principles for deciding on the grammatical structure, or Cb is independently defined in the text plan and this information is used to plan the sentence structure~~~According to the pipelining principle information cannot flow backwards between tasks.
The function for resolving IPAs ResolveIpa has similarly been tested on texts, where APAs were excluded~~~We have compared the obtained results with those obtained by testing bfp <TREF>Brennan et al , 1987</TREF> and str98 <REF>Strube, 1998</REF>~~~In all tests the intrasentential anaphors have been manually resolved~~~Expletive and cataphoric uses of pronouns have been marked and excluded from the tests.
Thus, even if there exists a perfect theory, it might not work well with noisy input, or it would not cover all the anaphoric phenomena~~~1Walker <REF>Walker, 1989</REF> compares Brennan, Friedman aad Pollards centering approach <TREF>Brennan et al , 1987</TREF> with Hobbs algorithm <REF>Hohbs, 1976</REF> on a theoretical basis~~~These requirements have motivated us to develop robust, extensible, and trainable anaphora resolution systems~~~Previously <REF>Aone and McKee, 1993</REF>, we reported our data-driven multilingual anaphora resolution system, which is robust, exteusible, and manually trainable.
But, more generally, we must have a theory that is able to handle all cases of pronoun use~~~A pronoun interpretation algorithm based on centering which relied on centering transition preferences was developed in Brennan et aL 1987 Using transition preferences in a pronoun generation rule would cover more cases of pronoun use than is covered by Rule 1, but the application of such transition preferences also proved unhelpful in explaining pronoun patterns in our corpus~~~<REF>Reichman 1985</REF> and <REF>Grosz  Sidner 1986</REF> indicate that discourse segmentation has an effect on the linguistic realization of referring expressions~~~While this is intuitively appealing, it is unclear how to apply this to the generation problem in part because it is unclear how to define discourse segments to a generation system.
This distinction underlies my proposals about the attentional consequences of pitch accents when applied to pronominals, in particular, that while most pitch accents may weaken or reinforce a cospecifiers status as the center of attention, a contrastively stressed pronominal may force a shift, even when contraindicated by textual features~~~To predict and track the center of attention in discourse, theories of centering <REF>Grosz et al , 1983</REF>; <TREF>Brennan et al , 1987</TREF>; <REF>Grosz et al , 1989</REF> and immediate focus <REF>Sidner, 1986</REF> rely on syntactic and grammatical features of the text such as pronominalization and surface sentence position~~~This may be sufficient for written discourse~~~For oral discourse, however, we must also consider the way intonation affects the interpretation of a sentence, especially the cases in which it alters the predictions of centering theories.
3~~~Processing Complex Sentences: A Reason for Extending Focusing Algorithms Although complex sentences are prevalent in written English, most other local focusing research focusing: Sidner 1979 and Carter 1987; centering: Grosz, Joshi, and Weinstein 1983, 1995, Brennan, Friedman, and Pollard 1987, Walker 1989, 1993, Kameyama 1986 2, Walker, Iida, and Cote 1994, Brennan 1998, Kameyama, Passonneau, and Poesio 1993, Linson 1993 and Hoffman 1998; and PUNDIT: Dahl 1986, Palmer et al 1986, and Dahl and Ball 1990 did not explicitly and/or adequately address how to process complex sentences~~~Thus, there is a need to extend focusing algorithms~~~An exception to this rule is the work of <REF>Strube 1996</REF> which applies functionalinformation-structure-based criteria on a per-clause basis, <REF>Kameyama 1998</REF>, and <REF>Strube 1998</REF>.
Overview of the data used~~~5 Preliminary Model Overviews The models evaluated in this paper are based on Centering Theory <REF>Grosz et al , 1995</REF>; <REF>Grosz  Sidner, 1986</REF> and the algorithms devised by Brennan and colleagues 1987 and adapted by <REF>Tetreault 2001</REF>~~~We examine a language-only model based on Tetreaults Left-Right Centering LRC model, a visual-only model that uses a measure of visual salience to rank the objects in the visual field as possible referential anchors, and an integrated model that balances the visual information along with the linguistic information to generate a ranked list of possible anchors~~~51 The Language-Only Model We chose the LRC algorithm <REF>Tetreault, 2001</REF> to serve as the basis for our language-only model.
Together this work suggests that the interlocutors shared visual context has a major impact on their patterns of referring behavior~~~Yet, a number of discourse-based models of reference primarily rely on linguistic information without regard to the surrounding visual environment eg , see <TREF>Brennan et al , 1987</TREF>; <REF>Hobbs, 1978</REF>; <REF>Poesio et al , 2004</REF>; <REF>Strube, 1998</REF>; <REF>Tetreault, 2005</REF>~~~Recently, multi-modal models have emerged that integrate visual information into the resolution process~~~However, many of these models are restricted by their simplifying assumption of communication via a command language.
This preference can be explained in terms of salience from the point of view of the centering theory~~~The latter proposes the ranking subject, direct object, indirect object <TREF>Brennan et al 1987</TREF> and noun phrases which are parts of prepositional phrases are usually indirect objects~~~Collocation pattern preference This preference is given to candidates which have an identical collocation pattern with a pronoun 2,0~~~The collocation preference here is restricted to the patterns noun phrase pronoun, verb and verb, noun phrase pronoun.
Since the constraints are eflective in the lifferent target from ours, the accuracy of identifying the referents of zero pronouns would be improved much more by using both of his constraints and the constraint we proposed~~~As for the identifying method available in general discourses, the centering theory<TREF>Brennan et al , 1987</TREF>; <REF>Walker et al , 1990</REF> and the property sharing theory<REF>Kameyama, 1988</REF> are proposed~~~Although this kind of theory has a good point that it is independent of the type o17 discourse, the linguistic constraints specitic to expressions like the pragmatic constraints l/roposed by Dohsaka or us are more accurate than theirs when the speeitlc constraints are applicable~~~3 General ontology in manuals and prinmry constraints In this section, we consider the general ontology which can be used in,dl types of manuals.
S, O and X Table 1B~~~The members of the CF list are ranked according to their grammatical role <TREF>Brennan et al , 1987</TREF> and their position in the grid3 The derived sequence of CF lists can then be used to compute other important Centering concepts:  The CB, ie the referent that links the current CF list with the previous one such as microsoft in b~~~Transitions <TREF>Brennan et al , 1987</TREF> and NOCBs, that is, cases in which two subsequent CF lists do not have any referent in common~~~Violations of CHEAPNESS <REF>Strube and Hahn, 1999</REF>, COHERENCE and SALIENCE <REF>Kibble and Power, 2000</REF>.
The members of the CF list are ranked according to their grammatical role <TREF>Brennan et al , 1987</TREF> and their position in the grid3 The derived sequence of CF lists can then be used to compute other important Centering concepts:  The CB, ie the referent that links the current CF list with the previous one such as microsoft in b~~~Transitions <TREF>Brennan et al , 1987</TREF> and NOCBs, that is, cases in which two subsequent CF lists do not have any referent in common~~~Violations of CHEAPNESS <REF>Strube and Hahn, 1999</REF>, COHERENCE and SALIENCE <REF>Kibble and Power, 2000</REF>~~~22 Metrics of coherence <REF>Karamanis 2003</REF> assumes a system which receives an unordered set of CF lists as its input and uses a metric to output the highest scoring ordering.
2 <unit finitefinite-yes idu210> <ne idne410 gfsubj>144</ne> is <ne idne411 gfpredicate> a torc</ne> </unit>~~~The ranking of the CFs other than the CP is defined according to the following preference on their gf <TREF>Brennan et al , 1987</TREF>: obj>iobj>other~~~CFs with the same gf are ranked according to the linear order of the corresponding NPs in the utterance~~~The second column of Table 1 shows how the utterances in example 1 are automatically translated by the scripts developed by Poesio et al.
It is often claimed in current work on in natural language generation that the constraints on felicitous text proposed by the theory are useful to guide text structuring, in combination with other factors see <REF>Karamanis, 2003</REF> for an overview~~~However, how successful Centerings constraints are on their own in generating a felicitous text structure is an open question, already raised by the seminal papers of the theory <TREF>Brennan et al , 1987</TREF>; <REF>Grosz et al , 1995</REF>~~~In this work, we explored this question by developing an approach to text structuring purely based on Centering, in which the role of other factors is deliberately ignored~~~In accordance with recent work in the emerging field of text-to-text generation <REF>Barzilay et al , 2002</REF>; <REF>Lapata, 2003</REF>, we assume that the input to text structuring is a set of clauses.
However, in this work we are treating CF lists as an abstract representation Following again the terminology in <REF>Kibble and Power 2000</REF>, we call the requirement that CBn be the same as CBn1 the principle of coherence and the requirement that CBn be the same as CPn the principle of salience~~~Each of these principles can be satisfied or violated while their various combinations give rise to the standard transitions of Centering shown in Table 2; Poesio et als scripts compute these violations6 We also make note of the preference between these transitions, known as Centerings Rule 2 <TREF>Brennan et al , 1987</TREF>: continue is preferred to retain, which is preferred to smoothshift, which is preferred to rough-shift~~~Finally, the scripts determine whether CBn is the same as CPn1, known as the principle of cheapness <REF>Strube and Hahn, 1999</REF>~~~The last column of Table 1 shows the violations of cheapness denoted with an asterisk in 17 23 Evaluating the coherence of a text and text structuring The statistics about transitions computed as just discussed can be used to determine the degree to which a text conforms with, or violates, Centerings principles.
P99-1079:56~~~Analysis of Syntax-Based Pronoun Resolution Methods Joel R Tetreault University of Rochester Department of Computer Science Rochester, NY, 14627 tetreaulcs, rochester, edu Abstract This paper presents a pronoun resolution algorithm that adheres to the constraints and rules of Centering Theory <REF>Grosz et al , 1995</REF> and is an alternative to Brennan et als 1987 algorithm~~~The advantages of this new model, the Left-Right Centering Algorithm LRC, lie in its incremental processing of utterances and in its low computational overhead~~~The algorithm is compared with three other pronoun resolution methods: Hobbs syntax-based algorithm, Strubes S-list approach, and the BFP Centering algorithm.
The noteworthy results were that Hobbs and LRC performed the best~~~The aim of this project is to develop a pronoun resolution algorithm which performs better than the <TREF>Brennan et al 1987</TREF> algorithm 1 as a cognitive model while also performing well empirically~~~A revised algorithm Left-Right Centering was motivated by the fact that the BFP algorithm did not allow for incremental processing of an utterance and hence of its pronouns, and also by the fact that it occasionally imposes a high computational load, detracting from its psycholinguistic plausibility~~~A second motivation for the project is to remedy the dearth of empirical results on pronoun resolution methods.
5~~~Identify Transition with the Cb and Cf resolved, use the criteria from <TREF>Brennan et al , 1987</TREF> to assign the transition~~~It should be noted that BFP makes use of Centering Rule 2 <REF>Grosz et al , 1995</REF>, LRC does not use the transition generated or Rule 2 in steps 4 and 5 since Rule 2s role in pronoun resolution is not yet known see <REF>Kehler 1997</REF> for a critique of its use by BFP~~~Computational overhead is avoided since no anchors or auxiliary data structures need to be produced and filtered.
The RETAIN is motivated as it enables a cheap SMOOTH SHIFT, and so we need a way of evaluating the whole sequence CONTINUE-RETAIN-SHIFT verSUS CONTINUE-CONTINUE-SHIFT~~~: :24,:Ceaateringin :NLG CT has developed primarily in the context of natural language interpretation, focussing on anaphora resolution see eg, <TREF>Brennan et al 1987</TREF>~~~Curiously, NLG researchers have tended to overlook GJWs proposal that Rule 2 provides a constraint on speakers, and on natural-language generation systems To empirically test the claim made by Rule 2 requires examination of differences in inference load of alternative multi-utterance sequences that differentially realize the same content~~~GJW, p 215.
This is also referred to as the backward-looking center or Cb~~~The notion of salience for the purposes of centering theory is most commonly defined according to a hierarchy of grammatical roles: SUBJECT > DIRECT OBJECT > INDIRECT OBJECT > OTHERS see eg, <TREF>Brennan et al 1987</TREF> For alternative approaches see eg, <REF>Strube and Hahn 1999</REF>, <REF>Walker et al 1994</REF>~~~2~~~There is a preference for consecutive utterances within a discourse segment to keep the same entity as the center, and for the center to be realised as Subject or preferred center Cp.
Cheapness is satisfied by a transition pair Un-1, Un, Un, Unl if the preferred center of Un is the Cb of Unl For example, this test is satisfied by a RETAIN-SHIFT sequence but not by CONTINUE-SHIFT, so it is predicted that the former pattern will be used to introduce a new center~~~This claim is consistent with the findings of <REF>Brennan 1998</REF>, <TREF>Brennan et al 1987</TREF>~~~If we consider examples la-e below, the sequence cd-e , including a RETAIN-SHIFT sequence, reads more fluently than c-d-e even though the latter scores better according to the canonical ranking~~~a John has had trouble arranging his vacation.
We found that it is much easier to annotate the building blocks of a theory of the local focus, and then use scripts to automatically compute the CB~~~There are two advantages to this approach: first of all, agreement on the building blocks is much easier to reach than agreement on the CBin our preliminary experiments we didnt go beyond   6 when trying to directly identify the CB using the definitions from <TREF>Brennan et al , 1987</TREF>~~~And secondly, this approach makes it possible to compute the CB according to different ways of instantiating what we call the parameters of Centering eg , ranking~~~We developed such scripts for the work discussed in <REF>Poesio et al , 2004b</REF>; they can be tested on the web site associated with that paper, http://cswwwessexacuk/staff/poesio/ cbc/.
It uses a hierarchy of grammatical roles quite similar to that of RAP, but this role hierarchy does not directly influence antecedent selection~~~Whereas th e hierarchy in RAP contributes to a multi-dimensional measure of the relative salience of all antecedent candidates, in <TREF>Brennan et al 1987</TREF>, it is used only to constrain the choice of the backward-looking center, Cb, of an utterance~~~It does not serve as a general preference measure for antecedence~~~The items in the forward center list, Cf, are ranked according to the hierarchy of grammatical roles.
The ranking of the Cf members is determined by the salience status of the entities in the utterance and mayvary crosslinguistically~~~Kameyama 28198529 and Brennan et al 28198729 proposed that the Cf ranking for English is determined by grammatical function as follows: 28229 Rule for ranking of forward-looking centers: SUBJ3EIND~~~OBJ3EOBJ3EOTHERS Later crosslinguistic studies based on empirical work 28<REF>Di Eugenio, 1998</REF>; <REF>Turan, 1995</REF>; <REF>Kameyama, 1985</REF>29 determined the following detailed ranking, with QIS standing for quanti0Ced inde0Cnite subjects 28people, everyone etc29 and PRO-ARB 28we, you29 for arbitrary plural pronominals~~~28329Revised rule for the ranking of forward-looking centers: SUBJ3EIND.
For example, as pointed out in Bonnie Webbers Penn presentation, there is a distinction between Tree Adjunction Grammar TAG as a linguistic theory and the several algorithms that have been used to implement TAG parsers: Extended CKY parser, Extended Earley parser, Two-pass extended Earley parser based on lexicalized TAGs, and a DCG parser using lexicalized TAGs~~~There is also a distinction between Centering as a theory for resolving anaphoric pronouns <REF>Joshi and Weinstein 1981</REF>; <REF>Gross et al 1983</REF>, and the attempts to use a centering approach to resolving pronouns in an implementation <TREF>Brennan et al 1987</TREF>~~~In addition, one way of looking inside a system is to look at the performance of one or more modules or components~~~Which components are obtained depends on the nature of the decomposition of the system.
PFNOCB, a second baseline, which enhances MNOCB with a global constraint on coherence that <REF>Karamanis, 2003</REF> calls the PageFocus PF~~~PFBFP which is based on PF as well as the original formulation of CT in <TREF>Brennan et al , 1987</TREF>~~~PFKP which makes use of PF as well as the recent reformulation of CT in <REF>Kibble and Power, 2000</REF>~~~<REF>Karamanis et al , 2004</REF> report that PFNOCB outperformed MNOCB but was overtaken by PFBFP and PFKP.
The function for resolving IPAsResolveIpa has similarly been tested on texts, where APAswereexcluded~~~We have compared the obtained results with those obtained by testing bfp <TREF>Brennan et al , 1987</TREF> and str98 <REF>Strube, 1998</REF>~~~In all tests the intrasentential anaphors have been manually resolved and expletive and cataphoric uses of pronouns have been marked and excluded from the test~~~Dialogue act units were marked and classified by three annotators following <REF>Eckert and Strube, 2000</REF>.
three arguments~~~Finally, the measure MBFP <TREF>Brennan et al , 1987</TREF> uses a lexicographic ordering on 4-tuples which indicate whether the transition is a CONTINUE, RETAIN, SMOOTH-SHIFT, or ROUGHSHIFT~~~cT and all four functions it is computed from take three arguments because the classification depends on COHERENCE~~~As the first transition in the discourse is coherent by default it has no Cb, we can compute cI by distinguishing RETAIN and CONTINUE via SALIENCE.
This is in contrast to theories of global coherence, which can consider relations between larger chunks of the discourse and eg structures them into a tree <REF>Mann and Thompson, 1988</REF>; <REF>Marcu, 1997</REF>; <REF>Webber et al , 1999</REF>~~~Measures of local coherence specify which ordering of the sentences makes for the most coherent discourse, and can be based eg on Centering Theory <REF>Walker et al , 1998</REF>; <TREF>Brennan et al , 1987</TREF>; <REF>Kibble and Power, 2000</REF>; <REF>Karamanis and Manurung, 2002</REF> or on statistical models <REF>Lapata, 2003</REF>~~~But while formal models of local coherence have made substantial progress over the past few years, the question of how to efficiently compute an ordering of the sentences in a discourse that maximises local coherence is still largely unsolved~~~The fundamental problem is that any of the factorial number of permutations of the sentences could be the optimal discourse, which makes for a formidable search space for nontrivial discourses.
Based on these concepts, CT classifies the transitions between subsequent utterances into different types~~~Table 1 shows the most common classification into the four types CONTINUE, RETAIN, SMOOTH-SHIFT, and ROUGH-SHIFT, which are predicted to be less and less coherent in this order <TREF>Brennan et al , 1987</TREF>~~~<REF>Kibble and Power 2000</REF> define three further classes of transitions: COHERENCE and SALIENCE, which are both defined in Table 1 as well, and NOCB, the class of transitions for which Cbui is undefined~~~Finally, a transition is considered to satisfy the CHEAPNESS constraint <REF>Strube and Hahn, 1999</REF> if Cbui  Cpui1.
One more filtering criterion is mutual information MI, which reflects the relatedness of two terms in their combination , kj ww  To keep a relation  kji wwwP, we require , kj ww be a meaningful combination~~~We use the following pointwise MI <TREF>Church and Hanks 1989</TREF>:  ,log, kj kj kj wPwP wwPwwMI  We only keep meaningful combinations such that 0, >kj wwMI  By these filtering criteria, we are able to reduce considerably the number of biterms and triterms~~~For example, on a collection of about 200MB, with a vocabulary size of about 148K, we selected only about 27M useful biterms and about 137M triterms, which remain tractable~~~33 Probability of Biterms In LM used in IR, each query term is attributed the same weight.
33 Scoring the semantic similarity of word pairs Measuring the semantic similarity of words on the basis of raw corpus data is obviously a much harder task than measuring the orthographic similarity of words~~~Mutual information first introduced to computational linguistics by <TREF>Church and Hanks 1989</TREF> is one of many measures that seems to be roughly correlated to the degree of semantic relatedness between words~~~The mutual information between two words A and B is given by: IA;B  log PrA;BPrAPrB 1 Intuitively, the larger the deviation between the empirical frequency of co-occurrence of two words and the expected frequency of co-occurrence if they were independent, the more likely it is that the occurrence of one of the two words is not independent from the occurrence of the other~~~Brown et alii 1990 observed that when mutual information is computed in a bi-directional fashion, and by counting co-occurrences of words within a 4Most of the pairs in this block  78  are actually morphologically related.
2In the case of an interrupted collocation, words can be separated by an arbitrary number of words, whereas 71 sin:e,:hey assumed that a collocation is a se-,lun:e of adjacent words that frequently apl:,ar tgether~~~<TREF>Church and Hanks, 1989</TREF> delhw:I ;t collocation as a pair of correlated words :mi,,set mutual information to evaluate such ,xi:a,1 :orrelations of word pairs of length two~~~They retrieved interrupted word pairs, as well as,minterrupted word pairs~~~<REF>Haruno et al , 1996</REF>,:onstructed collocations by combining adjacent n-grams with high value of mutual information.
Because of their low ambiguity and high specificity, these words are also particularly useful for conceptualizing a knowledge domain or for supporting the creation of a domain ontology~~~Candidate terminological expressions are usually captured with more or less shallow techniques, ranging from stochastic methods <TREF>Church and Hanks 1989</TREF>; <REF>Yamamoto and Church 2001</REF> to more sophisticated syntactic approaches <REF>Jacquemin 1997</REF>~~~155 Navigli and Velardi Learning Domain Ontologies WordNet domain corpus contrastive corpora terminology extraction candidate extraction terminology filtering semantic interpretation semantic disambiguation identification of taxonomic relations identification of conceptual relations Inductive learner Natural Language Processor ontology integration and updating 3 2 1 Lexical Resources Domain Concept Forest Figure 3 The architecture of OntoLearn~~~Obviously, richer syntactic information positively influences the quality of the result to be input to the statistical filtering.
Afterward, if a target adjective sense was not resolved, semantic indicator attributes were applied; no individual indicator nouns were used~~~The semantic attributes that were applied were animate, body part, color, concrete, human, and text type; <TREF>Church and Hanks 1989</TREF> had pointed to two of these attributes, person and body part also time, previously mentioned above in a seemingly casual listing of just five attributes potentially useful for describing the lexico-syntactic regularities of noun-verb relations~~~Table 1 shows that these few, general attributes cover almost three-quarters of all instances of the target adjectives~~~Disambiguation by these syntactic and semantic attributes is effectively as reliable as disambiguation using significant indicator nouns: having three apparent errors in disambiguation is not significantly worse than the errorless performance of the significant indicator nouns in the 100-sentence samples.
Content words that have a close syntactic relation to one another are useful candidates for examination and are intuitively more likely to bear a close semantic relation than words that are near one another but are not related syntactically~~~One much-studied example is the semantic relation between a verb and its arguments eg , <REF>Boguraev et al 1989</REF>; <TREF>Church and Hanks 1989</TREF>; Braden-<REF>Harder 1991</REF>; <REF>Hindle and Rooth 1991</REF>~~~Discrimination among senses of adjectives based on the nouns they modify or of which they are predicated has been the subject of less intensive and systematic study~~~Determining the potential of this line of evidence is the focus of this paper.
Like path coreference, semantic compatibility can be considered a form of world knowledge needed for more challenging pronoun resolution instances~~~We encode the semantic compatibility between a noun and its parse tree parent and grammatical relationship with the parent using mutual information MI <TREF>Church and Hanks, 1989</TREF>~~~Suppose we are determining whether ham is a suitable antecedent for the pronoun it in eat it~~~We calculate the MI as: MIeat:obj, ham  log Preat:obj:hamPreat:objPrham Although semantic compatibility is usually only computed for possessive-noun, subject-verb, and verb-object relationships, we include 121 different kinds of syntactic relationships as parsed in our news corpus3 We collected 488 billion parent:rel:node triples, including over 327 million possessive-noun values, 129 billion subject-verb and 877 million verb-direct object.
This line of research was motivated by a series of successful applications of mutual information statistics to other problems in natural language processing~~~In the last decade, research in speech recognition <REF>Jelinek 1985</REF>, noun classification <REF>Hindle 1988</REF>, predicate argument relations <TREF>Church  Hanks 1989</TREF>, and other areas have shown that mutual information statistics provide a wealth of information for solving these problems~~~22 Mutual Information Statistics The mutual information statistic <REF>Fano 1961</REF> is a measure of the interdependence of two signals in a message~~~It is a function of the probabilities of the two events: Mz, u  log u xzPvy In this paper, the events x and y will be part-of-speech n-grams instead of single parts-of-speech, as in some earlier work.
The idea behind it is that similar words tend to co-occur in certain patterns~~~Considerable efforts have been devoted to measure word similarity based on cooccurrence frequency of two words in a window <TREF>Church and Hanks, 1989</TREF>; <REF>Turney, 2001</REF>; <REF>Terra and Clarke, 2003</REF>; <REF>Matsuo et al, 2006</REF>~~~In addition to the classical window-based technique, some studies investigated the use of lexico-syntactic patterns eg, X or Y to get more accurate co-occurrence statistics <REF>Chilovski and Pantel, 2004</REF>; <REF>Bollegala et al, 2007</REF>~~~These two approaches are complementary with each other, because they are founded on different hypotheses and utilize different corpus statistics.
For example, Wilks et al~~~1989 use this ratio as a criterion for establishing links between words in a semantic network; <TREF>Church and Hanks 1989</TREF> use the logarithm of this ratio as a measure for word association~~~14 Justeson and Katz Co-occurrences of Antonymous Adjectives Under this formulation of the co-occurrence theory, acquiring the lexical relation of antonymy requires a certain amount of training for the association, and as the frequency of adjectives declines, so must the frequency of training for its associations~~~On the whole, then, very infrequent training should result in weaker associations; more generally, adjective frequency should correlate with the strength of lexical associations.
In the past, for this purpose a number of measures have been proposed~~~They were based on mutual information <TREF>Church  Hanks, 1989</TREF>, conditional probabilities <REF>Rapp, 1996</REF>, or on some standard statistical tests, such as the chi-square test or the loglikelihood ratio <REF>Dunning, 1993</REF>~~~For the purpose of this paper, we decided to use the loglikelihood ratio, which is theoretically well justified and more appropriate for sparse data than chi-square~~~In preliminary experiments it also led to slightly better results than the conditional probability measure.
Finally, methods and strategies for handling low-frequency data are suggested~~~The measures2  Mutual Information a0a2a1  <TREF>Church and Hanks, 1989</TREF>, the log-likelihood ratio test <REF>Dunning, 1993</REF>, two statistical tests: t-test and a3a5a4 -test, and co-occurrence frequency  are applied to two sets of data: adjective-noun AdjN pairs and preposition-noun-verb PNV triples, where the AMs are applied to PN,V pairs~~~See section 3 for a description of the base data~~~For evaluation of the association measures, a6 -best strategies section 41 are supplemented with precision and recall graphs section 42 over the complete data sets.
6 Probabifities were estimated using the Penn Treebank version of the Brown corpus~~~The pairs come from an example given by <TREF>Church and Hanks 1989</TREF>, illustrating the words that human subjects most frequently judged as being associated with the word doctor~~~The word sick also appeared on the list, but is excluded here because it is not a noun~~~Word 1 Word 2 doctor nurse doctor lawyer doctor man doctor medicine doctor hospital doctor health doctor sickness Similarity Most Informative Subsumer 94823 health professional 72240 professional person 29683 person, individual 10105 <entity 10105 <entity 00 virtual root 00 virtual root Doctors are minimally similar to medicine and hospitals, since these things are all instances of something having concrete existence, riving or nonliving WordNet class ent ty, but they are much more similar to lawyers, since both are kinds of professional people, and even more similar to nurses, since both are professional people working specifically within the health professions.
By no means an exhaustive list, the most commonly cited ranking and scoring algorithms are HITS <REF>Kleinberg 1998</REF> and PageRank <REF>Page et al 1998</REF>, which rank hyperlinked documents using the concepts of hubs and authorities~~~The most well-known keyword scoring methods within the IR community are the tf-idf <REF>Salton and McGill 1983</REF> and pointwise mutual information <TREF>Church and Hanks 1989</TREF> measures, which put more importance on matching keywords that occur frequently in a document relative to the total number of documents that contain the keyword by normalizing term frequencies with inverse document frequencies~~~Various methods including tf-idf have been comparatively evaluated by <REF>Salton and Buckley 1987</REF>~~~Creating nbest lists using the above algorithms produce result sets where each result is considered independently.
41 EIIR: Expected Independent Information Ranking Model Baseline Model Recall the task definition from Section 3~~~Finding a property r that most reduces the uncertainty in a query set Q can be modeled by measuring the strength of association between r and Q <REF>Following Pantel and Lin 2002</REF>, we use pointwise mutual information pmi to measure the association strength between two events q and r, where q is a term in Q and r is syntactic dependency, as follows <TREF>Church and Hanks 1989</TREF>:      N fqc N rwc N rqc Ff Ww rqpmi       , , , log,  41 where cq,r is the frequency of r in the feature vector of q as defined in Section 32, W is the set of all words in our corpus, F is the set of all syntactic dependencies in our corpus, and N   WwFf fwc , is the total frequency count of all features of all words~~~We estimate the association strength between a property r and a set of terms Q by taking the expected pmi between r and each term in Q as:       Qq rqpmiqPrQpmi ,,  42 where Pq is the probability of q in the corpus~~~Finally, the EIIR model chooses an n-best list by selecting the n properties from R that have highest pmiQ, r.
32 Contexts The context in which a word appears often imposesconstraintsonthesemantictypeoftheword~~~This basic idea has been exploited by many proposals for distributional similarity and clustering, eg, <TREF>Church and Hanks, 1989</TREF>; <REF>Lin, 1998</REF>; <REF>Pereira et al , 1993</REF>~~~Similar to <REF>Lin and Pantel 2001</REF>, we define the contexts of a word to be the undirected paths in dependency trees involving that word at either the beginning or the end~~~The following diagram shows an example dependency tree: Which city hosted the 1988 Winter Olympics.
RB, RBR, or RBS VB, VBD, VBN, or VBG anything The second step is to estimate the semantic orientation of the extracted phrases, using the PMI-IR algorithm~~~This algorithm uses mutual information as a measure of the strength of semantic association between two words <TREF>Church  Hanks, 1989</TREF>~~~PMI-IR has been empirically evaluated using 80 synonym test questions from the Test of English as a Foreign Language TOEFL, obtaining a score of 74 <REF>Turney, 2001</REF>~~~For comparison, Latent Semantic Analysis LSA, another statistical measure of word association, attains a score of 64 on the 3 http://wwwcsjhuedu/brill/RBT114tarZ 4 <REF>See Santorini 1995</REF> for a complete description of the tags.
same 80 TOEFL questions <REF>Landauer  Dumais, 1997</REF>~~~The Pointwise Mutual Information PMI between two words, word1 and word2, is defined as follows <TREF>Church  Hanks, 1989</TREF>: pword1  word2 PMIword1, word2  log2 pword1 pword2 1 Here, pword1  word2 is the probability that word1 and word2 co-occur~~~If the words are statistically independent, then the probability that they co-occur is given by the product pword1 pword2~~~The ratio between pword1  word2 and pword1 pword2 is thus a measure of the degree of statistical dependence between the words.
Thus we introduce d-bigram which is a bigram cooccurrence information concerning the distance<REF>Tsutsumi et al , 1993</REF>~~~Expression 1 calculates the score between two neighboring letters; UKi  E E Mwj,wid;d  x,qd 1 dl j-i--d--1 where wl as an eveN;, d as the distance between two eveN;s, dmax as the maximum distance used in the processing we set drnax - 5, and gd as the weight fimction on distance for this system gd  d-2<REF>Sano et al , 1996</REF>, to decrease tile influence of tile d-bigrams when the distance get longer <TREF>Church and Hanks, 1989</TREF>~~~When calculating the linking score between the letters wi and Wil, tile d-bigram information of the letter pairs around tim target two such as wi-l, wi2; 3 are added~~~Expression 2 calculates the mutual information between two events with d-bigram data; v; d d -2 where x, y as events, d for the distance between two events, and Px as the probability.
Linking Score Expression 2 is tbr calculating the linking score between two letters in a sentence ~~~Z 2 d:-:l ji-d-1 dmax : max distance used wl : the i-th letter in the sentence w gd : a certain weight for iV// concerning distance between letters The information between two remote words has less nmaning in a sentence when it comes to the semantic analysis<TREF>Church and Hanks, 1989</TREF>~~~According to the idea we put gd in the expression so that nearer pair can be more effective in calculating the score of the sentence~~~ hi,, I I I--1 B C  F G H Figure 3: Calculation of Linking Score A pair of far-away letters do not have strong relation between each other, neither syntactically nor semantically.
Word alignments that are shared by many different words are most probably mismatches~~~For this experiment we used Pointwise Mutual Information I <TREF>Church and Hanks, 1989</TREF>~~~IW, f  log PW, fPWPf,where W is the target word PW is the probability of seeing the word Pf is the probability of seeing the feature PW,f is the probability of seeing the word and the feature together~~~33 Word Alignment The multilingual approach we are proposing relies on automatic word alignment of parallel corpora from Dutch to one or more target languages.
For statistical features, previous work Section 2 suggests that the mutual information between the decision tokens xL0 and xR0 may be appropriate~~~The log of the pointwise mutual information <TREF>Church and Hanks, 1989</TREF> between the decision-boundary tokens xL0, xR0 is: MIxL0, xR0  log PrxL0xR0Prx L0PrxR0 This is equivalent to the sum: log CxL0xR0  log K log CxL0 log CxR0~~~For web-based features, the counts C~~~can be taken as a search engines count of the number of pages containing the term.
Tile focus of much of this work was to develop the methods themselves~~~<TREF>Church and Hanks 1989</TREF> explored tile use of mutual information statistics in ranking co-occurrences within five-word windows~~~<REF>Smadja 1992</REF> gathered co-occurrences within fiveword windows to find collocations, particularly in specific domains~~~<REF>Hindle 1990</REF> classified nouns on the basis of co-occurring patterns of subjectverb and verb-object pairs.
Collocations have been studied by computational linguists in different contexts~~~For instance, there is a substantial body of papers on the extraction of frequently co-occurring words from corpora using statistical methods eg , <REF>Choueka et al , 1983</REF>, <TREF>Church and Hanks, 1989</TREF>, <REF>Smadja, 1993</REF> to list only a few~~~These authors focus on techniques for providing material that can be used in other processing tasks such as x The research rcpmlcd in this paper was undmtaken as the project Collocations and the Lexicalisation of Semantic Operations ET10/75~~~Financial contributions weir by the Commission of the European Community, Association Suissetra Geneva and Oxford University Press.
Hence, greater the frequency, the more is the likelihood of the expression to be a MWE~~~612 Point-wise Mutual Information a16  Point-wise Mutual information of a collocation <TREF>Church and Hanks, 1989</TREF> is defined as, a16a18a17a19a11a21a20a23a22a25a24a27a26 a15a28a17a19a11a2a20a23a22a25a24a30a29a31a15a28a17a33a32a34a20a35a32a36a24 a15a28a17a19a11a2a20a35a32a36a24a30a29a37a15a28a17a33a32a34a20a23a22a25a24 where, a11 is the verb and a22 is the object of the collocation~~~The higher the Mutual information of a collocation, the more is the likelihood of the expression to be a MWE~~~613 Least mutual information difference with similar collocations a38  This feature is based on Lins work <REF>Lin, 1999</REF>.
Various statistical measures have been suggested for ranking expressions based on their compositionality~~~Some of these are Frequency, Mutual Information <TREF>Church and Hanks, 1989</TREF>, distributed frequency of object <REF>Tapanainen et al , 1998</REF> and LSA model <REF>Baldwin et al , 2003</REF> <REF>Schutze, 1998</REF>~~~In this paper, we define novel measures both collocation based and context based measures to measure the relative compositionality of MWEs of V-N type see section 6 for details~~~Integrating these statistical measures should provide better evidence for ranking the expressions.
These ranks are then compared with the human ranking~~~<REF>Breidt, 1995</REF> has evaluated the usefulness of the Point-wise Mutual Information measure as suggested by <TREF>Church and Hanks, 1989</TREF> for the extraction of V-N collocations from German text corpora~~~Several other measures like Log-Likelihood <REF>Dunning, 1993</REF>, Pearsons a2a4a3 <REF>Church et al , 1991</REF></REF>, Z-Score <REF>Church et al , 1991</REF></REF>, Cubic Association Ratio MI3, etc , have been also proposed~~~These measures try to quantify the association of two words but do not talk about quantifying the non-compositionality of MWEs.
However, the focus was more on balancing the effect of query expansion techniques such that different concepts in the query were equally benefited~~~Mutual information has been used previously in <TREF>Church and Hanks, 1989</TREF> to identify collocations of terms for identifying semantic relationships in text~~~Experiments were confined to bigrams~~~The use of MaST over a graph of mutual information values to incorporate the most significant dependencies between terms was first noted in <REF>Rijsbergen, 1979</REF>.
Mutual Information is attractive because it is not only easy to compute, but also takes into consideration corpus statistics and semantics~~~The mutual information between two terms <TREF>Church and Hanks, 1989</TREF> can be calculated using Equation 2~~~Ix,y  log nx,y N nx N ny N 2 nx,y is the number of times terms x and y occurred within a term window of 100 terms across the corpus, while nx and ny are the frequencies of x and y in the collection of size N terms~~~To tackle the situation where we have an arbitrary number of variables terms we extend the twovariable case to the multivariate case.
1~~~Mutual <REF>Information Church and Hanks 1989</REF> discussed the use of the mutual information statistic in order to identify a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type content word/content word to lexico-syntacfic co-occurrence constraints between verbs and prepositions content word/function word~~~Mutual information, lx;y, compares the probability of observing word x and word y together the joint probability with the probabilities of observing x and y independently chance~~~lx;y - log 2 Px,y ex ey If there is a genuine association between x and y, then the joint probability Px,y will he much larger than chance Px Py, and consequently lx;y >> 0, as illustrated in the table below.
75 Some Interesting Associations with Doctor in the 1987 AP Corpus N  15 million; w  6 Ix; y fx y fx x fly Y 2~~~Phrasal Verbs 80 24 111 honorary 621 doctor 80 16 1105 doctors 44 dentists 84 60 1105 doctors 241 nurses 71 16 1105 doctors 154 treating 67 12 275 examined 621 doctor 66 12 1105 doctors 317 treat 64 50 621 doctor 1407 bills 64 12 621 doctor 350 visits 63 38 1105 doctors 676 hospitals 61 12 241 nurses 1105 doctors Associations with Doctor Some Less Interesting -13 12 621 doctor 73785 with -14 82 284690 a 1105 doctors 14 24 84716 is 1105 doctors <TREF>Church and Hanks 1989</TREF> also used the mutual information statistic in order to identify phrasal verbs, following up a remark by Sinclair: How common are the phrasal verbs with set~~~Set is particularly rich in making combinations with words like about, in, up, out, on, off, and these words are themselves very common~~~How likely is set off to occur.
Extraction of V-N-Collocations from Text Corpora: A Feasibility Study for German Elisabeth Breidt Seminar fiir Sprachwissenschaft University of Tiibingen Kleine Wilhelmstr~~~113, D-72074 Tiibingen breidtarbucklesnsneuphilologieuni-tuebingende Abstract The usefulness of a statistical approach suggested by <TREF>Church and Hanks 1989</TREF> is evaluated for the extraction of verb-noun V-N collocations from German text corpora~~~Some motivations for the extraction of V-N collocations from corpora are given and a couple of differences concerning the German language are mentioned that have implications on the applicability of extraction methods developed for English~~~We present precision and recall results for V-N collocations with support verbs and discuss the consequences for further work on the extraction of collocations from German corpora.
Collocations present an area that is important both for lexicography to improve their coverage in modern dictionaries as well as for lexical acquisition in computational linguistics, where the goal is to build either large reusable lexical databases LDBs or specific lexica for specialized NLP-applications~~~We have tested the statistical approach Mutual Information MI, brought up by <TREF>Church and Hanks 1989</TREF> for linguistics, for a semiautomatic extraction of verb-noun V-N collocations from untagged German text corpora~~~We try to answer the question how much can be done with an untagged corpus and what might be gained by lemmatizing, POS-tagging or even superficial parsing~~~<REF>Choueka 1988</REF> describes how to automatically extract word combinations from English corpora as a preselection of collocation candidates to ease a lexicographers search for collocations.
AER  MergePos 054 045 049 05101  MergeMI 055 045 050 05045 Table 6: Results using the compositionality based features pressions of various types~~~Some of them are Frequency, Point-wise mutual information <TREF>Church and Hanks, 1989</TREF>, Distributed frequency of object <REF>Tapanainen et al , 1998</REF>, Distributed frequency of object using verb information <REF>Venkatapathy and Joshi, 2005</REF></REF>, Similarity of object in verbobject pair using the LSA model <REF>Baldwin et al , 2003</REF>, <REF>Venkatapathy and Joshi, 2005</REF></REF> and Lexical and Syntactic fixedness <REF>Fazly and Stevenson, 2006</REF>~~~These features have largely been evaluated by the correlation of the compositionality value predicted by these measures with the gold standard value suggested by human judges~~~It has been shown that the correlation of these measures is higher than simple baseline measures suggesting that these measures represent compositionality quite well.
In the past, various measures have been suggested for measuring the compositionality of multi-word expressions~~~Some of these are mutual information <TREF>Church and Hanks, 1989</TREF>, distributed frequency <REF>Tapanainen et al , 1998</REF> and Latent Semantic Analysis LSA model <REF>Baldwin et al , 2003</REF>~~~Even though, these measures have been shown to represent compositionality quite well, compositionality itself has not been shown to be useful in any application yet~~~In this paper, we explore this possibility of using the information about compositionality of MWEs verb based for the word alignment task.
Our main aim is here to investigate the use of long-distance semantic dependencies to dynamically adapt the prediction to the current semantic context of communication~~~Similar work has been done by <REF>Li and Hirst 2005</REF> and <REF>Matiasek and Baroni 2003</REF>, who exploit Pointwise Mutual Information PMI; <TREF>Church and Hanks, 1989</TREF>~~~Trnka et al~~~2005 dynamically interpolate a high number of topic-oriented models in order to adapt their predictions to the current topic of the text or conversation.
More is to be learned from the fact that you can drink wine than from the fact that you can drink it even though there are more clauses in our sample with  as an object of drink than with wine~~~To capture this intuition, we turn, following <TREF>Church and Hanks 1989</TREF>, to mutual information see <REF>Fano 1961</REF>~~~The mutual information of two events lx y is defined as follows: Px y lxy  log2 Px Py where Px y is the joint probability of events x and y, and Px and Py axe the respective independent probabilities~~~When the joint probability Px y is high relative to the product of the independent probabilities, I is positive; when the joint probability is relatively low, I is negative.
The aim of this measure is to indicate the relatedness between two elements composing a pair~~~Mutual information has been positively used in many NLP tasks such as collocation analysis <TREF>Church and Hanks, 1989</TREF>, terminology extraction <REF>Damerau, 1993</REF>, and word sense disambiguation <REF>Brown et al , 1991</REF>~~~3 Experimental Evaluation As many other corpus linguistic approaches, our entailment detection model relies partially on some linguistic prior knowledge the expected structure of the searched collocations, ie, the textual entailment patterns and partially on some probability distribution estimation~~~Only a positive combination of both these two ingredients can give good results when applying and evaluating the model.
In the first stage, pairwise lexical relations are retrieved using only statistical information~~~This stage is comparable to <TREF>Church and Hanks 1989</TREF> in that it evaluates a certain word association between pairs of words~~~As in <TREF>Church and Hanks 1989</TREF>, the words can appear in any order and they can be separated by an arbitrary number of other words~~~However, the statistics we use provide more information and allow us to have more precision in our output.
Example sentences containing the two words in the two possible positions are:  The provision is aimed at making a hostile takeover prohibitively expensive by enabling Borg Warners stockholders to buy the The pill would make a takeover attempt more expensive by allowing the retailers shareholders to buy more company stock Let us note that this filtering method is an original contribution of our work~~~Other works such as <TREF>Church and Hanks 1989</TREF> simply focus on an evaluation of the correlation of appearance of a pair of words, which is roughly equivalent to condition C1~~~See next section~~~However, taking note of their pattern of appearance allows us to filter out more irrelevant collocations with C2 and C3.
Finally, at a more general level, although disambiguation was originally considered as a performance task, the collocations retrieved have not been used for any specific computational task~~~<TREF>Church and Hanks 1989</TREF> describe a different set of techniques to retrieve collocations~~~A collocation as defined in their work is a pair of correlated words~~~That is, a collocation is a pair of words that appear together more often than expected.
Collocations in the lexicographic meaning are only dealt with in the lexical approach~~~Aside from the work we present in this paper, most of the work carried out within the lexical approach has been done in computer-assisted lexicography by <REF>Choueka, Klein, and Neuwitz 1983</REF> and Church and his colleagues <TREF>Church and Hanks 1989</TREF>~~~Both works attempted to automatically acquire true collocations from corpora~~~Our work builds on Chouekas, and has been developed contemporarily to Churchs.
This stage is comparable to <TREF>Church and Hanks 1989</TREF> in that it evaluates a certain word association between pairs of words~~~As in <TREF>Church and Hanks 1989</TREF>, the words can appear in any order and they can be separated by an arbitrary number of other words~~~However, the statistics we use provide more information and allow us to have more precision in our output~~~The output of this first stage is then passed in parallel to the next two stages.
This limitation is intrinsic to the technique used since mutual information scores are defined for two items~~~The second limitation is that many collocations identified in <TREF>Church and Hanks 1989</TREF> do not really identify true collocations, but simply pairs of words that frequently appear together such as the pairs doctor-nurse, doctor-bill, doctor-honorary, doctors-dentists, doctors-hospitals, etc These co-occurrences are mostly due to semantic reasons~~~The two words are used in the same context because they are of related meanings; they are not part of a single collocational construct~~~The work we describe in the rest of this paper is along the same lines of research.
The two words are often used together because they are associated with the same context rather than for pure structural reasons~~~Many collocations retrieved in <TREF>Church and Hanks 1989</TREF> were of this type, as they retrieved doctors-dentists, doctors-nurses, doctorbills, doctors-hospitals, nurses-doctor, etc , which are not collocations in the sense defined above~~~Such collocations are not of interest for our purpose, although they could be useful for disambiguation or other semantic purposes~~~Condition C2 filters out exactly this type of collocations.
In this case, we are interested in collocations between the head of a PP complement, a preposition and the head of the phrase being postmodified~~~In general, these words will not be adjacent in the text, so it will not be possible to use existing approaches unmodified eg <TREF>Church and Hanks 1989</TREF>, because these apply to adjacent words in unanalyzed text~~~<REF>Hindle and Rooth 1991</REF> report good results using a mutual information measure of collocation applied within such a structurally defined context, and their approach should carry over to our framework straightforwardly~~~One way of integrating structural collocational information into the system presented above would be to make use of the semantic component of the ANLT grammar This component pairs logical forms with each distinct syntactic analysis that represent, among other things, the predicate-argument structure of the input.
Finally, we write the conditional probability as a function of : Pci  1jsi1,si  11  n1    n where   PH1P H0  Psi1,siPs i1Psi  Psijsi1Ps i The conditional probability, Pci  1jsi1,si is a mapping g from  2 0,1 to p 2 0, 1~~~Beginning with <TREF>Church and Hanks, 1989</TREF>, numerous authors have used the pointwise mutual information between pairs of words to analyze word co-locations and associations~~~This ratio tells us whether si1 and si co-occur more or less often than would be expected by chance alone~~~Consider, for example, the tags DT determiner and NN noun, and the four possible ordered tagpairs.
Since we use grammatical context, the feature set is considerably larger than the simple word based proximity feature set for the newspaper corpus~~~43 Calculating Feature Vectors Having collected all nouns and their features, we now proceed to construct feature vectors and values for nouns from both corpora using mutual information <TREF>Church and Hanks, 1989</TREF>~~~We first construct a frequency count vector Ce  ce1,ce2,,cek, where k is the total number of features and cef is the frequency count of feature f occurring in word e Here, cef is the number of times word e occurred in context f We then construct a mutual information vector MIe  mie1,mie2,,miek for each word e, where mief is the pointwise mutual information between word e and feature f, which is defined as: mief  log cef Nsummationtext n i1 cif N  summationtextk j1 cej N 6 where n is the number of words and N  5We perform this operation so that we can compare the performance of our system to that of <REF>Pantel and Lin 2002</REF>~~~summationtextn i1 summationtextm j1 cij is the total frequency count of all features of all words.
The SO of a phrase is determined based upon the phrases pointwise mutual information PMI with the words excellent and poor~~~PMI is defined by <TREF>Church and Hanks 1989</TREF> as follows: a0a2a1a4a3a6a5a8a7a10a9a12a11a13a7a15a14a17a16a19a18a21a20a23a22a25a24 a14a27a26a29a28 a5a8a7a10a9a19a30a31a7a15a14a17a16 a28 a5a8a7 a9 a16 a28 a5a8a7 a14 a16a33a32 1 where a28 a5a8a7a10a9a19a30a31a7a15a14a12a16 is the probability that a7a34a9 and a7a35a14 co-occur~~~The SO for a a28a37a36a39a38a41a40a29a42a44a43 is the difference between its PMI with the word excellent and its PMI with the word poor The method used to derive these values takes advantage of the possibility of using the World Wide Web as a corpus, similarly to work such as <REF>Keller and Lapata, 2003</REF>~~~The probabilities are estimated by querying the AltaVista Advanced Search engine1 for counts.
To solve the problem, we make use of a kind of cooperative evolution strategy to design an evolutionary algorithm~~~Word compositions have long been a concern in lexicography<REF>Benson et al 1986</REF></REF>; <REF>Miller et al 1995</REF>, and now as a specific kind of lexical knowledge, it has been shown that they have an important role in many areas in natural language processing, eg, parsing, generation, lexicon building, word sense disambiguation, and information retrieving, etceg , <REF>Abney 1989, 1990</REF>; <REF>Benson et al 1986</REF></REF>; <REF>Yarowsky 1995</REF>; <TREF>Church and Hanks 1989</TREF>; Church, <REF>Gale, Hans, and Hindle 1989</REF>~~~But due to the huge number of words, it is impossible to list all compositions between words by hand in dictionaries~~~So an urgent problem occurs: how to automatically acquire word compositions.
While bound compositions are not predictable, ie, their reasonableness cannot be derived from the syntactic and semantic properties of the words in them<REF>Smadja 1993</REF>~~~Now with the availability of large-scale corpus, automatic acquisition of word compositions, especially word collocations from them have been extensively studiedeg , <REF>Choueka et al 1988</REF>; <TREF>Church and Hanks 1989</TREF>; <REF>Smadja 1993</REF>~~~The key of their methods is to make use of some statistical means, eg, frequencies or mutual information, to quantify the compositional strength between words~~~These methods are more appropriate for retrieving bound compositions, while less appropriate for retrieving free ones.
The classifier can also be used to rank these vectors according to their relative compositionality~~~3 Related <REF>Work Church and Hanks 1989</REF> proposed a measure of association called Mutual Information 9~~~Mutual Information MI is the logarithm of the ratio between the probability of the two words occurring together and the product of the probability of each word occurring individually~~~The higher the MI, the more likely are the words to be associated with each other.
Dictionaries produced by hand always substantially lag real language use~~~The last two points do not argue against the use of existing dictionaries, but show that the incomplete information that they provide needs to be supplemented with further knowledge that is best collected automatically The desire to combine hand-coded and automatically learned knowledge 1A point made by <TREF>Church and Hanks 1989</TREF>~~~Arbitrary gaps in listing can be smoothed with a program such as the work presented here~~~For example, among the 27 verbs that most commonly cooccurred with from, Church and Hanks found 7 for which this 235 suggests that we should aim for a high precision learner even at some cost in coverage, and that is the approach adopted here.
They are estimated by using Table 2~~~C8B4CRCYD4D3D7B5 BP CUB4CRBND4D3D7B5 CUB4CRBND4D3D7B5B7CUB4BMCRBND4D3D7B5 C8B4CRCYD2CTCVB5 BP CUB4CRBND2CTCVB5 CUB4CRBND2CTCVB5B7CUB4BMCRBND2CTCVB5 PMI based polarity value Using PMI, the strength of association between CR and positive sentences and negative sentences is defined as follows <TREF>Church and Hanks, 1989</TREF>~~~C8C5C1B4CRBND4D3D7B5 BP D0D3CV BE C8B4CRBND4D3D7B5 C8B4CRB5C8B4D4D3D7B5 C8C5C1B4CRBND2CTCVB5 BP D0D3CV BE C8B4CRBND2CTCVB5 C8B4CRB5C8B4D2CTCVB5 PMI based polarity value is defined as their difference~~~This idea is the same as <REF>Turney, 2002</REF>.
The reason for choosing this measure is that it can be used to compute the distance between any two co-occurrence vectors independent of any information about other words~~~This is in contrast to many other measures, eg, <REF>Lin 1998</REF>, which use the co-occurrences of features with other words to compute a weighting function such as mutual information MI <TREF>Church and Hanks, 1989</TREF>~~~Since we only have corpus data for the target phrases, it is not possible for us to use such a measure~~~However, the -skew divergence measure has been shown <REF>Weeds, 2003</REF> to perform comparably with measures which use MI, particularly for lower frequency target words.
If the absolute value of the relative distance in a sentence for a feature and an opinion word is less than Minimum-Offset, they are considered contextdependent~~~Many methods have been proposed to measure the co-occurrence relation between two words such as  2 Church and Mercer,1993 , mutual information <TREF>Church and Hanks, 1989</TREF></TREF>; <REF>Pantel and Lin, 2002</REF>, t-test <TREF>Church and Hanks, 1989</TREF></TREF>, and loglikelihood Dunning,1993~~~In this paper a revised formula of mutual information is used to measure the association since mutual information of a lowfrequency word pair tends to be very high~~~Table 1 gives the contingency table for two words or phrases w 1  and  w 2 , where A is the number of reviews where w 1  and w 2  co-occur; B indicates the number of reviews where w 1  occurs but does not co-occur with w 2 ; C denotes the number of reviews where w 2  occurs but does not co-occur with w 1 ; D is number of reviews where neither w 1  nor w 2  occurs; N  A  B  C  D With the table, the revised formula of mutual information is designed to calculate the association of w 1 with w 2  as formula 1.
Other types of phrases~~~Many efficient techniques exist to extract multiword expressions, collocations, lexical units and idioms <TREF>Church and Hanks, 1989</TREF>; <REF>Smadja, 1993</REF>; <REF>Dias et al , 2000</REF>; <REF>Dias, 2003</REF>~~~Unfortunately, very few have been applied to information retrieval with a deep evaluation of the results~~~Maximal Frequent Sequences.
An inter-domain entropy IDE measure will be proposed for this purpose~~~2 Conventional Clustering View for Constructing Lexicon Trees One conventional way to construct the lexicon hierarchy from web corpora is to collect the terms in all web documents and measure the degree of word association between word pairs using some well-known association metrics <TREF>Church and Hanks, 1989</TREF>; <REF>Smadja et al , 1996</REF> as the distance measure~~~Terms of high association are then clustered bottom-up using some clustering techniques to build the hierarchy~~~The clustered hierarchy is then submitted to lexicographers to assign a semantic label to each sub-cluster.
The definition can be easily extended to a set of expressions T Given a pair vt and vh we define the following entailment strength indicator Svt,vh~~~Specifically, the measure Snomvt,vh is derived from point-wise mutual information <TREF>Church and Hanks, 1989</TREF>: Snomvt,vh  log pvt,vhnompv tpvhpers 3 where nom is the event of having a nominalized textual entailment pattern and pers is the event of having an agentive nominalization of verbs~~~Probabilities are estimated using maximum-likelihood: pvt,vhnom  fCPnomvt,vhf C uniontextP nomvprimet,vprimeh, 852 pvt  fCFvt/fCuniontextFv, and pvhpers  fCFagentvh/fCuniontextFagentv~~~Counts are considered useful when they are greater or equal to 3.
We then construct a mutual information vector MIe  mi e1, mi e2, , mi em  for each word e, where mi ef is the pointwise mutual information between word e and feature f, which is defined as: N c N c N c ef m j ej n i if ef mi       1 1 log 1 where n is the number of words and N    n i m j ij c 11 is the total frequency count of all features of all words~~~Mutual information is commonly used to measure the association strength between two words <TREF>Church and Hanks 1989</TREF>~~~A well-known problem is that mutual information is biased towards infrequent elements/features~~~We therefore multiply mi ef with the following discounting factor: 1,min,min 1 11 11                        m j jf n i ei m j jf n i ei ef ef cc cc c c 2 32 Phase II Following <REF>Pantel and Lin 2002</REF>, we construct a committee for each semantic class.
This is the basis for Sadlers Analogical Semantics <REF>Sadler 1989</REF>, which according to his report has not proved effective~~~His results may be improved if more sophisticated methods and larger corpora are used to establish similarity between words such as in <TREF>Hindle 1990</TREF>~~~In particular, an enhancement of our disambiguation method, using similarity-based estimation <REF>Dagan, Marcus, and Markovitch 1993</REF>, was evaluated recently~~~In this evaluation the applicability of the disambiguation method was increased by 15, with only a slight decrease in the precision.
 1994 Association for Computational Linguistics Computational Linguistics Volume 20, Number 4 knowledge can be found in the use of statistical data on the occurrence of lexical relations in large corpora eg , <REF>Grishman, Hirschman, and Nhan 1986</REF>~~~The use of such relations mainly relations between verbs or nouns and their arguments and modifiers for various purposes has received growing attention in recent research <REF>Church and Hanks 1990</REF>; <REF>Zernik and Jacobs 1990</REF>; <TREF>Hindle 1990</TREF>; <REF>Smadja 1993</REF>~~~More specifically, two recent works have suggested using statistical data on lexical relations for resolving ambiguity of prepositional phrase attachment <REF>Hindle and Rooth 1991</REF> and pronoun references <REF>Dagan and Itai 1990, 1991</REF>~~~Clearly, statistics on lexical relations can also be useful for target word selection.
The use of such relations mainly relations between verbs or nouns and their arguments and modifiers for various purposes has received growing attention in recent research <REF>Church and Hanks 1990</REF>; <REF>Zernik and Jacobs 1990</REF>; <TREF>Hindle 1990</TREF>; <REF>Smadja 1993</REF>~~~More specifically, two recent works have suggested using statistical data on lexical relations for resolving ambiguity of prepositional phrase attachment <REF>Hindle and Rooth 1991</REF> and pronoun references <REF>Dagan and Itai 1990, 1991</REF>~~~Clearly, statistics on lexical relations can also be useful for target word selection~~~Consider, for example, the Hebrew sentence extracted from the foreign news section of the daily Ha-<REF>Aretz, September 1990</REF> transcripted to Latin letters: 1 Nose ze mana mi-shtei ha-mdinot mi-lahtom al hoze shalom.
We present an empirical evaluation on the task of attaching partof and causation relations, showing an improvement on F-score over a baseline model~~~NLP researchers have developed many algorithms for mining knowledge from text and the Web, including facts <REF>Etzioni et al 2005</REF>, semantic lexicons <REF>Riloff and Shepherd 1997</REF>, concept lists <REF>Lin and Pantel 2002</REF>, and word similarity lists <TREF>Hindle 1990</TREF>~~~Many recent efforts have also focused on extracting binary semantic relations between entities, such as entailments <REF>Szpektor et al 2004</REF>, is-a <REF>Ravichandran and Hovy 2002</REF>, part-of <REF>Girju et al 2003</REF>, and other relations~~~The output of most of these systems is flat lists of lexical semantic knowledge such as Italy is-a country and orange similar-to blue.
There have been many approachs to automatic detection of similar words from text~~~Our method is similar to <TREF>Hindle, 1990</TREF>, <REF>Lin, 1998</REF>, and <REF>Gasperin, 2001</REF> in the use of dependency relationships as the word features~~~Another approach used the words distribution to cluster the words <REF>Pereira, 1993</REF>, and Inoue <REF>Inoue, 1991</REF> also used the word distributional information in the Japanese-English word pairs to resolve the polysemous word problem~~~Wu <REF>Wu, 2003</REF> shows one approach to collect synonymous collocation by using translation information.
3 http:// wwwcisupennedu/ treebank/ rank candidate 1 batt 2 batterie 3 bat 4 BTBTBTBT cover 5 BTY 6 batterry 7 BT BT BT BT BT BT BT BT BT BT adapter 8 bezel 9 BT BT BT BT BT BT BT BT BT BT cheque 10 BTBTBTBT screw Table 3: batterys Synonymous Expression Candidates from the Entire Corpus Author A rank candidate 1 battery 2 controller 3 BT BT BT BT BT BT BT BT Cover 4 APM 5 BTBTBTBT screw 6 mark 7 BT BT BT BT BT BT BT BT BT BT cheque 8 diskette 9 checkmark 10 boot Author B rank candidate 1 batt 2 form 3 protector 4 DISKETTE 5 Mwave 6 BT BT BT BT BT BT BT BT BT BT adapter 7 mouse 8 BT BT BT BT BT BT BT BT BT BT cheque 9 checkmark 10 process Table 4: Noise Candidates from Each Authors Corpus word~~~The words we want to aggregate for text analysis are not rigorous synonyms, but the role is the same, so we have to consider the syntactic relation based on the assumptions that words with the same role tend to modify or be modified by similar words <TREF>Hindle, 1990</TREF>; <REF>Strzalkowski, 1992</REF>~~~On the other hand, window-based techniques are not suitable for our data, because the documents are written by several authors who have a variety of different writing styles eg selecting different prepositions and articles~~~Therefore we consider only syntactic features: dependency pairs, which consist of nouns, verbs, and their relationships.
All digits in both patterns and sentences are replaced with a common marker, such 810 that any two numerical values with the same number of digits will overlap during matching~~~Many methods have been proposed to compute distributional similarity between words, eg, <TREF>Hindle, 1990</TREF>, <REF>Pereira et al , 1993</REF>, <REF>Grefenstette, 1994</REF> and <REF>Lin, 1998</REF>~~~Almost all of the methods represent a word by a feature vector, where each feature corresponds to a type of context in which the word appeared~~~They differ in how the feature vectors are constructed and how the similarity between two feature vectors is computed.
et al, 1999; <REF>Torisawa, 2002</REF>~~~Others proposed distributional similarity measures between words <TREF>Hindle, 1990</TREF>; <REF>Lin, 1998</REF>; <REF>Lee, 1999</REF>; <REF>Weeds et al, 2004</REF>~~~Once such similarity is defined, it is trivial to perform clustering~~~On the other hand, some researchers utilized co-occurrence for word clustering.
In fact, corpus linguistics became a popular research field because of the claim that shallow techniques could overcome the lexical coverage bottleneck of traditional NLP techniques~~~Among the applications of collocational analysis for lexical acquisition are: the derivation of syntactic disambiguation cues <REF>Basili et al 1991, 1993a</REF>; <REF>Hindle and Rooths 1991</REF>,1993; <REF>Sekine 1992</REF> <REF>Bogges et al 1992</REF>, sense preference <REF>Yarowski 1992</REF>, acquisition of selectional restrictions <REF>Basili et al 1992b, 1993b</REF>; <REF>Utsuro et al 1993</REF>, lexical preference in generation <REF>Smadjia 1991</REF>, word clustering <REF>Pereira 1993</REF>; <TREF>Hindle 1990</TREF>; <REF>Basili et al 1993c</REF>, etc In the majority of these papers, even though the precedent or subsequent statistical processing reduces the number of accidental associations, very large corpora 10,000,000 words are necessary to obtain reliable data on a large enough number of words~~~In addition, most papers produce a performance evaluation of their methods but do not provide a measure of the coverage, ie the percentage of cases for which their method actually provides a right or wrong solution~~~It is quite common that results are discussed only for 10-20 cases.
This is known as the Distributional Hypothesis in linguistics <REF>Harris, 1968</REF>~~~For example, the words test and exam are similar because both of them follow verbs such as administer, cancel, cheat on, conduct,  and both of them can be preceded by adjectives such as academic, comprehensive, diagnostic, difficult,  Many methods have been proposed to compute distributional similarity between words <TREF>Hindle, 1990</TREF>; <REF>Pereira et al , 1993</REF>; <REF>Grefenstette, 1994</REF>; <REF>Lin, 1998</REF>~~~Almost all of the methods represent a word by a feature vector where each feature corresponds to a type of context in which the word appeared~~~They differ in how the feature vectors are constructed and how the similarity between two feature vectors is computed.
This is known as the Distributional Hypothesis in linguistics <REF>Harris, 1968</REF>~~~For example, the words test and exam are similar because both of them can follow verbs such as administer, cancel, cheat on, conduct, etc Many methods have been proposed to compute distributional similarity between words, eg, <TREF>Hindle, 1990</TREF>; <REF>Pereira et al , 1993</REF>; <REF>Grefenstette, 1994</REF>; <REF>Lin, 1998</REF>~~~Almost all of the methods represent a word by a feature vector where each feature corresponds to a type of context in which the word appeared~~~They differ in how the feature vectors are constructed and how the similarity between two feature vectors is computed.
<REF>Schutze 1998</REF> used bag-of-words contexts for sense discrimination~~~<TREF>Hindle 1990</TREF> grouped nouns into thesaurus-like lists based on the similarity of their syntactic contexts~~~Our approach is similar with the difference that we do not group noun arguments into finite categories, but instead leave the category boundaries blurry and allow overlaps~~~The DDNs are essentially a form of world knowledge which we extract automatically and apply to VSD.
or the cooccurrence of two words within a limited distance in the context~~~Statistical data about these various cooccurrence relations is employed for a variety of applications, such as speech recognition <REF>Jelinek, 1990</REF>, language generation <REF>Smadja and McKeown, 1990</REF>, lexicography <REF>Church and Hanks, 1990</REF>, machine translation Brown et al , ; <REF>Sadler, 1989</REF>, information retrieval <REF>Maarek and Smadja, 1989</REF> and various disambiguation tasks <REF>Dagan et al , 1991</REF>; <REF>Hindle and Rooth, 1991</REF>; <REF>Grishman et al , 1986</REF>; <REF>Dagan and Itai, 1990</REF>~~~A major problem for the above applications is how to estimate the probability of cooccurrences that were not observed in the training corpus~~~Due to data sparseness in unrestricted language, the aggregate probability of such cooccurrences is large and can easily get to 25 or more, even for a very large training corpus <REF>Church and Mercer, 1992</REF>.
To account for this problem we developed a simple heuristic that searches for words that are potentially similar to w, using thresholds on mutual information values and frequencies of cooccurrence pairs~~~The search is based on the property that when computing simwl, w2, words that have high mutual information values 5The nominator in our metric resembles the similarity metric in <TREF>Hindle, 1990</TREF>~~~We found, however, that the difference between the two metrics is important, because the denominator serves as a normalization factor~~~with both wl and w2 make the largest contributions to the value of the similarity measure.
Semantic variation is rarely studied in specialized domains~~~Works on word similarity and word sense disambiguation are generally based on statistical methods designed for large or even very large corpora <TREF>Hindle, 1990</TREF>; <REF>Agirre and Rigau, 1996</REF>~~~Therefore, they cannot be applied for technical documents which usually are medium size corpora~~~However, dealing with already linguistic filtered data, <REF>Assadi, 1997</REF> aims at statistically build rough clusters supposing that similar candidate terms have similar expansions.
We applied both a neural network model and a linguistic method, that is syntactic information, to a large corpora and extracted necessary information~~~To extract semantic information of words such as synonyms and antonyms from corpora, previous research used syntactic structures <TREF>Hindle 1990</TREF>, <REF>Hatzivassiloglou 1993</REF> and <REF>Tokunaga 1995</REF>, response time to associate synonyms and antonyms in psychological experiments <REF>Gross 1989</REF>, or extracting related words automatically from corpora <REF>Grefensette 1994</REF>~~~Most lexical classification is based on parts of speech, as they have very important semantic information~~~For examples, typically, an adjective refers to an attribute, a verb refers to a motion or an event, and a noun refers to an object.
3 RELATED WORK Interest in extracting lexical and especially collocational information from text has risen dramatically in the last two years, as sufficiently large corpora and sufficiently cheap computation have become available~~~Three recent papers in this area are <REF>Church and Hanks 1990</REF>, <TREF>Hindle 1990</TREF>, and <REF>Smadja and McKeown 1990</REF>~~~The latter two are concerned exclusively with collocation relations between open-class words and not with grammatical properties~~~Church is also interested primarily in open-class collocations, but he does discuss verbs that tend to be followed by infinitives within his mutual information framework.
Following the idea proposed in Harris Distributional Hypothesis <REF>Harris, 1985</REF>, that words occurring in similar contexts are semantically similar, many works have used different definitions of context to identify various types of semantic similarity~~~<TREF>Hindle 1990</TREF> uses a mutual-information based metric derived from the distribution of subject, verb and object in a large corpus to classify nouns~~~Pereira et al~~~1993 cluster nouns according to their distribution as direct objects of verbs, using information-theoretic tools the predecessors of the tools we use in this work.
Distributional similarity represents the relatedness of two words by the commonality of contexts the words share, based on the distributional hypothesis <REF>Harris, 1985</REF>, which states that semantically similar words share similar contexts~~~A number of researches which utilized distributional similarity have been conducted, including <TREF>Hindle, 1990</TREF>; <REF>Lin, 1998</REF>; <REF>Geffet and Dagan, 2004</REF> and many others~~~Although they have been successful in acquiring related words, various parameters such as similarity measures and weighting are involved~~~As Weeds et al.
In our experiments we set   095~~~32 m,n-cousin Classification The classifier for learning coordinate terms relies on the notion of distributional similarity, ie, the idea that two words with similar meanings will be used in similar contexts <TREF>Hindle, 1990</TREF>~~~We extend this notion to suggest that words with similar meanings should be near each other in a semantic taxonomy, and in particular will likely share a hypernym as a near parent~~~Our classifier for m,n-cousins is derived from the algorithm and corpus given in <REF>Ravichandran et al , 2005</REF>.
We agree with the differential definition of semantics : the meaning of the morpho-lexical units is not defined by reference to a concept, but rather by contrast with other units <REF>Rastier et al , 1994</REF>~~~In fact, we are considering word usage rather than word meanin <REF>Zernik, 1990</REF> following in this the distributional point of view, see <REF>Harris, 1968</REF>, <TREF>Hindle, 1990</TREF>~~~Statistical or probabilistic methods are often used to extract semantic clusters from corpora in order to build lexical resources for ANLP tools <TREF>Hindle, 1990</TREF>, <REF>Zernik, 1990</REF>, <REF>Resnik, 1993</REF>, or for automatic thesaurus generation <REF>Grefenstette, 1994</REF>~~~We use similar techniques, enriched by a preliminaxy morpho-syntaztic analysis, in order to perform knowledge acquisition and modeling for a specific task eg : electrical network planning.
In fact, we are considering word usage rather than word meanin <REF>Zernik, 1990</REF> following in this the distributional point of view, see <REF>Harris, 1968</REF>, <TREF>Hindle, 1990</TREF>~~~Statistical or probabilistic methods are often used to extract semantic clusters from corpora in order to build lexical resources for ANLP tools <TREF>Hindle, 1990</TREF>, <REF>Zernik, 1990</REF>, <REF>Resnik, 1993</REF>, or for automatic thesaurus generation <REF>Grefenstette, 1994</REF>~~~We use similar techniques, enriched by a preliminaxy morpho-syntaztic analysis, in order to perform knowledge acquisition and modeling for a specific task eg : electrical network planning~~~Moreover, we are dealing with language for specific purpose texts and not with general texts.
Our approach avoids hand-crafting a set of spe11 ci c indicator features; we simply use the distribution of the pronouns context~~~Our method is thus related to previous work based on <REF>Harris 1985</REF>s distributional hypothesis2 It has been used to determine both word and syntactic path similarity <TREF>Hindle, 1990</TREF>; <REF>Lin, 1998a</REF>; <REF>Lin and Pantel, 2001</REF>~~~Our work is part of a trend of extracting other important information from statistical distributions~~~<REF>Dagan and Itai 1990</REF> use the distribution of a pronouns context to determine which candidate antecedents can  t the context.
The hypothesis states that words that occur in the same contexts tend to have similar meaning~~~Researchers have mostly looked at representing words by their surrounding words <REF>Lund and Burgess 1996</REF> and by their syntactical contexts <TREF>Hindle 1990</TREF>; <REF>Lin 1998</REF>~~~However, these representations do not distinguish between the different senses of words~~~Our framework utilizes these principles and representations to induce disambiguated feature vectors.
2a~~~If the bound is too tight to allow the correct parse of some sentence, we would still like to allow an accurate partial parse: a sequence of accurate parse fragments <TREF>Hindle, 1990</TREF>; <REF>Abney, 1991</REF>; <REF>Appelt et al , 1993</REF>; <REF>Chen, 1995</REF>; <REF>Grefenstette, 1996</REF>~~~Furthermore, we would like to use the fact that some fragment sequences are presumably more likely than others~~~Our partial parses will look like the one in Fig.
<REF>Smadja 1992</REF> gathered co-occurrences within fiveword windows to find collocations, particularly in specific domains~~~<TREF>Hindle 1990</TREF> classified nouns on the basis of co-occurring patterns of subjectverb and verb-object pairs~~~<REF>Hatzivassiloglou and MeKeown 1993</REF> clustered adjectives into semantic classes, and Pereira et al~~~1993 clustered nouns on their appearance ill verb-object pairs.
Also, the patterns are learned with the specific goal of scaling to the terascale see Table 2~~~22 Co-occurrence-based approaches The second class of algorithms uses cooccurrence statistics <TREF>Hindle 1990</TREF>, <REF>Lin 1998</REF>~~~These systems mostly employ clustering algorithms to group words according to their meanings in text~~~Assuming the distributional hypothesis <REF>Harris 1985</REF>, words that occur in similar grammatical contexts are similar in meaning.
4 Towards an adequate similarity esfimatation for the building of ontologies The comparison with the similarity score of <TREF>Hindle, 1990</TREF> shows that SYCLADE similarity indicator is specifically relevant for ontology bootstrap and tuning~~~Hindle uses the observed frequencies within a specific syntactic pattern subject/verb, and verb/object to derive a cooccu,> rence score which is an estimate of mutual information <REF>Church and Hanks, 1990</REF>~~~We adapted this score to noun phrase patterns However the similarity measures based on cooccurrence scores and nominal phrase patterns are less relevant for an ontological analysis~~~The subgraph of the chirurgical acts words, which is easy to identify from the SYCLADE graph fig.
Automatic exploration of a sublanguage corpus constitutes a first step towards identifying the semantic classes and relationships which are relevant for this sublanguage~~~In the past five years, important research on the automatic acquisition of word classes based on lexical distribution has been published <REF>Church and Hanks, 1990</REF>; <TREF>Hindle, 1990</TREF>; <REF>Smadja, 1993</REF>; Greinstette, 1994; <REF>Grishman and Sterling, 1994</REF>~~~Most of these approaches, however, need large or even very large corpora in order for word classes to be discovered 1 whereas it is often the case that the data to be processed are insufficient to provide reliable lexical intbrmation~~~In other words, it is not always possible to resort to statistical methods.
2 Simplifying parse trees to classify words 21 The need for normalized syntactic contexts As Hindles work proves it, among others <REF>Grishman and Sterling, 1994</REF>; <REF>Grefenstette, 1994</REF>:, the mere existence of robust syntactic parsers makes it possible to parse large corpora in order to automate the discovery of syntactic patterns in the spirit of Harriss distributional hypothesis~~~Itowever, Harris methodology implies also to simplify and transform each parse tree 2, so as to obtain so-called elementary sentences exhibiting the main conceptual classes for the domain Sager lIaor instance, Hindle <TREF>Hindle, 1990</TREF> needs a six million word corpus in order to extract noun similarities from predicate-argunlent structures~~~2Changing passive into active sentences, using a verb instead of a nominalization, and so on~~~490 NP NPa AP4 I I Nr As I I stenose serre NPo PP2 Pa NP6 I de D9 NPlo le NPll AP12 t NPla AP14 A15 I I I N Ar gauche I I tronc eorninun Iigure 1: Parse tree for stenose serre de le hone commun gauche et al , 1987.
Although Stairmand <REF>Stairmand, 1997</REF> and Richardson <REF>Richardson and Smeaton, 1995</REF> have proposed the use of WordNet in information retrieval, they did not used WordNet in the query expansion framework~~~Our predicate-argument structure-based thesatmis is based on the method proposed by Hindie <TREF>Hindle, 1990</TREF>, although Hindle did not apply it to information retrieval~~~Instead, he used mutual information statistics as a Similarity coefficient, wheras we used the Dice coefficient for normalization purposes~~~Hindle only extracted the subject-verb and the object-verb predicatearguments, while we also extract adjective-noun predicate-arguments.
Among many kinds of lexical relations, synonyms are especially useful ones, having broad range of applications such as query expansion technique in information retrieval and automatic thesaurus construction~~~Various methods <TREF>Hindle, 1990</TREF>; <REF>Lin, 1998</REF>; <REF>Hagiwara et al , 2005</REF> have been proposed for synonym acquisition~~~Most of the acquisition methods are based on distributional hypothesis <REF>Harris, 1985</REF>, which states that semantically similar words share similar contexts, and it has been experimentally shown considerably plausible~~~However, whereas many methods which adopt the hypothesis are based on contextual clues concerning words, and there has been much consideration on the language models such as Latent Semantic Indexing <REF>Deerwester et al , 1990</REF> and Probabilistic LSI <REF>Hofmann, 1999</REF> and synonym acquisition method, almost no attention has been paid to what kind of categories of contextual information, or their combinations, are useful for word featuring in terms of synonym acquisition.
However, whereas many methods which adopt the hypothesis are based on contextual clues concerning words, and there has been much consideration on the language models such as Latent Semantic Indexing <REF>Deerwester et al , 1990</REF> and Probabilistic LSI <REF>Hofmann, 1999</REF> and synonym acquisition method, almost no attention has been paid to what kind of categories of contextual information, or their combinations, are useful for word featuring in terms of synonym acquisition~~~For example, <TREF>Hindle 1990</TREF> used cooccurrences between verbs and their subjects and objects, and proposed a similarity metric based on mutual information, but no exploration concerning the effectiveness of other kinds of word relationship is provided, although it is extendable to any kinds of contextual information~~~<REF>Lin 1998</REF> also proposed an information theorybased similarity metric, using a broad-coverage parser and extracting wider range of grammatical relationship including modifications, but he didnt further investigate what kind of relationships actually had important contributions to acquisition, either~~~The selection of useful contextual information is considered to have a critical impact on the performance of synonym acquisition.
Some of our initial data suggest that the hypothesis of deep semantic selection may in fact be correct, as well as indicating what the nature of the coercion rules may be~~~Using techniques described in <REF>Church and Hindle 1990</REF>, <REF>Church and Hanks 1990</REF>, and <REF>Hindle and Rooth 1991</REF>, Figure 4 shows some examples of the most frequent V-O pairs from the AP corpus~~~Corpus studies confirm similar results for weakly intensional contexts such as the complement of coercive verbs such as veto~~~These are interesting because regardless of the noun type appearing as complement, it is embedded within a semantic interpretation of the proposal to, thereby clothing the complement within an intensional context.
While full automatic extraction of semantic collocations is not yet feasible, some recent research in related areas is promising~~~<TREF>Hindle 1990</TREF> reports interesting results of this kind based on literal collocations, where he parses the corpus <REF>Hindle 1983</REF> into predicate-argument structures and applies a mutual information measure <REF>Fano 1961</REF>; <REF>Magerman and Marcus 1990</REF> to weigh the association between the predicate and each of its arguments~~~For example, as a list of the most frequent objects for the verb drink in his corpus, Hindle found beer, tea, Pepsi, and champagne~~~Based on the distributional hypothesis that the degree of shared contexts is a similarity measure for words, he develops a similarity metric for nouns based on their substitutability in certain verb contexts.
<REF>Although Stairmand 1997</REF> and <REF>Richardson 1995</REF> proposed the use of WordNet in information retrieval, they did not use WordNet in the query expansion framework~~~Our syntactic-relation-based thesaurus is based on the method proposed by <TREF>Hindle 1990</TREF>, although Hindle did not apply it to information retrieval~~~Hindle only extracted subject-verb and object-verb relations, while we also extract adjective-noun and noun-noun relations, in the manner of <REF>Grefenstette 1994</REF>, who applied his 99 Proceedings of EACL 99 Table 3: Average non-interpolated precision for expansion using single or combined thesauri~~~Topic Type Base Title 01175 Description 01428 All 01976 Expanded with WordNet Roget Syntac Cooccur Combined only only only only method 01276 01236 01386 01457 02314 86 52  179 240 969 01509 0,1477 01648 01693 02645 57 34 154 185 852 02010 01999 02131 02191 02724 17 12 78 108 378 syntactically-based thesaurus to information retrieval with mixed results.
232 Syntactically-based Thesaurus In contrast to the previous section, this method attempts to gather term relations on the basis of linguistic relations and not document cooccurrence statistics~~~Words appearing in similax grammatical contexts are assumed to be similar, and therefore classified into the same class <REF>Lin, 1998</REF>; <REF>Grefenstette, 1994</REF>; <REF>Grefenstette, 1992</REF>; <REF>Ruge, 1992</REF>; <TREF>Hindle, 1990</TREF>~~~First, all the documents are parsed using the Apple Pie Parser~~~The Apple Pie Parser is a natural language syntactic analyzer developed by Satoshi Sekine at New York University <REF>Sekine and Grishman, 1995</REF>.
The problemis that for large enough corpora the number of possible joint events is much larger than the number of event occurrences in the corpus, so many events are seen rarely or never, making their frequency counts unreliable estimates of their probabilities~~~<TREF>Hindle 1990</TREF> proposed dealing with the sparseness problem by estimating the likelihood of unseen events from that of similar events that have been seen~~~For instance, one may estimate the likelihood of a particular direct object for a verb from the likelihoods of that direct object for similar verbs~~~This requires a reasonable definition of verb similarity and a similarity estimation method.
With seemingly endless amounts of textual data at our disposal, we have a tremendous opportunity to automatically grow semantic term banks and ontological resources~~~To date, researchers have harvested, with varying success, several resources, including concept lists <REF>Lin and Pantel 2002</REF>, topic signatures <REF>Lin and Hovy 2000</REF>, facts <REF>Etzioni et al 2005</REF>, and word similarity lists <TREF>Hindle 1990</TREF>~~~Many recent efforts have also focused on extracting semantic relations between entities, such as entailments <REF>Szpektor et al 2004</REF>, is-a <REF>Ravichandran and Hovy 2002</REF>, part-of <REF>Girju et al 2006</REF>, and other relations~~~The following desiderata outline the properties of an ideal relation harvesting algorithm:  Performance: it must generate both high precision and high recall relation instances;  Minimal supervision: it must require little or no human annotation;  Breadth: it must be applicable to varying corpus sizes and domains; and  Generality: it must be applicable to a wide variety of relations ie , not just is-a or part-of.
We are not aware of other work that uses such collocations as we do~~~Features identified using distributional similarity have previously been used for syntactic and semantic disambiguation <TREF>Hindle 1990</TREF>; <REF>Dagan, Pereira, and Lee 1994</REF> and to develop lexical resources from corpora <REF>Lin 1998</REF>; <REF>Riloff and Jones 1999</REF>~~~We are not aware of other work identifying and using density parameters as described in this article~~~Since our experiments, other related work in NLP has been performed.
Typical feature weighting functions include the logarithm of the frequency of word-feature cooccurrence <REF>Ruge, 1992</REF>, and the conditional probability of the feature given the word within probabilistic-based measures <REF>Pereira et al , 1993</REF>, <REF>Lee, 1997</REF>, <REF>Dagan et al , 1999</REF>~~~Probably the most widely used association weight function is point-wise Mutual Information MI <REF>Church et al , 1990</REF>, <TREF>Hindle, 1990</TREF>, <REF>Lin, 1998</REF>, <REF>Dagan, 2000</REF>, defined by:  ,log, 2 fPwP fwPfwMI  A known weakness of MI is its tendency to assign high weights for rare features~~~Yet, similarity measures that utilize MI showed good performance~~~In particular, a common practice is to filter out features by minimal frequency and weight thresholds.
Finally, a novel feature weighting and selection function is presented, which yields superior feature vectors and better word similarity performance~~~Distributional Similarity has been an active research area for more than a decade <TREF>Hindle, 1990</TREF>, <REF>Ruge, 1992</REF>, <REF>Grefenstette, 1994</REF>, <REF>Lee, 1997</REF>, <REF>Lin, 1998</REF>, <REF>Dagan et al , 1999</REF>, <REF>Weeds and Weir, 2003</REF>~~~Inspired by Harris distributional hypothesis <REF>Harris, 1968</REF>, similarity measures compare a pair of weighted feature vectors that characterize two words~~~Features typically correspond to other words that co-occur with the characterized word in the same context.
Let Seenrp be the set of seen headwords for an argument rp of a predicate p Then we model the selectional preference S of rp for a possible headword w0 as a weighted sum of the similarities between w0 and the seen headwords: Srpw0  summationdisplay wSeenrp simw0,wwtrpw simw0,w is the similarity between the seen and the potential headword, and wtrpw is the weight of seen headword w Similarity simw0,w will be computed on the generalization corpus, again on the basis of extracted tuples p,rp,w~~~We will be using the similarity metrics shown in Table 1: Cosine, the Dice and Jaccard coefficients, and <TREF>Hindles 1990</TREF> and <REF>Lins 1998</REF> mutual information-based metrics~~~We write f for frequency, I for mutual information, and Rw for the set of arguments rp for which w occurs as a headword~~~In this paper we only study corpus-based metrics.
Computing MI scores is by now a standard procedure for measuring the co-occurrence between objects relative to their overall occurrence~~~MI is defined in general as follows: y I ix y  log2 Px Py We can use this definition to derive an estimate of the connectedness between words, in terms of collocations <REF>Smadja, 1993</REF>, but also in terms of phrases and grammatical relations <TREF>Hindle, 1990</TREF>~~~For instance the co-occurrence of verbs and the heads of their NP objects iN: size of the corpus, ie the number of stems: N Cobj v n  log2 /v /n N N All nouns are now classified by running a similaxity measure over their MI scores and the MI scores of each CoRELEx class~~~For this we use the Jaccard measure that compares objects relative to the attributes they share <REF>Grefenstette, 1994</REF>.
Evaluation of automatically generated lexical resources is a difficult problem~~~In <TREF>Hindle, 1990</TREF>, a small set of sample results are presented~~~In <REF>Smadja, 1993</REF>, automatically extracted collocations are judged by a lexicographer~~~In <REF>Dagan et al, 1993</REF> and <REF>Pereira et al, 1993</REF>, clusters of similar words are evaluated by how well they are able to recover data items that are removed from the input corpus one at a time.
We also constructed several other thesauri using the same corpus, but with the similarity measures in Figure 1~~~The measure simHindle is the same as the similarity measure proposed in <REF>Hindie, 1990</REF>, except that it does not use dependency triples with negative mutual information~~~The measure simHindle r is the same as simHindle except that all types of dependency relationships are used, instead of just subject and object relationships~~~The measures simcosine, simdice and simJacard are versions of similarity measures commonly used in information retrieval Frakes and Baeza-<REF>Yates, 1992</REF>.
and Conclusion There have been many approaches to automatic detection of similar words from text corpora~~~Ours is 772 similar to <REF>Grefenstette, 1994</REF>; <TREF>Hindle, 1990</TREF>; <REF>Ruge, 1992</REF> in the use of dependency relationship as the word features, based on which word similarities are computed~~~Evaluation of automatically generated lexical resources is a difficult problem~~~In <TREF>Hindle, 1990</TREF>, a small set of sample results are presented.
The similarity is usually calculated from a thesaurus~~~Since a handmade thesaurus is not slfitahle for machine use, and expensive to compile, automatical construction ofa thesaurus has been attempted using corpora <TREF>Hindle, 1990</TREF>~~~llowever, the thesaurus constructed by such ways does not contain so many nouns, and these nouns are specified by the used corpus~~~In other words, we cannot construct the general thesaurus from only a corpus.
In all, we obtained 2,708,135 bits of generalized cooccurrence data, which consisted of 115,330 types~~~23 Measuring the similarity between classes step 3 In step 3, we measure the similarity between two primitive classes by using the method given by Hindle <TREF>Hindle, 1990</TREF>~~~First, we define the nmtual information MI of a verb v and a primitive class C as follows~~~ZmY2 M ,Clogs N eq1 N N In the above equation, N is the total number of cooccurrence data bits, and fv and fC are the frequency of v and C in the whole cooccurrence data set respectively, and fv, C is the frequency of the cooccurrence data C, wo, v.
In the next section, we proceed to apply this technique for generating noun similarity lists~~~4 Building Noun Similarity Lists A lot of work has been done in the NLP community on clustering words according to their meaning in text <TREF>Hindle, 1990</TREF>; <REF>Lin, 1998</REF>~~~The basic intuition is that words that are similar to each other tend to occur in similar contexts, thus linking the semantics of words with their lexical usage in text~~~One may ask why is clustering of words necessary in the first place.
The UMass/MUC-3 parser would clearly need additional mechanisms to handle the ensuing part of speech and 7Other parsing errors occurred throughout the training set, but only those instances where the antecedent was not recognized as a constituent and the wh-word had an anteceden0 were discarded~~~8Interestingly, in work on the automated classification of nouns, <TREF>Hindle, 1990</TREF> also noted problems with empty words that depend on their complements for meaning~~~221 word sense disambiguation problems~~~However, recent research in these areas indicates that automated approaches for these tasks may be feasible see, for example, Brown, Della Pietra, <REF>Della Pietra,  Mercer, 1991</REF> and l-<REF>Iindle, 1983</REF>.
The corpus is relatively small it contains approximately 450,000 words and 18,750 sentences~~~In comparison, most corpus-based algorithms employ substantially larger corpora eg , 1 million words de <REF>Marcken, 1990</REF>, 25 million words <REF>Brent, 1991</REF>, 6 million words <TREF>Hindle, 1990</TREF>, 13 million words <REF>Hindle,  Rooth, 1991</REF>~~~Relative pronoun processing is especially important for the MUC-3 corpus because approximately 25 of the sentences contain at least one relative pronoun~~~3 In fact, the relative pronoun who occurs in approximately 1 out of every 10 sentences.
In previous research, word meanings have been statistically modeled based on syntactic information derived from a corpus~~~<TREF>Hindle 1990</TREF> used noun-verb syntactic relations, and <REF>Hatzivassiloglou and McKeown 1993</REF> used coordinated adjective-adjective modifier pairs~~~These methods are useful for the organization of words deep within a hierarchy, but do not seem to provide a solution for the top levels of the hierarchy~~~To find an objective hierarchical word structure, we utilize the complementary similarity measure CSM, which estimates a one-to-many relation, such as superordinatesubordinate relations <REF>Hagita and Sawaki 1995</REF>, <REF>Yamamoto and Umemura 2002</REF>.
Do we really need to fully parse the texts in every application~~~Some researchers apply shallow or partial parsers <REF>Smadja, 1991</REF>; <TREF>Hindle, 1990</TREF> to acquiring specific patterns from texts~~~These tell us that it is not necessary to completely parse the texts for some applications~~~This paper will propose a probabilistic partial parser and incorporate linguistic knowledge to extract noun phrases.
We also constructed several other thesauri using the same corpus, but with the similarity measures in Figure 1~~~The measure simHinate is the same as the similarity measure proposed in <TREF>Hindle, 1990</TREF>, except that it does not use dependency triples with negative mutual information~~~The measure simHindle,, is the same as simHindle except that all types of dependency relationships are used, instead of just subject and object relationships~~~The measures simcosine, simdice and simdacard are versions of similarity measures commonly used in information retrieval Frakes and Baeza-<REF>Yates, 1992</REF>.
and Conclusion There have been many approaches to automatic detection of similar words from text corpora~~~Ours is 772 similar to <REF>Grefenstette, 1994</REF>; <TREF>Hindle, 1990</TREF>; <REF>Ruge, 1992</REF> in the use of dependency relationship as the word features, based on which word similarities are computed~~~Evaluation of automatically generated lexical resources is a difficult problem~~~In <TREF>Hindle, 1990</TREF>, a small set of sample results are presented.
Evaluation of automatically generated lexical resources is a difficult problem~~~In <TREF>Hindle, 1990</TREF>, a small set of sample results are presented~~~In <REF>Smadja, 1993</REF>, automatically extracted collocations are judged by a lexicographer~~~In <REF>Dagan et al , 1993</REF> and Pereira et al ,  993, clusters of similar words are evaluated by how well they are able to recover data items that are removed from the input corpus one at a time.
2 Acquiring syntactic associations Clustered association data are collected by first extracting from the corpus all the syntactically related word pairs~~~Combining statistical and parsing methods has been done by <TREF>Hindle, 1990</TREF>; Hindle and Rooths,1991 and <REF>Smadja and McKewon, 1990</REF>; Smadja,1991~~~The novel aspect of our study is that we collect not only operational pairs, but triples, such as Nprep N, VprepN etc In fact, the preposition convey important information on the nature of the semantic link between syntactically related content words~~~By looking at the preposition, it is possible to restrict the set of semantic relations underlying a syntactic relation eg forpurpose,beneficiary.
The results of these studies have important applications in lexicography, to detect lexicosyntactic regularities <REF>Church and Hanks, 1990</REF>1 Calzolari and Bindi,1990, such as, for example support verbs eg make-decision prepositional verbs eg rely-upon idioms, semantic relations eg partof and fixed expressions eg kick the bucket~~~In Hindle,1990; <REF>Zernik, 1989</REF>; Webster el <REF>Marcus, 1989</REF> cooccurrence analyses augmented with syntactic parsing is used for the purpose of word classification~~~All these studies are based on th strong assumption that syntactic similarity in wor patterns implies semantic similarity~~~In Guthrie el al , 1991, sets of consistently contiguous word, neighbourhood are extracted from machinereadable dictionaries, to help semantic disambiguation in information retrieval.
In <REF>Smadja, 1989</REF>, <REF>Zernik and Jacobs, 1990</REF>, the associations are filtered by selecting the word pairs x,y whose frequency of occurrence is above fks, where f is the average appearance, s is the standard deviation, and k is an empirically determined factor~~~<TREF>Hindle, 1990</TREF>; Hindle and Rooths,1991 and <REF>Smadja, 1991</REF> use syntactic markers to increase the significance of the data~~~<REF>Guthrie et al , 1991</REF> uses the subject classification given in machine-readable dictionaries eg economics, engineering, etc~~~to reinforce cooccurence links.
More sophisticated linguistic information comes in several forms, all of which may need to be represented if performance in an automatic categorisation experiment is to be improved~~~Typical examples of linguistically sophisticated annotation include tagging words with their syntactic category although this has not been found to be effective for 1R, lemma of the word eg corpus for corpora, phrasal information eg identifying noun groups and phrases <REF>Lewis 1992c</REF>, <REF>Church 1988</REF>, and subject-predicate identification eg <TREF>Hindle 1990</TREF>~~~For the RAPRA corpus, we currently identify noun groups and adjective groups~~~This is achieved in a manner similar to Churchs 1988 PARTS algorithm used by Lewis 1992bc, in the sense that its main properties are robustness and corpus sensitivity.
In NLP, representing verb semantics with their thematic roles is a consolidated practice, even though theoretical researches <REF>Pustejovski 1991</REF> propose more rich and formal representation frameworks~~~More recent papers <TREF>Hindle 1990</TREF>, <REF>Pereira and Tishby 1992</REF> proposed to cluster nouns on the basis of a metric derived from the distribution of subject, verb and object in the texts~~~Both papers use as a source of information large corpora, but differ in the type of statistical approach used to determine word similarity~~~These studies, though valuable, leave several open problems: 70 1 A metric of conceptual closeness based on mere syntactic similarity is questionable, particularly if applied to verbs.
1993 and <REF>Lee 1999</REF>, among others~~~We use the cosine similarity measure for windowbased contexts and the following commonly used similarity measures for the syntactic vector space: <TREF>Hindles 1990</TREF> measure, the weighted Lin measure <REF>Wu and Zhou, 2003</REF>, the -Skew divergence measure <REF>Lee, 1999</REF>, the Jensen-Shannon JS divergence measure <REF>Lin, 1991</REF>, Jaccards coef cient van <REF>Rijsbergen, 1979</REF> and the Confusion probability <REF>Essen and Steinbiss, 1992</REF>~~~The Jensen-Shannon measure JS x1, x2  summationtext yY summationtext xx1,x2 parenleftbigg P yx log parenleftbigg Pyx 1 2 Pyx1Pyx2 parenrightbiggparenrightbigg subsequently performed best for our task~~~We compare the different ranking methodologies and data sets with respect to a manually-de ned gold standard list of 20 goal-type verbs and 20 nouns.
First, this approach leverages a little a priori grammatical knowledge using statistical inference~~~Most work on corpora of naturally occurring language 244 Michael R Brent From Grammar to Lexicon either uses no a priori grammatical knowledge <REF>Brill and Marcus 1992</REF>; <REF>Ellison 1991</REF>; <REF>Finch and Chater 1992</REF>; <REF>Pereira and Schabes 1992</REF>, or else it relies on a large and complex grammar <REF>Hindle 1990, 1991</REF>~~~One exception is <REF>Magerman and Marcus 1991</REF>, in which a small grammar is used to aid learning~~~1 A second difference is that the work reported here uses inferential rather than descriptive statistics.
In other words, it uses statistical methods to infer facts about the language as it exists in the minds of those who produced the corpus~~~Many other projects have used statistics in a way that summarizes facts about the text but does not draw any explicit conclusions from them <REF>Finch and Chater 1992</REF>; <TREF>Hindle 1990</TREF>~~~On the other hand, <REF>Hindle 1991</REF> does use inferential statistics, and <REF>Brill 1992</REF> recognizes the value of inference, although he does not use inferential statistics per se~~~Finally, many other projects in machine learning of natural language use input that is annotated in some way, either with part-of-speech tags <REF>Brill 1992</REF>; <REF>Brill and Marcus 1992</REF>; <REF>Magerman and Marcus 1990</REF> or with syntactic brackets <REF>Pereira and Schabes 1992</REF>.
This is the basis for Sadlers <REF>Analogical Semantics 1989</REF> which has not yet proved effective~~~His results may be improved if more sophisticated techniques and larger corpora are used to establish similarity between words such as in <TREF>Hindle, 1990</TREF>~~~Conflicting data~~~In very few cases two alternatives were supported equally by the statistical data, thus preventing a selection.
Consequently, a possible though partial alternative to using manually constructed knowledge can be found in the use of statistical data on the occurrence of lexical relations in large corpora~~~The use of such relations mainly relations between verbs or nouns and their arguments and modifiers for various purposes has received growing attention in recent research <REF>Church and Hanks, 1990</REF>; <REF>Zernik and Jacobs, 1990</REF>; <TREF>Hindle, 1990</TREF>~~~More specifically, two recent works have suggested to use statistical data on lexical relations for resolving ambiguity cases of PP-attachment <REF>Hindle and Rooth, 1990</REF> and pronoun references <REF>Dagan and Itai, 1990a</REF>; <REF>Dagan and Itai, 1990b</REF>~~~Clearly, statistical methods can be useful also for target word selection.
The use of such relations mainly relations between verbs or nouns and their arguments and modifiers for various purposes has received growing attention in recent research <REF>Church and Hanks, 1990</REF>; <REF>Zernik and Jacobs, 1990</REF>; <TREF>Hindle, 1990</TREF>~~~More specifically, two recent works have suggested to use statistical data on lexical relations for resolving ambiguity cases of PP-attachment <REF>Hindle and Rooth, 1990</REF> and pronoun references <REF>Dagan and Itai, 1990a</REF>; <REF>Dagan and Itai, 1990b</REF>~~~Clearly, statistical methods can be useful also for target word selection~~~Consider, for example, the Hebrew sentence extracted from the foreign news section of the daily <REF>Haaretz, September 1990</REF> transcripted to Latin letters.
Many studies extract synonyms from large monolingual corpora by using context information around targetterms<REF>CroachandYang, 1992</REF>; <REF>ParkandChoi, 1996</REF>; <REF>Waterman, 1996</REF>; <REF>Curran, 2004</REF>~~~Some researchers <TREF>Hindle, 1990</TREF>; <REF>Grefenstette, 1994</REF>; <REF>Lin, 1998</REF> classify terms by similarities based on their distributional syntactic patterns~~~These methods often extract not only synonyms, but also semantically related terms, such as antonyms, hyponyms and coordinate terms such as cat and dog Some studies make use of bilingual corpora or dictionaries to nd synonyms in a target language <REF>Barzilay and McKeown, 2001</REF>; <REF>Shimohata and Sumita, 2002</REF>; <REF>Wu and Zhou, 2003</REF>; <REF>Lin et al, 2003</REF>~~~Lin et al.
Semantic Relatedness Information~~~There bas recently been work in the detection of semantically related nouns via, for example, shared argument structures <TREF>Hindle 1990</TREF>, and shared dictionary definition context Wilks e al 1990~~~These approaches attempt to infer relationships among exical terms by looking at very large text samples and determining which ones are related in a statistically significant way~~~The technique introduced in this paper can be seen as having a similar goal but an entirely different approach, since only one sample need be found in order to determine a salient relationship and that sample may be infrequently occurring or nonexistent.
However, many studies investigate synonym extraction from only one resource~~~The most frequently used resource for synonym extraction is large monolingual corpora <TREF>Hindle, 1990</TREF>; <REF>Crouch and Yang, 1992</REF>; <REF>Grefenstatte, 1994</REF>; <REF>Park and Choi, 1997</REF>; <REF>Gasperin et al , 2001</REF> and <REF>Lin, 1998</REF>~~~The methods used the contexts around the investigated words to discover synonyms~~~The problem of the methods is that the precision of the extracted synonymous words is low because it extracts many word pairs such as cat and dog, which are similar but not synonymous.
It has two sources of evidence: the similarity of the strings themselves ie , edit distance and the similarity of the assertions they appear in~~~This second source of evidence is sometimes referred to as distributional similarity <TREF>Hindle, 1990</TREF>~~~Section 32 presents a simple model for predicting whether a pair of strings co-refer based on string similarity~~~Section 33 then presents a model called the Extracted Shared Property ESP Model for predicting whether a pair of strings co-refer based on their distributional similarity.
3 It is worth noting at this point that there are several well-known measures from the NLP literature that we have omitted from our experiments~~~Arguably the most widely used is the mutual information <TREF>Hindle, 1990</TREF>; <REF>Church and Hanks, 1990</REF>; <REF>Dagan et al , 1995</REF>; <REF>Luk, 1995</REF>; D <REF>Lin, 1998a</REF>~~~It does not apply in the present setting because it does not measure the similarity between two arbitrary probability distributions in our case, PVIn  and PVIm, but rather the similarity between a joint distribution PX1,X2 and the corresponding product distribution PX1PX2~~~Hamming-type metrics <REF>Cardie, 1993</REF>; <REF>Zavrel and Daelemans, 1997</REF> are intended for data with symbolic features, since they count feature label mismatches, whereas we are dealing feature Values that are probabilities.
Furthermore, this effort is repeated when a system is ported to another domain~~~This criticism leads us to automatic approaches for building thesauri from large corpora <REF>Hirschman et al , 1975</REF>; <TREF>Hindle, 1990</TREF>; <REF>Hatzivassiloglou and McKeown, 1993</REF>; <REF>Pereira et al , 1993</REF>; Tokunaga et aL, 1995; <REF>Ushioda, 1996</REF>~~~Past attempts have basically taken the following steps <REF>Charniak, 1993</REF>~~~1 extract word co-occurrences 2 define similarities distances between words on the basis of co-occurrences 3 cluster words on the basis of similarities The most crucial part of this approach is gathering word co-occurrence data.
The Distributional Hypothesis <REF>Harris 1985</REF> states that words that occur in the same contexts tend to be similar~~~There have been many approaches to compute the similarity between words based on their distribution in a corpus <TREF>Hindle 1990</TREF>; <REF>Landauer and Dumais 1997</REF>; <REF>Lin 1998</REF>~~~The output of these programs is a ranked list of similar words to each word~~~For example, Lins approach outputs the following similar words for wine and suit: wine: beer, white wine, red wine, Chardonnay, champagne, fruit, food, coffee, juice, Cabernet, cognac, vinegar, Pinot noir, milk, vodka, suit: lawsuit, jacket, shirt, pant, dress, case, sweater, coat, trouser, claim, business suit, blouse, skirt, litigation,  The similar words of wine represent the meaning of wine.
A proper filter must be able to access information in the text using any word of a set of similar words~~~A number of knowledge-rich <REF>Jacobs and Rau, 1990</REF>, <REF>Calzolari and Bindi, 1990</REF>, <REF>Mauldin, 1991</REF> and knowledge-poor <REF>Brown et al , 1992</REF>, <TREF>Hindle, 1990</TREF>, <REF>Ruge, 1991</REF>, <REF>Grefenstette, 1992</REF> methods have been proposed for recognizing when words are similar~~~The knowledge-rich approaches require either a conceptual dependency representation, or semantic tagging of the words, while the knowledge-poor approaches require no previously encoded semantic information, and depend on frequency of co-occurrence of word contexts to determine similarity~~~Evaluations of results produced by the above systems are often been limited to visual verification by a human subject or left to the human reader.
3 Distributional Word Similarity Words that tend to appear in the same contexts tend to have similar meanings <REF>Harris 1968</REF>~~~For example, the words corruption and abuse are similar because both of them can be subjects of verbs like arouse, become, betray, cause, continue, cost, exist, force, go on, grow, have, increase, lead to, and persist, etc, and both of them can modify nouns like accusation, act, allegation, appearance, and case, etc Many methods have been proposed to compute distributional similarity between words, eg, <TREF>Hindle, 1990</TREF>, <REF>Pereira et al 1993</REF>, <REF>Grefenstette 1994</REF> and <REF>Lin 1998</REF>~~~Almost all of the methods represent a word by a feature vector where each feature corresponds to a type of context in which the word appeared~~~84 31 Proximity-based Similarity It is natural to use dependency relationship Meluk, 1987 as features, but a parser has to be available.
The underlying idea is based largely on the central claim of the distributional hypothesis <REF>Harris 1968</REF>, that is: The meaning of entities, and the meaning of grammatical relations among them, is related to the restriction of combinations of these entities relative to other entities~~~This hypothesized relationship between distributional similarity and semantic similarity has given rise to a large body of work on automatic thesaurus generation <TREF>Hindle 1990</TREF>; <REF>Grefenstette 1994</REF>; <REF>Lin 1998a</REF>; <REF>Curran and Moens 2002</REF>; <REF>Kilgarriff 2003</REF>~~~There are inherent problems in evaluating automatic thesaurus extraction techniques, and much research assumes a gold standard that does not exist see Kilgarriff 2003 and Weeds 2003 for more discussion of this~~~A further problem for distributional similarity methods for automatic thesaurus generation is that they do not offer any obvious way to distinguish between linguistic relations such as synonymy, antonymy, and hyponymy see Caraballo 1999 and Lin et al 2003 for work on this.
As can be seen, similarity between neighbor sets is significantly higher at high recall settings low  within the model than at highprecision settings high , which suggests that dist  has high-recall CR characteristics~~~45 Hindles <REF>Measure Hindle 1990</REF> proposed an MI-based measure, which he used to show that nouns could be reliably clustered based on their verb co-occurrences~~~We consider the variant of 458 Weeds and Weir Co-occurrence Retrieval Figure 1 Variation with parameters  and  in development set mean similarity between neighbor sets of the additive t-test based CRM and of dist   Hindles Measure proposed by <REF>Lin 1998a</REF>, which overcomes the problem associated with calculating MI for word-feature combinations that do not occur: sim hind w 1, w 2   summationdisplay Tw 1 Tw 2  minIc, w 1 , Ic, w 2  38 where Tw 1  c : Ic, n > 0~~~This expression is the same as the numerator in the expressions for precision and recall in the difference-weighted MI-based CRM: P dw mi w 1, w 2   summationtext TP Iw 1, c  minIw 1, c,Iw 2, c Iw 1, c summationtext Fw 1  Iw 1, c  summationtext TP minIw 1, c, Iw 2, c summationtext Fw 1  Iw 1, c 39 R dw mi w 1, w 2   summationtext TP Iw 2, c  minIw 2, c,Iw 1, c Iw 2, c summationtext Fw 2  Iw 2, c  summationtext TP minIw 2, c, Iw 1, c summationtext Fw 2  Iw 2, c 40 since TP  Tw 1   Tw 2 .
A statistical technique using a language model that assigns a zero probability to these previously unseen events will rule the correct parse or interpretation of the utterance impossible~~~Similarity-based smoothing <TREF>Hindle 1990</TREF>; <REF>Brown et al 1992</REF>; <REF>Dagan, Marcus, and Markovitch 1993</REF>; <REF>Pereira, Tishby, and Lee 1993</REF>; <REF>Dagan, Lee, and Pereira 1999</REF> provides an intuitively appealing approach to language modeling~~~In order to estimate the probability of an unseen co-occurrence of events, estimates based on seen occurrences of similar events can be combined~~~For example, in a speech recognition task, we might predict that cat is a more likely subject of growl than the word cap, even though neither co-occurrence has been seen before, based on the fact that cat is similar to words that do occur as the subject of growl eg , dog and tiger, whereas cap is not.
statistical factors, although statistical factors are calculated in terms of the predicate argument structure in which each noun appears~~~Predicate argument structures, which consist of complements case filler nouns and case markers and verbs, have also been used in the task of noun classification <TREF>Hindle 1990</TREF>~~~This can be expressed by Equation 3, where ff is the vector for the noun in question, and items ti represent the statistics for predicate argument structures including n ff  h, t2,, ti  3 In regard to ti, we used the notion of TF~~~IDF <REF>Salton and McGill 1983</REF>.
2 Previous Work There have been several approaches to automatically discovering lexico-semantic information from text <REF>Hearst 1992</REF>; <REF>Riloff and Shepherd 1997</REF>; <REF>Riloff and Jones 1999</REF>; <REF>Berland and Charniak 1999</REF>; <REF>Pantel and Lin 2002</REF>; <REF>Fleischman et al 2003</REF>; <REF>Girju et al 2003</REF>~~~One approach constructs automatic thesauri by computing the similarity between words based on their distribution in a corpus <TREF>Hindle 1990</TREF>; <REF>Lin 1998</REF>~~~The output of these programs is a ranked list of similar words to each word~~~For example, Lins approach outputs the following top-20 similar words of orange: D peach, grapefruit, yellow, lemon, pink, avocado, tangerine, banana, purple, Santa Ana, strawberry, tomato, red, pineapple, pear, Apricot, apple, green, citrus, mango A common problem of such lists is that they do not discriminate between the senses of polysemous words.
First, most of theln assume that the input corpora me aligned sentence by sentence, which reduces their applicability remarkably~~~Although a number of automatic sentence alignment methods have been proposed <TREF>Brown et al 1991</TREF> ; <REF>Gale  Church 1991</REF> b; <REF>Kay  Roscheisen 1993</REF>; <REF>Chen 1993</REF>, they are not very reliable for real noisy bilingual texts~~~Second, the statistical methods usually require a very large corpus as their input~~~However, it is not easy to obtain a very large corpus.
In fact, it could be argued that, ultimately, text alignment is no easier than the more general problem of natural language understanding~~~In addition, most research efforts were directed towards the easiest problem, that of sentence-to-sentence alignment <TREF>Brown et al , 1991</TREF>; <REF>Gale and Church, 1991</REF>; <REF>Debili, 1992</REF>; Kay and lscheisen, 1993; <REF>Simard et al , 1992</REF>; <REF>Simard and Plamondon, 1996</REF>~~~Alignment at the word and term level, which is extremely useful for applications such as lexieal resource extraction, is still a largely unexplored research area<REF>Melamed, 1997</REF>~~~In order to live up to the expectations of the 711 various application fields, alignment technology will therefore have to improve substantially.
Several automatic methods have been proposed for this task in recent years~~~However, most of these methods address only the sub-problem of alignment <REF>Catizone et al 1989</REF>, <TREF>Brown et al 1991</TREF>, <REF>Gale  Church 1991</REF>, <REF>Debili  Sammouda 1992</REF>, <REF>Simard et al 1992</REF>, Kay  R<REF>Sscheisen 1993</REF>, <REF>Wu 1994</REF>~~~Alignment algorithms assume the availability of text unit boundary information and their output has less expressive power than a general bitext map~~~The only published solution to the more difficult general bitext mapping problem <REF>Church 1993</REF> can err by several typeset pages.
et al, 1999; <REF>Torisawa, 2002</REF>~~~Others proposed distributional similarity measures between words <REF>Hindle, 1990</REF>; <REF>Lin, 1998</REF>; <TREF>Lee, 1999</TREF>; <REF>Weeds et al, 2004</REF>~~~Once such similarity is defined, it is trivial to perform clustering~~~On the other hand, some researchers utilized co-occurrence for word clustering.
Moreover, it may not work well for dealing with general predicate phrases because it is hard to enumerate all phrases to determine the weights of features w,fWe thus simply adopted the co-occurrence frequency of the phrase and the feature as in <REF>Fujita and Sato, 2008</REF>~~~Skew divergence The skew divergence, a variant of KL divergence, was proposed in <TREF>Lee, 1999</TREF> based on an insight: the substitutability of one word for another need not be symmetrical~~~The divergence is given by the following formula: d skew t,sD P s bardblP t 1 P s , where P s and P t are the probability distributions of features for the given original and substituted words s and t, respectively~~~0    1 is a parameter for approximating KL divergence DThe score can be recast into a similarity score via, for example, the following function <REF>Fujita and Sato, 2008</REF>: Par skew stexpd skew t,s.
We preferred taxonomic class-based methods over distributional clustering mainly because we wanted to compare directly methods that use distributional information inherent in the corpus without making external assumptions with regard to how concepts and their similarity are represented with methods that quantify similarity relationships based on information present in a hand-crafted taxonomy~~~Furthermore, as <REF>Lee and Pereiras 1999</REF> results indicate that distributional clustering 365 Lapata The Disambiguation of Nominalizations and distance-weighted averaging obtain similar levels of performance, we restricted ourselves to the latter~~~We evaluated the contribution of the different smoothing methods on the nominalization task by exploring how each method and their combination influences disambiguation performance~~~Sections 3133 review discounting, class-based smoothing, and distance-weighted averaging.
The problem arises when the probability of word combinations that do not occur in the training data needs to be estimated~~~The smoothing methods proposed in the literature overviews are provided by <REF>Dagan, Lee, and Pereira 1999</REF> and <TREF>Lee 1999</TREF> can be generally divided into three types: discounting <REF>Katz 1987</REF>, class-based smoothing <REF>Resnik 1993</REF>; <REF>Brown et al 1992</REF>; 364 Computational Linguistics Volume 28, Number 3 <REF>Pereira, Tishby, and Lee 1993</REF>, and distance-weighted averaging <REF>Grishman and Sterling 1994</REF>; <REF>Dagan, Lee, and Pereira 1999</REF>~~~Discounting methods decrease the probability of previously seen events so that the total probability of observed word co-occurrences is less than one, leaving some probability mass to be redistributed among unseen co-occurrences~~~Class-based smoothing and distance-weighted averaging both rely on an intuitively simple idea: interword dependencies are modeled by relying on the corpus evidence available for words that are similar to the words of interest.
Furthermore, some nominalizations are conventionalized eg , business administration, health organization and are therefore attested more frequently than their verb-subject or verb-object counterparts~~~We re-created the frequencies of unseen verb-argument pairs by experimenting with three types of smoothing techniques proposed in the literature: back-off smoothing <REF>Katz 1987</REF>, class-based smoothing <REF>Resnik 1993</REF>; <REF>Lauer 1995</REF>, and distanceweighted averaging <REF>Grishman and Sterling 1994</REF>; <REF>Dagan, Lee, and Pereira 1999</REF>~~~We present these three smoothing variants and their underlying assumptions in the following section~~~3.
A key feature of this type of smoothing is the function that measures distributional similarity from co-occurrence frequencies~~~Several measures of distributional similarity have been proposed in the literature <REF>Dagan, Lee, and Pereira 1999</REF>; <TREF>Lee 1999</TREF>~~~We used two measures, the Jensen-Shannon divergence and the confusion probability~~~The choice of these two measures was motivated by work described in <REF>Dagan, Lee, and Pereira 1999</REF>, in which the JensenShannon divergence outperforms related similarity measures such as the confusion probability or the L 1 norm on a pseudodisambiguation task that uses verb-object pairs.
<REF>Grishman and Sterling 1994</REF> in particular employ the confusion probability to re-create the frequencies of verb-noun co-occurrences in which the noun is the object or the subject of the verb in question~~~In the following we describe these two similarity measures and show how they can be used to re-create the frequencies for unseen verb-argument tuples for a more detailed description see <REF>Dagan, Lee, and Pereira 1999</REF>~~~331 Confusion Probability~~~The confusion probability P C is an estimate of the probability that a word w 1 can be substituted for a word w prime 1, in the sense of being found in the same contexts.
In class-based smoothing, classes are used as the basis according to which the co-occurrence probability of unseen word combinations is estimated~~~Classes can be induced directly from the corpus using distributional clustering <REF>Pereira, Tishby, and Lee 1993</REF>; <REF>Brown et al 1992</REF>; <REF>Lee and Pereira 1999</REF> or taken from a manually crafted taxonomy <REF>Resnik 1993</REF>~~~In the latter case the taxonomy is used to provide a mapping from words to conceptual classes~~~Distance-weighted averaging differs from distributional clustering in that it does not explicitly cluster words.
33 Distance-Weighted Averaging Distance-weighted averaging induces classes of similar words from word co-occurrences without making reference to a taxonomy~~~Instead, it is based on the assumption that if a word w prime 1 is similar to word w 1, then w prime 1 can provide information about the frequency of unseen word pairs involving w 1 <REF>Dagan, Lee, and Pereira 1999</REF>~~~A key feature of this type of smoothing is the function that measures distributional similarity from co-occurrence frequencies~~~Several measures of distributional similarity have been proposed in the literature <REF>Dagan, Lee, and Pereira 1999</REF>; <TREF>Lee 1999</TREF>.
In language modeling, smoothing techniques are typically evaluated by showing that a language model that uses smoothed estimates incurs a reduction in perplexity on test data over a model that does not employ smoothed estimates <REF>Katz 1987</REF>~~~<REF>Dagan, Lee, and Pereira 1999</REF> use perplexity to compare back-off smoothing against distance-weighted averaging methods within the context of language modeling for speech recognition and show that the latter outperform the former~~~They also compare different distance-weighted averaging methods on a pseudoword disambiguation task in which the language model decides which of two verbs v 1 and v 2 is more likely to take a noun n as its object~~~The method being tested must reconstruct which of the unseen v 1, n and v 2, n is a valid verb-object combination.
The method being tested must reconstruct which of the unseen v 1, n and v 2, n is a valid verb-object combination~~~The same task is used by <REF>Lee and Pereira 1999</REF> in a detailed comparison between distributional clustering and distance-weighted averaging that demonstrates that the two methods yield comparable results~~~In our experiments we re-created co-occurrence frequencies for unseen verb-subject and verb-object pairs using three maximally different approaches: back-off smoothing, class-based smoothing using a predefined taxonomy, and distance-weighted averaging~~~We preferred taxonomic class-based methods over distributional clustering mainly because we wanted to compare directly methods that use distributional information inherent in the corpus without making external assumptions with regard to how concepts and their similarity are represented with methods that quantify similarity relationships based on information present in a hand-crafted taxonomy.
We used two measures, the Jensen-Shannon divergence and the confusion probability~~~The choice of these two measures was motivated by work described in <REF>Dagan, Lee, and Pereira 1999</REF>, in which the JensenShannon divergence outperforms related similarity measures such as the confusion probability or the L 1 norm on a pseudodisambiguation task that uses verb-object pairs~~~The confusion probability has been used by several authors to smooth word co367 Lapata The Disambiguation of Nominalizations occurrence probabilities <REF>Essen and Steinbiss 1992</REF>; <REF>Grishman and Sterling 1994</REF> and shown to give promising performance~~~<REF>Grishman and Sterling 1994</REF> in particular employ the confusion probability to re-create the frequencies of verb-noun co-occurrences in which the noun is the object or the subject of the verb in question.
McCarthy determines the sense profile of a verb/slot pair using a minimum description length tree cut model over the frequency-populated hierarchy <REF>Li and Abe, 1998</REF>~~~The two profiles for a verb are aligned to permit comparison using skew divergence as a probability distance measure <TREF>Lee 1999</TREF>~~~This step is explained in more detail in the next section, with an example~~~The value of the distance measure is compared to a threshold, which determines classification of a verb as causative the two profiles are similar or non-causative the two profiles are dissimilar, leading to best performance of 73 accuracy, on a set of hand-selected verbs.
<REF>Briefly, Clark and Weir 2002</REF> populate the WordNet hierarchy based on corpus frequencies of all nouns for a verb/slot pair, and then determine the appropriate probability estimate at each node in the hierarchy by using a24 a102 to determine whether to generalize an estimate to a parent node in the hierarchy~~~We compare SPD to other measures applied directly to the unpropagated probability profiles given by the Clark-Weir method: the probability distribution distance given by skew divergence skew <TREF>Lee, 1999</TREF>, as well as the general vector distance given by cosine cos~~~These are the measures aside from SPD that performed best in our pilot experiments~~~It is worth noting that the method of <REF>Clark and Weir 2002</REF> does not yield a tree cut, but instead generally populates the WordNet hierarchy with non-zero probabilities.
A drawback of this approach for generalizing to other sense profile comparisons is the assumption in relative entropy of an asymmetry between the two probability distributions~~~<REF>Similarly, McCarthy 2000</REF> uses skew divergence a variant of KL divergence proposed by <TREF>Lee, 1999</TREF> to compare the sense profile of one argument of a verb eg , the subject position of the intransitive to another argument of the same verb eg , the object position of the transitive, to determine if the verb participates in an argument alternation involving the two positions~~~For example, the causative alternation in sentences 1 and 2 illustrates how the subject of the intransitive is the same underlying semantic argument ie , the Themethe argument undergoing the action as the object of the transitive: 1 The snow melted~~~2 The sun melted the snow.
Due to the original KL distance is asymmetric and is not defined when zero frequency occurs~~~Some enhanced KL models were developed to prevent these problems such as Jensen-Shannon <REF>Jianhua, 1991</REF>, which introducing a probabilistic variable m, or  -Skew Divergence <TREF>Lee, 1999</TREF>, by adopting adjustable variable ~~~Research shows that Skew Divergence achieves better performance than other measures~~~<REF>Lee, 2001</REF> 1yxS rgenceDSkewDive yxxKL aaa  2/,2/yx,JS Shannon-DJensen yxm myKLmxKL   To convert distance to similarity value, we adopt the formula inspired by <REF>Mochihashi, and Matsumoto 2002</REF>.
Figure 2 exemplifies this process for two TOMs TCM1 and TCM2 in an imaginary hierarchy~~~The UBC is at the classes B, c and D To quantify the similarity between the probability distributions for the target slots we use the a-skew divergence aSD proposed by <TREF>Lee 1999</TREF>~~~1 This measure, defined in equation 2, is a smoothed version of the Kulback-Liebler divergence, plx and p2x are the two probability distributions which are being compared~~~The  constant is a value between 0 and 1 We also experimented with euclidian distance, the L1 norm, and cosine measures.
synonyms from the hypernyms verbs and nouns or closely related classes adjectives of all synsets of the target, ranked with the BNC frequency data~~~We also produced best and oot baselines using the distributional similarity measures l1, jaccard, cosine, lin <REF>Lin, 1998</REF> and SD <TREF>Lee, 1999</TREF> 4~~~We took the word with the largest similarity or smallest distance for SD and l1 for best and the top 10 for oot~~~For mw detection and identification we used WordNet to detect if a multiword in WordNet which includes the target word occurs within a window of 2 words before and 2 words after the target word.
All counts are log-likelihood transformed~~~We experiment with two distance measures to compute vector similarity, namely the Jaccard Coefficient and Cosine Distance, both of which have been shown to yield good performance in NLP tasks <TREF>Lee, 1999</TREF>; <REF>McDonald and Lowe, 1998</REF>~~~Evaluation Procedure~~~We evaluate our models by correlating the predicted plausibility values with the human judgements, which range between 1 and 7.
The distributional similarity was measured by means of three different similarity measures: the Jaccards coefficient, L1 distance, and the skew divergence~~~This choice of similarity measures was motivated by results of studies by <REF>Levy et al 1998</REF> and <TREF>Lee 1999</TREF> which compared several well known measures on similar tasks and found these three to be superior to many others~~~Another reason for this choice is that there are different ideas underlying these measures: while the Jaccards coefficient is a binary measure, L1 and the skew divergence are probabilistic, the former being geometrically motivated and the latter being a version of the information theoretic Kullback Leibler divergence cf~~~, <TREF>Lee 1999</TREF>.
The same procedure is applied for symmetric KL divergence and JS divergence~~~The second approach is from <TREF>Lee 1999</TREF>~~~Here similarity for KL is defined as Simp,q  C KLpq, where C is a free parameter to be tuned~~~4 Experimental Setup 41 Materials Following Chen et al.
Comparing the different divergence measures for LDA, we found that KL and JS perform significantly better than symmetrised KL divergence~~~Interestingly, the performance of the asymmetric KL divergence and the symmetric JS divergence is very close, which makes it difficult to conclude whether the relation discovery domain is a symmetric domain or an asymmetric domain like <TREF>Lees 1999</TREF> task of improving probability estimates for unseen word co-occurrences~~~A shortcoming of all the models we will describe here is that they are derived from the basic bag-of-words models and as such do not account for word order or other notions of syntax~~~Related work on relation discovery by Zhang et al.
The optimal configuration varies by the divergence measure with D  50 and C  14 for KL divergence, D  200 and C  4 for symmetrised KL, and D  150 and C  2 for JS divergence~~~For all divergence measures, <TREF>Lees 1999</TREF> method outperformed Dagan et als 1997 method~~~Also for all divergence measures, the model hyper-parameter  was found to be optimal at 00001~~~The  hyper-parameter was always set to 50/T following <REF>Griffiths and Steyvers 2004</REF>.
wx,f stands for the weight frequency in our experiment of f in F x  While Par Lin is symmetric, it has been argued that itisimportant todetermine thedirection ofparaphrase~~~As an asymmetric measure, we examine skew divergence defined by the following equation <TREF>Lee, 1999</TREF>: d skew t,sD P s bardblP t 1 P s , where P x denotes a probability distribution estimated 6 from a feature set F x HowwellP t approximates P s is calculated based on the KL divergence, D The parameter  is set to 099, following tradition, because the optimization of  is difficult~~~To take consistent measurements, we define the paraphrasability score Par skew as follows: Par skew stexpd skew t,s~~~6 We estimate them simply using maximum likelihood estimation, ie, P x fwx,f/ P f prime F x wx,f prime .
Note that if any a56 a8 a64a80a50, then a57 a11a81a1a59a55a60a52a61a56a4a5 is infinite; in general, the KLdivergence is very sensitive to small probabilities, and careful attention must be paid to smoothing if it is to be used with text co-occurrence data~~~The Jensen-Shannon divergencean average of the divergences of a55 and a56 from their mean distribution does not share this sensitivity and has previously been used in tests of lexical similarity <TREF>Lee, 1999</TREF>~~~Furthermore, unlike the KL-divergence, it is symmetric, presumably a desirable property in this setting, since synonymy is a symmetric relation, and our test design exploits this symmetry~~~However, a57 a11a83a82a45a17 a1a59a55a60a52a61a56a4a5, the Hellinger distance 3, is also symmetric and robust to small or zero estimates.
For a48 a49 a1a51a50a23a52a54a53a19a5 and word-conditional context distributions a55 and a56, we have the so-called a48 -divergences <REF>Zhu and Rohwer, 1998</REF>: a57 a58 a1a59a55a60a52a61a56a4a5a63a62a59a64 a53a65a14 a7 a55 a58 a56 a11a38a66 a58 a48a67a1a45a53a18a14a16a48a42a5 1 Divergences a57 a68 and a57 a11 are defined as limits as a48a6a69 a50 and a48a6a69a70a53 :a57 a11 a1a59a55a60a52a61a56a4a5a71a64 a57 a68 a1a51a56a67a52a51a55a72a5a71a64a74a73 a55a76a75a78a77a47a79 a55 a56 In other words, a57 a11a19a1a59a55a60a52a61a56a4a5 is the KL-divergence of a55 from a56  Members of this divergence family are in some sense preferred by theory to alternative measures~~~It can be shown that the a48 -divergences or divergences defined by combinations of them, such as the Jensen-Shannon or skew divergences <TREF>Lee, 1999</TREF> are the only ones that are robust to redundant contexts ie , only divergences in this family are invariant <REF>Csiszar, 1975</REF>~~~Several notions of lexical similarity have been based on the KL-divergence~~~Note that if any a56 a8 a64a80a50, then a57 a11a81a1a59a55a60a52a61a56a4a5 is infinite; in general, the KLdivergence is very sensitive to small probabilities, and careful attention must be paid to smoothing if it is to be used with text co-occurrence data.
By default, we bracket a token sequence with pseudo-tokens <bos> and <eos>2 Contextual tokens in the window may be either observed or disregarded, and the policy governing which to admit is one of the dimensions we explore here~~~The decision whether or not to observe a particular contextual token is made before counting commences, and is not sensitive to the circumstances of a particular occurrence eg , its participation in some syntactic relation <REF>Lin, 1997</REF>; <TREF>Lee, 1999</TREF>~~~When a contextual token is observed, it is always counted as a single occurrence~~~Thus, in contrast with earlier approaches <REF>Sahlgren, 2001</REF>; <REF>Ehlert, 2003</REF>, we do not use a weighting scheme that is a function of distance from the reference token.
We do not know whether or to what extent this particular parameter setting is universally best, best only for English, best for newswire English, or best only for the specific test we have devised~~~We have restricted our attention to a relatively small space of similarity measures, excluding many previously proposed measures of lexical affinity but see Weeds, et al 2004, and <TREF>Lee 1999</TREF> for some empirical comparisons~~~Lee observed that measures from the space of invariant divergences particularly the JS and skew divergences perform at least as well as any of a wide variety of alternatives~~~As noted, we experimented with the JS divergence and observed accuracies that tracked those of the Hellinger closely.
One could look at differences in the ranking over all words, using a meaTraining Testing FINANCE SPORTS Finance 355 Sports 409 SemCor 142 153 100 Table 4: WSD accuracy for words with a different first sense to the BNC~~~sure such as pairwise agreement of rankings or a ranking correlation coefficient, such as Spearmans One could also use the rankings to estimate probability distributions and compare the distributions with measures such as alpha-skew divergence <TREF>Lee, 1999</TREF>~~~A simple definition would be where the rankings assign different predominant senses to a word~~~Taking this simple definition of deviation, we demonstrate how this might be done for our corpora.
The distributional hypothesis <REF>Harris, 1968</REF> says the following: The meaning of entities, and the meaning of grammatical relations among them, is related to the restriction of combinations of these entities relative to other entities~~~Over recent years, many applications <REF>Lin, 1998</REF>, <TREF>Lee, 1999</TREF>, <REF>Lee, 2001</REF>, <REF>Weeds et al , 2004</REF>, and <REF>Weeds and Weir, 2006</REF> have been investigating the distributional similarity of words~~~Similarity means that words with similar meaning tend to appear in similar contexts~~~In NLG, the considerationofsemanticsimilarityisusuallypreferred to just distributional similarity.
The pairs are generally either related in one type of relationship, or completely unrelated~~~In general we may be able to identify related phrases for example with distributional similarity <TREF>Lee, 1999</TREF>, but would like to be able to automatically classify the related phrases by the type of the relationship~~~For this task we identify a larger set of candidate-related phrases~~~32 Query Log Data To find phrases that are similar or substitutable for web searchers, we turn to logs of user search sessions.
We draw a distinction between the task of identifying terms which are topically related and identifying the specific semantic class~~~For example, the terms dog, puppy, canine, schnauzer, cat and pet are highly related terms, which can be identified using techniques that include distributional similarity <TREF>Lee, 1999</TREF> and withindocument cooccurrence measures such as pointwise mutual information <REF>Turney et al , 2003</REF>~~~These techniques, however, do not allow us to distinguish the more specific relationships:  hypernymdog,puppy This work was carried out while these authors were at Yahoo~~~Research.
In all studies done so far, however, the first classifier  the confusion sets  were constructed manually by the researchers~~~Other word predictions tasks have also constructed manually the list of confusion sets <REF>Lee and Pereira, 1999</REF>; <REF>Dagan et al , 1999</REF>; <TREF>Lee, 1999</TREF> and justifications where given as to why this is a reasonable way to construct it~~~Even-<REF>Zohar and Roth, 2000</REF> present a similar task in which the confusion sets generation was automated~~~Their study also quantified experimentally the advantage in using early classifiers to restrict the size of the confusion set.
Therefore, the  term in skew divergence implicitly defines a parameter stating how many orders of magnitude smaller than pj to count qj if qj  0~~~We define the Zero-KL divergence with respect to 2<REF>In Lees 1999</REF> original presentation, skew divergence is defined not as sp,q but rather as sq,p~~~We reverse the argument order for consistency with the other measures discussed here~~~586 gamma: ZKLp,q  summationdisplay i pi braceleftbigg logpi qi qi negationslash 0  qi  0 Note that this is exactly KL-divergence when KLdivergence is defined and, like skew divergence, approximates KL divergence in the limit as   .
One is Jensen-Shannon divergence <REF>Lin, 1991</REF>, a symmetric measure based on KL-divergence defined as the average of the KL divergences of each distribution to their average distribution~~~Jensen-Shannon is well defined for all distributions becausetheaverageofpi andqi isnon-zerowhenevereither number is These measures and others are surveyed in <REF>Lee, 2001</REF>, who finds that Jensen-Shannon is outperformed by the Skew divergence measure introduced by Lee in 1999~~~The skew divergence2 accounts for zeros in q by mixing in a small amount of p sp,q  Dp bardbl q  1p  summationtexti pi log piqi1pi Lee found that as   1, the performance of skew divergence on natural language tasks improves~~~In particular, it outperforms most other models and even beats pure KL divergence modified to avoid zeros with sophisticated smoothing models.
22 Nearest-neighbors averaging As noted earlier, the nearest-neighbors averaging method is an alternative to clustering for estimating the probabilities of unseen cooccurfences~~~Given an unseen pair n, v, we calculate an estimate 15vln  as an appropriate average of pvln I where n I is distributionally similar to n Many distributional similarity measures can be considered <TREF>Lee, 1999</TREF>~~~In this paper, we focus on the one that gave the best results in our earlier work <REF>Dagan et al , 1999</REF>, the Jensen-Shannon divergence <REF>Rao, 1982</REF>; <REF>Lin, 1991</REF>~~~The Jensen-Shannon divergence of two discrete distributions p and q over the same domain is defined as 1 gSp, q   It is easy to see that JSp, q is always defined.
Opinion-piece data are used for training, and a different set of opinion-piece data and the subjective-element data are used for testing~~~With distributional similarity, words are judged to be more or less similar based on their distributional patterning in text <TREF>Lee 1999</TREF>; <REF>Lee and Pereira 1999</REF>~~~Our Table 5 Random sample of fixed-3-gram collocations in OP1~~~one-noun of-prep his-det worst-adj of-prep all-det quality-noun of-prep the-det to-prep do-verb so-adverb in-prep the-det company-noun you-pronoun and-conj your-pronoun have-verb taken-verb the-det rest-noun of-prep us-pronoun are-verb at-prep least-adj but-conj if-prep you-pronoun as-prep a-det weapon-noun continue-verb to-to do-verb purpose-noun of-prep the-det could-modal have-verb be-verb it-pronoun seem-verb to-prep to-pronoun continue-verb to-prep have-verb be-verb the-det do-verb something-noun about-prep cause-verb you-pronoun to-to evidence-noun to-to back-adverb that-prep you-pronoun are-verb i-pronoun be-verb not-adverb of-prep the-det century-noun of-prep money-noun be-prep 291 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language Table 6 Random sample of unique generalized collocations in OP1.
Thus, to decide whether to retain a word as a PSE, we consider the precision not of the individual word, but of the word together with a cluster of words similar to it~~~Many variants of distributional similarity have been used in NLP <TREF>Lee 1999</TREF>; <REF>Lee and Pereira 1999</REF>~~~<REF>Dekang Lins 1998</REF> method is used here~~~In contrast to many implementations, which focus exclusively on verb-noun relationships, Lins method incorporates a variety of syntactic relations.
To evaluate word prediction as a simple language model~~~We chose the verb prediction task which is similar to other word prediction tasks eg ,<REF>Golding and Roth, 1999</REF> and, in particular, follows the paradigm in <REF>Lee and Pereira, 1999</REF>; <REF>Dagan et al , 1999</REF>; <TREF>Lee, 1999</TREF>~~~There, a list of the confusion sets is constructed first, each consists of two different verbs~~~The verb vl is coupled with v2 provided that they occur equally likely in the corpus.
Results are shown in percentage of improvement in accuracy over the baseline~~~Table 2 compares our method to methods that use similarity measures <REF>Dagan et al , 1999</REF>; <TREF>Lee, 1999</TREF>~~~Since we could not use the same corpus as in those experiments, we compare the ratio of improvement and not the WER~~~The baseline in this studies is different, but other than that the experiments are identical.
The cosine measure <REF>Salton and McGill, 1983</REF> returns the cosine of the angle between two vectors~~~The Jensen-Shannon JS divergence measure <REF>Rao, 1983</REF> and the -skew divergence measure <TREF>Lee, 1999</TREF> are based on the Kullback-Leibler KL divergence measure~~~The KL divergence, or relative entropy, Dpjjq, between two probability distribution functions p and q is de ned <REF>Cover and Thomas, 1991</REF> as the ine ciency of assuming that the distribution is q when the true distribution is p: Dpjjq  Pcplog pq~~~However, Dpjjq  1 if there are any contexts c for which pc > 0 and qc  0.
However, due to the lack of a tight de nition for the concept of distributional similarity and the broad range of potential applications, a large number of measures of distributional similarity have been proposed or adopted see Section 2~~~Previous work on the evaluation of distributional similarity methods tends to either compare sets of distributionally similar words to a manually created semantic resource <REF>Lin, 1998</REF>; <REF>Curran and Moens, 2002</REF> or be oriented towards a particular task such as language modelling <REF>Dagan et al , 1999</REF>; <TREF>Lee, 1999</TREF>~~~The rst approach is not ideal since it assumes that the goal of distributional similarity methods is to predict semantic similarity and that the semantic resource used is a valid gold standard~~~Further, the second approach is clearly advantageous when one wishes to apply distributional similarity methods in a particular application area.
As the vector for each target word must sum to 1, the marginal distributions of target words have little effect on the resulting similarity estimates~~~Many 649 similarity measures and weighting functions have been proposed for distributional vectors; comparative studies include <TREF>Lee 1999</TREF>, <REF>Curran 2003</REF> and <REF>Weeds and Weir 2005</REF>~~~22 Kernel Methods for Computing Similarity and Distance In this section we describe two classes of functions, positive semi-definite and negative semidefinite kernels, and state some relationships between these classes~~~The mathematical treatment follows Berg et al.
It seems likely that distance measures that are known to work well for comparing co-occurrence distributions will also give us suitable psd similarity measures~~~Negative semi-definite kernels are bydefinitionsymmetric, whichrulestheKullbackLeibler divergence and <TREF>Lees 1999</TREF>-skew divergence out of consideration~~~The nsd condition 2 ismetifthedistancefunctionisasquaredmetricin a Hilbert space~~~In this paper we use a parametric familyofsquaredHilbertianmetricsonprobability distributions that has been discussed by <REF>Hein and Bousquet 2005</REF>.
23 Distributional Kernels Given the effectiveness of distributional similarity measures for numerous tasks in NLP and the interpretation of kernels as similarity functions, it seems natural to consider the use of kernels tailored for co-occurrence distributions when performing semantic classification~~~As shown in Section 22 the standardly used linear and Gaussian kernelsderivefromtheL2 distance, yet<TREF>Lee1999</TREF> has shown that this distance measure is relatively poor at comparing co-occurrence distributions~~~Information theory provides a number of alternative distance functions on probability measures, of which the L1 distance also called variational distance, Kullback-Leibler divergence and JensenShannon divergence are well-known in NLP and 1Negated nsd functions are sometimes called conditionally psd; they constitute a superset of the psd functions~~~650 Distance Definition Derived linear kernel L2 distance2 summationtextcPcw1Pcw22 summationtextc Pcw1Pcw2 L1 distance summationtextcPcw1Pcw2 summationtextc minPcw1,Pcw2 Jensen-Shannon summationtextc Pcw1log2 2Pcw1Pcw1Pcw2  summationtextc Pcw1log2 Pcw1Pcw1Pcw2  divergence Pcw2log2 2Pcw2Pcw1Pcw2 Pcw2log2 Pcw2Pcw1Pcw2 Hellinger distance summationtextcradicalbigPcw1radicalbigPcw22 summationtextcradicalbigPcw1Pcw2 Table 1: Squared metric distances on co-occurrence distributions and corresponding linear kernels were shown by Lee to give better similarity estimates than the L2 distance.
A key feature of this type of smoothing is the function which measures distributional similarity from cooccurrence frequencies~~~Several measures of distributional similarity have been proposed in the literature <REF>Dagan et al , 1999</REF>; <TREF>Lee, 1999</TREF>~~~We used two measures, the Jensen-Shannon divergence and the confusion probability~~~Those two measures have been previously shown to give promising performance for the task of estimating the frequencies of unseen verb-argument pairs <REF>Dagan et al , 1999</REF>; <REF>Grishman and Sterling, 1994</REF>; <REF>Lapata, 2000</REF>; <TREF>Lee, 1999</TREF>.
We used two measures, the Jensen-Shannon divergence and the confusion probability~~~Those two measures have been previously shown to give promising performance for the task of estimating the frequencies of unseen verb-argument pairs <REF>Dagan et al , 1999</REF>; <REF>Grishman and Sterling, 1994</REF>; <REF>Lapata, 2000</REF>; <TREF>Lee, 1999</TREF>~~~In the following we describe these two similarity measures and show how they can be used to recreate the frequencies for unseen adjective-noun pairs~~~Jensen-Shannon Divergence.
If a bigram is unseen in a given corpus, conventional approaches recreate its frequency using techniques such as back-off, linear interpolation, class-based smoothing or distance-weighted averaging see Dagan et al~~~1999 and <TREF>Lee 1999</TREF> for overviews~~~The approach proposed here does not recreate the missing counts, but instead retrieves them from a corpus that is much larger but also much more noisy than any existing corpus: it launches queries to a search engine in order to determine how often a bigram occurs on the web~~~We systematically investigated the validity of this approach by using it to obtain frequencies for predicate-argument bigrams adjective-noun, nounnoun, and verb-object bigrams.
The two measures are shown in Figure 2~~~The Skew divergence represents a generalisation of the Kullback-Leibler divergence and was proposed by <TREF>Lee 1999</TREF> as a linguistically motivated distance measure~~~We use a value of   :99~~~We explored in detail the influence of different types and sizes of context by varying the context specification and path value functions.
This makes semantic spaces more flexible, different types of contexts can be selected and words do not have to physically co-occur to be considered contextually relevant~~~However, existing models either concentrate on specific relations for constructing the semantic space such as objects eg , <TREF>Lee, 1999</TREF> or collapse all types of syntactic relations available for a given target word <REF>Grefenstette, 1994</REF>; <REF>Lin, 1998</REF>~~~Although syntactic information is now used to select a words appropriate contexts, this information is not explicitly captured in the contexts themselves which are still represented by words and is therefore not amenable to further processing~~~A commonly raised criticism for both types of semantic space models ie , word-based and syntaxbased concerns the notion of semantic similarity.
Contexts are defined as a small number of words surrounding the target word <REF>Lund and Burgess, 1996</REF>; <REF>Lowe and McDonald, 2000</REF> or as entire paragraphs, even documents <REF>Landauer and Dumais, 1997</REF>~~~Context is typically treated as a set of unordered words, although in some cases syntactic information is taken into account <REF>Lin, 1998</REF>; <REF>Grefenstette, 1994</REF>; <TREF>Lee, 1999</TREF>~~~A word can be thus viewed as a point in an n-dimensional semantic space~~~The semantic similarity between words can be then mathematically computed by measuring the distance between points in the semantic space using a metric such as cosine or Euclidean distance.
There are a number of studies that, starting from this hypothesis, have built automatic or semi-automatic procedures for clustering words <REF>Brill and Marcus, 1992</REF>; <REF>Pereira et al , 1993</REF>; <REF>Martin et al , 1998</REF>, especially in the field of cognitive sciences <REF>Redington et al , 1998</REF>; <REF>Gobet and Pine, 1997</REF>; <REF>Clark, 2000</REF>~~~They examine the distributional behaviour of some target words, comparing the lexical distribution of their respective collocates using quantitative measures of distributional similarity <TREF>Lee, 1999</TREF>~~~In <REF>Brill and Marcus, 1992</REF> it is given a semiautomatic procedure that, starting from lexical statistical data collected from a large corpus, aims to arrange target words in a tree more precisely a dendrogram, instead of clustering them automatically~~~This procedure requires a linguistic examination of the resulting tree, in order to identify the word classes that are most appropriate to describe the phenomenon under investigation.
The formula is symmetric but does not satisfy the triangle inequality~~~For speed the estimate may be calculated from the shared features alone <TREF>Lee, 1999</TREF>~~~After calculating all the pairwise estimates, we retained lists of the 100 most similar nouns for each of the nouns in the corpus data~~~No other data is used in the similarity calculations.
There are many approaches to computing semantic similarity between words based on their distribution in a corpus~~~For a general overview of similarity measures, see <REF>Manning and Schutze, 1999</REF>, and for some recent and extensive overviews and evaluations of similarity measures for ia automatic thesaurus construction, see <REF>Weeds, 2003</REF>; <REF>Curran, 2003</REF>; <REF>Lee, 2001</REF>; <REF>Dagan et al , 1999</REF>~~~They show that the information radius and the skew distance are among the best for finding distributional proxies for words~~~If we assume that a word w is represented as a sum of its contexts and that we can calculate the similarities between such word representations, we get a list Lw of words with quantifications of how similar they are to w Each similarity <REF>CompuTerm 2004</REF> 3rd International Workshop on Computational Terminology 63 list Lw contains a mix of words related to the senses of the word w If we wish to identify groups of synonyms and other related words in a list of similarityrated words, we need to find clusters of similar words that are more similar to one another than they are to other words.
The intuition behind the cosine measure is that the similarity between two distributions of words should be independent of the length of either document~~~However, researchers have demonstrated that cosine is not the best relevance metric for other applications, so we evaluated two other topical similarity scores: Jacquards coefficient, which performed better than most other similarity measures in a different task for <TREF>Lee 1999</TREF> and Nave Bayes, which gave better results than cosine in topic-adapted language models for <REF>Seymore and Rosenfeld 1997</REF>~~~We evaluated all three similarity metrics using Switchboard topics as the training data and each of our corpora for testing using cross-validation~~~We found that cosine is consistently better than both Jacquards coefficient and Nave Bayes, across all corpora tested.
1993 and <TREF>Lee 1999</TREF>, among others~~~We use the cosine similarity measure for windowbased contexts and the following commonly used similarity measures for the syntactic vector space: <REF>Hindles 1990</REF> measure, the weighted Lin measure <REF>Wu and Zhou, 2003</REF>, the -Skew divergence measure <TREF>Lee, 1999</TREF>, the Jensen-Shannon JS divergence measure <REF>Lin, 1991</REF>, Jaccards coef cient van <REF>Rijsbergen, 1979</REF> and the Confusion probability <REF>Essen and Steinbiss, 1992</REF>~~~The Jensen-Shannon measure JS x1, x2  summationtext yY summationtext xx1,x2 parenleftbigg P yx log parenleftbigg Pyx 1 2 Pyx1Pyx2 parenrightbiggparenrightbigg subsequently performed best for our task~~~We compare the different ranking methodologies and data sets with respect to a manually-de ned gold standard list of 20 goal-type verbs and 20 nouns.
We use verb-object relations in both active and passive voice constructions as did Pereira et al~~~1993 and <TREF>Lee 1999</TREF>, among others~~~We use the cosine similarity measure for windowbased contexts and the following commonly used similarity measures for the syntactic vector space: <REF>Hindles 1990</REF> measure, the weighted Lin measure <REF>Wu and Zhou, 2003</REF>, the -Skew divergence measure <TREF>Lee, 1999</TREF>, the Jensen-Shannon JS divergence measure <REF>Lin, 1991</REF>, Jaccards coef cient van <REF>Rijsbergen, 1979</REF> and the Confusion probability <REF>Essen and Steinbiss, 1992</REF>~~~The Jensen-Shannon measure JS x1, x2  summationtext yY summationtext xx1,x2 parenleftbigg P yx log parenleftbigg Pyx 1 2 Pyx1Pyx2 parenrightbiggparenrightbigg subsequently performed best for our task.
Although -Skew outperforms the simpler measures in ranking nouns, its performance on verbs is worse than the performance of Weighted Lin~~~<REF>While Lee 1999</REF> argues that -Skews asymmetry can be advantageous for nouns, this probably does not hold for verbs: verb hierarchies have much shallower structure than noun hierarchies with most verbs concentrated on one level <REF>Miller et al , 1990</REF>~~~This would explain why JS, which is symmetric compared to the -Skew metric, performed better in our experiments~~~In the evaluation presented here we therefore use Google Scholar data and the JS measure.
Table 1 summarises our expectations of the values of KL divergence and V, for the various substitutability relationships~~~KL divergence, unlike most similarity functions, is sensitive to the order of arguments related by hyponymy <TREF>Lee, 1999</TREF>~~~The 152 Something happened and something else happened~~~Something happened or something else happened.
149 This paper proposes that substitutability can be predicted through statistical analysis of the contexts in which connectives appear~~~Similar methods have been developed for predicting the similarity of nouns and verbs on the basis of their distributional similarity, and many distributional similarity functions have been proposed for these tasks <TREF>Lee, 1999</TREF>~~~However substitutability is a more complex notion than similarity, and we propose a novel variance-based function for assisting in this task~~~This paper constitutes a first step towards predicting substitutability of cnonectives automatically.
Here, xi and yj denote two words and c stands for a context~~~Similarly to <TREF>Lee, 1999</TREF>, we use unsmoothed relative frequencies to derive probability estimates P In the de nition of the dice coef cient, Fxi  c : Pcxi > 0~~~We are mainly interested in the symmetric measures dxi, yj  dyj, xi because of a symmetric positive semi-de nite matrix required by kernel methods~~~Consequently, such measures as the skew divergence were excluded from the consideration <TREF>Lee, 1999</TREF>.
There are a number of measures proposed over the years, including such metrics as cosine, dice coef cient, and Jaccard distance~~~Distributional similarity measures have been extensively studied in <TREF>Lee, 1999</TREF>; <REF>Weeds et al, 2004</REF>~~~We have chosen the following metrics: dice, cosine and l2 euclidean whose de nitions are given in Table 1~~~Here, xi and yj denote two words and c stands for a context.
We are mainly interested in the symmetric measures dxi, yj  dyj, xi because of a symmetric positive semi-de nite matrix required by kernel methods~~~Consequently, such measures as the skew divergence were excluded from the consideration <TREF>Lee, 1999</TREF>~~~The Euclidean measure as de ned in Table 1 does not necessarily vary from 0 to 1~~~It was therefore normalized by dividing an l2 score in Table 1 by a maximum score and retracting it from 1.
42 Experiment I: Distributional measures and their impact on the final performance Distributional similarity measures have been used for various tasks in the past~~~For instance, <TREF>Lee, 1999</TREF> employs them to detect similar nouns based on the verb-object cooccurrence pairs~~~The results suggest the Jaccards coef cient to be one of the best performing measures followed by some others including cosine~~~Euclidean distance fell into the group with the largest error rates.
This shows that we have extracted a reasonable number of features for each phrase, since distributional similarity techniques have been shown to work well for words which occur more than 100 times in a given corpus <REF>Lin, 1998</REF>; <REF>Weeds and Weir, 2003</REF>~~~We then computed the distributional similarity between each co-occurrence vector using the -skew divergence measure <TREF>Lee, 1999</TREF>~~~The -skew divergence measure is an approximation to the KullbackLeibler KL divergence meassure between two distributions p and q: Dpq  summationdisplay x pxlogpxqx 5We currently retain all of the distinctions between grammatical relations output by RASP~~~10 The -skew divergence measure is designed to be used when unreliable maximum likelihood estimates MLE of probabilities would result in the KL divergence being equal to .
Given this framework, many different methods of measuring distributional similarity have been proposed; see <REF>Dagan 2000</REF>, <REF>Weeds 2003</REF>, or <REF>Mohammad and Hirst 2005</REF> for a review~~~For example, the set of words that co-occur with w 1 and those that co-occur with w 2 may be regarded as a feature vector of each and their similarity measured as the cosine between the vectors; or a measure may be based on the KullbackLeibler divergence between the probability distributions Pww 1 andPww 2 , as, for example, <TREF>Lees 1999</TREF> -skew divergence~~~<REF>Lin 1998b</REF> uses his similarity theorem equation 19 above to derive a measure based on the degree of overlap of the sets of words with which w 1 and w 2 , respectively, have positive mutual information~~~22 Words that are distributionally similar do indeed often represent semantically related concepts, and vice versa, as the following examples demonstrate.
Second, whereas semantic relatedness is symmetric, distributional similarity is a potentially asymmetrical relationship~~~If distributional similarity is conceived of as substitutability, as <REF>Weeds and Weir 2005</REF> and <TREF>Lee 1999</TREF> emphasize, then asymmetries arise when one word appears in a subset of the contexts in which the other appears; for example, the adjectives that typically modify apple are a subset of those that modify fruit,sofruit substitutes for apple better than apple substitutes for fruit~~~While some distributional similarity measures, such as cosine, are symmetric, many, such as -skew divergence and the co-occurrence retrieval models developed by Weeds and Weir, are not~~~But this is simply not an adequate model of semantic relatedness, for which substitutability is far too strict a requirement: window and house are semantically related, but they are not plausibly substitutable in most contexts.
As a means of acknowledging the polysemy of language, in this paper the term concept will refer to a particular sense of a given word~~~We want to be very clear that, throughout this paper, when we say that two words are similar, this is a short way of saying that they denote similar concepts; we are not talking about similarity of distributional or co-occurrence behavior of the words, for which the term word similarity has also been used <REF>Dagan 2000</REF>; <REF>Dagan, Lee, and Pereira 1999</REF>~~~While similarity of denotation might be inferred from similarity of distributional or co-occurrence behavior <REF>Dagan 2000</REF>; <REF>Weeds 2003</REF>, the two are distinct ideas~~~We return to the relationship between them in Section 62.
Baseline1 and Baseline2 in our system use different back-off schema~~~The following formula is introduced in <TREF>Lee 1999</TREF> for word similarity-based smoothing: 4 , ,    1         tt tt wSw tt wSw tttt tt wwsim wtagPwwsim wtagP where Sw is a set of candidate similar words and simw,w is the similarity between word w and w~~~Word similarity-based smoothing approach is used in our system to make advantage of the huge unlabeled corpus~~~In order to plug the word similarity-based smoothing into our HMM model, we made several extensions to formula 4.
The constant and multiplying factors are required, since the CRM defines a similarity in the range 0,1, whereas the L 1 Norm defines a distance in the range 0,2 where 0 distance is equivalent to 1 on the similarity scale~~~44 The -skew Divergence Measure The -skew divergence measure <REF>Lee 1999, 2001</REF> is a popular approximation to the Kullback-Leibler divergence measure 8 <REF>Kullback and Leibler 1951</REF>; <REF>Cover and Thomas 1991</REF>~~~It is an approximation developed to be used when unreliable MLE probabilities 7 Distance measures, also referred to as divergence and dissimilarity measures, can be viewed as the inverse of similarity measures; that is, an increase in distance correlates with a decrease in similarity~~~8 The Kullback-Leibler divergence measure is also often referred to as relative entropy 456 Weeds and Weir Co-occurrence Retrieval would result in the actual Kullback-Leibler divergence measure being equal to Itis defined <TREF>Lee 1999</TREF> as: dist  q, r  Drq  1 r 35 for 0    1, and where: Dpq  summationdisplay x pxlog px qx 36 In effect, the q distribution is smoothed with the r distribution, which results in it always being non-zero when the r distribution is non-zero.
In our experiments, the development-set similarity using the harmonic mean in the additive MI-based CRM was 0312 for high-frequency nouns and 0153 for low-frequency nouns, and the development-set similarity using the harmonic mean in the additive t-test based CRM was 0294 for high-frequency nouns and 0129 for low-frequency nouns~~~52 Pseudo-Disambiguation Task Pseudo-disambiguation tasks have become a standard evaluation technique <REF>Gale, Church, and Yarowsky 1992</REF>; Sch utze 1992; <REF>Pereira, Tishby, and Lee 1993</REF>; Sch utze 1998; <TREF>Lee 1999</TREF>; <REF>Dagan, Lee, and Pereira 1999</REF>; <REF>Golding and Roth 1999</REF>; <REF>Rooth et al 1999</REF>; <REF>EvenZohar and Roth 2000</REF>; <REF>Lee 2001</REF>; <REF>Clark and Weir 2002</REF> and, in the current setting, we may use a nouns neighbors to decide which of two co-occurrences is the most likely~~~Although pseudo-disambiguation is an artificial task, it has relevance in at least two application areas~~~First, by replacing occurrences of a particular word in a test suite with 465 Computational Linguistics Volume 31, Number 4 a pair of words from which a technique must choose, we recreate a simplified version of the word sense disambiguation task; that is, we choose between a fixed number of homonyms based on local context.
In order to study the relationship between parameter settings and error rate, we combine three of the sets to form a development set and two of the sets to form a test set~~~The development set is used to optimize parameters and the test set 10 <REF>Unlike Lee 1999</REF>, we do not delete instances from the test data that occur in the training data~~~This is discussed in detail in <REF>Weeds 2003</REF>, but our main justification for this approach is that a single co-occurrence of n, v 1  compared to zero co-occurrences of n, v 2  is not necessarily sufficient evidence to conclude that the population probability of n, v 1  is greater than that of n, v 2 ~~~11 Ten being less than the minimum number 14 of possibly indistinct co-occurrences for any target noun in the original test data.
This is advantageous in the computation of similarity, since computing the sums over all co-occurrence types rather than just those co-occurring with at least one of the words is 1 very computationally expensive and 2 due to their vast number, the effect of these zero frequency co-occurrence types tends to outweigh the effect of those co-occurrence types that have actually occurred~~~Giving such weight to these shared non-occurrences seems unintuitive and has been shown by <TREF>Lee 1999</TREF> to be undesirable in the calculation of distributional similarity~~~Hence, when using the 448 Weeds and Weir Co-occurrence Retrieval ALLR as the weight function, we use the additional restriction that Pc, w > 0 when selecting features~~~24 Difference-Weighted Models In additive models, no distinction is made between features that have occurred to the same extent with each word and features that have occurred to different extents with each word.
It is an approximation developed to be used when unreliable MLE probabilities 7 Distance measures, also referred to as divergence and dissimilarity measures, can be viewed as the inverse of similarity measures; that is, an increase in distance correlates with a decrease in similarity~~~8 The Kullback-Leibler divergence measure is also often referred to as relative entropy 456 Weeds and Weir Co-occurrence Retrieval would result in the actual Kullback-Leibler divergence measure being equal to Itis defined <TREF>Lee 1999</TREF> as: dist  q, r  Drq  1 r 35 for 0    1, and where: Dpq  summationdisplay x pxlog px qx 36 In effect, the q distribution is smoothed with the r distribution, which results in it always being non-zero when the r distribution is non-zero~~~The parameter  controls the extent to which the measure approximates the Kullback-Leibler divergence measure~~~When  is close to 1, the approximation is close while avoiding the problem with zero probabilities associated with using the Kullback-Leibler divergence measure.
Further, the noun hyponymy hierarchy in WordNet, which will be used as a pseudo-gold standard for comparison, is widely recognized in this area of research~~~Some previous work on distributional similarity between nouns has used only a single grammatical relation eg , <TREF>Lee 1999</TREF>, whereas other work has considered multiple grammatical relations eg , <REF>Lin 1998a</REF>~~~We consider only a single grammatical relation because we believe that it is important to evaluate the usefulness of each grammatical relation in calculating similarity before deciding how to combine information from 5 This results in a single 80:20 split of the complete data set, in which we are guaranteed that the original relative frequencies of the target nouns are maintained~~~6 The use of grammatical relations to model context precludes finding similarities between words of different parts of speech.
A statistical technique using a language model that assigns a zero probability to these previously unseen events will rule the correct parse or interpretation of the utterance impossible~~~Similarity-based smoothing <REF>Hindle 1990</REF>; <REF>Brown et al 1992</REF>; <REF>Dagan, Marcus, and Markovitch 1993</REF>; <REF>Pereira, Tishby, and Lee 1993</REF>; <REF>Dagan, Lee, and Pereira 1999</REF> provides an intuitively appealing approach to language modeling~~~In order to estimate the probability of an unseen co-occurrence of events, estimates based on seen occurrences of similar events can be combined~~~For example, in a speech recognition task, we might predict that cat is a more likely subject of growl than the word cap, even though neither co-occurrence has been seen before, based on the fact that cat is similar to words that do occur as the subject of growl eg , dog and tiger, whereas cap is not.
However, the next section will present a small-scale study that compares the performance of several smoothing techniques with the performance of Web counts on a standard task from the literature~~~34 Pseudodisambiguation In the smoothing literature, re-created frequencies are typically evaluated using pseudodisambiguation <REF>Clark and Weir 2001</REF>; <REF>Dagan, Lee, and Pereira 1999</REF>; <TREF>Lee 1999</TREF>; <REF>Pereira, Tishby, and Lee 1993</REF>; <REF>Prescher, Riezler, and Rooth 2000</REF>; <REF>Rooth et al 1999</REF>~~~477 Keller and Lapata Web Frequencies for Unseen Bigrams The aim of the pseudodisambiguation task is to decide whether a given algorithm re-creates frequencies that make it possible to distinguish between seen and unseen bigrams in a given corpus~~~A set of pseudobigrams is constructed according to a set of criteria detailed below that ensure that they are unattested in the training corpus.
Conclusions This article explored a novel approach to overcoming data sparseness~~~If a bigram is unseen in a given corpus, conventional approaches re-create its frequency using techniques such as back-off, linear interpolation, class-based smoothing or distanceweighted averaging see Dagan, Lee, and Pereira 1999 and Lee 1999 for overviews~~~The approach proposed here does not re-create the missing counts but instead retrieves them from a corpus that is much larger but also much more noisy than any existing corpus: it launches queries to a search engine in order to determine how often the bigram occurs on the Web~~~We systematically investigated the validity of this approach by using it to obtain frequencies for predicate-argument bigrams adjective-noun, noun-noun, and verbobject bigrams.
Other, more sophisticated class-based methods do away with the simplifying assumption that the argument co-occurring with a given predicate adjective, noun, verb is distributed evenly across its conceptual classes and attempt to find the right level of generalization in a concept hierarchy, by discounting, for example, the contribution of very general classes <REF>Clark and Weir 2001</REF>; <REF>McCarthy 2000</REF>; <REF>Li and Abe 1998</REF>~~~Other smoothing approaches such as discounting <REF>Katz 1987</REF> and distance-weighted averaging <REF>Grishman and Sterling 1994</REF>; <REF>Dagan, Lee, and Pereira 1999</REF> re-create counts of unseen word combinations by exploiting only corpus-internal evidence, without relying on taxonomic information~~~Our goal was to demonstrate that frequencies retrieved from the Web are a viable alternative to conventional smoothing methods when data are sparse; we do not claim that our Web-based method is necessarily superior to smoothing or that it should be generally preferred over smoothing methods~~~However, the next section will present a small-scale study that compares the performance of several smoothing techniques with the performance of Web counts on a standard task from the literature.
They demonstrate that the counts re-created using this smoothing technique correlate significantly with plausibility judgments for adjective-noun bigrams~~~They also show that this class-based approach outperforms distance-weighted averaging <REF>Dagan, Lee, and Pereira 1999</REF>, a smoothing method that re-creates unseen word co-occurrences on the basis of distributional similarity without relying on a predefined taxonomy, in predicting plausibility~~~In the current study, we used the smoothing technique of <REF>Lapata, Keller, and McDonald 2001</REF> to re-create not only adjective-noun bigrams, but also noun-noun 475 Keller and Lapata Web Frequencies for Unseen Bigrams Table 12 Correlation of counts re-created using class-based smoothing with Web counts~~~Adjective-Noun Noun-Noun Verb-Object Seen Bigrams AltaVista 344 362 361 Google 330 343 349 Unseen Bigrams AltaVista 439 386 412 Google 444 421 397 p <05 one-tailed.
Despite their imperfect output, heuristic methods for the extraction of syntactic relations are relatively common in statistical NLP~~~Several statistical models employ frequencies obtained from the output of partial parsers and other heuristic methods; these include models for disambiguating the attachment site of prepositional phrases <REF>Hindle and Rooth 1993</REF>; <REF>Ratnaparkhi 1998</REF>, models for interpreting compound nouns <REF>Lauer 1995</REF>; <REF>Lapata 2002</REF> and polysemous adjectives <REF>Lapata 2001</REF>, models for the induction of selectional preferences <REF>Abney and Light 1999</REF>, methods for automatically clustering words according to their distribution in particular syntactic contexts <REF>Pereira, Tishby, and Lee 1993</REF>, automatic thesaurus extraction <REF>Grefenstette 1994</REF>; <REF>Curran 2002</REF>, and similarity-based models of word co-occurrence probabilities <TREF>Lee 1999</TREF>; <REF>Dagan, Lee, and Pereira 1999</REF>~~~In this article we investigate alternative ways for obtaining bigram frequencies that are potentially useful for such models despite the fact that some of these bigrams are identified in a heuristic manner and may be noisy~~~22 Sampling Bigrams from the NANTC We also obtained corpus counts from a second corpus, the North American News Text Corpus NANTC.
By using Japanese HTML documents, we empirically show that our proposed method can obtain a significant number of hyponymy relations which would otherwise be missed by alternative methods~~~Hyponymy relations can play a crucial role in various NLP systems, and there have been many attempts to develop automatic methods to acquire hyponymy relations from text corpora <REF>Hearst, 1992</REF>; <TREF>Caraballo, 1999</TREF>; <REF>Imasumi, 2001</REF>; <REF>Fleischman et al , 2003</REF>; <REF>Morin and Jacquemin, 2003</REF>; <REF>Ando et al , 2003</REF>~~~Most of these techniques have relied on particular linguistic patterns, such as NP such as NP The frequencies of use for such linguistic patterns are relatively low, though, and there can be many expressions that do not appear in such patterns even if we look at large corpora~~~The effort of searching for other clues indicating hyponymy relations is thus significant.
<REF>Pantel and Lin, 2002</REF> improves on the latter by clustering by committee~~~<TREF>Caraballo 1999</TREF> uses conjunction and appositive annotations in the vector representation~~~2We did not compare against methods that use richer syntactic information, both because they are supervised and because they are much more computationally demanding~~~3We are not aware of any multilingual evaluation previously reported on the task.
For instance, <REF>Lin 1998</REF> used dependency relation as word features to compute word similarities from large corpora, and compared the thesaurus created in such a way with WordNet and Roget classes~~~<TREF>Caraballo 1999</TREF> selected head nouns from conjunctions and appositives in noun phrases, and used the cosine similarity measure with a bottomup clustering technique to construct a noun hierarchy from text~~~<REF>Curran and Moens 2002</REF> explored a new similarity measure for automatic thesaurus extraction which better compromises with the speed/performance tradeoff~~~<REF>You and Chen 2006</REF> used a feature clustering method to create a thesaurus from a Chinese newspaper corpus.
Previous work on automatic methods for building semantic lexicons could be divided into two main groups~~~One is automatic thesaurus acquisition, that is, to identify synonyms or topically related words from corpora based on various measures of similarity eg <REF>Riloff and Shepherd, 1997</REF>; <REF>Lin, 1998</REF>; <TREF>Caraballo, 1999</TREF>; <REF>Thelen and Riloff, 2002</REF>; <REF>You and Chen, 2006</REF>~~~Another line of research, which is more closely related to the current study, is to extend existing thesauri by classifying new words with respect to their given structures eg <REF>Tokunaga et al, 1997</REF>; <REF>Pekar, 2004</REF>~~~An early effort along this line is <REF>Hearst 1992</REF>, who attempted to identify hyponyms from large text corpora, based on a set of lexico-syntactic patterns, to augment and critique the content of WordNet.
In contrast, in this paper we focus on the problem of determining the categories of interest~~~Another thread of work is on finding synonymous terms and word associations, as well as automatic acquisition of IS-A or genus-head relations from dictionary definitions and free text <REF>Hearst, 1992</REF>; <TREF>Caraballo, 1999</TREF>~~~That work focuses on finding the right position for a word within a lexicon, rather than building up comprehensible and coherent faceted hierarchies~~~A major class of solutions for creating subject hierarchies uses data clustering.
One way to overcome this problem might be to give judges information about a sequence of higher ancestors, in order to make the judgement easier~~~It is difficult to compare these results with results from other studies such as that of <TREF>Caraballo 1999</TREF>, as the data used is not the same~~~However, it seems that our figures are in the same range as those reported in previous studies~~~<REF>Charniak  Roark 1998</REF>, evaluating the semantic lexicon against gold standard resources the MUC-4 and the WSJ corpus, reports that the ratio of valid to total entries for their system lies between 20 and 40.
For example, while the plural form of the word boy ie boys is a valid hyponym of the hypernym group, the singular form would not be~~~As was also reported by <TREF>Caraballo 1999</TREF>, the judges sometimes found proper nouns as hyponyms hard to evaluate~~~Eg it might be hard to tell if Simon Le Bon is a valid hyponym to the hypernym rock star if his identity is unknown to the judge~~~One way to overcome this problem might be to give judges information about a sequence of higher ancestors, in order to make the judgement easier.
Generally, principle 1-2 above are meant to prevent the hierarchies from containing ambiguity~~~The built-in ambiguity in the hyponymy hierarchy presented in <TREF>Caraballo, 1999</TREF> is primarily an effect of the fact that all information is composed into one tree~~~Part of the ambiguity could have been solved if the requirement of building one tree had been relaxed~~~Principle 2, regarding keeping the hierarchy ambiguity-free, is especially important, as we are working with acquisition from a corpus that is not domain restricted.
<REF>Charniak  Roark 1998</REF>, evaluating the semantic lexicon against gold standard resources the MUC-4 and the WSJ corpus, reports that the ratio of valid to total entries for their system lies between 20 and 40~~~<TREF>Caraballo 1999</TREF> let three judges evaluate ten internal nodes in the hyponymy hierarchy, that had at least twenty descendants~~~Cases where judges had problems with proper nouns as hyponyms, corresponding to these mentioned above, were corrected~~~When the best hypernym was evaluated, the result reported for a majority of the judges was 33.
Continue at 1~~~<TREF>Caraballo 1999</TREF> uses a hierarchical clustering technique to build a hyponymy hierarchy of nouns~~~The internal nodes are labeled by the syntactic constructions from <REF>Hearst 1992</REF>~~~Each internal node in the hierarchy can be represented by up to three nouns.
Automatically extracting world knowledge from MRDs was attempted by projects such as MindNet at Microsoft Research <REF>Richardson, Dolan, and Vanderwende 1998</REF>, and Barrierre and <REF>Popowichs 1996</REF> project, which learns from childrens dictionaries~~~IS-A hierarchies have been learned automatically from MRDs <REF>Hearst 1992</REF> and from corpora Caraballo 1999 among others~~~14 http://wwwclcamacuk/Research/NL/acquilex/acqhomehtml 240 Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences Research on merging information from various lexical resources is related to the present work in the sense that the consistency issues to be resolved are similar~~~One example is the construction of Unified Medical Language System UMLS 15 <REF>Lindberg, Humphreys, and McCray 1993</REF>, in the medical domain.
Based on these seeds, she proposes a bootstrapping algorithm to semi-automatically acquire new more specific patterns~~~Similarly, <TREF>Caraballo, 1999</TREF> uses predefined patterns such as X is a kind of Y or X, Y, and other Zs to identify hypernym/hyponym relationships~~~This approach to information extraction is based on a technique called selective concept extraction as defined by <REF>Riloff, 1993</REF>~~~Selective concept extraction is a form of text skimming that selectively processes relevant text while effectively ignoring surrounding text that is thought to be irrelevant to the domain.
However, it is well known that any knowledge-based system suffers from the so-called knowledge acquisition bottleneck, ie the difficulty to actually model the domain in question~~~As stated in <TREF>Caraballo, 1999</TREF>, WordNet has been an important lexical knowledge base, but it is insufficient for domain specific texts~~~So, many attempts have been made to automatically produce taxonomies <REF>Grefenstette, 1994</REF>, but <TREF>Caraballo, 1999</TREF> is certainly the first work which proposes a complete overview of the problem by 1 automatically building a hierarchical structure of nouns based on bottom-up clustering methods and 2 labeling the internal nodes of the resulting tree with hypernyms from the nouns clustered underneath by using patterns such as B is a kind of A~~~2008.
As stated in <TREF>Caraballo, 1999</TREF>, WordNet has been an important lexical knowledge base, but it is insufficient for domain specific texts~~~So, many attempts have been made to automatically produce taxonomies <REF>Grefenstette, 1994</REF>, but <TREF>Caraballo, 1999</TREF> is certainly the first work which proposes a complete overview of the problem by 1 automatically building a hierarchical structure of nouns based on bottom-up clustering methods and 2 labeling the internal nodes of the resulting tree with hypernyms from the nouns clustered underneath by using patterns such as B is a kind of A~~~2008~~~Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 30 Unported license http://creativecommonsorg/licenses/by-ncsa/30/.
For example, the phrase France, Germany, Italy, and other European countries suggests that France, Germany and Italy are part of the class of European countries~~~Such hierarchical examples are quite sparse, and greater coverage was later attained by <REF>Riloff and Shepherd 1997</REF> and <REF>Roark and Charniak 1998</REF> in extracting relations not of hierarchy but of similarity, by finding conjunctions or co-ordinations such as cloves, cinammon, and nutmeg and cars and trucks This work was extended by <TREF>Caraballo 1999</TREF>, who built classes of related words in this fashion and then reasoned that if a hierarchical relationship could be extracted for any member of this class, it could be applied to all members of the class~~~This technique can often mistakenly reason across an ambiguous middle-term, a situation that was improved upon by <REF>Cederberg and Widdows 2003</REF>, by combining pattern-based extraction with contextual filtering using latent semantic analysis~~~Prior work in discovering non-compositional phrases has been carried out by <REF>Lin 1999</REF> and Baldwin et al.
The extraction patterns used in our research are linguistically richer patterns, requiring shallow parsing and syntactic role assignment~~~In recent years several techniques have been developed for semantic lexicon creation eg , <REF>Hearst, 1992</REF>; <REF>Riloff and Shepherd, 1997</REF>; <REF>Roark and Charniak, 1998</REF>; <TREF>Caraballo, 1999</TREF>~~~Semantic word learning is different from subjective word learning, but we have shown that MetaBootstrapping and Basilisk could be successfully applied to subjectivity learning~~~Perhaps some of these other methods could also be used to learn subjective words.
For this reason, knowledge-poor approaches such as the distributional approach are particularly suited for this task~~~Its previous applications eg , <REF>Grefenstette 1993</REF>, <REF>Hearst and Schuetze 1993</REF>, <REF>Takunaga et al 1997</REF>, <REF>Lin 1998</REF>, <TREF>Caraballo 1999</TREF> demonstrated that cooccurrence statistics on a target word is often sufficient for its automatical classification into one of numerous classes such as synsets of WordNet~~~Distributional techniques, however, are poorly applicable to rare words, ie, those words for which a corpus does not contain enough cooccurrence data to judge about their meaning~~~Such words are the primary concern of many practical NLP applications: as a rule, they are semantically focused words and carry a lot of important information.
<REF>Hearst 1992</REF> used textual patterns eg such as to identify common class members~~~<REF>Caraballo and Charniak 1999</REF> and <TREF>Caraballo 1999</TREF> augmented these lexical patterns with more general lexical co-occurrence statistics such as relative entropy~~~<REF>Berland and Charniak 1999</REF> use Hearst style techniques to learn meronym relationships part-whole from corpora~~~There has also been work in building ontologies from structured Correct Answer Question Debbie Reynolds What actress once held the title of Miss Burbank.
In addition, <REF>Strzalkowski and Wang 1996</REF> used a bootstrapping technique to identify types of references, and <REF>Riloff and Jones 1999</REF> adapted bootstrapping techniques to lexicon building targeted to information extraction~~~In the same vein, researchers at Brown University <REF>Caraballo and Charniak, 1999</REF> <REF>Berland and Charniak, 1999</REF>, <TREF>Caraballo, 1999</TREF> and <REF>Roark and Charniak, 1998</REF> focused on target constructions, in particular complex noun thrases, and searched for information not only on identifying classes of nouns, lint also hypernyms, noun specificity and meronymy~~~We have a diflbrent perspective than these lines of inquiry~~~They were specifying various semantic relationships and seeking ways to collect similar pairs.
To do this, many approaches to lexical acquisition employ the distributional model of word meaning induced from the distribution of the word across various lexical contexts of its occurrence found in the corpus~~~The approach is now being actively explored for a wide range of semantics-related tasks including automatic construction of thesauri <REF>Lin, 1998</REF>; <TREF>Caraballo, 1999</TREF>, their enrichment <REF>Alfonseca and Manandhar, 2002</REF>; <REF>Pekar and Staab, 2002</REF>, acquisition of bilingual lexica from nonaligned <REF>Kay and Rscheisen, 1993</REF> and nonparallel corpora <REF>Fung and Yee, 1998</REF>, learning of information extraction patterns from un-annotated text <REF>Riloff and Schmelzenbach, 1998</REF>~~~However, because of irregularities in corpus data, corpus statistics cannot guarantee optimal performance, notably for rare lexical items~~~In order to improve robustness, recent research has attempted a variety of ways to incorporate external knowledge into the distributional model.
However, such clustering algorithms fail to name their classes~~~<TREF>Caraballo 1999</TREF> was the first to use clustering for labeling is-a relations using conjunction and apposition features to build noun clusters~~~<REF>Recently, Pantel and Ravichandran 2004</REF> extended this approach by making use of all syntactic dependency features for each noun~~~3 Syntactical co-occurrence approach Much of the research discussed above takes a similar approach of searching text for simple surface or lexico-syntactic patterns in a bottom-up approach.
larity measure could be defined so that, for example: simexecutives, spouses > simbusloads, spouses then it is potentially useful for coordination disambiguation~~~The idea that nouns co-occurring in conjunctions tend to be semantically related has been noted in <REF>Riloff and Shepherd, 1997</REF> and used effectively to automatically cluster semantically similar words <REF>Roark and Charniak, 1998</REF>; <TREF>Caraballo, 1999</TREF>; <REF>Widdows and Dorow, 2002</REF>~~~The tendency for conjoined nouns to be semantically similar has also been exploited for coordinate noun phrase disambiguation by <REF>Resnik 1999</REF> who employed a measure of similarity based on WordNet to measure which were the head nouns being conjoined in certain types of coordinate noun phrase~~~In this paper we look at different measures of word similarity in order to discover whether they can detect empirically a tendency for conjoined nouns to be more similar than nouns which co-occur but are not conjoined.
This includes the extraction of hyponymy and synonymy relations <REF>Hearst 1992</REF>; <TREF>Caraballo 1999</TREF>, among others as well as meronymy <REF>Berland and Charniak 1999</REF>; <REF>Meyer 2001</REF>~~~10 One approach to the extraction of instances of a particular lexical relation is the use of patterns that express lexical relations structurally explicitly in a corpus <REF>Hearst 1992</REF>; <REF>Berland and Charniak 1999</REF>; <TREF>Caraballo 1999</TREF>; <REF>Meyer 2001</REF>, and this is the approach we focus on here~~~As an example, the pattern NP 1 and other NP 2 usually expresses a hyponymy/similarity relation between the hyponym NP 1 and its hypernym NP 2 <REF>Hearst 1992</REF>, and it can therefore be postulated that two noun phrases that occur in such a pattern in a corpus should be linked in an ontology via a hyponymy link~~~Applications of the extracted relations to anaphora resolution are less frequent.
372 Markert and Nissim Knowledge Sources for Anaphora Resolution consuming hand-modeling~~~This includes the extraction of hyponymy and synonymy relations <REF>Hearst 1992</REF>; <TREF>Caraballo 1999</TREF>, among others as well as meronymy <REF>Berland and Charniak 1999</REF>; <REF>Meyer 2001</REF>~~~10 One approach to the extraction of instances of a particular lexical relation is the use of patterns that express lexical relations structurally explicitly in a corpus <REF>Hearst 1992</REF>; <REF>Berland and Charniak 1999</REF>; <TREF>Caraballo 1999</TREF>; <REF>Meyer 2001</REF>, and this is the approach we focus on here~~~As an example, the pattern NP 1 and other NP 2 usually expresses a hyponymy/similarity relation between the hyponym NP 1 and its hypernym NP 2 <REF>Hearst 1992</REF>, and it can therefore be postulated that two noun phrases that occur in such a pattern in a corpus should be linked in an ontology via a hyponymy link.
The literature on automated text categorization is enormous, but assumes that a set of categories has already been created, whereas the problem here is to determine the categories of interest~~~There has also been extensive work on finding synonymous terms and word associations, as well as automatic acquisition of IS-A or genus-head relations from dictionary definitions and glosses <REF>Klavans and Whitman, 2001</REF> and from free text <REF>Hearst, 1992</REF>; <TREF>Caraballo, 1999</TREF>~~~<REF>Sanderson and Croft 1999</REF> propose a method called subsumption for building a hierarchy for a set of documents retrieved for a query~~~For two terms x and y, x is said to subsume y if the following conditions hold: a2a4a3a6a5a8a7a9a11a10a13a12a15a14a17a16a18a20a19a21a2a4a3a6a9a22a7a5a23a10a25a24a27a26.
Other kinds of models that have been studied in the context of lexical acquisition are those based on lexico-syntactic patterns of the kind X, Y and other Zs, as in the phrase bluejays, robins and other birds~~~These types of models have been used for hyponym discovery <REF>Hearst, 1992</REF>; <REF>Roark and Charniak, 1998</REF>, meronym discovery <REF>Berland and Charniak, 1999</REF>, and hierarchy building <TREF>Caraballo, 1999</TREF>~~~These methods are very interesting but of limited applicability, because nouns that do not appear in known lexico-syntactic patterns cannot be learned~~~7 Conclusion All the approaches cited above focus on some aspect of the problem of lexical acquisition.
Hale, Ge, and Charniak <REF>Ge et al , 1998</REF> devised a technique to learn the gender of words~~~Caraballo <TREF>Caraballo, 1999</TREF> and Hearst <REF>Hearst, 1992</REF> created techniques to learn hypernym/hyponym relationships~~~None of these previous algorithms used extraction patterns or similar contexts to infer semantic class associations~~~Several learning algorithms have also been developed for named entity recognition eg , <REF>Collins and Singer, 1999</REF>; <REF>Cucerzan and Yarowsky, 1999</REF>.
Alternatively, some systems are based on the observation that related terms appear together in particular contexts~~~These systems extract related terms directly by recognising linguistic patterns eg X, Y and other Zs which link synonyms and hyponyms <REF>Hearst, 1992</REF>; <TREF>Caraballo, 1999</TREF>~~~Our previous work <REF>Curran and Moens, 2002</REF> has evaluated thesaurus extraction performance and ef ciency using several different context models~~~In this paper, we evaluate some existing similarity metrics and propose and motivate a new metric which outperforms the existing metrics.
These methods use clustering algorithms to group words according to their meanings in text, label the clusters using its members lexical or syntactic dependencies, and then extract an is-a relation between each cluster member and the cluster label~~~<TREF>Caraballo 1999</TREF> proposed the first attempt, which used conjunction and apposition features to build noun clusters~~~<REF>Recently, Pantel and Ravichandran 2004</REF> extended this approach by making use of all syntactic dependency features for each noun~~~The advantage of clustering approaches is that they permit algorithms to identify is-a relations that do not explicitly appear in text, however they generally fail to produce coherent clusters from fewer than 100 million words; hence they are unreliable for small corpora.
In Section 3, we show how latent semantic analysis can be used to filter potential relationships according to their semantic plausibility~~~In Section 4, we show how correctly extracted relationships can be used as seed-cases to extract several more relationships, thus improving recall; this work shares some similarities with that of <TREF>Caraballo 1999</TREF>~~~In Section 5 we show that combining the techniques of Section 3 and Section 4 improves both precision and recall~~~Section 6 demonstrates that 1Another possible view is that hyponymy should only refer to core relationships, not contingent ones so pheasant a60 bird might be accepted but pheasant a60 food might not be, because it depends on context and culture.
We use the broader subset definition because contingent relationships are an important part of world-knowledge and are therefore worth learning, and because in practice we found the distinction difficult to enforce~~~Another definition is given by <TREF>Caraballo 1999</TREF>: ~~~a word A is said to be a hypernym of a word B if native speakers of English accept the sentence B is a kind of A  linguistic tools such as lemmatization can be used to reliably put the extracted relationships into a normalized or canonical form for addition to a semantic resource~~~2 Pattern-Based Hyponymy Extraction The first major attempt to extract hyponyms from text was that of <REF>Hearst 1992</REF>, described in more detail in <REF>Hearst, 1998</REF>, who extracted relationships from the text of Groliers Encyclopedia.
4 Improving Recall Using Coordination Information One of the main challenges facing hyponymy extraction is that comparatively few of the correct relations that might be found in text are expressed overtly by the simple lexicosyntactic patterns used in Section 2, as was apparent in the results presented in that section~~~This problem has been addressed by <TREF>Caraballo 1999</TREF>, who describes a system that first builds an unlabelled hierarchy of noun clusters using agglomerative bottom-up clustering of vectors of noun coordination information~~~The leaves of this hierarchy corresponding to nouns are assigned hypernyms using Hearst-style lexicosyntactic patterns~~~Internal nodes in the hierarchy are then labelled with hypernyms of the leaves they subsume according to a vote of these subsumed leaves.
This paper suggests many possibilities for future work~~~First of all, it would be interesting to apply LSA to a system for building an entire hypernym-labelled ontology in roughly the way described in <TREF>Caraballo, 1999</TREF>, perhaps by using an LSA-weighted voting method to determine which hypernym would be used to label each node~~~We are considering how to extend our techniques to such a task~~~Also, systematic comparison of the lexicosyntactic patterns used for extraction to determine the relative productiveness and accuracy of each pattern might prove illuminating, as would comparison across different corpora to determine the impact of the topic area and medium/format of documents on the effectiveness of hyponymy extraction.
Further, we explore a problem faced by the automatic thesaurus generation community, which is that distributional similarity methods do not seem to o er any obvious way to distinguish between the semantic relations of synonymy, antonymy and hyponymy~~~Previous work on this problem <TREF>Caraballo, 1999</TREF>; <REF>Lin et al , 2003</REF> involves identifying speci c phrasal patterns within text eg, Xs and other Ys is used as evidence that X is a hyponym of Y Our work explores the connection between relative frequency, distributional generality and semantic generality with promising results~~~The rest of this paper is organised as follows~~~In Section 2, we present ten distributional similarity measures that have been proposed for use in NLP.
The sparseness of these patterns prevents this from being an effective approach to the problem we address here~~~<REF>In Caraballo 1999</REF>, we construct a hierarchy of nouns, including hypernym relations~~~However, there are several areas where that work could benefit from the research presented here~~~The hypernyms used to label the internal nodes of that hierarchy are chosen in a simple fashion; pattern-matching as in <REF>Hearst 1992</REF> is used to identify candidate hypernyms of the words dominated by a particular node, and a simple voting scheme selects the hypernyms to be used.
This project is meant to provide a tool to support other methods~~~<REF>See Caraballo 1999</REF> for a detailed description of a method to construct such a hierarchy~~~2 Previous work To the best of our knowledge, this is the first attempt to automatically rank nouns based on specificity~~~<REF>Hearst 1992</REF> found individual pairs of hypernyms and hyponyms from text using pattern-matching techniques.
our disposal, WordNet <REF>Fellbaum, 1998</REF> contains very little information that would be considered as being about attributesonly information about parts, not about qualities such as height, or even to the values of such attributes in the adjective networkand this information is still very sparse~~~On the other hand, the only work on the extraction of lexical semantic relations we are aware of has concentrated on the type of relations found in WordNet: hyponymy <REF>Hearst, 1998</REF>; <TREF>Caraballo, 1999</TREF> and meronymy <REF>Berland and Charniak, 1999</REF>; <REF>Poesio et al, 2002</REF>~~~2 The work discussed here could be perhaps best described as an example of empirical ontology: using linguistics and philosophical ideas to improve the results of empirical work on lexical / ontology acquisition, and vice versa, using findings from empirical analysis to question some of the assumptions of theoretical work on ontology and the lexicon~~~Specifically, we discuss work on the acquisition of nominal concept attributes whose goal is twofold: on the one hand, to clarify the notion of attribute and its role in lexical semantics, if any; on the other, to develop methods to acquire such information automatically eg , to supplement WordNet.
Finally, some systems extract synonyms directly without extracting and comparing contextual representations for each term~~~Instead, these systems recognise terms within certain linguistic patterns eg X, Y and other Zs which associate synonyms and hyponyms <REF>Hearst, 1992</REF>; <TREF>Caraballo, 1999</TREF>~~~Thesaurus extraction is a good task to use to experiment with scaling context spaces~~~The vectorspace model with nearest neighbour searching is simple, so we neednt worry about interactions between the contexts we select and a learning algorithm such as independence of the features.
Node numbers represent hierarchical structure of terms Contextual information has been mainly used to represent the characteristics of terms~~~<TREF>Caraballo, 1999</TREF>A <REF>Grefenstette, 1994</REF> <REF>Hearst, 1992</REF> <REF>Pereira, 1993</REF> and <REF>Sanderson, 1999</REF> used contextual information to find hyponymy relation between terms~~~<TREF>Caraballo, 1999</TREF>B also used contextual information to determine the specificity of nouns~~~Contrary, compositional information of terms has not been commonly discussed.
This paper describes a classifier that assigns semantic thesaurus categories to unknown Chinese words~~~<REF>The Caraballo 1999</REF>s system adopted the contextual information to assign nouns to their hyponyms~~~<REF>Roark and Charniak 1998</REF> used the co-occurrence of words as features to classify nouns~~~While context is clearly an important feature, this paper focuses on non-contextual features, which may play a key role for unknown words that occur only once 1 The Sinica Corpus is a balanced corpus contained five million part-of-speech words in Mandarin Chinese.
My analysis of the Sinica Corpus shows that contrary to expectation, most of unknown words in Chinese are common nouns, adjectives, and verbs rather than proper nouns~~~Other previous research has focused on features related to unknown word contexts <TREF>Caraballo 1999</TREF>; <REF>Roark and Charniak 1998</REF>~~~While context is clearly an important feature, this paper focuses on non-contextual features, which may play a key role for unknown words that occur only once and hence have limited context~~~The feature I focus on, following <REF>Ciaramita 2002</REF>, is morphological similarity to words whose semantic category is known.
The existing approaches to ontology induction include those that start from structured data, merging ontologies or database schemas <REF>Doan et al 2002</REF>~~~Other approaches use natural language data, sometimes just by analyzing the corpus <REF>Sanderson and Croft 1999</REF>, <TREF>Caraballo 1999</TREF> or by learning to expand WordNet with clusters of terms from a corpus, eg, <REF>Girju et al 2003</REF>~~~Information extraction approaches that infer labeled relations either require substantial handcreated linguistic or domain knowledge, eg, <REF>Craven and Kumlien 1999</REF> <REF>Hull and Gomez 1993</REF>, or require human-annotated training data with relation information for each domain <REF>Craven et al 1998</REF>~~~The number of relations in H that our system missed relations that were more than distance 1 away in the system ontology, is 3493.
<REF>CompuTerm 2004</REF> 3rd International Workshop on Computational Terminology 49 234 Explicit Patterns Relations This knowledge source infers specific relations between terms based on characteristic cue-phrases which relate them~~~For example, the cue-phrase such as <REF>Hearst 1992</REF> <TREF>Caraballo 1999</TREF> suggest a kind-of relation, eg, a ligand such as triethylphosphine tells us that triethylphosphene is a kind of ligand~~~Likewise, in the TREC domain, air toxics such as benzene can suggest that benzene is a kind of air toxic~~~However, since such cue-phrase patterns tend to be sparse in occurrence, we do not use them in the evaluations described below.
Corpus statistics can be used to weight the links~~~For example, based on <TREF>Caraballo 1999</TREF>, each parent of a leaf node could be viewed as a cluster label for its children, with the weight of a parent-child link being determined based on how strongly the child is associated with the cluster~~~10 The mean distance in H between terms that are distance 1 apart in M is 517, with a standard deviation of 212~~~The mean distance in M between terms which are distance 1 apart in H is 385, with a standard deviation of 169.
The goal of this work is to become able to automatically acquire hyponymy relations for a wide range of words or phrases from HTML documents on the WWW~~~We do not use particular lexicosyntactic patterns, as previous attempts have <REF>Hearst, 1992</REF>; <TREF>Caraballo, 1999</TREF>; <REF>Imasumi, 2001</REF>; <REF>Fleischman et al , 2003</REF>; <REF>Morin and Jacquemin, 2003</REF>; <REF>Ando et al , 2003</REF>~~~The frequencies of use for such lexicosyntactic patterns are relatively low, and there can be many words or phrases that do not appear in such patterns even if we look at a large number of texts~~~The effort of searching for other clues indicating hyponymy relations is thus significant.
Fully unsupervised semantic clustering eg, <REF>Lin, 1998</REF>; <REF>Lin and Pantel, 2002</REF>; <REF>Davidov and Rappoport, 2006</REF> has the disadvantage that it may or may not produce the types and granularities of semantic classes desired by a user~~~Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes eg, <TREF>Caraballo, 1999</TREF>; <REF>Cimiano and Volker, 2005</REF>; <REF>Mann, 2002</REF>, and learning semantic relations such as meronymy <REF>Berland and Charniak, 1999</REF>; <REF>Girju et al, 2003</REF>~~~Our research focuses on semantic lexicon induction, which aims to generate lists of words that belong to a given semantic class eg, lists of FISH or VEHICLE words~~~Weakly supervised learning methods for semantic lexicon generation have utilized co-occurrence statistics <REF>Riloff and Shepherd, 1997</REF>; <REF>Roark and Charniak, 1998</REF>, syntactic information <REF>Tanev and Magnini, 2006</REF>; <REF>Pantel and Ravichandran, 2004</REF>; <REF>Phillips and Riloff, 2002</REF>, lexico-syntactic contextual patterns eg, resides in <location> or moved to <location> <REF>Riloff and Jones, 1999</REF>; <REF>Thelen and Riloff, 2002</REF>, and local and global contexts <REF>Fleischman and Hovy, 2002</REF>.
Accordingly, we try to extract a hierarchical relation of words automatically and statistically~~~In previous research, ways of extracting from definition sentences in dictionaries <REF>Tsurumaru et al , 1986</REF>; <REF>Shoutsu et al , 2003</REF> or from a corpus by using patterns such as a part of, is-a, or and <REF>Berland and Charniak, 1999</REF>; <TREF>Caraballo, 1999</TREF> have been proposed~~~Also, there is a method that uses the dependence relation between words taken from a corpus <REF>Matsumoto et al , 1996</REF>~~~In contrast, we propose a method based on the inclusion relation of appearance patterns from corpora.
Roark and Charniak <REF>Roark and Charniak, 1998</REF> followed up on this work by using a parser to explicitly capture these structures~~~Caraballo <TREF>Caraballo, 1999</TREF> also exploited these syntactic structures and applied a cosine vector model to produce semantic groupings~~~In our view, these previous systems used weak syntactic models because the syntactic structures sometimes identified desirable semantic associations and sometimes did not~~~To compensate, statistical models were used to separate the meaningful semantic associations from the spurious ones.
There are inherent problems in evaluating automatic thesaurus extraction techniques, and much research assumes a gold standard that does not exist see Kilgarriff 2003 and Weeds 2003 for more discussion of this~~~A further problem for distributional similarity methods for automatic thesaurus generation is that they do not offer any obvious way to distinguish between linguistic relations such as synonymy, antonymy, and hyponymy see Caraballo 1999 and Lin et al 2003 for work on this~~~Thus, one may question 1 You shall know a word by the company it keeps<REF>Firth 1957</REF> 440 Weeds and Weir Co-occurrence Retrieval the benefit of automatically generating a thesaurus if one has access to large-scale manually constructed thesauri eg , WordNet <REF>Fellbaum 1998</REF>, Rogets <REF>Roget 1911</REF>, the Macquarie <REF>Bernard 1990</REF> and Moby 2 ~~~Automatic techniques give us the opportunity to model language change over time or across domains and genres.
The number of dependency types may be reduced in future work~~~3 The Probability Model The DAG-like nature of the dependency structures makes it difficult to apply generative modelling techniques <REF>Abney, 1997</REF>; <TREF>Johnson et al , 1999</TREF>, so we have defined a conditional model, similar to the model of <REF>Collins 1996</REF> see also the conditional model in <REF>Eisner 1996b</REF>~~~While the model of <REF>Collins 1996</REF> is technically unsound <REF>Collins, 1999</REF>, our aim at this stage is to demonstrate that accurate, efficient wide-coverage parsing is possible with CCG, even with an over-simplified statistical model~~~Future work will look at alternative models4 4The reentrancies creating the DAG-like structures are fairly limited, and moreover determined by the lexical categories.
The features are shown with hidden variables corresponding to wordspecific hidden values, such as shares1 or bought3~~~In our experiments, we made use of features such as those in Figure 2 in combination with the following four definitions of the hiddenvalue 3We also performed some experiments using the conjugate gradient descent algorithm <TREF>Johnson et al , 1999</TREF>~~~However, we did not find a significant difference between the performance of either method~~~Since stochastic gradient descent was faster and required less memory, our final experiments used the stochastic gradient method.
Mainstream approaches in statistical parsing are based on nondeterministic parsing techniques, usually employing some kind of dynamic programming, in combination with generative probabilistic models that provide an n-best ranking of the set of candidate analyses derived by the parser <REF>Collins, 1997</REF>; <REF>Collins, 1999</REF>; <REF>Charniak, 2000</REF>~~~These parsers can be enhanced by using a discriminative model, which reranks the analyses output by the parser <TREF>Johnson et al , 1999</TREF>; <REF>Collins and Duffy, 2005</REF>; <REF>Charniak and Johnson, 2005</REF>~~~Alternatively, discriminative models can be used to search the complete space of possible parses <REF>Taskar et al , 2004</REF>; <REF>McDonald et al , 2005</REF>~~~A radically different approach is to perform disambiguation deterministically, using a greedy parsing algorithm that approximates a globally optimal solution by making a sequence of locally optimal choices, guided by a classifier trained on gold standard derivations from a treebank.
A recent development in data-driven parsing is the use of discriminative training methods <REF>Riezler et al , 2002</REF>; <REF>Taskar et al , 2004</REF>; <REF>Collins and Roark, 2004</REF>; <REF>Turian and Melamed, 2006</REF>~~~One popular approach is to use a log-linear parsing model and maximise the conditional likelihood function <TREF>Johnson et al , 1999</TREF>; <REF>Riezler et al , 2002</REF>; <REF>Clark and Curran, 2004b</REF>; Malouf and van <REF>Noord, 2004</REF>; <REF>Miyao and Tsujii, 2005</REF>~~~Maximising the likelihood involves calculating feature expectations, which is computationally expensive~~~Dynamic programming DP in the form of the inside-outside algorithm can be used to calculate the expectations, if the features are sufficiently local <REF>Miyao and Tsujii, 2002</REF>; however, the memory requirements can be prohibitive, especially for automatically extracted, wide-coverage grammars.
If a complete structure is represented with a feature forest of a tractable size, the parameters can be efficiently estimated by dynamic programming~~~A series of studies on parsing with wide-coverage LFG <TREF>Johnson et al , 1999</TREF>; <REF>Riezler et al , 2000</REF>; <REF>Riezler et al , 2002</REF> have had a similar motivation to ours~~~Their models have also been based on a discriminative model to select a parsing result from all candidates given by the grammar~~~A significant difference is that we apply maximum entropy estimation for feature forests to avoid the inherent problem with estimation: the exponential explosion of parsing results given by the grammar.
Wellknown computational linguistic models such as MLE MCLE Y  yi; X  xi X  xi Y  yi; X  xi Figure 1: The MLE makes the training data yi; xi as likely as possible relative to , while the MCLE makes yi; xi as likely as possible relative to other pairs y0; xi~~~Maximum-Entropy Markov Models <REF>McCallum et al , 2000</REF> and Stochastic Unification-based Grammars <TREF>Johnson et al , 1999</TREF> are standardly estimated with conditional estimators, and it would be interesting to know whether conditional estimation affects the quality of the estimated model~~~It should be noted that in practice, the MCLE of a model with a large number of features with complex dependencies may yield far better performance than the MLE of the much smaller model that could be estimated with the same computational effort~~~Nevertheless, as this paper shows, conditional estimators can be used with other kinds of models besides MaxEnt models, and in any event it is interesting to ask whether the MLE differs from the MCLE in actual applications, and if so, how.
However, because these constraints can be non-local or context-sensitive, developing stochastic versions of UBGs and associated estimation procedures is not as straight-forward as it is for, eg, PCFGs~~~Recent work has shown how to define probability distributions over the parses of UBGs <REF>Abney, 1997</REF> and efficiently estimate and use conditional probabilities for parsing <TREF>Johnson et al , 1999</TREF>~~~Like most other practical stochastic grammar estimation procedures, this latter estimation procedure requires a parsed training corpus~~~Unfortunately, large parsed UBG corpora are not yet available.
Maximum entropy ME models, variously known as log-linear, Gibbs, exponential, and multinomial logit models, provide a general purpose machine learning technique for classification and prediction which has been successfully applied to fields as diverse as computer vision and econometrics~~~In natural language processing, recent years have seen ME techniques used for sentence boundary detection, part of speech tagging, parse selection and ambiguity resolution, and stochastic attribute-value grammars, to name just a few applications <REF>Abney, 1997</REF>; <REF>Berger et al , 1996</REF>; <REF>Ratnaparkhi, 1998</REF>; <TREF>Johnson et al , 1999</TREF>~~~A leading advantage of ME models is their flexibility: they allow stochastic rule systems to be augmented with additional syntactic, semantic, and pragmatic features~~~However, the richness of the representations is not without cost.
2 Maximum likelihood estimation Suppose we are given a probability distribution p over a set of events X which are characterized by a d dimensional feature vector function f : X Rd In addition, we have also a set of contexts W and a function Y which partitions the members of X In the case of a stochastic context-free grammar, for example, X might be the set of possible trees, the feature vectors might represent the number of times each rule applied in the derivation of each tree, W might be the set of possible strings of words, and Yw the set of trees whose yield is w2W~~~A conditional maximum entropy model qxjw for p has the parametric form <REF>Berger et al , 1996</REF>; <REF>Chi, 1998</REF>; <TREF>Johnson et al , 1999</TREF>: qxjw  exp T f x y2Yw expT f y 1 where  is a d-dimensional parameter vector and T f x is the inner product of the parameter vector and a feature vector~~~Given the parametric form of an ME model in 1, fitting an ME model to a collection of training data entails finding values for the parameter vector  which minimize the Kullback-Leibler divergence between the model q and the empirical distribution p: Dpjjq   w;x px;wlog pxjwq xjw or, equivalently, which maximize the log likelihood: L   w;x pw;xlogqxjw 2 The gradient of the log likelihood function, or the vector of its first derivatives with respect to the parameter  is: G  Ep f  Eq f  3 Since the likelihood function 2 is concave over the parameter space, it has a global maximum where the gradient is zero~~~Unfortunately, simply setting G  0 and solving for  does not yield a closed form solution, so we proceed iteratively.
23 Log-Linear CCGs We can generalize CCGs to weighted, or probabilistic, models as follows~~~Our models are similar to several other approaches <REF>Ratnaparkhi et al , 1994</REF>; <TREF>Johnson et al , 1999</TREF>; <REF>Lafferty et al , 2001</REF>; <REF>Collins, 2004</REF>; <REF>Taskar et al , 2004</REF>~~~We will write x to denote a sentence, and y to denote a CCG parse for a sentence~~~We use GENx; to refer to all possible CCG parses for x under some CCG lexicon .
Therefore there are a large number of features available that could be used by stochastic models for disambiguation~~~Other researchers have worked on extracting features useful for disambiguation from unification grammar analyses and have built log linear models aka Stochastic Unification Based Grammars <TREF>Johnson et al , 1999</TREF>; <REF>Riezler et al , 2000</REF>~~~Here we also use log linear models to estimate conditional probabilities of sentence analyses~~~Since feature selection is almost prohibitive for these models, because of high computational costs, we use PCFG models to select features for log linear models.
One is for a simple model with a relatively small number of features, and the other is for a model with a large number of features~~~The usefulness of priors in maximum entropy models is not new to this work: Gaussian prior smoothing is advocated in <REF>Chen and Rosenfeld 2000</REF>, and used in all the stochastic LFG work <TREF>Johnson et al , 1999</TREF>~~~However, until recently, its role and importance have not been widely understood~~~For example, <REF>Zhang and Oles 2001</REF> attribute the perceived limited success of logistic regression for text categorization to a lack of use of regularization.
After filtering by the generator, the remaining fstructures were weighted by the stochastic disambiguation component~~~Similar to stochastic disambiguation for constraint-based parsing <TREF>Johnson et al , 1999</TREF>; <REF>Riezler et al , 2002</REF>, an exponential aka log-linear or maximumentropy probability model on transferred structures is estimated from a set of training data~~~The data for estimation consists of pairs of original sentences y and goldstandard summarized f-structures s which were manually selected from the transfer output for each sentence~~~For training data sj,yjmj1 and a set of possible summarized structures Sy for each sentence y, the objective was to maximize a discriminative criterion, namely the conditional likelihood L of a summarized f-structure given the sentence.
If on the other hand most features are noisy but at least weakly correlated with the target, it may be reasonable to attempt to reduce noise by averaging over all of the features~~~ME estimators with L2 regularization, which have been widely used in NLP tasks eg , <REF>Chen and Rosenfeld 2000</REF>; <REF>Charniak and Johnson 2005</REF>; <TREF>Johnson et al 1999</TREF>, tend to produce models that have this property~~~In addition, the perceptron algorithm and its variants, eg, the voted or averaged perceptron, is becoming increasingly popular due to their competitive performance, simplicity in implementation and low computational cost in training eg , <REF>Collins 2002</REF>~~~While recent studies claim advantages for L1 regularization, this study is the first of which we are aware to systematically compare it to a range of estimators on a diverse set of NLP tasks.
The problems with history-based models and the desire to be able to specify features as arbitrary predicates of the entire tree have been noted before~~~In particular, previous work <REF>Ratnaparkhi, Roukos, and Ward 1994</REF>; <REF>Abney 1997</REF>; Della Pietra, <REF>Della Pietra, and Lafferty 1997</REF>; <TREF>Johnson et al 1999</TREF>; <REF>Riezler et al 2002</REF> has investigated the use of Markov random fields MRFs or log-linear models as probabilistic models with global features for parsing and other NLP tasks~~~Log-linear models are often referred to as maximum-entropy models in the NLP literature~~~Similar methods have also been proposed for machine translation <REF>Och and Ney 2002</REF> and language understanding in dialogue systems <REF>Papineni, Roukos, and Ward 1997, 1998</REF>.
Experiments on the Penn WSJ treebank show that the model achieves state-of-the-art performance, for both constituent and dependency accuracy~~~In global linear models GLMs for structured prediction, eg, <TREF>Johnson et al, 1999</TREF>; <REF>Lafferty et al, 2001</REF>; <REF>Collins, 2002</REF>; <REF>Altun et al, 2003</REF>; <REF>Taskar et al, 2004</REF>, the optimal label y for an input x is y  arg max yYx w fx,y 1 where Yx is the set of possible labels for the input x; fx,y  Rd is a feature vector that represents the pair x,y; and w is a parameter vector~~~This paper describes a GLM for natural language parsing, trained using the averaged perceptron~~~The parser we describe recovers full syntactic representations, similar to those derived by a probabilistic context-free grammar PCFG.
This section describes the relationship between our work and this previous work~~~In reranking approaches, a first-pass parser is used to enumerate a small set of candidate parses for an input sentence; the reranking model, which is a GLM, is used to select between these parses eg, <REF>Ratnaparkhi et al, 1994</REF>; <TREF>Johnson et al, 1999</TREF>; <REF>Collins, 2000</REF>; <REF>Charniak and Johnson, 2005</REF>~~~A crucial advantage of our approach is that it considers a very large set of alternatives in Yx, and can thereby avoid search errors that may be made in the first-pass parser1 Another approach that allows efficient training of GLMs is to use simpler syntactic representations, in particular dependency structures McDon1Some features used within reranking approaches may be difficult to incorporate within dynamic programming, but it is nevertheless useful to make use of GLMs in the dynamicprogramming stage of parsing~~~Our parser could, of course, be used as the first-stage parser in a reranking approach.
Moreover, property design can be carried out in a targeted way, ie properties can be designed in order to improve the disambiguation of grammatical relations that, so far, are disambiguated particularly poorly or that are of special interest for the task that the systems output is used for~~~By demonstrating that property design is the key to good log-linear models for deepsyntactic disambiguation, our work confirms that specifying the features of a SUBG stochastic unification-based grammar is as much an empirical matter as specifying the grammar itself<TREF>Johnson et al , 1999</TREF>~~~Acknowledgements The work described in this paper has been carried out in the DLFG project, which was funded by the German Research Foundation DFG~~~Furthermore, I thank the audiences at several ParGram meetings, at the Research Workshop of the Israel Science Foundation on Large-scale Grammar Development and Grammar Engineering at the University of Haifa and at the SFB 732 Opening Colloquium in Stuttgart for their important feedback on earlier versions of this work.
Note that HPSG is one of the lexicalized grammar formalisms, in which lexical entries determine the dominant syntactic structures~~~Previous studies <REF>Abney, 1997</REF>; <TREF>Johnson et al , 1999</TREF>; <REF>Riezler et al , 2000</REF>; Malouf and van <REF>Noord, 2004</REF>; <REF>Kaplan et al , 2004</REF>; <REF>Miyao and Tsujii, 2005</REF> defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model <REF>Berger et al , 1996</REF>~~~The probability that a parse result T is assigned to a given sentence w  w1,,wn is Probabilistic HPSG phpsgTw  1Z w exp parenleftBiggsummationdisplay u ufuT parenrightBigg Zw  summationdisplay Tprime exp parenleftBiggsummationdisplay u ufuTprime parenrightBigg, where u is a model parameter, fu is a feature function that represents a characteristic of parse tree T, and Zw is the sum over the set of all possible parse trees for the sentence~~~Intuitively, the probability is defined as the normalized product of the weights expu when a characteristic corresponding to fu appears in parse result T The model parameters, u, are estimated using numerical optimization methods <REF>Malouf, 2002</REF> to maximize the log-likelihood of the training data.
The main difficulty of developing parsers in these formalisms was how to model a well-defined probabilistic model for graph structures such as feature structures~~~This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model <REF>Berger et al , 1996</REF> with many features for parse trees <REF>Abney, 1997</REF>; <TREF>Johnson et al , 1999</TREF>; <REF>Riezler et al , 2000</REF>; Malouf and van <REF>Noord, 2004</REF>; <REF>Kaplan et al , 2004</REF>; <REF>Miyao and Tsujii, 2005</REF>~~~Following this discriminative approach, techniques for efficiency were investigated for estimation <REF>Geman and Johnson, 2002</REF>; <REF>Miyao and Tsujii, 2002</REF>; Malouf and van <REF>Noord, 2004</REF> and parsing <REF>Clark and Curran, 2004b</REF>; <REF>Clark and Curran, 2004a</REF>; <REF>Ninomiya et al , 2005</REF>~~~An interesting approach to the problem of parsing efficiency was using supertagging Clark and Cur60 ran, 2004b; <REF>Clark and Curran, 2004a</REF>; <REF>Wang, 2003</REF>; <REF>Wang and Harper, 2004</REF>; <REF>Nasr and Rambow, 2004</REF>; <REF>Ninomiya et al , 2006</REF>; <REF>Foth et al , 2006</REF>; <REF>Foth and Menzel, 2006</REF>, which was originally developed for lexicalized tree adjoining grammars LTAG <REF>Bangalore and Joshi, 1999</REF>.
Firstly, the rudimentary character of functional annotations in standard treebanks has hindered the direct use of such data for statistical estimation of linguistically fine-grained statistical parsing systems~~~Rather, parameter estimation for such models had to resort to unsupervised techniques <REF>Bouma et al , 2000</REF>; <REF>Riezler et al , 2000</REF>, or training corpora tailored to the specific grammars had to be created by parsing and manual disambiguation, resulting in relatively small training sets of around 1,000 sentences <TREF>Johnson et al , 1999</TREF>~~~Furthermore, the effort involved in coding broadcoverage grammars by hand has often led to the specialization of grammars to relatively small domains, thus sacrificing grammar coverage ie the percentage of sentences for which at least one analysis is found on free text~~~The approach presented in this paper is a first attempt to scale up stochastic parsing systems based on linguistically fine-grained handcoded grammars to the UPenn Wall Street Journal henceforth WSJ treebank <REF>Marcus et al , 1994</REF>.
The clustering model itself is then used to yield smoothed probabilities as values for property functions on head-argument-relation tuples of LFG parses~~~32 Discriminative Estimation Discriminative estimation techniques have recently received great attention in the statistical machine learning community and have already been applied to statistical parsing <TREF>Johnson et al , 1999</TREF>; <REF>Collins, 2000</REF>; <REF>Collins and Duffy, 2001</REF>~~~In discriminative estimation, only the conditional relation of an analysis given an example is considered relevant, whereas in maximum likelihood estimation the joint probability of the training data to best describe observations is maximized~~~Since the discriminative task is kept in mind during estimation, discriminative methods can yield improved performance.
Recent work in statistical approaches to parsing and tagging has begun to consider methods which incorporate global features of candidate structures~~~Examples of such techniques are Markov Random Fields <REF>Abney 1997</REF>; Della <REF>Pietra et al 1997</REF>; <TREF>Johnson et al 1999</TREF>, and boosting algorithms <REF>Freund et al 1998</REF>; <REF>Collins 2000</REF>; <REF>Walker et al 2001</REF>~~~One appeal of these methods is their flexibility in incorporating features into a model: essentially any features which might be useful in discriminating good from bad structures can be included~~~A second appeal of these methods is that their training criterion is often discriminative, attempting to explicitly push the score or probability of the correct structure for each training sentence above the score of competing structures.
A second appeal of these methods is that their training criterion is often discriminative, attempting to explicitly push the score or probability of the correct structure for each training sentence above the score of competing structures~~~This discriminative property is shared by the methods of <TREF>Johnson et al 1999</TREF>; <REF>Collins 2000</REF>, and also the Conditional Random Field methods of <REF>Lafferty et al 2001</REF>~~~In a previous paper <REF>Collins 2000</REF>, a boosting algorithm was used to rerank the output from an existing statistical parser, giving significant improvements in parsing accuracy on Wall Street Journal data~~~Similar boosting algorithms have been applied to natural language generation, with good results, in <REF>Walker et al 2001</REF>.
The framework is derived by the transformation from ranking problems to a margin-based classification problem in <REF>Freund et al 1998</REF>~~~It is also related to the Markov Random Field methods for parsing suggested in <TREF>Johnson et al 1999</TREF>, and the boosting methods for parsing in <REF>Collins 2000</REF>~~~We consider the following set-up: a15 Training data is a set of example input/output pairs~~~In tagging we would have training examples a147 a71 a28 a30a37a17 a28a19a148 where each a71 a28 is a sentence and each a17 a28 is the correct sequence of tags for that sentence.
Experiments of the parsing of realworld sentences can properly evaluate the effectiveness and possibility of parsing models for HPSG~~~2 Disambiguation models for HPSG Discriminative log-linear models are now becoming a de facto standard for probabilistic disambiguation models for deep parsing <TREF>Johnson et al , 1999</TREF>; <REF>Riezler et al , 2002</REF>; <REF>Geman and Johnson, 2002</REF>; <REF>Miyao and Tsujii, 2002</REF>; <REF>Clark and Curran, 2004b</REF>; <REF>Kaplan et al , 2004</REF>~~~Previous studies on probabilistic models for HPSG <REF>Toutanova and Manning, 2002</REF>; <REF>Baldridge and Osborne, 2003</REF>; Malouf and van <REF>Noord, 2004</REF> also adopted log-linear models~~~HPSG exploits feature structures to represent linguistic constraints.
Following Abney, we propose a loglinear framework which incorporates long-range dependencies as features without loss of consistency~~~Log-linear models have previously been applied to statistical parsing <TREF>Johnson et al , 1999</TREF>; <REF>Toutanova et al , 2002</REF>; <REF>Riezler et al , 2002</REF>; <REF>Osborne, 2000</REF>~~~Typically, these approaches have enumerated all possible parses for model estimation and finding the most probable parse~~~For grammars extracted from the Penn Treebank in our case CCGbank <REF>Hockenmaier, 2003</REF>, enumerating all parses is infeasible.
As expected, we observed that the regularization term increases the accuracy, especially when the training data is small; but we did not observe much difference when we used different regularization terms~~~The results we report are with the Gaussian prior regularization term described in <TREF>Johnson et al , 1999</TREF>~~~Our goal in this paper is not to build the best tagger or recognizer, but to compare different loss functions and optimization methods~~~Since we did not spend much effort on designing the most useful features, our results are slightly worse than, but comparable to the best performing models.
This method generates 50-best lists that are of substantially higher quality than previously obtainable~~~We used these parses as the input to a MaxEnt reranker <TREF>Johnson et al , 1999</TREF>; <REF>Riezler et al , 2002</REF> that selects the best parse from the set of parses for each sentence, obtaining an f-score of 910 on sentences of length 100 or less~~~We describe a reranking parser which uses a regularized MaxEnt reranker to select the best parse from the 50-best parses returned by a generative parsing model~~~The 50-best parser is a probabilistic parser that on its own produces high quality parses; the maximum probability parse trees according to the parsers model have an f-score of 0897 on section 23 of the Penn Treebank <REF>Charniak, 2000</REF>, which is still state-of-the-art.
Given set W of words and set F of feature structures, an HPSG is formulated as a tuple, G  L,R, where L  l  w,Fw  W,F  F is a set of lexical entries, and R is a set of schemata, ie, r  R is a partial function: F F  F Given a sentence, an HPSG computes a set of phrasal signs, ie, feature structures, as a result of parsing~~~Previous studies <REF>Abney, 1997</REF>; <TREF>Johnson et al , 1999</TREF>; <REF>Riezler et al , 2000</REF>; <REF>Miyao et al , 2003</REF>; Malouf and van <REF>Noord, 2004</REF>; <REF>Kaplan et al , 2004</REF>; <REF>Miyao and Tsujii, 2005</REF> defined a probabilistic model of unification-based grammars as a log-linear model or maximum entropy model <REF>Berger et al , 1996</REF>~~~The probability of parse result T assigned to given sentence w  w1,,,wn is pTw  1Z w exp parenleftBiggsummationdisplay i ifiT parenrightBigg Zw  summationdisplay T prime exp parenleftBiggsummationdisplay i ifiTprime parenrightBigg, where i is a model parameter, and fi is a feature function that represents a characteristic of parse tree T Intuitively, the probability is defined as the normalized product of the weights expi when a characteristic corresponding to fi appears in parse result T Model parameters i are estimated using numer104 ical optimization methods <REF>Malouf, 2002</REF> so as to maximize the log-likelihood of the training data~~~However, the above model cannot be easily estimated because the estimation requires the computation of pTw for all parse candidates assigned to sentence w Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences.
